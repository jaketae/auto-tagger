title,body,probability_distribution,linear_algebra,algorithms,monte_carlo,tensorflow,c,bayesian,sql,simulation,data_viz,markov_chain,jupyter,docker,pytorch,apple,r,deep_learning,machine_learning,spark,nlp,regression,from_scratch,numerical_methods,military,analysis,euler,statistics,update
2020-05-27-natural-gradient,"in a previous post, we took a look at fisher's information matrix. today, we will be taking a break from the r frenzy and continue our exploration of this topic, meshing together related ideas such as gradient descent, kl divergence, hessian, and more. the typical formula for batch gradient descent looks something like this: this is the familiar gradient descent algorithm that we know of. while this approach works and certainly makes sense, there are definite limitations; hence the introduction of other more efficient algorithms such as sgd, adam, and et cetera. however, these algorithms all have one thing in common: they adjust the parameter in the parameter space according to euclidean distance. in other words, gradient descent essentially looks at regions that are some euclidean distance away from the current parameter and chooses the direction of steepest descent. this is where the notion of natural gradients come into play: if our goal is to minimize the cost function, which is effectively equivalent to maximizing the likelihood, why not search within the distribution space of the likelihood function instead? after all, this makes more sense since gradient descent in parameter space is likely to be easily perturbed by the mode of parametrization, such as using precision instead of variance in a normal distribution, whereas searching in the distribution space would not be subject to this limitation. so the alternative to this approach would be to search the distribution space and find the distribution that which makes value of the cost function the smallest. this is the motivation behind the notion of a natural gradient. now you might be wondering how all this has anything to do with the fisher matrix, which we looked at in the previous post. well, it turns out there are some deep, interesting questions to be posed and connections to be uncovered. if we're going to search around the distribution space, one natural question to consider is what distance metric we will use for our search. in case of batch gradient descent, we used euclidean distance. this made sense since we were simply measuring the distance between two parameters, which are effectively scalars or vector quantities. if we want to search the distribution space, on the other hand, we would have to measure the distance between two probability distributions, one that is defined by the previous parameter and the other defined by the newly found parameter after natural gradient descent. well, we know one great candidate for this task right off the bat, and that is kl divergence. recall that kl divergence is a way of quantifying the pseudo distance between two probability distributions. the formula for kl divergence is shown below. and while we're at it, let's throw cross entropy and entropy into the picture as well, both for review and clarity's sake: for a short, simple review of these concepts, refer to this previous article, or aurelien geron's video on youtube. in most cases, is the true distribution which we seek to model, while is some more tractable distribution at our disposal. in the classic context of ml, we want to minimize the kl divergence. in this case, however, we're simply using kl divergence as a means of measuring distance between two parameters in defined within a distribution space. as nicely stated in layman's term in this medium article, ... instead of “i’ll follow my current gradient, subject to keeping the parameter vector within epsilon distance of the current vector,” you’d instead say “i’ll follow my current gradient, subject to keeping the distribution my model is predicting within epsilon distance of the distribution it was previously predicting” i see this as an intuitive way of nicely summarizing why we're using kl divergence in searching the distribution space, as opposed to using euclidean distance in searching the parameter space. now it's time for us to connect the dots between kl divergence and fisher's matrix. before we diving right into computations, let's think about how or why these two concepts might be related at all. one somewhat obvious link is that both quantities deal with likelihood, or to be more precise, log likelihood. due to the definition of entropy, kl divergence ends up having a log likelihood term, while fisher's matrix is the negative expected hessian of the log likelihood function, or the covariance matrix of fisher's score, which is the gradient of the log likelihood. either way, we know that likelihood is the fundamental bridge connecting the two. let's try to compute the kl divergence between and . conceptually, we can think of as the previous point of the parameter and as the newly updated parameter. in this context, the kl divergence would tell us the effect of one iteration of natural gradient descent. this time, instead of using integral, let's try to simplify a bit by expressing quantities as expectations. we see the familiar log likelihood term. given the fact that the fisher matrix is the negative expected hessian of the log likelihood, we should be itching to derive this expression twice to get a hessian out of it. let's first obtain the gradient, then get its jacobian to derive a hessian. this derivation process was heavily referenced from agustinus kristiadi's blog. let's do this one more time to get the hessian. this conclusion tells us that the curvature of kl divergence is defined by fisher's matrix. in hindsight, this is not such a surprising result given that the kl divergence literally had a term for expected log likelihood. applying the leibniz rule twice to move the derivative into the integral, we quickly end up with fisher's matrix. at this point, you might be wondering about the implications of this conclusion. it's great that kl divergence and the fisher matrix are closely related via the hessian, but what implication does it have for the gradient descent algorithm in distribution space? to answer this question, we first need to perform a quick multivariate second order taylor expansion on kl divergence. recall that the simple, generic case of multivariate taylor expansion looks as follows: this is simply a generalization of the familiar univariate taylor series approximation we saw earlier. continuing our discussion of kl divergence, let's try to expand the divergence term using taylor approximation. here, is small distance in the distribution space defined by kl divergence as the distance metric. this can be a bit obfuscating notation wise because of the use of as our variable, assuming as a fixed constant, and evaluating the gradient and the hessian at the point where since we want to approximate the value of kl divergence at the point where where . but really, all that is happening here is that in order to approximate kl divergence, we're starting at the point where , and using the slope and curvature obtained at that point to approximate the value of kl divergence at distance away. picturing the simpler univariate situation in the cartesian plane might help. the bottom line is that the kl divergence is effectively defined by the fisher matrix. the implication of this is that now, the gradient descent algorithm is subject to the constraint where is some constant. now, the update rule would be to solve for the argument minima operation, we will resort to the classic method for optimization: lagrangians. in this case, the lagrangian would be this immediately follows from using the constraint condition. to make progress, let's use taylor approximation again, both on the term for the loss function and the kl divergence. the good news is that we have already derived the expression for the latter. noting the fact that there are several constants in this expression, we can simplify this into to minimize this expression, we set its gradient equal to zero. note that we are deriving with respect to . therefore, we are finally done with our derivation. this equation tells us that the direction of steepest descent is defined by the inverse of the fisher matrix multiplied by the gradient of the loss function, up to some constant scaling factor. this is different from the vanilla batch gradient descent we are familiar with, which was simply defined as although the difference seems very minor after all, all that was changed was the addition of fisher's matrix yet the underlying concept, as we have seen in the derivation, is entirely different. this was definitely a math heavy post. even after having written this entire post, i'm still not certain if i have understood the details and subtleties involved in the derivation. and even the details that i understand now will become confusing and ambiguous later when i return back to it. hopefully i can retain most of what i have learned from this post. before i close this post, i must give credit to agustinus kristiadi, whose blog post was basically the basis of this entire writing. i did look at a few stack overflow threads, but the vast majority of what i have written are either distillations or adaptations from their blog. it's a great resource for understanding the mathematics behind deep learning. i hope you enjoyed reading this blog. see you in the next one!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0
2020-05-05-c,"so i've been spending some time this past week or so picking up a new language: c. c is considered by many to be one of the most basic and fundamental of all languages, not only because it is what powers the linux kernel, but also because it has inspired and motivated the birth of many other languages that came after, such as c++, java, and python. in fact, many popular python packages such as numpy or tensorflow are implemented through c or c++ under the hood. i thought learning c would be a fruitful endeavor during these sad quarantine hours, so here is my reflection, plus a short crash course for those who want to get a taste of what c is like. let me put this out there first: c is not an easy language to pick up. this might be in large part due to the fact that i come from a python background: python is a dynamically typed language, meaning that programmers don't have to worry much about things like data types. in fact, it's possible to do something like and python will compile these without error. c, on the other hand, is very picky about data types and will print out a myriad of error statements to complain that something has gone astray. to be fair, however, this is not a problem exclusive to c: as far as i know, other languages like java or even swift will complain that such reassignments are impossible. it's fairer to say that python is the oddball here. but then there're pointers. pointers are one of those things that might seem doable and somewhat intuitive at first, then throws you off when you actually start coding, then gives you a false sense of security that you've grasped the concept, only to surprise you from behind with another set of complications. even simple i/o functions like seem daunting at first. i'll shamelessly admit that it took me a full day or so to wrap my head around what was going on in these lines of code the first day i decided to venture into the world of c. oh, and did i mention the fact that arrays and strings are a whole different animal in the world of c? i'll stop here and go into more detail in later portions of this post, but in short, the conclusion is simple: pointers are confusing. and thus also is c. then why learn c in the first place? after all, c is a pretty old language, and although it is used in the development of operating systems, kernels, and embedded systems, its applicability is more limited compared to something like python or even c++. i personally think learning c helps us better understand how computers work and what's actually going on under the hood. because c is a rather low level language, it is more machine friendly and thus reveals more of what happens when lines of code are executed, a lot more than, say, python. granted, one can see memory addresses of where variables are stored as well, using something like but this is if we really try. c is designed in a less beginner friendly fashion, so that it requires the programmer to think about things like garbage collection and memory management on the stack, heap, and so on. it also forces you to think more about overflow, which occurs when information assigned to a variable exceeds the maximum amount of information that can be stored in a given datatype's representation. these are all basic computer science concepts, but because i was never trained to think rigorously or systematically about these issues, learning c was a challenging, refreshing experience. enough of my story, let's take a look at some of c's basic syntax structures. this is not really going to be a full fledged course, so perhaps crash course is a misnomer. but you get the idea: we'll review some basic syntax to get a sense of how things work in c. note that this may not be the most beginner friendly introduction to c programming: if you're looking for a resource for starters, i highly recommend introduction to programming in c offered by duke university on coursera. one last note for my fellow java programmers before we dive in: java is c's step cousin. if you know java's syntax, a lot of c's syntax will seem familiar to you. but do not be fooled by the exterior familiarity: c's behavior is often times different from that of java's, especially when it comes to things like pointers. this looks identical to java for the most part. functions are also pretty straightforward. one point of caution: variables that are declared, initialized, and modified within the function are killed once the function's sequence is terminated, and thus its stack frame removed from computer's memory. in other words, although this may appear confusing at first, this makes a lot of sense if you start thinking in terms of stack frames. from the point of view of the function, in this case , the function has no knowledge of the actual variable that it is passed: it only knows the value of that parameter. therefore, to way to go about this problem is to use pointers instead. if the whole and business seem confusing to you, don't worry: the next section is dedicated to pointers and memory addresses. in c, everything is pass by value. this is why we could not modify a value within a function: the argument that is given is simply the value of that variable, not the reference to the variable itself. therefore, if we want to modify objects outside of a function in c, we need to pass in what are called pointers. this stack overflow post explains the notion of emulating pass by reference in c by passing pointer values as we have done in the short example above. put quite simply, a pointer is an object that as the name implies literally points to another object in the computer's memory. the way this works is that the pointer stores the memory address of the object that it is pointing to. for example, recall this previous example as you can see, the memory address is a hexadecimal number that simply tells us where a certain data is stored. the number should be a 64 bits, or 8 bytes, but because i'm not using a proper data type such as , the length of the memory addresses are truncated. let's look more into the code. in this code snippet, is a pointer that points to the integer . the written in the declaration of indicates that is a pointer to an integer. and since is a pointer, we initialize it with the memory address of , which can be obtained via the ampersand symbol, . one of the most confusing things about c is making sense of the distinction between a pointer and an array. unlike in some other languages, where arrays are given first class citizen support, the notion of arrays is slightly more muddled in c. i've sifted through countless so posts to understand whether the following statement is true: in c, arrays are equivalent to pointers. indeed, this proposition seems very true if you try something like this: this result seems to suggest that there is no difference between an array, a pointer to an array, or the pointer to the first element of the array. indeed, it appears as if an array is just really a pointer. in fact, it is even possible to store an array into a pointer : however, a pointer is not an array, and the two should not be confused. as one of the gurus on so memorably said, pointers are pointers and arrays are arrays. indeed, this statement will understandably trigger the gcc compiler to complain that something is wrong: specifically, it states that . this message simply means that a pointer is not an array, which is why it is impossible for a pointer to store an array. then, what is the relationship between a pointer and an array? the answer is that an array often decays into a pointer to the first element of that array. this decay happens, for instance, when an array is passed into a function as a parameter. let's consider the example of a function that prints a 2d array. note that the function is passed in a 2d array, . however, the function accepts a pointer to an array of size . why is this the case? well, we simply recall the fact that an array decays to a pointer to the first element of that array. in this case, because is a 2d array, its first element is a 1d array of length . therefore, accepts a pointer to an array as its argument. something that i was very confused when first starting out with pointers and c in general was the conflation of double pointers with 2d arrays. the two should not be confused. for example, in the function above, if we define the parameter as something like , we would be greeted with a compiler error. 1d arrays decay into pointers, but this does not mean that 2d arrays decay into double pointers; instead, as we have seen, they decay into , or pointer to an array. one important fact that deserves its own subsection, related to the topic of pointers and arrays, is the operator. earlier we have noted that arrays frequently decay into the pointer to the first element of that array. however, there are exceptions when this is not the case: sometimes, the array retains its form and produces results differently than we would expect if we were to consider it a pointer to the first element. for example, consider the following code snippet: on my work station running on macos, is 4 bytes, which is why the size of is 40. this is clearly not the result we would get if we calculated the size of the pointer to the first array. in other words, the general rule of thumb that arrays decay into pointers to their first elements holds true for the most part, with several exceptions, the most notable and common of which is the operator. another point of confusion i ran into was the difference between these two seem to achieve the same end goal: creating a string variable that contains . however, under the hood, and are very different. in other words, and are two distinct ways of initializing strings: the first method creates a array that stores a string literal, as the syntax suggests; the second method, on the other hand, creates a pointer to a string literal, which is stored in the read only portion of the computer's memory. a corollary of this difference is that assigning or modifying contents of the string is only legal in the case of the first array type declaration. concretely, the array type initialization method should more accurately be understood as a short hand for the last element is the null terminator, which is basically a way for computers of knowing that the string has ended and to ensure that the contents of the array is a valid c string. note that null terminators are necessary only to make some data a valid c string; hence, it is not a requirement of the datatype to have a null terminator as its last element. another confusion that might throw of some beginners is the use of in declaring and initializing strings. the difference, to put it in human language terms, is that is a pointer to a constant character; , on the other hand, is a constant pointer to a character. the implications of this are best demonstrated via an example: is a pointer to a constant, which means that the pointer itself can be modified, whereas the object itself is pointing to cannot. therefore, one cannot deference that pointer to modify its content, which is what is attempting to do in the example above. for , which is a constant pointer to a character, the situation is slightly different. because itself is a variable, it cannot be assigned a new memory address to point to. we can combine both worlds to create something like in this case, cannot be dereferenced to alter the value it is pointing to, nor can we modify the pointer itself to refer to a new memory location. this is because is a constant pointer to a constant character. by now, you should be a wizard at reading character pointers. let's take a break from pointers, arrays, and strings and talk about something simpler. in c, there is a shorthand for if else conditional statements, known as ternary operators. this is just another way of saying the ternary operator is a lot more concise than the conditional statement. of course, once we get into more complicated code, there will most definitely be cases when we want to use if else instead of the ternary operator, but it is a good tool to have nonetheless. personally, i've found it useful when declaring some simple macro functions. then, we can now use and pretend like it is a built in function. previously, we mentioned the fact that variables that are declared and initialized in a function expires once the stack frame pertaining to that function is terminated after the end of the sequence. however, turns out that there is a way to keep these variables remaining, even after the function is fully executed and the stack frame removed. the key behind this magic is , which stands for memory allocation. what does is that it allocates a memory for some object on the heap, not the typical stack. this way, the object will keep on living, even outside the function scope after the execution is over. to remove that object, we need to manually that variable. this is how memory management and garbage collection works in c: through the manual control of the programmer. this will allocate space for an integer array of length . why would we want to do this in the first place? note that, when we declare and initialize arrays, we need to specify its length. if we do not know the size of the array before compile time, however, this becomes a problem. one way to solve this issue is use precisely as shown above to dynamically allocate space for the array according to, say, the value inputted by the user via . note that we must the ed variable so that the heap is flushed. also allows us to declare variables and let them live beyond the scope of the function. consider the following dummy function: the reason why this doesn't work is that no longer exists by the time the function is executed. instead, a workaround is to allocate on the heap using . this way, we will be able to return , since it is dynamically allocated on the heap. of course, we will have to free it at one point for garbage collection once everything is over. is a way of re ing a pointer. this might happen in cases where we want to extend the size of a dynamically allocated array like pointer . say we have an array of size , but we want to expand it to . in this case, we might do something like this: this will expand the size of to while copying its original contents. note that may very well change the memory address of to a completely different location on the heap. this post in no way covers even an atom of perks and beauties of the c programming language. instead, i wrote this post in the hopes of helping those who might be thrown off by the complexities and subtleties of c, as well as for myself, the helpless c novice who always revisit the same so threads each time he runs into the same question he literally searched for the day prior. learning c has been a challenging yet also a very rewarding experience so far, and i plan on continuing it so that i can build a solid foundation. i've also found that leetcoding or solving easy coding questions with c is an interesting experience, as it requires more thinking that solving them with python. i will be referencing c for some posts on data structures i plan on writing, so hopefully this will build a solid foundation for what's more to come. in the meantime, take care, and see you in the next one.",0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2020-01-15-first-keras,"lately, i have been on a datacamp spree after unlocking a two month free unlimited trial through microsoft's visual studio dev essentials program. if you haven't already, make sure to check it out, as it offers a plethora of tools, journal subscriptions, and software packages for developers. anyhow, one of the courses i decided to check out on datacamp was titled ""introduction to deep learning with python,"" which covered basic concepts in deep learning such as forward and backward propagation. the latter half of the tutorial was devoted to the introduction of the keras api and the implementation of neural networks. i created this notebook immediately after finishing the tutorial for memory retention and self review purposes. first, we begin by importing the library as well as other affiliated functions in the module. note that keras uses tensorflow as backend by default. the warning in the code block below appears because this notebook was written on google colab, which informs users that the platform will be switching over to tensorflow 2 in the future. as you might be able to guess from one of the imported modules, the objective of the neural network will be to classify hand written digits. in doing so, we will be dealing with a classic in machine learning literature known as the the mnist data set, which contains images of hand written digits from 0 to 9, each hand labeled by researchers. the variable denotes the total number of class labels available in the classification task, which is 10. specifies the number of iterations the gradient descent algorithm will run for. let's begin by loading the data from . downloading data from https://s3.amazonaws.com/img datasets/mnist.npz 11493376/11490434 ============================== 1s 0us/step now we have to slightly modify the loaded data so that its dimensions and values are made suitable to be fed into a neural network. changing the dimensionality of data can be achieved through the function, which takes in the number of rows and columns as its argument. we convert the numbes into type , then normalize it so that its values are all between 0 and 1. although we won't get into too much detail as to why normalization is important, an elementary intuition we might develop is that normalization effectively squishes all values into the same bound, making data much more processable. we also implement one hot encoding on through the function. let's quickly check if the necessary adjustments were made by looking up the dimensions of and , respectively. looks like the data has been reshaped successfully. now, it's finally time to get into the nuts and bolts of a neural network. the simplest neural network is the model, which means that every neuron in one layer is connected to all other neurons in the previous layer. building a simple neural network is extremely easy in a high level api like keras. the model below has 784 input nodes. the input layer is then connected to a hidden layer with 512 neurons, which is then connected to a second hidden layer with also 512 neurons. note that the hidden layer uses the function as its activation function. the dropout layers ensure that our model does not overfit to the data. the last output layer has 10 neurons, each corresponding to digits from 0 to 9. the activation fuction of this last layer is the softmax function, which allows us to interpret the final results as a categorical distribution. let's double check that the layers have been formed correctly as per our intended design. model: ""sequential_3"" _________________________________________________________________ ================================================================= dense_6 401920 _________________________________________________________________ dropout_5 0 _________________________________________________________________ dense_7 262656 _________________________________________________________________ dropout_6 0 _________________________________________________________________ dense_8 5130 ================================================================= total params: 669,706 trainable params: 669,706 non trainable params: 0 _________________________________________________________________ everything looks good, which means we are now ready to compile and train our model. before we do that, however, it is always a good idea to use the module to ensure that gradient descent stops when no substantial weight adjustments are being made to our model. in other words, when the model successfully finds the local minimum , the will kick in and stop gradient descent from proceeding with further epochs. we are now ready to go! let's compile the model by making some configurations, namely the , , and . simply put, an specifies which flavor of the gradient descent algorithm we want to choose. the simplest version is known as , or the stochastic gradient descent. can be considered an improved version of the stochastic gradient descent in that its learning rate changes depending on the slope of the loss function, defined here as cross entropy. if you recall, cross entropy is basically a measurement of the pseudo distance between two distributions, i.e. how different two distributions are. but because cross entropy is often not easy to intuitively wrap our minds around, let's pass the metric to the function, as shown below. it's time to train the neural network with the training data, and , over a specified number of epochs. as promised, we will use the to stop graident descent from making unnecessary computations down the road. we also specify that and are components of the validation set. train on 60000 samples, validate on 10000 samples epoch 1/15 60000/60000 ============================== 8s 129us/step loss: 0.2149 acc: 0.9349 val_loss: 0.0992 val_acc: 0.9688 epoch 2/15 60000/60000 ============================== 7s 124us/step loss: 0.1048 acc: 0.9676 val_loss: 0.0815 val_acc: 0.9750 epoch 3/15 60000/60000 ============================== 7s 122us/step loss: 0.0820 acc: 0.9743 val_loss: 0.0907 val_acc: 0.9726 epoch 4/15 60000/60000 ============================== 7s 123us/step loss: 0.0669 acc: 0.9794 val_loss: 0.0795 val_acc: 0.9782 epoch 5/15 60000/60000 ============================== 7s 122us/step loss: 0.0576 acc: 0.9823 val_loss: 0.0883 val_acc: 0.9767 epoch 6/15 60000/60000 ============================== 7s 123us/step loss: 0.0498 acc: 0.9843 val_loss: 0.0704 val_acc: 0.9799 epoch 7/15 60000/60000 ============================== 7s 123us/step loss: 0.0470 acc: 0.9851 val_loss: 0.0752 val_acc: 0.9814 epoch 8/15 60000/60000 ============================== 7s 122us/step loss: 0.0425 acc: 0.9867 val_loss: 0.0857 val_acc: 0.9800 keras shows us how much our neural network improves over each epoch. this is convenient, but can we do better? the answer is a sure yes. let's quickly plot a graph to see how model accuracy improves over time, while cross entropy loss decreases with more epochs. as the last step, we might want to save our trained model. this can be achieved with a single line of code. we can load pre saved models as well. that's it for today! obviously there are a lot more we can do with , such as building deeper neural networks or non sequential models such as cnn or gan, but these are topics we might look at a later date when i grow more proficient with the keras api and deep learning in general. for now, consider this to be a gentle introduction to neural networks with keras. thanks for reading! catch you up in the next one. datacamp: http://datacamp.com visual studio dev essentials program: https://visualstudio.microsoft.com/dev essentials/ gradient descent algorithm: https://en.wikipedia.org/wiki/gradient_descent cross entropy: https://jaketae.github.io/study/information entropy/ keras api: https://keras.io google colab: http://colab.research.google.com",0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0
2020-02-18-autoencoder,"in today's post, we will take yet another look at an interesting application of a neural network: autoencoders. there are many types of autoencoders, but the one we will be looking at today is the simplest variant, the vanilla autoencoder. despite its simplicity, however, there is a lot of insight to glean from this example in fact, it is precisely the simplicity that allows us to better understand how autoencoders work, and potentially extend that understanding to to analyze other flavors of autoencoders, such as variational autoencoder networks which we might see in a future post. without further ado, let's get started. we begin by importing all modules and configurations necessary for this tutorial. how do autoencoders work? there are entire books dedicated to this topic, and this post in no way claims to introduce and explore all the fascinating complexities of this model. however, one intuitive way to understand autoencoders is to consider them as, lo and behold, encoders that map complex data points into vectors living in some latent dimension. for example, a 28 by 28 pixel rgb channel image might be compressed into a five dimensional latent vector. the five numbers composing this vector somehow encodes the core information needed to then decode this vector back into the original 28 by 28 pixel rgb channel image. of course, some information is inevitably going to be lost after all, how can five numbers describe the entirety of an image? however, what's important and fascinating about autoencoders is that, with appropriate training and configuration, they manage to find ways to best compress input data into latent vectors that can be decoded to regenerate a close approximation of the input data. for the purposes of this demonstration, let's configure the latent dimension of the encoder to be 128 dimensions in other words, each 28 by 28, single channel image will be encoded into vectors living in 128 dimensional space. it's time to build the autoencoder model. in summary, an autoencoder is composed of two components: an encoder and a decoder. the encoder transfers input data into the latent dimension, and the decoder performs the exact reverse: it takes vectors in the latent space and rearranges it to bring it back into its original dimension, which is, in this case, a 28 by 28, single channel image. the followign code snippet implements this logic using the functional api. let's declare the encoder and autoencoder model by invoking the function with the specified image shape and the dimensionality of the latent space. just to get a sense of what operations are taking place dimensionality wise, here is a look at the output shapes of the autoencoder model. notice that the input is of shape , and that the final output is also of the same shape , as expected. model: ""model"" _________________________________________________________________ ================================================================= input_1 0 _________________________________________________________________ conv2d 160 _________________________________________________________________ batch_normalization 64 _________________________________________________________________ max_pooling2d 0 _________________________________________________________________ conv2d_1 4640 _________________________________________________________________ batch_normalization_1 128 _________________________________________________________________ max_pooling2d_1 0 _________________________________________________________________ conv2d_2 9248 _________________________________________________________________ flatten 0 _________________________________________________________________ dense 200832 _________________________________________________________________ dense_1 202272 _________________________________________________________________ reshape 0 _________________________________________________________________ conv2d_transpose 9248 _________________________________________________________________ batch_normalization_2 128 _________________________________________________________________ up_sampling2d 0 _________________________________________________________________ conv2d_transpose_1 9248 _________________________________________________________________ batch_normalization_3 128 _________________________________________________________________ up_sampling2d_1 0 _________________________________________________________________ conv2d_transpose_2 4624 _________________________________________________________________ conv2d_3 145 ================================================================= total params: 440,865 trainable params: 440,641 non trainable params: 224 _________________________________________________________________ here's the image of the model for the fancy bells and whistles. now that the autoencoder model is fully ready, it's time to see what it can do! although autoencoders present countless exciting possibilities for application, we will look at a relatively simple use of an autoencoder in this post: denoising. there might be times when the photos we take or image data we use are tarnished by noise undesired dots or lines that undermine image quality. an autoencoder can be trained to remove these noises fairly easily as we will see in thi post. first, let's import the mnist data set for this tutorial. nothing much exciting is happening below, except for the fact that we are rearranging and preprocessing the dataset so as to maximize training efficiency. next, we will add noise to the data. note that the mnist dataset does not contain noise by default: we will have to artificially and intentionally tarnish the dataset to produce a noisy training set for the autoencoder model. the function precisely performs this function. using the function, we can create a noisy sample. note that was set to 0.5, although i'd imagine other values within reasonable range would work equally well as well. training the model is very simple: the training data is , the noisy dataset, and the predicted label is . through this configuration, we essentially expect the autoencoder to be able to see noisy images, after which encoding and decoding is performed via a transformation to a latent dimension to ultimately reproduce a pristine image devoid of any noise. for experimental puposes, i tried using the callback on google colab. is a platform that gives developers full view of what happens during and after the training process. it makes observing metrics like loss and accuracy a breeze. i highly recommend that you check out this tutorial on how to use and configure this functionality on your notebook. train on 54000 samples, validate on 6000 samples epoch 1/35 54000/54000 ============================== 9s 170us/sample loss: 0.1358 val_loss: 0.1091 epoch 2/35 54000/54000 ============================== 6s 117us/sample loss: 0.1046 val_loss: 0.1041 epoch 3/35 54000/54000 ============================== 6s 118us/sample loss: 0.1004 val_loss: 0.1001 epoch 4/35 54000/54000 ============================== 6s 118us/sample loss: 0.0982 val_loss: 0.1001 epoch 5/35 54000/54000 ============================== 6s 116us/sample loss: 0.0966 val_loss: 0.0995 epoch 6/35 54000/54000 ============================== 6s 117us/sample loss: 0.0956 val_loss: 0.0991 epoch 7/35 54000/54000 ============================== 6s 117us/sample loss: 0.0946 val_loss: 0.0969 epoch 8/35 54000/54000 ============================== 6s 117us/sample loss: 0.0939 val_loss: 0.0971 epoch 9/35 54000/54000 ============================== 6s 117us/sample loss: 0.0932 val_loss: 0.0966 epoch 10/35 54000/54000 ============================== 6s 117us/sample loss: 0.0928 val_loss: 0.0959 epoch 11/35 54000/54000 ============================== 6s 116us/sample loss: 0.0922 val_loss: 0.0966 epoch 12/35 54000/54000 ============================== 6s 117us/sample loss: 0.0917 val_loss: 0.0958 epoch 13/35 54000/54000 ============================== 6s 116us/sample loss: 0.0914 val_loss: 0.0958 epoch 14/35 54000/54000 ============================== 6s 116us/sample loss: 0.0910 val_loss: 0.0970 epoch 15/35 54000/54000 ============================== 6s 116us/sample loss: 0.0907 val_loss: 0.0961 epoch 16/35 54000/54000 ============================== 6s 116us/sample loss: 0.0903 val_loss: 0.0983 epoch 17/35 54000/54000 ============================== 6s 118us/sample loss: 0.0900 val_loss: 0.0987 epoch 18/35 54000/54000 ============================== 7s 121us/sample loss: 0.0898 val_loss: 0.0963 epoch 19/35 54000/54000 ============================== 6s 116us/sample loss: 0.0895 val_loss: 0.0953 epoch 20/35 54000/54000 ============================== 6s 116us/sample loss: 0.0893 val_loss: 0.0959 epoch 21/35 54000/54000 ============================== 6s 117us/sample loss: 0.0890 val_loss: 0.0954 epoch 22/35 54000/54000 ============================== 6s 116us/sample loss: 0.0888 val_loss: 0.0953 epoch 23/35 54000/54000 ============================== 6s 116us/sample loss: 0.0887 val_loss: 0.0954 epoch 24/35 54000/54000 ============================== 6s 117us/sample loss: 0.0885 val_loss: 0.0958 epoch 25/35 54000/54000 ============================== 6s 117us/sample loss: 0.0882 val_loss: 0.0958 epoch 26/35 54000/54000 ============================== 6s 116us/sample loss: 0.0880 val_loss: 0.0966 epoch 27/35 54000/54000 ============================== 6s 117us/sample loss: 0.0879 val_loss: 0.0956 epoch 28/35 54000/54000 ============================== 6s 116us/sample loss: 0.0877 val_loss: 0.0956 epoch 29/35 54000/54000 ============================== 6s 116us/sample loss: 0.0876 val_loss: 0.0954 epoch 30/35 54000/54000 ============================== 6s 117us/sample loss: 0.0874 val_loss: 0.0959 epoch 31/35 54000/54000 ============================== 6s 118us/sample loss: 0.0873 val_loss: 0.0959 epoch 32/35 54000/54000 ============================== 6s 116us/sample loss: 0.0872 val_loss: 0.0960 epoch 33/35 54000/54000 ============================== 6s 116us/sample loss: 0.0871 val_loss: 0.0958 epoch 34/35 54000/54000 ============================== 6s 117us/sample loss: 0.0869 val_loss: 0.0980 epoch 35/35 54000/54000 ============================== 6s 116us/sample loss: 0.0867 val_loss: 0.0981 now that the training is over, what can we do with this autoencoder? well, let's see if the autoencoder is now capable of removing noise from tainted image files. but before we jump right into that, let's first build a simple function that displays images for our convenience. using the function, we can now display 25 test images that we will feed into the autoencoder. let's add noise to the data. finally, the time has come! the autoencoder will try to ""denoise"" the contaminated images. let's see if it does a good job. lo and behold, the autoencoder produces pristine images, almost reverting them back to their original state! i find autoencoders interesting for two reasons. first, they can be used to compress images into lower dimensions. our original image was of size 28 by 28, summing up to a total of 784 pixels. somehow, the autoencoder finds ways to decompress this image into vectors living in the predefined 128 dimensions. this is interesting in and of itself, since it presents ways that we might be able to compress large files with minimal loss of information. but more importantly, as we have seen in this tutorial, autoencoders can be used to perform certain tasks, such as removing noise from data, and many more. in the next post, we will take a look at a variant of this vanilla autoencoder model, known as variational autoencoders. variataional autoencoders are a lot more powerful and fascinating because they can actually be used to generate data instead of merely processing them. i hope you enjoyed reading this post. stay tuned for more!",0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2020-08-15-rerent-summer,"for the past month and a half, i've been working as a backend developer for rerent, a yale som based hospitality startup. working alongside motivated, inspirational people of rerent has been such a transformative experience me, which is why i decided to share some of that experience through this post. i decided that now is also good timing since we just pushed out an mvp a few days ago; now the team is on a brief hiatus before getting back to work for the fall. reorientation phase, if you will. i have admittedly been slacking off these past few weeks with this blog, largely because i was spending a lot of my time learning django, react, and many more. when i first came across django a few months back through the ppetrackr project, i never understood how django worked and why it was so powerful. as a volunteer who was exclusively focused on building out the dashboard with plotly and dash, i lacked the holistic understanding of how the plotly dashboard blended into the bigger frame of the project that was django. i was still more of a flask fan than a django fan i was admittedly intimidated by its ostensible complexity. fast forward a few months and now here i am, a django backend engineer intern who now loves django to its fullest. django is such a powerful battery packed backend framework that offers so much functionality out of the box. below are some of the features that i love about django. although i’ve seen people say otherwise, django’s orm is a powerful yet highly intuitive way of interacting with the database. it makes things like foreign keys, one to one, and many to many relationships so easy to deal with. in reality, forming a many to many relationship requires there to be an intermediate table that represents such a relationship. however, django takes care of all that under the hood, lifting the burden off of the developer’s shoulders. in our web application, we had to implement a decently complex model with a chain of foreign keys and many to many relationships. in summary, a user could have multiple properties, and each property could have multiple trip plans associated with them. simultaneously, we had staff users who would be assigned a number of zip codes. each property, then, was associated with one of these zip codes. while this sounds like a decently complicated relationship, with django querying becomes extremely easy. for example, to obtain all the locations of the user this is assuming that the user is logged in, and that we have access to the via some view function. in the case of class based views, we would use instead. one feature i really like about django orm is the ability to query backwards, or upstream. normally, we would get the associated with some awayplan via a foreign key lookup. for instance, the following query would give us the location of the first awayplan in the database. this simply flows from the fact that the location is a foreign key attribute associated with the model. however, we can also query the other way around: in this case, we get the first default location of the user, then obtain a object which contains all the objects associated with that property. i thoroughly enjoyed writing these query statements because they felt more like little brain teaser puzzles. of course, when the schema gets extremely complicated, so will the django query, but nonetheless i still feel confident in the flexibility and the level of abstraction that the django orm offers. django’s views are surprisingly similar to those of flask. the biggest difference is probably the fact that django offers a lot of class based views out of the box. personally, i’m still not the hugest fan of class based views, simply because i feel like it doesn’t provide quite enough the level of granularity i want, but it is a robust, powerful way of writing views no doubt. as i will explain in the next section on the rest framework, i do believe using class based views makes a lot of sense in some contexts. decorators are, in simple terms, ways of wrapping view functions to place restrictions on which users can and cannot access them. for example, we might only want to expose some endpoints to users who are logged in. in that case we might use a decorator like , which is unsurprisingly a function that is offered by django right out of the box. in our case, we wanted to place more stringent restrictions on some views: not only should the user be logged in, but they should also be, for instance, a rerenter or staff to be able to access a certain url. otherwise, we want to show them an access denied page. while this isn’t exactly the code that we used, it covers the gist of the idea: we use the decorator, then apply the custom user type check decorator to make sure that only authorized users can access the view. database migrations was something that really bite us hard in the beginning. the main reasoning was that we weren’t version controlling migration files at all; we had intentionally d it, thinking that migration files are best kept out of our github repository. turns out that this was a very bad idea. because the developers on the team each had their own migrations, changes to the database often messed up how django dealt with migrations. it was even worse with deployment because the heroku app was also having a lot of hiccups whenever we added a column to a table, for instance. in the process, we learned a bunch of django commands like or migrations. after realizing that version controlling migration files took care of the issue, the development process sped up quite a bit. i also learned about , which gave access to a bunch of shortcut commands, almost like aliases, except that i didn’t have to clutter my or . with this in the directory, we can now run to get rid of all migration files as well as the database. this is a destructive operation and should be avoided, but when drastic schema changes are being made in a development environment, i found it to be quite useful. all credits go to mitesh. after the mvp release, i began looking more into frontend frameworks, most notably react.js. and as i was taking a deeper dive into how react works , i got to know more about how the backend and frontend interact with each other. django does not require one to have a lot of knowledge of frontend or javascript in general, at least the way it is set up right out of the box. instead, it comes with dtl, or the django templating language, which is a convenient way of rendering context variables passed over from the backend. for example, in a view function, we might say something like the first argument is the template to be rendered, and the second argument in the form of a dictionary is the context. the variables in the context can be accessed in the templates via . instead of dtl, i prefer to use jinja2, just because the jinja2 syntax is a bit more pythonic, and also because jinja2’s rendering speed is superior to that of the default django templating backend. to read more about specific benchmarks, i recommend that you check out this post. the takeaway is that jinja2 is probably a better choice, although it does require some more configuration to work as smoothly as django. it might also be somewhat frustrating because there are lesser tutorials on how to get jinja2 working with third party packages like django crispy forms. i’ve seen the word rest being thrown around for a while, but it is these past few weeks that i really started to wrap my head around the concept. as i’m still in the learning stage of things, the explanations that follow may have inaccuracies and deficiencies, yet it is my hope that it would provide a beginner friendly introduction to the idea of a rest api. a rest api can loosely be understood as a set of endpoints that allow users to make http requests, such as but not limited to get and post. in other words, the rest api is an entrypoint through which users can interact with the database. of course, a robust api would not allow users to do whatever they want with the database; instead, the api which is effectively a web application would make sure that the user is permitted to make specific requests. the rest api responds to user requests by sending a json response. json stands short for ""javascript serializable object notation."" if you’ve worked with javascript before, you’ll probably be familiar with javascript objects, which are basically what json is. if you’re more of a pythonista like i am, just think of dictionaries. json is a way of serializing data, which works well with browsers and http requests. the rest api provides a way for the backend to communicate with the frontend. the user wouldn’t be directly accessing the endpoints of the rest api, since, as stated earlier, the rest api’s way of responding to requests is by returning some json. therefore, the job of the frontend is to render that json response in a user friendly visual interface. and that’s there all is to it: when a user presses a button, the frontend ""pings"" the backend, and the backend responds with a json object. the frontend then displays the json object, whether it be a list of tweets, a user authentication token, or some error message. by default, django does not provide a restful api in the sense that there is no complete decoupling between the frontend and the backend. the backend communicates directly with the frontend via dtl. the frontend gains limited interactivity with the backend via things like form submissions. the good news is that django has so much community development support that whatever you’re looking for, there’s always going to be a django package for it. in the case of restifying django, there is an incredibly widely used, industry standard package known as the django rest framework, which is often abbreviated as drf. drf allows django developers to easily build up a restful api with only a few minor tweaks. and this is where my earlier point on function and class based views comes in. in the case of vanilla django, the view function might have to deal with some aspects of the frontend since we need to be careful with context variables. in the case of a rest api, however, the heavy lifting is all down by the frontend, which is why the backend can be streamlined quite a bit. in those instances, the default class based views can be very convenient. for example, here is an example of a class view taken from the drf docs. at a glance, this doesn’t seem different from a function based view at all. well, the reason is that we didn’t make use of higher level abstractions that drf provides, just like django itself. if we use some , for instance, we can simplify the code above to something as follows: the , as the name implies, provides a method, which is essentially what we want to show upon a http get request by the client. similarly, the method is made available by inheriting from the . as you can see, these functionalities are something that only inheritance and object oriented programming with classes can provide; function based views cannot and do not provide this level of abstraction and convenience. and that is all we need! just like django, drf is filled with features that make building a rest api but a simple task. i’d like to add one last bit of drf’s magic: viewsets and routers. viewsets provide an additional layer of abstraction even farther on top of what drf class based views provide. an easy way to understand viewsets is to view them as a collection of different views. then, the router automatically maps these views with each url pattern. then, we can set up the router as follows: i’m still not entirely sure how this automatica mapping works that will require me to look at the drf source code but it is helpful to know that there are even higher levels of abstraction that drf provides. i personally don’t think i’ll be using viewsets and routers that much because its level of abstraction takes away a lot of room for customization and micro optimization, yet it is a useful feature no doubt, especially if all you need is to build a quick rest api. i was originally planning to write about things like agile and react, but realized that this post was getting a bit too long. while combining drf with react will be an interesting engineering challenge, we will probably be spending more time debugging and improving the mvp, which is a vanilla django app with a jinja2 templating backend as it stands. in general, working in a collaborative environment on a product has been a delightful challenge. although it is disappointing that i haven’t been able to write more on this blog on what have been my mainstream topics so far math and machine learning i hope to continue writing in some fashion, as i consider writing to be an incredibly rewarding, creative endeavor that also allows me to reflect, improve, and contemplate. thanks for reading, and catch you up in the next one!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
2019-11-22-gamma,"in a previous post, we looked at the poisson distribution as a way of modeling the probability of some event's occurrence within a specified time frame. specifically, we took the example of phone calls and calculated how lucky i was on the day i got only five calls during my shift, as opposed to the typical twelve. while we clearly established the fact that the poisson distribution was a more accurate representation of the situation than the binomial distribution, we ran into a problem at the end of the post: how can we derive or integrate the poisson probability distribution, which is discontinuous? to recap, let's reexamine the poisson distribution function: as you can see, this function is discontinuous because of that one factorial term shamelessly flaunting itself in the denominator. the factorial, we might recall, is as an operation is only defined for integers. therefore, although we can calculate expression such as , we have no idea what the expression evaluates to. or do we? here is where the gamma function kicks in. this is going to be the crux of today's post. let's jump right into it by analyzing the gamma function, specifically euler's integral of the second kind: at a glance, it is not immediately clear as to why this integral is an interpolation of the factorial function. however, if we try to evaluate this expression through integration by parts, the picture becomes clearer: notice that the first term evaluates to 0. moreover, the integral term can be expressed in terms of the gamma function since applying all the simplifications leave us with notice that this is a recursive representation of the factorial, since we can further unravel using the same definition. in other words, so it is now clear that the gamma function is indeed an interpolation of the factorial function. but the gamma function deserves a bit more attention and analysis than the simple evaluation we have performed above. specifically, i want to introduce a few more alternative forms of expressing and deriving the gamma function. there are many ways to approach this subject, and it would be impossible to exhaust through the entire list of possible representations. for the purposes of this post, we look at two forms of the gamma function i find intriguing. the first version, presented below, is euler's definition of the gamma function as an infinite product. to see how this comes from, we backtrack this equality by dividing the right hand side by . at this point, we can reduce the fraction by eliminating from both the denominator and the numerator, which leaves us with the following expression: therefore, we have this is another representation of the gamma function that is distinct from the integral we saw earlier. the last form that we will see involves some tweaking of the harmonic series, or the simplest case of the riemann zeta function where . we start from . we derive both sides by to obtain the following: notice the harmonic series embedded in the expression above. to further simplify this expression, we derive an interpolation of the harmonic series. let the interpolation be denoted as : we derive both sides by to obtain the following: the expression above is the sum of a geometric series with radius . we integrate both sides by to obtain . we are almost done! now that we have the expression for the harmonic series, we can plug it back into the original equation on to finish off this derivation. integrate both sides by to obtain the following: exponentiate both sides by to remove the logarithm, and we finally get the alternative representation of the gamma function: so we see that there are many other alternate modes of expressing the same function, . but the truth that remains unchanged is that the gamma function is essentially a more expansive definition of the factorial that allows for operations on any real numbers. there are many concepts and theories surrounding the gamma function, such as the euler mascheroni constant, mellin transformation, and countless many more, but these might be tabled for another discussion as they each deserve a separate post. the gamma distribution is, just like the binomial and poisson distribution we saw earlier, ways of modeling the distribution of some random variable . deriving the probability density function of the gamma distribution is fairly simple. we start with two parameters, and , using which we can construct a gamma function. we can divide both sides by to obtain the following expression: we can then apply the substitution to obtain notice that we can consider the integrand to be a probability distribution function since the result of integration over the prescribed domain yields 1, the total probability. seen this way, we can finally delineate a definition of the gamma distribution as follows, with a trivial substitution of parameters. if this derivation involving the gamma function does not click with you, we can take the alternative route of starting from the functional definition of the gamma distribution: the gamma distribution models the waiting time until the occurrence of the th event in a poisson process. in other words, given some rate which denotes the rate at which an event occurs, what is the distribution of the waiting time going to look like? the gamma distribution holds an answer to this question. let’s jump right into derivation. the first ingredient we need is the equation for the poisson probability mass function, which we might recall goes as follows: denotes the number of events that occur in unit time, while denotes the rate of success. to generalize this formula by removing the unit time constraint, we can perform a rescaling on to produce the following: where denotes the probability that events occur within time . notice that setting gives us the unit time version of the formula presented above. the link between the poisson and gamma distribution, then, is conveniently established by the fact that the time of the th arrival is lesser than if more than events happen within the time interval . this proposition can be expressed as an identity in the following form. notice that the left hand side is a cumulative distribution function of the gamma distribution expressed in terms of . given the derivative relationship between the cdf and pdf, we can obtain the probability distribution function of gamma by deriving the right hand side sigma expression with respect to . after some thinking, we can convince ourselves that unraveling the sigma results in a chain reaction wherein adjacent terms nicely cancel one another, ultimately collapsing into a single term, which also happens to be the first term of the expression: recalling that , we can rewrite the expression as follows: notice the structural identity between the form we have derived and the equation of the gamma distribution function introduced above, with parameters , , and . in the language of poisson, these variables translate to , , and , respectively. to develop and intuition of the gamma distribution, let’s quickly plot the function. if we determine the values for the parameters and , the term reduces to some constant, say , allowing us to reduce the pdf into the following form: this simplified expression reveals the underlying structure behind the gamma distribution: a fight for dominance between two terms, one that grows polynomially and the other that decays exponentially. plotting the gamma distribution for different values of and gives us a better idea of what this relationship entails. executing this code block produces the following figure. figure 1: gamma distribution for different parameters notice that, as increases, the gamma distribution starts to look more like a normal distribution. at , the collapses to 1, resulting in an exponential distribution. a short tangential digression: the exponential distribution is a special case of a gamma distribution that models the waiting time until the first event in a poisson process. it can also be considered as the continuous version of the geometric distribution. but maybe more on this later on a separate post. returning back to the gamma distribution, we see that altering the also produces a transformative effect on the shape of the gamma pdf. specifically, increasing causes the distribution to move to the right along the axis. this movement occurs because, for every , a larger value results in a decrease in the corresponding value of , which graphically translates to a rightward movement along the axis as shown. we started by looking at the poisson probability mass function, and started our venture into the gamma function by pondering the question of interpolating the factorial function. from there, we were able to derive and develop an intuition for the gamma distribution, which models the waiting time required until the occurrence of the th event in a poisson process. this may all sound very abstract because of the theoretical nature of our discussion. so in the posts to follow, we will explore how these distributions can be applied in different contexts. specifically, we will take a look at bayesian statistics and inference to demonstrate how distributions can be employed to express prior or posterior probabilities. at the same time, we will also continue our exploration of the distribution world by diving deeper into other probability distributions, such as but not limited to exponential, chi square, normal, and beta distributions, in no specific order. at the end of the journey, we will see how these distributions are all beautifully interrelated. catch you up in the next one! interpolation: https://en.wikipedia.org/wiki/interpolation gamma function: https://en.wikipedia.org/wiki/gamma_function harmonic series: https://en.wikipedia.org/wiki/harmonic_series_ riemann zeta function: https://en.wikipedia.org/wiki/riemann_zeta_function previous post: https://jaketae.github.io/study/poisson/ binomial: https://en.wikipedia.org/wiki/binomial_distribution poisson distribution: https://en.wikipedia.org/wiki/poisson_distribution euler mascheroni constant: https://en.wikipedia.org/wiki/euler–mascheroni_constant mellin transformation: https://en.wikipedia.org/wiki/mellin_transform poisson: https://en.wikipedia.org/wiki/poisson_distribution cumulative probability distribution function: https://en.wikipedia.org/wiki/cumulative_distribution_function probability mass function: https://en.wikipedia.org/wiki/probability_mass_function bayesian statistics and inference: https://jaketae.github.io/study/bayes/",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2020-03-09-kl-mle,"these days, i've been spending some time trying to read published research papers on neural networks to gain a more solid understanding of the math behind deep learning. this is a rewarding yet also a very challenging endeavor, mostly because i have not studied enough math to really understand all of what is going on. while reading the groundbreaking research paper wasserstein gan by martin arjovsky, i came across this phrase: ... asymptotically, maximum likelihood estimation amounts to minimizing the kullback leibler divergence... i was particularly interested in the last portion of this sentence, that mle amounts to minimizing kl divergence. we discussed mle multiple time on this blog, including this introductory post and a related post on map. neither is kl divergence an entirely unfamiliar topic. however, i had not thought about these two concepts together in one setting. in this post, let's try to hash out what the quote from the paper means. let's start with a very quick review of what mle and kl divergence each are. after all, it's been a while since i've written the linked posts, and for a fruitful, substantive discussion on this topic, it's necessary to make sure that we have a solid grasp of what mle and kl divergence are. mle is a technique used to find the optimal parameter of a distribution that best describes a set of data. to cut to the chase, this statement can be expressed as follows: from here, we can start making assumptions, such as that observations in are i.i.d, which is the assumption that we make to build models such as naïve bayes, and so on. for now, it suffices to clarify that the goal of maximum likelihood estimation is to find the optimal parameter of a distribution that best captures some given data. kl divergence is a concept that arises from the field of information theory that is also heavily applied in statistics and machine learning. kl divergence is particularly useful because it can be used to measure the dissimilarity between to probability distributions. the familiar equation for kl divergence goes as follows: in bayesian terms, kl divergence might be used to compare the prior and the posterior distribution, where represents the posterior and , the prior. in machine learning, is often the true distribution which we seek to model, and is the approximation of that true distribution, which is also the prediction generated by the model. note that kl divergence is not a true measure of distance, since it is asymmetric. in other words, the focus of this post is obviously not on distance metrics, and i plan on writing a separate post devoted to this topic. but as a preview of what is to come, here is an appetizer to get you interested. an alternative to kl divergence that satisfies the condition of symmetry is the jensen shannon divergence, which is defined as follows: where one can intuit jsd as being a measurement that somewhat averages the two asymmetric quantities of kl divergence. we will revisit jsd in the future when we discuss the mathematics behind gans. but for now, it suffices to know what kl divergence is and what it measures. now that we have reviewed the essential concepts that we need, let's get down to the proof. let's start with the statement of the parameter that minimizes the kl divergence between the two distribution and the approximate distribution : not a lot has happened in this step, except for substituting the expression with its definition as per . observe that in the last derived expression in , the term does not affect the argument of the minima, which is why it can safely be omitted to yield the following simplified expression: we can change the argument of the minima operator to the maxima given the negative sign in the expression for the expected value. to proceed further, it is necessary to resort to the law of large numbers, or lln for short. the law states that the average of samples obtained from a large number of repeated trials should be close to the expected value of that random variable. in other words, the average will approximate the expected value as more trials are performed. more formally, lln might be stated in the following fashion. suppose we perform an experiment involving the random variable and repeat it times. then, we would obtain a set of indecent and identically distributed samples as shown below: then, lln states that a more precise statement of the law uses chebyshev's inequality: for the curious, here is the general formulation of chebyshev's inequality outside the context of lln: for the purpose of this post, it is not necessary to go into how chebyshev's inequality is derived or what it means. however, it isn't difficult to see how one might reformulate to derive to prove the law of large numbers. all that the inequality is saying is that no more than a certain fraction of samples can fall outside more than a certain distance away from the mean of the distribution. with this understanding in mind, let's return to the original problem and wrap up the proof. let's apply the law of large numbers to modify the expected value expression sitting in : voila! we have shown that minimizing the kl divergence amounts to finding the maximum likelihood estimate of . this was not the shortest of journeys, but it is interesting to see how the two concepts are related. indeed, it sort of makes intuitive sense to think that minimizing the distance between the true and approximated distribution is best done through maximum likelihood estimation, which is a technique used to find the parameter of the distribution that best describes given data. i personally find little derivations and proofs like these to be quite interesting, which is why i plan on doing more posts on the mathematics of deep learning and its related concepts in the future. thanks for reading, and catch you up in the next one.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
2020-02-22-vae,"in a previous post, we took a look at autoencoders, a type of neural network that receives some data as input, encodes them into a latent representation, and decodes this information to restore the original input. autoencoders are exciting in and of themselves, but things can get a lot more interesting if we apply a bit of twist. in this post, we will take a look at one of the many flavors of the autoencoder model, known as variational autoencoders, or vae for short. specifically, the model that we will build in this tutorial is a convolutional variational autoencoder, since we will be using convolutional layers for better image processing. the model architecture introduced in this tutorial was heavily inspired by the one outlined in françois chollet's deep learning with python, as well as that from a separate article on the keras blog. let's start by importing the modules necessary for this demonstration. the objective of today's task is to build an autoencoder model that produces mnist hand written digits. the hidden dimension, or the latent space of the model, is going to a random vector living in two dimensional space. let's specify this setup, along with some other miscellaneous configurations, before we proceed with constructing the model architecture. it's time to build our model... or not quite now. before we start stacking layers for the encoder and the decoder, we need to define a sampling function that will perform the meat of the variational inference involved in vae. let's start out by taking a look at the sampling function we will use to define one of the layers of the variational autoencoder network. simply put, the above below takes as arguments and in the form of a bundled list. as you can guess from the name of the variables, these two parameters refer to the mean and log variance of the random vector living in our predefined latent space. note that we are assuming a diagonal gaussian here: in other words, the covariance matrix of the multi dimensional gaussian is assumed to be diagonal, meaning that each elements of the vector are independent. if any of this sounds foreign to you, i recommend that you read this post on the gaussian distribution. let's continue our discussion with the sampling function. the goal here is to sample a random vector in the latent space from the distribution specified by the two parameters, mean and log variance. the sampling process can be expressed as follows: where denotes the mean, corresponding to , denotes a tensor of random numbers sampled from the standard normal distribution, and denotes the standard deviation . essentially, the goal here is to use a resampling technique such that we can sample from a standard normal distribution centered around mean 0 and a standard deviation of 1, but consequentially sample from a distribution of living in the latent space. if you are wondering how translates to the return statement, then the following equation might resolve your curiosity. this is the promised elaboration on the relationship between log variance and standard deviation: therefore, multiplying 0.5 is just a simple algebraic manipulation to morph log variance to standard deviation. the reason why we use log variance instead of just variance or standard deviation is to ensure numerical stability in computation. now that this part has been cleared, let's start stacking away layers! just like the autoencoder, vaes are composed of two discrete components: the encoder and the decoder. here, we take a look at the first piece of the puzzle, the encoder network. there are several things to note about this model. first, i decided to use a loop to simplify the process of stacking layers. instead of repeating the same code over multiple lines, i found this approach to be more succinct and concise. second, we define a custom layer at the end, shown as , that uses the function we defined earlier. this is the final key that enables us to build an encoder model that receives as input a 28 by 28 image, then output a two dimensional latent vector representation of that image to pass onto the decoder network. below is the summary of what our model looks like. note that the model outputs a total of three quantities: , , and . we need the first two parameters to later sample from the latent distribution; , of course, is needed to train the decoder. model: ""encoder"" __________________________________________________________________________________________________ ================================================================================================== input_1 0 __________________________________________________________________________________________________ conv2d 320 input_100 __________________________________________________________________________________________________ conv2d_1 18496 conv2d00 __________________________________________________________________________________________________ flatten 0 conv2d_100 __________________________________________________________________________________________________ dense 50192 flatten00 __________________________________________________________________________________________________ z_mean 34 dense00 __________________________________________________________________________________________________ z_log_var 34 dense00 __________________________________________________________________________________________________ z 0 z_mean00 z_log_var00 ================================================================================================== total params: 69,076 trainable params: 69,076 non trainable params: 0 __________________________________________________________________________________________________ the decoder network looks similar to the the encoder, except that much of the architecture is in reverse order. most notably, we use to undo the convolution done by the encoder. this allows us to effectively scale up the input back to its original dimension, which is what we want to do with a generative model like a vae. one subtly worth mentioning is the fact that we use a sigmoid activation in the end. this is because we want the pixel values of the output to be between 0 and 1, just as the original input was normalized before it was fed into the encoder network via division by 255. the summary of the decoder network is presented below: model: ""decoder"" _________________________________________________________________ ================================================================= input_2 0 _________________________________________________________________ dense_1 9408 _________________________________________________________________ reshape 0 _________________________________________________________________ conv2d_transpose 36928 _________________________________________________________________ conv2d_transpose_1 18464 _________________________________________________________________ conv2d_transpose_2 289 ================================================================= total params: 65,089 trainable params: 65,089 non trainable params: 0 _________________________________________________________________ now that we have both the encoder and the decode network fully defined, it's time to wrap them together into one autoencoder model. this can simply achieved by defining the input as the input of the encoder the normalized mnist images and defining the output as the output of the decoder when fed a latent vector. concretely, this process might look as follows: let's look a the summary of the cvae. note that the encoder and the decoder look like individual layers in the grand scheme of the vae architecture. model: ""model_1"" _________________________________________________________________ ================================================================= input_1 0 _________________________________________________________________ encoder , , 65089 ================================================================= total params: 134,165 trainable params: 134,165 non trainable params: 0 _________________________________________________________________ we have almost everything we need, but there is one crucial step that is missing: compiling the model with an optimizer and a loss function. normally, defining a loss function is very easy: in most cases, we use pre made loss functions that are available through the tensorflow api, such as cross entropy or mean squared error. in the case of variational autoencoders, however, this is not such an easy task: how do we judge the robustness or the effectiveness of the decoder, which is essentially a generative algorithm? of course, we could stop training once the figures it generates becomes reasonable, i.e. the mock mnist digits it creates looks compelling to the human eye. however, this is a subjective metric at best, and we can't expect there to be a ml engineer peering at the screen, looking at the outputs of the decoder per each epoch. to tackle this challenge, we need to dive into some math. let's take a look. first, let's carefully review what our goal is for this task. the motivating idea behind variational autoencoders is that we want to model a specific distribution, namely the distribution of the latent space given some input. as you recall, this latent space is a two dimensional vector modeled as a multivariate diagonal gaussian. using bayes' theorem, we can express this distribution as follows: by now, it is pretty clear what the problem its: the evidence sitting in the denominator is intractable. therefore, we cannot directly calculate or derive in its closed form; hence the need for variational inference. the best we can do is to find a distribution that best approximates . how do we find this distribution? well, we know one handy concept that measures the difference or the pseudo distance between two distributions, and that is kullback leibler divergence. as we discussed in this post on entropy, kl divergence tells us how different two distributions are. so the goal here would be find a distribution that minimizes the following expression: using the definition of conditional probability, we can simplify as follows: the trick is to notice that is a constant that can break out of the expectation calculation. let's continue by deriving an expression for the evidence term. a useful property to know about kl divergence is the fact that it is always non negative. we will get into why this is the case in a moment. for now, let's assume non negativity to be true and transform into an inequality: the term on the right of the inequality is known as the evidence lower bound, or elbo for short. why are we interested in elbo? first, note that , the evidence, is a constant. therefore, minimizing kl divergence amounts to maximizing elbo. this is the key to variational inference: instead of calculating the intractable integral in , we can find a distribution that which minimizes kl divergence by maximizing elbo, which is a tractable operation. let's prove why kl divergence is always greater or equal to zero, which is a condition we assumed to be true in the derivation of elbo above. for the sake of completeness, i present two ways of proving the same property. in the context of probability, jensen's inequality can be summarized as follows. given a convex function , we won't get into rigorous proofs here, but it's not difficult to see why this inequality stands with some basic geometric intuition. due to its bow like shape, the expected value of a convex function evaluated across a given interval will always be greater or equal to the function evaluated at the expected value of the random variable. !img how is jensen's inequality related to the non negativity of kl divergence? let's return back to the definition of kl divergence. for simplicity and to reduce notational burden, we briefly depart from conditional probabilities and return back to generic distributions and . notice that the definition of kl divergence itself is an expected value expression. also, note that is a convex function itself is concave, but the negative sign flips the concavity the other way. with these observations in mind, we can apply jensen's inequality to derive the following: therefore, we have shown that kl divergence is always greater or equal to zero, which was our end goal. there is another version of a proof that i found a lot more intuitive and easier to follow than the previous approach. this derivation was borrowed from this post. we start from the simple observation that a logarithmic function is always smaller than a linear one. in other words, this is no rocket science, and one can easily verify by simply plotting the two functions on a cartesian plane. using , we can proceed in a different direction from the definition of kl divergence. once again, we have shown that kl divergence is positive! proving this isn't really necessary in the grand scheme of exploring the mathematics behind vaes, yet i thought it would help to have this adjunctive section to better understand kl divergence and familiarize ourselves with some standard algebraic manipulations that are frequently invoked in many derivations. let's jump back into variational inference and defining the cost function with elbo. recall from the setup of our variational autoencoder model that we have defined the latent vector as living in two dimensional space following a multivariate gaussian distribution. it's time to apply the elbo equation to this specific context and derive a closed form expression of our loss function. let's recall the formula for elbo: after some rearranging, we can decompose elbo into two terms, one of which is a kl divergence: now, it's finally time for us to dive deep into math: let's unpack the closed form expression in . note that the elbo expression applies to just about any distribution, but since we chose a multivariate gaussian to be the base distribution, we will see how it unfolds specifically in this context. let's begin by assuming the distribution of our models to be gaussian. namely, because is an approximation of , we naturally assume the same model for the approximate distribution: now we can derive an expression for the negative kl divergence sitting in the elbo expression: this may seem like a lot, but it's really just plugging in the distributions into the definition of kl divergence as an expectation and using some convenient properties of logarithms to perform simple algebraic simplifications. to proceed further, observe that the first term is a constant that can escape out of the expectation: from the definition of variance and expectation, we know that therefore, we can simplify as follows: let's zoom in on the expected value term in . our goal is to use again so that we can flesh out another one half from that term. this can be achieved through some clever algebraic manipulation: but since the the expected value of is constant and that of is zero, we can now plug this simplified expression back into the calculation of kl divergence, in : since we will standardize our input such that and , we can plug these quantities into and show that we are almost done with deriving the expression for elbo. i say almost, because we still have not dealt with the trailing term in : at this point, it is extremely useful to recall the definition of cross entropy, which is generically defined as follows: therefore, we see that the trailing term in is just a cross entropy between two distributions! this was a circumlocutions journey, but that is enough math we will need for this tutorial. it's time to get back to coding. all that math was for this simple code snippet shown below: as you can see, this short code snippet shows, in essence, how we can define a compile a model with a custom loss function. in this case, refers to the reconstruction loss, which is the cross entropy term we saw earlier. , as you might expect, simply refers to kl divergence. notice how there is a multiplying factor in the expression, just like we did when we derived it in the section above. with some keen observations and comparisons, you will easily see that the code is merely a transcription of , with some minor differences given dimensionality. one important fact to note is that the gradient descent algorithm, by default, seeks to minimize the loss function. however, we discussed above how the objective of vae is to maximize elbo. therefore, we modify elbo into a loss function that is to be minimized by defining the loss function as the negative of elbo. in other words, the cost function is defined as ; hence the difference in sign. it's finally time to test the model. let's first begin with data preparation and preprocessing. now, we should have the training and test set ready to be fed into our network. next, let's define a simple callback application using the monitor so that training can be stopped when no substantial improvements are being made to our model. this was included because training a vae can take some time, and we don't want to waste computing resources seeing only submarginal increments to the model performance. training begins! train on 60000 samples, validate on 10000 samples epoch 1/30 60000/60000 ============================== 13s 210us/sample loss: 191.6765 val_loss: 170.1529 epoch 2/30 60000/60000 ============================== 11s 180us/sample loss: 163.9683 val_loss: 160.2263 epoch 3/30 60000/60000 ============================== 11s 181us/sample loss: 159.0007 val_loss: 158.0777 epoch 4/30 60000/60000 ============================== 11s 180us/sample loss: 156.8238 val_loss: 156.3414 epoch 5/30 60000/60000 ============================== 11s 181us/sample loss: 155.4041 val_loss: 154.7498 epoch 6/30 60000/60000 ============================== 11s 181us/sample loss: 154.2847 val_loss: 153.9668 epoch 7/30 60000/60000 ============================== 11s 180us/sample loss: 153.4675 val_loss: 153.8024 epoch 8/30 60000/60000 ============================== 11s 179us/sample loss: 152.7539 val_loss: 152.6393 epoch 9/30 60000/60000 ============================== 11s 181us/sample loss: 152.2562 val_loss: 152.6557 epoch 10/30 60000/60000 ============================== 11s 180us/sample loss: 151.7278 val_loss: 151.7882 epoch 11/30 60000/60000 ============================== 11s 179us/sample loss: 151.3973 val_loss: 151.6642 epoch 12/30 60000/60000 ============================== 11s 177us/sample loss: 150.9899 val_loss: 151.3316 epoch 13/30 60000/60000 ============================== 11s 177us/sample loss: 150.6191 val_loss: 152.0779 epoch 14/30 60000/60000 ============================== 11s 179us/sample loss: 150.3378 val_loss: 151.6977 after 14 epochs, training has stopped, meaning that no meaningful improvements were being made. let's visualize the representation of the latent space learned by the vae. visualizing this representation is easy in this case because we defined the latent space to be two dimensional; in other words, all points can be plotted on a cartesian plane. let's take a look: this plot shows us how each numbers are distributed across the latent space. notice that numbers that belong to the same class seem to be generally clustered around each other, although there is a messy region in the middle. this is a reasonable result: while we would expect ones to be fairly easy to distinguish from, say, eights, numbers like zeros and sixes might look very similar, and hence appear mixed as a lump in the fuzzy region in the middle. one cool thing about vaes is that we can use their learned representation to see how numbers slowly morph and vary across a specified domain. this is why vaes are considered to be generative models: if we feed the vae some two dimensional vector living in the latent space, it will spit out a digit. whether or not that digit appears convincing depends on the random vector the decoder was provided as input: if the vector is close to the learned mean, , then the result will be convincing; if not, we might see a confusing blob of black and white. let's see what exactly is going on in the fuzzy region of the image, because that is apparently where all the digits mingle together and seem indistinguishable from one another. put differently, if we vary the random vector little by little across that region, we will be able to see how the digit slowly morphs into another number. how cool is that? we were able to get a vae to show us how one digit can shift across a certain domain of the latent space. this is one of the many cool things we can do with a generative model like a variational autoencoder. in this post, we took a deep dive into the math behind variational autoencoders. it was a long journey, but definitely worth it because it exposed us to many core concepts in deep learning and statistics. at the same time, i found it fascinating to see how a model could learn from a representation to generate numbers, as we saw in the very last figure. in a future post, we will look at generative adversarial networks, or gans, which might be considered as the pinnacle of generative models and a successor to autoencoders. gans resemble autoencoders in that it is also composed of two models. one core difference, however, is that in gans, the two models are in a competing relationship, whereas in autoencoders, the encoder and the decoder play distinct, complementary roles. if any of this sounds exciting, make sure to check out the next post. i hope you enjoyed reading. catch you up in the next one!",0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2020-06-28-genetic-algorithm,"the traveling salesman problem is a famous problem in computer science. the problem might be summarized as follows: imagine you are a salesperson who needs to visit some number of cities. because you want to minimize costs spent on traveling , you want to find out the most efficient route, one that will require the least amount of traveling. you are given a coordinate of the cities to visit on a map. how can you find the optimal route? the most obvious solution would be the brute force method, where you consider all the different possibilities, calculate the estimated distance for each, and choose the one that is the shortest path. while this is a definite way to solve tsp, the issue with this approach is that it requires a lot of compute the runtime of this brute force algorithm would be , which is just utterly terrible. in this post, we will consider a more interesting way to approach tsp: genetic algorithms. as the name implies, genetic algorithms somewhat simulate an evolutionary process, in which the principle of the survival of the fittest ensures that only the best genes will have survived after some iteration of evolutionary cycles across a number of generations. genetic algorithms can be considered as a sort of randomized algorithm where we use random sampling to ensure that we probe the entire search space while trying to find the optimal solution. while genetic algorithms are not the most efficient or guaranteed method of solving tsp, i thought it was a fascinating approach nonetheless, so here goes the post on tsp and genetic algorithms. before we dive into the solution, we need to first consider how we might represent this problem in code. let's take a look at the modules we will be using and the mode of representation we will adopt in approaching tsp. the original, popular tsp requires that the salesperson return to the original starting point destination as well. in other words, if the salesman starts at city a, he has to visit all the rest of the cities until returning back to city a. for the sake of simplicity, however, we don't enforce this returning requirement in our modified version of tsp. below are the modules we will be using for this post. we will be using , more specifically a lot of functions from for things like sampling, choosing, or permuting. arrays are also generally faster than using normal python lists since they support vectorization, which will certainly be beneficial when building our model. for reproducibility, let's set the random seed to 42. now we need to consider the question of how we might represent tsp in code. obviously, we will need some cities and some information on the distance between these cities. one solution is to consider adjacency matrices, somewhat similar to the adjacency list we took a look at on the post on breadth first and depth first search algorithms. the simple idea is that we can construct some matrix that represent distances between cities and such that represents the distance between those two cities. when , therefore, it is obvious that will be zero, since the distance from city to itself is trivially zero. here is an example of some adjacency matrix. for convenience purposes, we will represent cities by their indices. now it's time for us to understand how genetic algorithms work. don't worry, you don't have to be a biology major to understand this; simple intuition will do. the idea is that, we can use some sort of randomized approach to generate an initial population, and motivate an evolutionary cycle such that only superior genes will survive successive iterations. you might be wondering what genes are in this context. most typically, genes can be thought of as some representation of the solution we are trying to find. in this case, an encoding of the optimal path would be the gene we are looking for. evolution is a process that finds an optimal solution for survival through competition and mutation. basically, the genes that have superior traits will survive, leaving offspring into the next generation. those that are inferior will be unable to find a mate and perish, as sad as it sounds. then how do these superior or inferior traits occur in the first place? the answer lies in random mutations. the children of one parent will not all have identical genes: due to mutation, which occurs by chance, some will acquire even more superior features that puts them far ahead of their peers. needless to say, such beneficiaries of positive mutation will survive and leave offspring, carrying onto the next generation. those who experience adversarial mutation, on the other hand, will not be able to survive. in genetic algorithm engineering, we want to be able to simulate this process over an extended period of time without hard coding our solution, such that the end result after hundred or thousands of generations will contain the optimal solution. of course, we can't let the computer do everything: we still have to implement mutational procedures that define an evolutionary process. but more on that later. first, let's begin with the simple task of building a way of modeling a population. first, let's define a class to represent the population. i decided to go with a class based implementation to attach pieces of information about a specific generation of population to that class object. specifically, we can have things like to represent the full population, to represent th chosen, selected superior few, to store the score of the best chromosome in the population, to store the best chromosome itself, and , the adjacency matrix that we will be using to calculate the distance in the context of tsp. here is a little snippet of code that we will be using to randomly generate the first generation of population. let's see if this everything works as expected by generating a dummy population. array now we need some function that will determine the fitness of a chromosome. in the context of tsp, fitness is defined in very simple terms: the shorter the total distance, the fitter and more superior the chromosome. recall that all the distance information we need is nicely stored in . we can calculate the sum of all the distances between two adjacent cities in the chromosome sequence. next, we evaluate the population. simply put, evaluation amounts to calculating the fitness of each chromosome in the total population, determining who is best, storing the score information, and returning some probability vector whose each element represents the probability that the th element in the population bag is chosen as a parent. we apply some basic preprocessing to ensure that the worst performing chromosome has absolutely no chance of being selected. when we call , we get a probability vector as expected. from the result, it appears that the last element is the best chromosome; the second chromosome in the population bag is the worst. array when we call , notice that we get the last element in the population, as previously anticipated. array we can also access the score of the best chromosome. in this case, the distance is said to be 86.25. note that the lower the score, the better, since these scores represent the total distance a salesman has to travel to visit all the cities. 86.25 now, we will select number of parents to be the basis of the next generation. here, we use a simple roulette model, where we compare the value of the probability vector and a random number sampled from a uniform distribution. if the value of the probability vector is higher, the corresponding chromosome is added to . we repeat this process until we have parents. as expected, we get 4 parents after selecting the parents through . array now is the crucial part: mutation. there are different types of mutation schemes we can use for our model. here, we use a simple swap and crossover mutation. as the name implies, swap simply involves swapping two elements of a chromosome. for instance, if we have , we might swap the first two elements to end up with . the problem with swap mutation, however, is the fact that swapping is a very disruptive process in the context of tsp. because each chromosome encodes the order in which a salesman has to visit each city, swapping two cities may greatly impact the final fitness score of that mutated chromosome. therefore, we also use another form of mutation, known as crossovers. in crossover mutation, we grab two parents. then, we slice a portion of the chromosome of one parent, and fill the rest of the slots with that of the other parent. when filling the rest of the slots, we need to make sure that there are no duplicates in the chromosome. let's take a look at an example. imagine one parent has and the other has . let's also say that slicing a random portion of the first parent gave us . then, we fill up the rest of the empty indices with the other parent, paying attention to the order in which elements occur. in this case, we would end up with . let's see how this works. now, we wrap the swap and crossover mutation into one nice function to call so that we perform each mutation according to some specified threshold. let's test it on . when we call , we end up with the population bag for the next generation, as expected. 3, 1, 2, 0, 4, 0, 1, 2, 3, 4, 3, 1, 4, 0, 2, 3, 1, 2, 0, 4, 3, 1, 2, 0, 4 now it's finally time to put it all together. for convenience, i've added some additional parameters such as or , but for the most part, a lot of what is being done here should be familiar and straightforward. the gist of it is that we run a simulation of population selection and mutation over generations. the key part is and . basically, we obtain the children from the mutation and pass it over as the population bag of the next generation in the constructor. now let's test it on our tsp example over 20 generations. as generations pass, the fitness score seems to improve, but not by a lot. generation 0: 105.04 generation 1: 105.04 generation 2: 104.13 generation 3: 104.13 generation 4: 104.13 generation 5: 104.13 generation 6: 104.13 generation 7: 104.13 generation 8: 104.13 generation 9: 104.13 generation 10: 104.13 generation 11: 104.13 generation 12: 104.13 generation 13: 104.13 generation 14: 104.13 generation 15: 104.13 generation 16: 104.13 generation 17: 104.13 generation 18: 104.13 generation 19: 104.13 3, 0, 2, 1, 4 let's try running this over an extended period of time, namely 100 generations. for clarity, let's also plot the progress of our genetic algorithm by setting to . generation 0: 117.11000000000001 generation 20: 99.06 generation 40: 86.25 generation 60: 86.25 generation 80: 86.25 4, 1, 3, 2, 0 after something like 30 iterations, it seems like algorithm has converged to the minimum, sitting at around 86.25. apparently, the best way to travel the cities is to go in the order of . but this was more of a contrived example. we want to see if this algorithm can scale. so let's write some functions to generate city coordinates and corresponding adjacency matrices. generates number of random city coordinates in the form of a numpy array. now, we need some functions that will create an adjacency matrix based on the city coordinates. let's perform a quick sanity check to see if works as expected. here, give vertices of a unit square as input to the function. while we're at it, let's also make sure that indeed does create city coordinates as expected. array array now, we're finally ready to use these functions to randomly generate city coordinates and use the genetic algorithm to find the optimal path using with the appropriate parameters. let's run the algorithm for a few iterations and plot its history. generation 0: 50574.20948201705 generation 100: 37016.74080507189 generation 200: 29461.92301346868 generation 300: 27377.57152367764 generation 400: 25832.95851026539 generation 500: 24982.527095698166 generation 600: 23996.17871767765 generation 700: 23747.520578015312 generation 800: 22876.05723826875 generation 900: 22585.508967184436 91, 82, 25, 69, 52, 78, 2, 57, 75, 29, 27, 81, 35, 92, 18, 68, 34, 79, 58, 55, 0, 54, 74, 13, 37, 23, 67, 19, 61, 97, 64, 86, 93, 65, 17, 1, 3, 8, 59, 7, 98, 66, 49, 22, 5, 62, 41, 96, 12, 95, 36, 44, 77, 48, 31, 16, 39, 99, 53, 6, 43, 42, 83, 73, 60, 71, 76, 14, 33, 89, 38, 47, 28, 9, 85, 11, 72, 21, 88, 51, 63, 4, 15, 70, 56, 24, 94, 87, 90, 80, 50, 26, 30, 10, 40, 84, 46, 32, 20, 45 we can see that the genetic algorithm does seems to be optimizing the path as we expect, since the distance metric seems to be decreasing throughout the iteration. now, let's actually try plotting the path along with the corresponding city coordinates. here's a helper function to print the optimal path. and calling this function, we obtain the following: at a glance, it's really difficult to see if this is indeed the optimal path, especially because the city coordinates were generated at random. i therefore decided to create a much more contrived example, but with many coordinates, so that we can easily verify whether the path decided on by the algorithm is indeed the optimal path. namely, we will be arranging city coordinates to lie on a semi circle, using the very familiar equation let's create 100 such fake cities and run the genetic algorithm to optimize the path. if the algorithm does successfully find an optimal path, it will be a single curve from one end of the semi circle fully connected all the way up to its other end. generation 0: 2964.489767366144 generation 500: 871.7211766732178 generation 1000: 688.4158889546616 generation 1500: 600.4773168672432 generation 2000: 531.3472866075754 generation 2500: 502.99936690851723 generation 3000: 418.2191546248274 generation 3500: 414.9874637897337 generation 4000: 411.94420801395387 generation 4500: 411.87289018133225 generation 5000: 407.6196328244909 generation 5500: 407.54195052584106 0, 1, 2, 3, 4, 36, 37, 38, 39, 40, 41, 42, 43, 47, 48, 49, 50, 51, 52, 53, 99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82, 81, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 54, 55, 56, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 46, 45, 44, 11, 10, 9, 8, 7, 6, 5 the algorithm seems to have converged, but the returned does not seem to be the optimal path, as it is not a sorted array from 0 to 99 as we expect. plotting this result, the fact that the algorithm hasn't quite found the most optimal solution becomes clearer. this point notwithstanding, it is still worth noting that the algorithm has found what might be referred to as optimal segments: notice that there are some segments of the path that contain consecutive numbers, which is what we would expect to see in the optimal path. an optimal path would look as follows. comparing the two, we see that the optimal path returned by the genetic algorithm does contain some wasted traveling routes, namely the the chords between certain non adjacent cities. nonetheless, a lot of the adjacent cities are connected . considering the fact that there are a total of possibilities, the fact that the algorithm was able to narrow it down to a plausible route that beats the baseline is still very interesting. genetic algorithms belong to a larger group of algorithms known as randomized algorithms. prior to learning about genetic algorithms, the word ""randomized algorithms"" seemed more like a mysterious black box. after all, how can an algorithm find an answer to a problem using pseudo random number generators, for instance? this post was a great opportunity to think more about this naive question through a concrete example. moreover, it was also interesting to think about the traveling salesman problem, which is a problem that appears so simple and easy, belying the true level of difficulty under the surface. there are many other ways to approach tsp, and genetic algorithms are just one of the many approaches we can take. it is also not the most effective way, as iterating over generations and generations can often take a lot of time. the contrived semi circle example, for instance, took somewhere around five to ten minutes to fully run on my 13 inch macbook pro. nonetheless, i think it is an interesting way well worth the time and effort spent on implementation. i hope you've enjoyed reading this post. catch you up in the next one!",0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
2019-12-21-information-entropy,"the other day, my friend and i were talking about our mutual friend jeremy. ""he's an oddball,"" my friend sean remarked, to which i agreed. out of nowhere, jeremy had just told us that he would not be coming back to korea for the next three years. ""he is just about the most random person i know."" and both of us, being aspiring statistics majors, began wondering: is there a systematic way of measuring randomness? it is from here that we went down the rabbit hole of google and wikipedia search. i ended up landing on entropy land, which is going to be the topic for today's post. it's a random post on the topic of randomness. to begin our discussion of randomness, let's take a look at how scientists measure the randomness involved in natural phenomena, such as particle movement. if you are a chemist or physicist, you might be familiar with the concept of entropy, one of the core elements of thermodynamics and a topic that recurs throughout many subfields of natural sciences. although i'm no science major, one of the few things that i recall from high school chemistry class is the gibbs free energy equation for calculating the spontaneity of reactions, which went as follows: where the term for entropy, denoted as satisfies the condition that we won't get into the details of these equations, but an intuition that we can glean from this equation is that the randomness of a particle is determined by the number of potential states that are possible for that given particle. in other words, a gas particle that freely moves across space at atp is going to be more random than a near static water particle composing an ice cub. we might take a step further and say that the gas particle carries a larger amount of information than the particle in a solid since more information is required to express its randomness. entropy in science, denoted above as , provides us with a valuable intuition on the topic of randomness and information. in fact, it is no coincidence that the notion of randomness in information theory, a subfield of math that we are going to be dipping our toes in, borrowed the term ""entropy"" to express randomness exhibited in data. just like entropy was used to quantify randomness in the scientific phenomena, the notion of entropy is used in information theory to denote randomness in data. the origin of information entropy can be traced back to claude shannon's paper published in 1948, titled ""a mathematical theory of communication."" while working at bell labs, shannon was experimenting with methods to most efficiently encode and transmit data without loss of information. it is in this context that shannon proposed the notion of entropy, which he roughly defined as the smallest possible size of lossless encoding of a message that can be achieved for transmission. of course, there is a corresponding mathematical definition for entropy. but before we jump straight into entropy, let's try to develop some preliminary intuition on the concept of information, which is the building block of entropy. what is information? warren weaver, who popularized shannon's works and together developed the field of information theory, pointed out that information is not related to what is said, but what could be said. this element of uncertainty involved in one's degree of freedom is what makes the notion of information inseparable from probability and randomness. as ian goodfellow put it in deep learning, the basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred. in other words, a low probability event expresses a lot of information, while a high probability event expresses low information as its occurrence provides little information of value to the informed. put differently, rare events require more information to represent than common ones. consider, for example, how we might represent the amount of information involved in a fair coin toss. we know for a fact that where and denote the events that the coin lands on heads and tails, respectively. how might we be able to express information involved in the event that the coin lands on tails? how about heads? there are many ways to approach this problem, but an easy way would be to use binary numbers. for example, we might ascribe meaning to 0 and 1 such that 0 represents heads and 1 represents tails. of course, there might be other ways to encode information, such as setting 111 to heads and 000 to tails, but obviously this is wasting information. in other words, it is not the most efficient method of encoding. even under a single digit binary number scheme, we would be able to decode a series of transmitted information without loss. 'h', 't', 'h', 'h', 't' it is not difficult to see, therefore, that all we need to express the result of a fair coin toss is a single digit of binary numbers 0 and 1. typically, we use bits to denote the number of digits required to express information in binary numbers. in this case, the information involved in is equal to 1 bit; by symmetry, the same goes for . if bits sounds similar to bytes or gigabytes we use for storage, you're exactly on the right path. in fact, the relationship between bit and byte is established directly by the fact that where denotes bits and denotes bytes. this is why we use bytes to represent the amount of disk storage in computers, for instance. it is also worth mentioning that the alternative name for bits is shannons, named eponymously after the mathematician who pioneered the field of information theory, as mentioned above. now that we have some idea of what information is and how we can quantify it using binary numbers in bits, it's time to get into the math. information can be calculated through the formula where is the information need to express the random event , and is the probability that event occurs, i.e. . there are different versions of this formula, such as the one that uses euler's constant as the log base instead of 2. whereas the unit of information as measured through is in bits, that calculated through as shown below is in the unit of nats. for the purposes of this post, we will be using equation instead of two. this is primarily because we will be using the binary number analogy to build an intuition for information computation. let's quickly create a visualization that shows the relationship between probability and information in bits. as equation describes this relationship quite concisely, let's try plotting it on a graph. so that's how we calculate randomness in a random event the amount of information that is needed to represent randomness as probability. if you think about it for a second, this is a very intuitive definition of randomness: the more random and infrequent an event is, the more information would be required to represent it. with this in mind, now we move onto the bigger picture: entropy in the context of random variables. in the previous section, we looked at how random events can be represented as information in bits. what's important here was that we were dealing with isolated random events instead of random variables. for example, in the fair coin toss example, we dealt with information involved with and , not the binomial random variable itself. this is an important distinction to make, because entropy is essentially a probabilistically weighted average of all the random events that a random variable can take. in other words, entropy is defined as the weighted average of information given by each random event: for the continuous case, we would use an integral instead. say denotes the random variable of interest in a fair coin toss. then, we are most interested in how much bits, on average, we would need to encode information generated from the distribution of this random variable. using , we can easily answer this question by calculating the follows: this tells us that the entropy involved in a fair coin toss is 1 bit, i.e, on average, we only need a single digit of binary number to encode information given by a fair coin toss. but how might this number change for a biased coin? we would not expect the entropy of the random variable given by the result of a biased coin to be 1. for example, consider a coin that always lands on heads. the entropy of the random variable in this case would be exactly 0 bits, since we don't need any information to express an event that is certain. let's try to figure out this dynamic between success probability of in a coin toss and entropy by creating a plot. the first observation to make is that the graph is symmetrical. this is no surprise, since we would expect the entropy of a random variable involving a coin that lands tails with probability to be equal to that which lands on heads with equal probability , i.e. whether the bias concerns heads or tails should not affect the calculation of entropy. moreover, we see that the graph peaks when , meaning that a fair coin toss involves the most randomness. in other words, this translates to saying that a skewed distribution is less random and thus more predictable than a symmetric one. this makes sense, since the result of a biased coin is more predictable and less surprising than that of a fair one, with the extreme case being the coin that always lands on one side. seen in this light, entropy is just the total amount of information content expressed by the distribution of a random variable. because we were dealing with a very simple example of biased coins, entropy values we calculated did not go past 1 bits, but we can easily imagine situations were it might, such as a dice roll or more complicated real life examples. in this light, entropy is one metric with which we can quantify randomness, which is the mission we set out to accomplish in this post. but entropy is a lot more than just an interesting concept on randomness. entropy has wide ranging applications, especially in the field of machine learning and neural networks. among these, we will be looking at cross entropy and kullback leibler divergence, two derivative concepts of entropy that are arguably most commonly used. let's start with the big picture first when these concepts are used, what is the relationship between the two, why they are important and move onto the details later. first, we have to know that cross entropy, unlike just entropy, deals with two probability distributions instead of one. specifically, cross entropy is a way of measuring the pseudo distance between two probability distributions. in a typical scenario, we might have a true probability distribution that we are trying to model, and our deep learning algorithm might produce some approximate probability distribution . we might evaluate the effectiveness of our model by calculating the distance between and . seen in this light, cross entropy can be interpreted as a target cost function to be minimized. here is the equation that explains the relationship between entropy, cross entropy, and kl divergence. where denotes cross entropy; ,entropy, and the last term, kl divergence. now, let's try to understand what each of them means. kl divergence has many interpretations. one possible definition of kl divergence is that it measures the average number of extra information content required to represent a message with distribution instead of . in machine learning: a probabilistic perspective, kevin p. murphy describes kl divergence as follows: ... the kl divergence is the average number of extra bits needed to encode the data, due to the fact that we used distribution to encode the data instead of the true distribution . put differently, kl divergence is the amount of information that is lost when is used to approximate . therefore, if and are close, kl divergence would be low, whereas the converse would be true when the two distributions are different. we can also extend this notion a bit farther to apply it in the context of bayesian inference. recall that bayesian inference is the process by which we start from some prior distribution and update our beliefs about the distribution with more data input to derive a posterior. in this context, kl divergence can be viewed as the amount of information gained as we move from the prior to the posterior . let's derive the mathematical definition of kl divergence using likelihoods. the derivation process to be introduced is based on this source. we begin with the likelihood ratio: we can consider as representing how probable the sample came from distribution than , given that was sampled from some unknown distribution. if , the more likely it is that the data came from ; if , the more probable it is that the sample came from . say we have multiple independent observations of data. then, we can use to compute the likelihood ratio for each sample. calculating the product of these ratios will tell us which distribution is more likely given all available data points. in other words, a technique we saw when we were exploring the topic of likelihood maximization was log likelihood. log likelihoods are useful because we can reexpress products as sums, using the property of logs. makes sense, but it weighs all likelihood ratios equally. in reality, most samples are not equiprobable; some values are more likely than others, unless in the context of uniform distributions. to account for this, let's reframe as an expected values calculation, i.e. give different weight to each likelihood ratio depending on the probability of observing that data point. let's make look better by unpacking the fraction sitting in the log function as a subtraction of two terms. the final key is to realize the secret that in other words, we have derived the mathematical definition of kl divergence! the mathematical definition of cross entropy can simply be derived by plugging in into . this yields recall that the definition of entropy goes as plugging in this definition to yields the simplified definition of cross entropy: if kl divergence represents the average amount of additional information needed to represent an event with instead of , cross entropy tells us the average amount of total information needed to represent a stochastic event with instead of . this is why cross entropy is a sum of the entropy of the distribution plus the kl divergence between and . instead of dwelling in the theoretical realm regurgitating different definitions and interpretations of cross entropy and kl divergence, let's take a look at a realistic example to gain a better grasp of these concepts. say we have constructed a neural network to solve a task, such as mnist hand written digit classification. let's say we have fed our neural network an image corresponding to the number 2. in that case, the true distribution that we are trying to model, represented in vector form, will be as shown below. the statement is there to make sure that the probabilities sum up to 1. let's assume that our neural network made the following prediction about image. these two distributions, although similar, are different. but the question is, how different? creating a visualization might give us some idea about the difference between the two distributions. the two distributions are quite similar, meaning that our neural network did a good job of classifying given data. however, we can get a bit more scientific by calculating the cross entropy to see exactly how well our model performed with the given data. to achieve this, let's quickly write some functions to calculate kl divergence and cross entropy. we will be reusing the function we defined above. on a trivial note, we prevent python from running into math domain errors, we add to the provided distribution if the list contains 0. entropy: 2.975308333795324e 08 kl: 0.4150374749297422 cross entropy: 0.4150375046828255 the result is unsurprising. if we recall that the definition of entropy is the amount of information content needed to encode information, we will quickly realize that is a distribution with probability 1, which is why it makes sense that entropy converges to 0. therefore, in this case, kl divergence equals cross entropy, which computes to approximately 0.415. what actually happened beneath the hood? if we recall the definition of cross entropy, we can easily see that, among the 10 terms, 9 of them were eliminated since given the setup of the distribution . in the end, the only term that mattered was the third term, which was given by why does this quantity make sense as an error term defining a loss function? in other words, why is cross entropy used as a loss function in classification tasks? to see this, let's assume that our neural network was perfect in its prediction, or more realistically, trained to excessively overfit given data, i.e, equals . then, cross entropy would have been calculated as indeed, we can use our function to verify the coherency of this statement. entropy: 2.975308333795324e 08 kl: 0.0 cross entropy: 2.975308333795324e 08 in other words, when our predicted distribution equals the true distribution we are trying to model, cross entropy becomes 0, as a desirable cost function would behave. this is why crosss entropy is often used in classification models where we have a one hot encoded vector that represents a true distribution, as exemplified by , and a prediction that models this distribution, denoted as in our example. it is not difficult to see why cross entropy is a useful cost function frequently used in the field of machine learning. entropy is an interesting concept with which we can quantify randomness in data. this process is no rocket science, but simply a process that involves calculations with probabilities. although the link may not be immediately apparent, randomness is just another way of expressing probabilities and uncertainty, and it is from this premise that information and entropy take off. beyond that, however, entropy is now used extensively in the field of machine learning, specifically as a loss function. although it was not noted explicitly above, cross entropy calculates the same quantity as the logarithmic loss function. essentially, cross entropy is useful in that it provides us with some intuitive information of how far two distributions are apart. this distance is a metric with which we can evaluate the effectiveness of our model, which also means that the effectiveness of a model will be increased as cross entropy is increasingly minimized. machine learning is often referred to as a black box that we can simply use without much knowledge on how it works, but i personally find studying these underlying clockwork behind the hood to be much more interesting than blindly applying to to churn out numbers and predictions. i hope you enjoyed reading. merry christmas and happy holidays! deep learning: http://www.deeplearningbook.org bits: https://en.wikipedia.org/wiki/bit nats: https://en.wikipedia.org/wiki/nat_ cost function: https://en.wikipedia.org/wiki/loss_function bayesian inference: https://jaketae.github.io/study/bayes/ machine learning: a probabilistic perspective: https://www.cs.ubc.ca/~murphyk/mlbook/ this source: https://adventuresinmachinelearning.com/cross entropy kl divergence/ likelihood maximization: https://jaketae.github.io/study/likelihood/ cross entropy: https://en.wikipedia.org/wiki/cross_entropy kullback leibler divergence: https://en.wikipedia.org/wiki/kullback–leibler_divergence",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
2020-10-20-pytorch-modeling,"these past few weeks, i've been powering through pytorch notebooks and tutorials, mostly because i enjoyed the pytorch api so much and found so many of it useful and intuitive. well, the problem was that i ended up writing something like ten notebooks without ever publishing them on this blog. so really, i'm going over some old notebooks i've coded out more than a month ago to finally make it live. that's enough excuses, let's get into the basics of pytorch modeling in this notebook with the cifar10 dataset and some basic cnns. the setup is pretty simple here. we import some modules and functions from pytorch, as well as to be able to show some basic training plots. one thing i have noticed is that a lot of people do something like which i personally don't really get, because you can easily just do if you ask me, i think the latter is more elegant and less cluttered . i don't think the two import statements are functionally different, but if i do figure out any differences, i will make sure to update future notebooks. one of the many nice things about pytorch is the clean, intuitive api. pytorch comes with good gpu support, and one of the main ways through which this can be done is by creating a object. device because i am running this notebook on my macbook pro, which obviously does not come with nvidia cuda enabled graphics cards, the device is set as the cpu. now, i can ""move"" tensors and models up to the gpu by doing something like and these statements would allow inference and training to occur within the gpu. and below are some constants i will be using in this notebook. namely, we will run training for a total of 4 epochs, with a batch size of 32 and a learning rate of 0.001. now that we have all the things we need, let's jump into some data preparation and modeling. another thing i love about pytorch is the sheer ease with which you can preprocess data. pytorch makes it incredibly easy to combine and stack multiple transforms to create custom transformations to be applied to the dataset. the easiest way to go about this is to use the method, which looks like this: here, we are applying to transformations: the first changes the dataset and casts it into pytorch tensors,, and the second one normalizes the dataset to have a mean of 0.5 and a standard deviation of also 0.5 across all three channels of rgb. how can we apply this transform? well, we can pass it to initialize the datasets as shown below: files already downloaded and verified files already downloaded and verified because i already have the cifar10 downloaded in the directory of my local, pytorch does not download the dataset again. we could go with the dataset as is, but we can use the class to further batch and shuffle the dataset, which we normally want 99 percent of the time. this is as simple as calling and passing in the dataset we want to load. if we loop through the , for instance, we can see that it is giving us a nice batch of 32 photos. note that the dimensions are in the form of . as for the labels, we get 32 values where each number corresponds to an image. torch.size torch.size as the last step, let's just make sure that we know what each of these labels correspond to. the is a tuple of strings that translate label indices to actual strings we can interpret. for example, if we see the label , we know that it denotes , which is . modeling in pytorch is the probably the part i love the most. tensorflow's sequential api is a great way to start, and pytorch also provides the same sort of way of building sequential models. however, once you try to build anything that's more complicated than that, i think pytorch's class based way of approaching modeling makes a lot more intuitive sense and provides more room for experimentation and customization. before getting into too much detail, below is a very simple cnn we will refer to as an example throughout this post. the first thing you will realize is that the model itself is a class that inherits from . this is a pattern you will see all the time with pytorch models. is a super class from which we can inherit to build anything from full fledged models to custom blocks or layers to be used in some other larger model. in the initialization function, we also define a number of layers that will be used in forward propagation. you might be wondering why these have to initialized in the initialization function, as opposed to the forward function itself. while i don't have a complete, technically cogent explanation to that question, intuitively, we can understand a model's layers as being components of the model itself. after all, the weights of these layers are adjusted with each iteration or epoch. in that sense, we want the layers to be attached to the model instance itself; hence the oop design of pytorch's model class. in this particular instance, we define a number of convolutional layers, a pooling layer, and two fully connected layers used for classification output. the declaration of the layers themselves are not too different from other frameworks, such as tensorflow. also, i've written out all the named arguments so that it is immediately clear what each argument is configuring. once we've declared all the necessary components in the initialization function, the next steps to actually churn out forward propagation results given some input. in pytorch, this is done by defining the function. as you can see above, we basically call on the layers we've declared in the initialization function via and pass in any parameters we want. since this is a very simple cnn, you will see that there is nothing exciting going on; we are simply getting the output of the previous layer and passing that as input into the next. after going through some convolutions and fully connected layers, we can return the result. there are one caveats worth mentioning here, which is the use of . there is a that i could have easily used, and indeed there is an entire discussion on the pytorch forum on the difference between the two. the bottom line is that they are pretty similar for our purposes. the most salient difference between the two is that the functional approach cannot be used when declaring a sequential model. however, since we are defining a custom forward function, this limitation does not apply. personally, i prefer the functional because it means that there is one less layer to declare in the initialization function. however, it's probably better to err on the side of the if you're not totally sure. now that we've designed and implemented a model, it's time to train it. this is the part where people might argue that tensorflow 2 or keras is superior to pytorch. in keras, you can simply call to train the model. however, in pytorch, this is not necessarily the case, unless you really want to imitate the keras api and define a function yourself. pytorch is more low level in that you need to define a custom training loop manually. however, i actually prefer this low levelness because it requires me to really think through what is happening in each iteration, namely what the dimension of each batch is, what the model expects to receive as input in the forward computation, and what loss function is appropriate given the output and label. let's see what all of this means. first, we begin by actually initializing an instance of the model, a loss function named , and an , which is adam in this case. a quick note of caution: if you dig into the pytorch documentation or look at other example classifiers, you will realize, like me, there are two loss functions you can typically use: and , or negative log likelihood loss. the difference between the former and latter is that, while the former applies a softmax function to the output before calculating the actual loss, the latter does not. in our case, since we simply output the raw logits instead of applying a softmax calculation, we need to use the former. let's return where we were. before we jump into training and defining the training loop, it's always a good idea to see if the output of the model is what you'd expect. in this case, we can simply define some random dummy input and see if the output is correct. torch.size now that we've verified the input and output dimensions, we can move onto defining the training loop. defining the training loop may seem difficult at first, especially if you're coming from a keras background, but actually a lot of it is boiler plate, and things are not as difficult as they may seem. first, we define a list to hold the loss values per iteration. we will be using this list for visualization later. the exciting part comes next. for each epoch, we load the images in the . note that the loader returns a tuple of images and labels, which we can unpack directly within the loop itself. we then move the two objects to , which would be necessary if we were running this one a cuda enabled computer. then, we calculate the loss by calling , the loss function, and append the loss to the list. note that we have to call since itself is a one by one pytorch tensor. epoch 1/4, batch 500/1563, loss: 1.2798 epoch 1/4, batch 1000/1563, loss: 1.1682 epoch 1/4, batch 1500/1563, loss: 1.3911 epoch 2/4, batch 500/1563, loss: 1.0510 epoch 2/4, batch 1000/1563, loss: 0.9175 epoch 2/4, batch 1500/1563, loss: 1.1130 epoch 3/4, batch 500/1563, loss: 0.9330 epoch 3/4, batch 1000/1563, loss: 0.6095 epoch 3/4, batch 1500/1563, loss: 0.7042 epoch 4/4, batch 500/1563, loss: 0.7850 epoch 4/4, batch 1000/1563, loss: 0.5785 epoch 4/4, batch 1500/1563, loss: 0.9072 then comes the important part where we perform backprop. the idea is that we would clear the previous gradient values, if any calculate the gradient for the current iteration apply the gradient to adjust weights the three steps correspond to each of the lines in the code above, starting from . as you might be able to guess from the name of the function, we zero the gradients to make sure that we aren't accumulating gradient values from one iteration to the next. calling corresponds to calculating the new gradient values, and performs the backprop. the last block of code is simply a convenient print function i've written to see the progress of training at certain intervals. as you can see, the loss seems to be decreasing for the most part, although it is jumpy at times. indeed, plotting makes it clear that the loss has been decreasing, though not entirely in a steady, monotonous fashion. in retrospect, we could have probably added a batch normalization layer to stabilize and expedite training. however, since this post is largely meant as an introduction to pytorch modeling, not model optimization or design, the example suffices. now we have finally reached the last step of the development cycle: testing and evaluating the model. this last step will also require us to write a custom loop as we receive batches of data from the object we've created above. the good news, however, is that the testing loop is not going to look too much different from the training loop; the only difference will be that we will not be backpropagating per each iteration. we will also be using to make sure that the model is in the evaluation mode, not its default training mode. this ensures that things like batch normalization and dropout work correctly. let's talk briefly about the details of this loop. here, the metric we're collecting is accuracy. first, we generally see how many correct predictions the model generates. then, we also see per class accuracy; that is, whether our model is good at predicting any particular class. this ensures that the model's performance is balanced throughout all labels. accuracy: 69.39% accuracy of plane: 75.6% accuracy of car: 81.3% accuracy of bird: 41.8% accuracy of cat: 62.6% accuracy of deer: 56.599999999999994% accuracy of dog: 63.9% accuracy of frog: 74.1% accuracy of horse: 75.8% accuracy of ship: 89.4% accuracy of truck: 72.8% and here is the result! an overall accuracy of 70 percent is definitely not impressive, and we certainly could have done better by building a deeper model, or by using more complex architectures. however, this isn't the worst performance considering the fact that we only had three convolutional layers. the more important takeaway from this tutorial is how to prepare data, build models, and train and evaluate them through a custom loop. from the tone and style of my writing, it is perhaps immediately clear to you that i am not officially a pytorch fanboy. yes, i will admit that i loved keras for its simplicity, but after having spent more time learning python and dl, i now much prefer the freedom provided by pytorch's reasonably abstract api. i hope this notebook provided you with a nice, simple introduction to pytorch. in the coming notebooks, we will take a deeper dive into implementing models with pytorch, starting from rnns all the way up to classic sota vision models like inceptionnet, resnet, and seq2seq models. i can definitely tell you that these are coming, because, as funny as this sounds, i already have all the notetbooks and code ready; i just have to annotate them. i hope you've enjoyed reading this post. catch you up in the next one!",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2019-11-15-markov-chain,"in a previous post, we briefly explored the notion of markov chains and their application to google's pagerank algorithm. today, we will attempt to understand the markov process from a more mathematical standpoint by meshing it together the concept of eigenvectors. this post was inspired and in part adapted from this source. in linear algebra, an eigenvector of a linear transformation is roughly defined as follows: a nonzero vector that is mapped by a given linear transformation onto a vector that is the scalar multiple of itself this definition, while seemingly abstract and cryptic, distills down into a simple equation when written in matrix form: here, denotes the matrix representing a linear transformation; , the eignevector; , the scalar value that is multiplied onto the eigenvector. simply put, an eigenvector of a linear transformation is one that is allow me to use this term in the loosest sense to encompass positive, negative, and even imaginary scalar values ""stretched"" by some factor when the transformation is applied, i.e. multiplied by the matrix which maps the given linear transformation. the easiest example i like to employ to demonstrate this concept is the identity matrix . for the purpose of demonstration, let be an arbritrary vector and the three by three identity matrix. multiplying by produces the following result: the result is unsurprising, but it reveals an interesting way of understanding : identity matrices are a special case of diagonalizable matrices whose eigenvalues are 1. because the multiplying any arbitrary vector by the identity matrix returns the vector itself, all vectors in the dimensional space can be considered an eigenvector to the matrix , with = 1. a formal way to calculate eigenvectors and eigenvalues can be derived from the equation above. since is assumed as a nonzero vector, we can deduce that the matrix is a singular matrix with a nontrivial null space. in fact, the vectors in this null space are precisely the eigenvectors that we are looking for. here, it is useful to recall that the a way to determine the singularity of a matrix is by calculating its determinant. using these set of observations, we can modify the equation above to the following form: by calculating the determinant of , we can derive the characteristic polynomial, from which we can obtain the set of eigenvectors for representing some linear transformation . now that we have reviewed some underlying concepts, perhaps it is time to apply our knowledge to a concrete example. before we move on, i recommend that you check out this post i have written on the markov process, just so that you are comfortable with the material to be presented in this section. in this post, we turn our attention to the game of chutes and ladders, which is an example of a markov process which demonstrates the property of ""memorylessness."" this simply means that the progress of the game depends only on the players' current positions, not where they were or how they got there. a player might have ended up where they are by taking a ladder or by performing a series of regular dice rolls. in the end, however, all that matters is that the players eventually hit the hundredth cell. figure 1: chutes and ladders game to perform a markov chain analysis on the chutes and ladders game, it is first necessary to convert the information presented on the board as a stochastic matrix. how would we go about this process? let's assume that we start the game at the th cell by rolling a dice. there are six possible events, each with probability of . more specifically, we can end up at the index numbers 38, 2, 3, 14, 5, or 6. in other words, at position 0, where and denote the current and next position of the player on the game board, respectively. we can make the same deductions for other cases where . we are thus able to construct a 101 by 101 matrix representing the transition probabilities of our chutes and ladders system, where each column represents the system at a different state, i.e. the th entry of the th column vector represents the probabilities of moving from cell to cell . to make this more concrete, let's consider a program that constructs the stochastic matrix , without regards to the chutes and ladders for now. the indexing is key here: for each column, th rows were assigned the probability of . let's say that a player is in the th cell. assuming no chutes or ladders, a single roll of a dice will place him at one of the cells from to ; hence the indexing as presented above. however, this algorithm has to be modified for bigger or equal to 95. for example if , there are only three probabilities: , , and , each of values , , and respectively. the statements are additional corrective mechanisms to account for this irregularity. so now we're done with the stochastic matrix! ... or not quite. things get a bit more complicated once we throw the chutes and ladders into the mix. to achieve this, we first build a dictionary containing information on the jump from one cell to another. in this dictionary, the keys correspond to the original position; the values, the index of the cell after the jump, either through a chute or a ladder. for example, represents the first ladder on the game board, which moves the player from the first cell to the thirty eighth cell. to integrate this new piece of information into our code, we need to build a permutation matrix that essentially ""shuffles up"" the entries of the stochastic matrix in such a way that the probabilities can be assigned to the appropriate entries. for example, does not reflect the fact that getting a 1 on a roll of the dice will move the player up to the thirty eighth cell; it supposes that the player would stay on the first cell. the new permutation matrix would adjust for this error by reordering . for an informative read on the mechanics of permutation, refer to this explanation from wolfram alpha. let's perform a quick sanity check to verify that contains the right information on the first ladder, namely the entry in the dictionary. notice the in the th entry hidden among a haystack of 100 s! this result tells us that is indeed a permutation matrix whose multiplication with will produce the final stochastic vector that correctly enumerates the probabilities encoded into the chutes and ladders game board. here is our final product: we can visualize the stochastic matrix using the package. this produces a visualization of our stochastic matrix. figure 2: visualization of the stochastic matrix so there is our stochastic matrix! now that we have a concrete matrix to work with, let's start by identifying its eigenvectors. this step is key to understanding markov processes since the eigenvector of the stochastic matrix whose eigenvalue is 1 is the stationary distribution vector, which describes the markov chain in a state of equilibrium. for an intuitive explanation of this concept, refer to this previous post. let's begin by using the package to identify the eigenvalues and eigenvectors of the stochastic matrix. this code block produces the following output: the first entry of this array, which is the value , deserves our attention, as it is the eigenvalue which corresponds to the stationary distribution eigenvector. since the index of this value is , we can identify its eigenvector as follows: notice that this eigenvector is a representation of a situation in which the player is in the th cell of the game board! in other words, it is telling us that once the user reaches the th cell, they will stay on that cell even after more dice rolls hence the stationary distribution. on one hand, this information is impractical given that a player who reaches the end goal will not continue the game to go beyond the th cell. on the other hand, it is interesting to see that the eigenvector reveals information about the structure of the markov chain in this example. markov chains like these are referred to as absorbing markov chains because the stationary equilibrium always involves a non escapable state that ""absorbs"" all other states. one might visualize this system as having a loop on a network graph, where it is impossible to move onto a different state because of the circular nature of the edge on the node of the absorbing state. at this point, let's remind ourselves of the end goal. since we have successfully built a stochastic matrix, all we have to do is to set some initial starting vector and perform iterative matrix calculations. in recursive form, this statement can be expressed as follows: the math inclined thinkers in this room might consider the possibility of conducting an eigendecomposition on the stochastic matrix to simply the calculation of matrix powers. there is merit to considering this proposition, although later on we will see that this approach is inapplicable to the current case. eigendecomposition refers to a specific method of factorizing a matrix in terms of its eigenvalues and eigenvectors. let's begin the derivation: let be the matrix of interest, a matrix whose columns are eigenvectors of , and , a matrix whose diagonal entries are the corresponding eigenvalues of . let's consider the result of multiplying and . if we view multiplication as a repetition of matrix times vector operations, we yield the following result. but recall that are eigenvectors of , which necessarily implies that therefore, the result of can be rearranged and unpacked in terms of : in short, therefore, we have , which is the formula for eigendecomposition of a matrix. one of the beauties of eigendecomposition is that it allows us to compute matrix powers very easily. concretely, because and nicely cross out, all we have to compute boils down to ! this is certainly good news for us, since our end goal is to compute powers of the stochastic matrix to simulate the markov chain. however, an important assumption behind eigendecomposition is that it can only be performed on nonsingular matrices. although we won't go into the formal proofs here, having a full span of independent eigenvectors implies full rank, which is why we must check if the stochastic matrix is singular before jumping into eigendecomposition. unfortunately, the stochastic matrix is singular because , the number of columns or rows. this implies that our matrix is degenerate, and that the best alternative to eigendecomposition is the singular value decomposition. but for the sake of simplicity, let's resort to the brute force calculation method instead and jump straight into some statistical analysis. we first write a simple function that simulates the chutes and ladders game given a starting position vector . because a game starts at the th cell by default, the function includes a default argument on as shown below: calling this function will give us , which is a 101 by 1 vector whose th entry represents the probability of the player being on the th cell after a single turn. now, we can plot the probability distribution of the random variable , which represents the number of turns necessary for a player to end the game. this analysis can be performed by looking at the values of since the last entry of this vector encodes the probability of the player being at the th cell, i.e. successfully completing the game after rounds. this block produces the following figure: figure 3: game completion percentage after n turns i doubt that anyone would play chutes and ladders for this long, but after about 150 rolls of the dice, we can expect with a fair amount of certainty that the game will come to an end. the graph above presents information on cumulative fractions, but we can also look at the graph for marginal probabilities by examining its derivative: and the result: figure 3: fraction of games completed at n turns from the looks of it, the maximum of the graph seems to exist somewhere around . to be exact, . this result tells us that we will finish the game in 19 rolls of the dice more often than any other number of turns. we can also use this information to calculate the expected value of the game length. recall that or if the probability density function is continuous, in this case, we have a discrete random variable, so we adopt the first formula for our analysis. the formula can be achieved in python as follows: this result tells us that the typical length of a chutes and ladders game is approximately 36 turns. but an issue with using expected value as a metric of analysis is that long games with infinitesimal probabilities are weighted equally to short games of substantial probability of occurrence. this mistreatment can be corrected for by other ways of understanding the distribution, such as median: this function tries to find the point in the cumulative distribution where the value is closest to , i.e. the median of the distribution. the result tells us that about fifty percent of the games end after 29 turns. notice that this number is smaller than because it discredits more of the long games with small probabilities. the markov chain represents an in interesting way to analyze systems that are memoryless, such as the one in today's post, the chutes and ladders game. although it is a simple game, it is fascinating to see just how much information and data can be derived from a simple image of the game board. in a future post, we present another way to approach similar systems, known as monte carlo simulations. but that's for another time. peace! this source: https://jakevdp.github.io/blog/2017/12/18/simulating chutes and ladders/ previous post: https://jaketae.github.io/blog/math/pagerank and markov/ this post: https://jaketae.github.io/blog/math/pagerank and markov/ defined: https://en.wikipedia.org/wiki/eigenvalues_and_eigenvectors stochastic matrix: http://mathworld.wolfram.com/stochasticmatrix.html explanation from wolfram alpha: http://mathworld.wolfram.com/permutationmatrix.html memorylessness: https://en.wikipedia.org/wiki/markov_property characteristic polynomial: http://mathworld.wolfram.com/characteristicpolynomial.html eigendecomposition: https://en.wikipedia.org/wiki/eigendecomposition_of_a_matrix singular value decomposition: https://en.wikipedia.org/wiki/singular_value_decomposition absorbing markov chains: https://en.wikipedia.org/wiki/absorbing_markov_chain",0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2020-08-28-bayes-multi-bandit,"recently, i fortuitously came across an interesting blog post on the multi armed bandit problem, or mab for short. i say fortuitous because the contents of this blog nicely coincided with a post i had meant to write for a very long time: revisiting the beta distribution, conjugate priors, and all that good stuff. i decided that the mab would be a refreshing way to discuss this topic. ""bayesian"" is a buzz word that statisticians and ml people love anyway, me shamelessly included. in this post, we will start off with a brief introduction into what the mab problem is, why it is relevant, and how we can use some basic bayesian analysis with beta and bernoulli distributions to derive a nice sampling algorithm, known as thompson sampling. let's dive right into it. the multi armed bandit problem is a classical gambling setup in which a gambler has the choice of pulling the lever of any one of slot machines, or bandits. the probability of winning for each slot machine is fixed, but of course the gambler has no idea what these probabilities are. to ascertain out the values of these parameters, the gambler must learn through some trial and error. given this setup, what is the optimal way of going about the problem? one obvious way is to start randomly pulling on some slot machines until they get a rough idea of what the success probabilities are. however, this is obviously not the most systematic approach, and there is not even a clear guideline as to how they should go about pulling these levers. here is where the bayesian approach comes in handy. it isn't difficult to frame this as a bayesian problem. given a pull of the slot machine, the result of which we will denote as , a bernoulli random variable, we can then formulate the problem as follows: and as the classical bayesian analysis goes, the more data we collect through repeated experiments, the closer the posterior distribution will get to the target distribution. through this process, we are able to approximate the parameter . now we have to think about what distributions we want to use for the prior and likelihood. let's start with the easier one: the likelihood. the natural choice that makes sense is the binomial distribution. each pull on the lever can be considered a bernoulli trial, and as we start exploring with more pulls to accumulate data, we will essentially be sampling from a binomial distribution, with success and failures out of a total of trials. therefore, more concretely, we can say the more interesting part is the prior distribution. intuitively, the one might be inclined to say that we need a uniform prior, and indeed that answer would technically be true. however, i qualify with ""technically"" since the real answer has an added layer of complexity. having a uniform prior only makes sense on the very first try, when there is no historical data to estimate the parameter from. however, once we start pulling the lever and seeing the results of each consecutive pull, we should be able to build some prior expectation as to what the true value of the parameter is. this suggests that the uniform prior approach is missing some important key pieces. to really drive this point home, let's discuss more about the notion of conjugate priors, specifically in the context of the beta and binomial distributions. the notion of conjugate priors is something that recurs extremely often in bayesian analysis. this is mainly because conjugate priors offer a nice closed form solution to posterior distribution computation. simply put, a conjugate prior to some likelihood function is a type of prior that, when multiplied with a given likelihood, produces a posterior distribution that is of the same form as itself. for example, a beta prior, when multiplied with a binomial likelihood, produces a beta posterior. this is what we want in bayesian analysis, since we can now assume that posterior to be our new prior in the next iteration of experiments. in this section, we will build on top of this high level understanding of conjugacy to show that a beta distribution is indeed a conjugate prior to the binomial likelihood function. on that end, let's start with a quick definition review of the beta distribution and the beta function. the beta distribution looks as follows: where comparing and the integral representation of the beta function in , you will see that there is a clear resemblance, and this is no coincidence. in the beta distribution, the beta function which appears in the denominator is simply a normalizing constant that ensures that integrating the probability distribution function from 0 to 1 produces 1. the specifics of the beta function and how it relates to the gamma function might be a topic for a another post. for now, it suffices to show the general probability function as well as its building blocks, namely the beta function itself. these will become relevant later when we derive the posterior with a binomial likelihood function to show conjugacy. to prove conjugacy, it suffices to show that multiplying a beta prior with a binomial likelihood produces a beta posterior. roughly speaking, the big picture we will be using is essentially equation . for notation consistency, i decided to use for conditional probability and for function parametrization. for a rather pedantic discussion on this notational convention, i recommend that you take a quick look at this cross validated post. let's now plug in the beta and binomial distributions into to see what comes out of it. this is going to be a lot of computation, but the nice part of it is that a lot of terms cancel out each other. we can easily see that there are constant that exist both in the numerator and the denominator. we can pull these constants out of the integral to simplify the expression. these constants include the binomial and the reciprocal beta normalizing constants. one useful observation to make here is the fact that the numerator itself looks a lot like something we have seen before: the beta distribution. in fact, you might also realize that the denominator is nothing but just a normalizing constant that ensures that our posterior distribution, when integrated from 0 to 1, integrates up to 1 as the axiom of probability states. we can also see the denominator as the definition of the beta function. in other words, therefore, we end up with now we know that there is a closed form solution for bayesian problems involving conjugate priors and likelihood functions, such as a beta prior coupled with the binomial likelihood. but we want to be able to interpret the posterior distribution. we might start from rather simple metrics like the mean to get a better idea of what the posterior tells us. note that we can always generalize such quantities into moments. given , the expected value of the beta random variable can be expressed as proving this is pretty straightforward if we simply use the law of the unconscious statistician. using the definition of expectation, we can derive the following: here, we use the gamma representation of the beta function. this conclusion gives an even nicer, more intuitive interpretation of the bayesian update we saw earlier with the beta prior and binomial likelihood. namely, given the new posterior we know that sampling from that new posterior will give us a mean value that somewhat resembles the crude approach we would have taken without the prior expectation. here, the crude approach is referring to the raw frequentist estimate we would have made had we not taken the bayesian approach to the problem. it is obvious that the two hyperparameters of the prior are acting as an initial weight of sorts, making sure that when little data is available, the prior overshadows observations, but when ample amount of data is collected and available, eventually yields to those observations to estimate the parameter. now we can turn our attention back to the multi armed bandit problem. now we can build on top of our knowledge of the beta binomial update and refine what the frequentist greedy approach. we will also write out some simple functions to simulate the bandit problem and thus demonstrate the effectiveness of the bayesian approach. note that a lot of the code in this post was largely inspired by peter's blog. before we get into any modeling, let's first import the modules we'll need and set things up. instead of using the approach outlined in the blog i've linked to, i decided to use objects to model bandits. the rationale is that this approach seems to make a little bit more intuitive sense to me. also, working with django these days has made me feel more affinity with python classes. at any rate, let's go ahead. now, we can initialize a bandit with some predetermined parameter, . of course, our goal would be to determine the true value of this parameter through sampling and bayesian magic. in this case, we have created a fair bandit with a of 0.5. 0.5 we can also simulate level pulls by repeatedly calling on . note that this will accumulate the result of each bernoulli trial in the list object. 0, 1, 1, 0, 0, 1, 0, 0, 1, 1 notice that we also have . this is an a list object that c now that we have performed all the basic sanity checks we need, let's quickly go ahead and create three bandit objects for demonstration purposes. now we get into the details of how to perform the bayesian update. more specifically, we're interested in how we are going to use posterior probabilities to make decisions on which slot machine to pull on. this is where thompson sampling comes in. in the simple, greedy frequentist approach, we would determine which bandit to pull on given our historical rate of success. if the first slot machine approximately yielded success 60 percent of the time, whereas the second one gave us 40, we would choose the first. of course, this approach is limited by the fact that, perhaps we only pulled on the second machine 5 times and got only 2 success out of them, whereas we pulled on the first bandit a hundred times and got 60 successes. maybe it turns out that the second bandit actually has a higher success rate, and that we were simply unlucky those five turns. thompson sampling remedies this problem by suggesting a different approach: now that we have bayesian posteriors, we can now directly sample from those posteriors to get an approximation of the parameter values. since we are sampling from a distribution instead of relying on a point estimate as we do for the greedy approach, this allows for both exploration and exploitation to happen at reasonable frequencies. if a posterior distribution has large variance, this means that we will explore that particular bandit slightly more than others. if a posterior has a large mean a high success parameter then we will exploit that machine a bit more to earn more profit, or, in this context, to minimize regret. before we move on any farther, perhaps' it's worth discussing what the term ""regret"" means in this context. simply put, regret refers to the amount that we have comparatively lost by making a sub optimal choice from the get go. here is a visual diagram i came across on analytics vidhya. the maximum reward would obviously be achieved if we pull on the slot machine with the highest success parameter from trial 1. however, this does not happen since the gambler dives into the game without this prior knowledge. hence, they have to learn what the optimal choice is through exploration and exploitation. it is of course in this learning process that the greedy algorithm or bayesian analysis with thompson sampling comes into play. the amount that we have theoretically lost or, in other words, the extent to which we are far away from the maximum amount we could have earned is denoted as regret. thus, to maximize reward is to minimize regret, and vice versa. now let's simulate a hundred pulls on the lever using bayesian analysis using the beta binomial model and thompson sampling. nothing much fancy here, all we're doing is thompson sampling from the beta distribution via , then obtaining the index of the bandit with the highest parameter, then pulling the machine that corresponds to that index. we will also keep cumulative track of our results to reproduce the regret diagram shown above. and we're done with the hundred round of simulations! hopefully our simulator gambler has made some good choices by following bayesian update principles, with the beta binomial model and thompson sampling under their belt. let's take a look at the posterior distribution for each bandit. we can easily plot them using , as shown below. notice that i have also included the uniform prior for reference purposes. the result is as we would expect: the bandit with the highest success parameter of 0.7 seems to have been pulled on the most, which explains why its variance is the smallest out of the bunch. moreover, the mean of that particular posterior is also close to 0.7, its true value. notice that the rest of the posteriors also somewhat have this trend, although more uncertainty is reflected into the shape of the distributions via the spread. it is interesting to see how we can go from a uniform prior, or , to almost normal shaped distributions as we see above. to corroborate our intuition that the best bandit was indeed the most pulled, let's quickly see the proportion of the pulls through a pie chart. as you can see, there seems to be a positive correlation between the success parameter and the number of pulls. this is certainly good, since we want the gambler to pull on the best bandit the majority of times while avoiding the worse ones as much as possible. this certainly seems to be the case, given that the bandit with the worse parameter 0.1 was pulled the least. last but not least, let's revisit the cumulative regret graph introduced earlier. we can draw our own cumulative regret graph by first simulating what would have been the optimal result in other words, we need to obtain the amount of reward the gambler would have earned had they simply pulled on the best bandit the entire time. let's quickly simulate that first. and it turns out that the maximum amount they would have earned, in this particular instance, is 74. i say in this particular instance, since the expected value of the maximum reward is simply 70, given that the highest success parameter is 0.7. 74 this minor detail notwithstanding, we can now use the quantity and the list in order to recreate our own cumulative regret graph, as shown below. it is obvious that the cumulative regret is highest when we start since the current reward is at 0. however, after some trial and error, the gambler starts to figure out which bandit is the best and starts pulling more of those, ultimately ending up at the point that is quite close to the maximum reward, though not quite due to the earlier opportunities that may have been lost due to exploration and sampling. in this post, we took a look at the multi armed bandit problem and how it relates to bayesian analysis with the beta and binomial distributions. i personally enjoyed writing this post, not only because i hadn't written in a long time, but also because it helped me revisit some statistics, which is something that i desperately needed to do i'm spending way too much time dealing with django and selenium these days. time and again, i realize that there is an element of intuitiveness to bayesian statistics that, though not obvious at first, starts to make more sense as i explore more into that realm. of course, frequentist statistics offers a quick and easy way of interpreting certain metrics, but the notion of expectation is something that i had thought was complicated and unnecessary at first, but have gradually come to understand, embrace, and even support. perhaps writing this post has reaffirmed my bayesian identity as a budding statistician. i hope you've enjoyed reading this post. catch you up in the next one!",1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
2020-09-21-bleu,"recently, i joined the language, information, and learning at yale lab, led by professor dragomir radev. although i'm still in what i would consider to be the incipient stages of ml/dl/nlp studies meaning it will take time for me to be able to actively participate in an contribute to research and publications i think it will be a great learning experience from which i can glean valuable insight into what research at yale looks like. one of the first projects i was introduced to at the lab is domain independent table summarization. as the name implies, the goal is to train a model such that it can extract some meaningful insight from the table and produce a human readable summary. members are the lab seem to be making great progress in this project, and i'm excited to see where it will go. in the meantime, i decided to write a short post on bleu, a metric that i came across while reading some of the survey papers related to this topic. let's dive into it. before going into code and equations, a high level overview of what bleu is might be helpful here. bleu, which stands for bilingual evaluation understudy, is an metric that was introduced to quantitatively evaluate the quality of machine translations. the motivation is clear: as humans, we are able to get an intuitive sense of whether or not a given translation is accurate and of high quality; however, it is difficult to translate this arbitrary linguistic intuition to train nlp models to produce better translations. this is where bleu comes to the rescue. the way bleu works is simple. given some candidate translation of a sentence and a group of reference sentences, we use a bag of word approach to see how many occurences of bows co occur in both the translation and reference sentences. bow is a simple yet highly effective way of ensuring that the machine translation contains key phrases or words that reference translations also contain. in other words, bleu compares candidate translations with human produced, annotated reference translations and compares how many hits there are in the candidate sentence. the more bow hits there are, the better the translation. of course, there are many more details that go beyond this. for instance, bleu is able to account for situations in which meaningless words are repeated throughout the machine translation to simply increase bow hits. it can also penalize translations that are too short. by combining this bow precision based approach with some penalization terms, bleu provides a robust means of evaluating machine translations. with this high level overview in mind, let's start implementing bleu from scratch. first, let's begin by defining some simple preprocessing and helper functions that we will be using throughout this tutorial. the first on the list is , which converts a given sentence into lowercase and splits it into tokens, which are, in this case, english words. we could make this more robust using regular expressions to remove punctuations, but for the purposes of this demonstration, let's make this simpler. i decided to use anonymous functions for the sake of simplicity and code readability. next, let's write a function that creates n grams from a given sentence. this involves tokenizing the given sentence using , then looping through the tokens to create a bag of words. and here is a quick sanity check of what we've done so far. 'this is', 'is an', 'an example', 'example sentence' the bleu score is based on a familar concept in machine learning: precision. formally, precision is defined as where and stand for true and false positives, respectively. in the context of machine translations, we can consider positives as roughly corresponding to the notion of hits or matches. in other words, the positives are the bag of word n grams we can construct from a given candidate translation. true positives are n grams that appear in both the candidate and some reference translation; false positives are those that only appear in the candidate translation. let's use this intuition to build a simple precision based metric. first, we need to create some n grams from the candidate translation. then, we iterate through the n grams to see if they exist in any of the n grams generated from reference translations. we count the total number of such hits, or true positives, and divide that quantity by the total number of n grams produced from the candidate translation. below are some candidate sentences and reference translations that we will be using as an example throughout this tutorial. comparing with , it is pretty clear that the former is the better translation. let's see if the simple precision metric is able to capture this intuition. 0.9444444444444444 0.5714285714285714 and indeed that seems to be the case! however, the simple precision based metric has some huge problems. as an extreme example, consider the following candidate translation. 1.0 obviously, is a horrible translation, but the simple precision metric fails to flag it. this is because precision simply involves checking whether a hit occurs or not: it does not check for repeated bag of words. hence, the original authors of bleu introduces modified precision as a solution, which uses clipped counts. the gist of it is that, if some n gram is repeated many times, we clip its count through the following formula: here, refers to the number of hits we assign to a certain n gram. we sum this value over all distinct n grams in the candidate sentence. note that the distinction requirement effectively weeds out repetitive translations such as we looked at earlier. refers to the number of occurrences of a n gram in the candidate sentence. for example, in , the unigram appears 13 times, and so . this value, however, is clipped by , which is the maximum number of occurrence of that n gram in any one of the reference sentences. in other words, for each reference, we count the number of occurrence of that n gram and take the maximum value among them. this can seem very confusing, but hopefully it's clearer once you read the code. here is my implementation using . notice that we use a in order to remove redundancies. corresponds to ; corresponds to . using this modified metric, we can see that the is now penalized quite a lot through the clipping mechanism. 0.07692307692307693 but there are still problems that modified precision doesn't take into account. consider the following example translation. 0.9444444444444444 to us, it's pretty obvious that is a bad translation. although some of the key words might be there, the order in which they are arranged violates english syntax. this is the limitation of using unigrams for precision analysis. to make sure that sentences are coherent and read fluently, we now have to introduce the notion of n grams, where is larger than 1. this way, we can preserve some of the sequential encoding in reference sentences and make better comparison. the fact that unigrams are a poor way of evaluating translations becomes immediately clear once we plot the in n grams against modified precision. as you can see, precision score decreases as gets higher. this makes sense: a larger simply means that the window of comparison is larger. unless whole phrases co occur in the translation and reference sentences which is highly unlikely precision will be low. people have generally found that a suitable value lies somewhere around 1 and 4. as we will see later, packages like use what is known as cumulative 4 gram bleu score, or bleu 4. the good news is that our current implementation is already able to account for different values. this is because we wrote a handy little function, . by passing in different values to , we can deal with different n grams. now we're almost done. the last example to consider is the following translation: this is obviously a bad translation. however, due to the way modified precision is currently being calculated, this sentence will likely earn a high score. to prevent this from happening, we need to apply what is known as brevity penalty. as the name implies, this penalizes short candidate translations, thus ensuring that only sufficiently long machine translations are ascribed a high score. although this might seem confusing, the underlying mechanism is quite simple. the goal is to find the length of the reference sentence whose length is closest to that of the candidate translation in question. if the length of that reference sentence is larger than the candidate sentence, we apply some penalty; if the candidate sentence is longer, than we do not apply any penalization. the specific formula for penalization looks as follows: the brevity penalty term is multiplied to the n gram modified precision. therefore, a value of 1 means that no penalization is applied. let's perform a quick sanity check to see whether the brevity penalty function works as expected. 0.11080315836233387 1 finally, it's time to put all the pieces together. the formula for bleu can be written as follows: first, some notation clarifications. specifies the size of the bag of word, or the n gram. denotes the weight we will ascribe to the modified precision produced under that gram configuration. in other words, we calculate the weighted average of log precision, exponentiate that sum, and apply some brevity penalty. although this can sound like a lot, really it's just putting all the pieces we have discussed so far together. let's take a look at the code implementation. the weighting happens in the part within the generator expression within the statement. in this case, we apply weighting across that goes from to . now we're done! let's test out our final implementation with for from 1 to 4, all weighted equally. 0.5045666840058485 the package offers functions for bleu calculation by default. for convenience purposes, let's create a wrapper functions. this wrapping isn't really necessary, but it abstracts out many of the preprocessing steps, such as applying . this is because the bleu calculation function expects tokenized input, whereas and are untokenized sentences. and we see that the result matches that derived from our own implementation! 0.5045666840058485 in this post, we took a look at bleu, a very common way of evaluating the fluency of machine translations. studying the implementation of this metric was a meaningful and interesting process, not only because bleu itself is widely used, but also because the motivation and intuition behind its construction was easily understandable and came very naturally to me. each component of bleu addresses some problem with simpler metrics, such as precision or modified precision. it also takes into account things like abnormally short or repetitive translations. one area of interest for me these days is seq2seq models. although rnn models have largely given way to transformers, i still think it's a very interesting architecture worth diving into. i've also recently ran into a combined lstm cnn approach for processing series data. i might write about these topics in a future post. i hope you've enjoyed reading this post. catch you up later!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,0
2020-01-20-bayesian-regression,"in today's post, we will take a look at bayesian linear regression. both bayes and linear regression should be familiar names, as we have dealt with these two topics on this blog before. the bayesian linear regression method is a type of linear regression approach that borrows heavily from bayesian principles. the biggest difference between what we might call the vanilla linear regression method and the bayesian approach is that the latter provides a probability distribution instead of a point estimate. in other words, it allows us to reflect uncertainty in our estimate, which is an additional dimension of information that can be useful in many situations. by now, hopefully you are fully convinced that bayesian linear regression is worthy of our intellectual exploration. let's take a deep dive into bayesian linear regression, then see how it works out in code using the library. in this section, we will derive the formula for bayesian linear regression step by step. if you are feeling rusty on linear algebra or bayesian analysis, i recommend that you go take a quick review of these concepts before proceeding. note that i borrowed heavily from this video for reference. for any regression problem, we first need a data set. let denote this pre provided data set, containing entries where each entry contains an dimensional vector and a corresponding scalar. concretely, where the goal of bayesian linear regression is to find the predictive posterior distribution for . this is where the difference between bayesian linear regression and the normal equation method becomes most apparent. whereas vanilla linear regression only gives us a single point estimate given an input vector, bayesian linear regression gives an entire distribution. for the purposes of our demonstration, we will define the predictive posterior to take the following form as shown below, with precision pre given. precision is simply the reciprocal of variance and is commonly used as an alternative way of parametrizing gaussian distributions. in other words, we assume the model our goal will be to derive a posterior for this distribution by performing bayesian inference on , which corresponds to the slope of the linear regression equation, where denotes noise and randomness in the data, thus affecting our final prediction. to begin bayesian inference on parameter , we need to specify a prior. our uninformed prior will look as follows. where denotes precision, the inverse of variance. note that we have a diagonal covariance matrix in place of variance, the distribution for will be a multivariate gaussian. the next ingredient we need for our recipe is the likelihood function. recall that likelihood can intuitively be understood as an estimation of how likely it is to observe the given data points provided some parameter for the true distribution of these samples. the likelihood can easily be computed by referencing back to equation above. note that the dot product of with itself yields the sum of the exponents, which is precisely the quantity we need when computing the likelihood. where is a design matrix given by and is a column vector given by before calculating the posterior, let's recall what the big picture of bayesian inference looks like. where denotes the parameter of interest for inference. in plain terms, the proposition above can be written as in other words, the posterior distribution can be obtained by calculating the product of the prior distribution and the likelihood function. in many real world cases, this process can be intractable, but because we are dealing with two gaussian distributions, the property of conjugacy ensures that this problem is not only tractable, but also that the resulting posterior would also be gaussian. although this may not be immediately apparent, observe that the exponent is a quadratic that follows the form after making appropriate substitutions therefore, we know that the posterior for is indeed gaussian, parameterized as follows: let's try to obtain the map estimate of of , i.e. simplify notice the similarity with the mle estimate, which is the solution to normal equation, which i otherwise referred to as vanilla linear regression: this is no coincidence: in a previous post on map and mle, we observed that the map and mle become identical when we have a uniform prior. in other words, the only cause behind the divergence between map and mle is the existence of a prior distribution. we can thus consider the additional term in absent in as a vestige of the prior we defined for . map versus mle is a recurring theme that appears throughout the paradigmatic shift from frequentist to bayesian, so it merits discussion. now that we have a posterior distribution for which we can work with, it's time to derive the predictive distribution. we go about this by marginalizing using the property of conditional probability, as illustrated below. this may seem like a lot, but most of it was simple calculation and distributing vectors over parentheses. it's time to use the power of conjugacy again to extract a normal distribution out of the equation soup. let's complete the square of the exponent according to the gaussian form after making the appropriate substitutions again, observing this is not a straightforward process, especially if we had no idea what the final distribution is going to look like. however, given that the resulting predictive posterior will take a gaussian form, we can backtrack using this knowledge to obtain the appropriate substitution parameters in . continuing, where the last equality stands because we can pull out terms unrelated to by considering them as constants. why do we bother to pull out the exponent? this is because the integral of a probability density function evaluates to 1, leaving us only with the exponential term outside the integral. to proceed further from here, let's take some time to zoom in on for a second. substituting , we get we can now plug this term back into as shown below. although it may seem as if we made zero progress by unpacking , this process is in fact necessary to complete the square of the exponent according to the gaussian form after making the substitutions by now, you should be comfortable with this operation of backtracking a quadratic and rearranging it to complete the square, as it is a standard operation we have used in multiple parts of this process. finally, we have derived the predictive distribution in closed form: with more simplification using the , it can be shown that and there's the grand formula for bayesian linear regression! this result tells us that, if we were to simply get the best point estimate of the predicted value , we would simply have to calculate , which is the tranpose product of the map estimate of the weights and the input vector! in other words, the answer that bayesian linear regression gives us is not so much different from vanilla linear regression, if we were to reduce the returned predictive probability distribution into a single point. but of course, doing so would defeat the purpose of performing bayesian inference, so consider this merely an intriguing food for thought. as promised, we will attempt to visualize bayesian linear regression using the library. doing so will not only be instructive from a perspective of honing probabilistic programming skills, but also help us better understand and visualize bayesian inference invovled in linear regression as explored in the context of this article. note that, being a novice in , i borrowed heavily from this resource available on the official documentation. first, let's begin by importing all necessary modules. let's randomly generate two hundred data points to serve as our toy data set for linear regression. below is a simple visualization of the generated data points alongside the true line which we will seek to approximate through regression. now is the time to use the library. in reality, all of the complicated math we combed through reduces to an extremely simple, single line command shown below. under the hood, the using variations of random sampling to produce an approximate estimate for the predictive distribution. auto assigning nuts sampler... initializing nuts using jitter+adapt_diag... multiprocess sampling nuts: sd, x, intercept sampling 2 chains: 100%|██████████| 7000/7000 00:04 we see two lines for each plot because the sampler ran over two chains by default. what do those sampled values mean for us in the context of linear regression? well, let's plot some sampled lines using the function conveniently made available through the library. we see that the gray lines, sampled by , all seem to be a good estimate of the true regression line, colored in gold. we might also notice that the sampled regression lines seem to stay below the true regression line for smaller values of . this is because we have more samples beneath the true regression line that we have above it. bayesian linear regression is able to account for such variations in data and uncertainty, which is a huge advantage over the simple mle linear regression method. the true power of bayesian linear regression might be summarized as follows: instead of returning just a single line using the mle weight estimate of data, bayesian linear regression models the entire data set to create a distribution of linear functions so to speak, allowing us to sample from that distribution to obtain sample linear regression lines. this is an approach that makes much more sense, since it allows us to take into account the uncertainty in our linear regression estimate. the reason why the normal equation method is unable to capture this uncertainty is that as you might recall from the derivation of the formula for vanilla linear regression the tools we used did not involve any probabilistic modeling. recall that we used only linear algebra and matrix calculus to derive the model for vanilla linear regression. bayesian linear regression is more complicated in that it involves computations with probability density functions, but the end result is of course more rewarding. that's it for today! i hope you enjoyed reading this post. catch you up in the next one. bayes: https://jaketae.github.io/study/bayes/ linear regression: https://jaketae.github.io/study/linear regression/ video: https://www.youtube.com/watch?v=dtkgq9tdyci&list=pld0f06aa0d2e8ffba&index=59 conjugacy: https://en.wikipedia.org/wiki/conjugate_prior map and mle: https://jaketae.github.io/study/map mle/ resource: https://docs.pymc.io/notebooks/glm linear.html multivariate gaussian: https://jaketae.github.io/study/gaussian distribution/",0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
2020-01-02-MCMC,"finally, here is the post that was promised ages ago: an introduction to monte carolo markov chains, or mcmc for short. it took a while for me to understand how mcmc models work, not to mention the task of representing and visualizing it via code. to add a bit more to the excuse, i did dabble in some other topics recently, such as machine learning models or information theory, which is also partially why this post was delayed quite a bit. nevertheless, it's finally here and ready to go. in this post, we will take a look at the metropolis hastings algorithm, the simplest variant among the family of mcmc models. let's see what the bayesian hype is all about. it's been a while since we last discussed bayesian inference, so it's probably a good idea to start with a brief recap. bayesian statistics commences from bayes' theorem, a seminal statement in probability theory that can be expressed as follows alternatively, bayes' theorem can be stated more generally in the context of some partitioned sample space, for simplicity, i omit the equation for the case involving continuous random variables. simply put, the summation experssion in the denominator would simply be replaced with that involving integration. the power of the proposition underlying bayes's theorem really comes into light when we consider it in the context of bayesian analysis. the objective of bayesian statistical analysis is to update our beliefs about some probability, known as the posterior, given a preestablished belief, called the prior, and a series of data observations, which might be decomposed into likelihood and evidence. concretely, this statement is equivalent to where denotes the likelihood function. in plain language, bayesian statistics operates on the assumption that all probabilities are reflections of subjective beliefs about the distribution of some random variable. a prior expectation or belief we might have about this distribution is referred to as the prior. then, we can update our prior belief based on sample observations, resulting in a posterior distribution. roughly speaking, the posterior can be considered as the ""average"" between the prior and the observed data. this process, which we went over in detail in this post, is at the heart of bayesian inference, a powerful tool through which data and distributions can be understood. theoretically, everything should work fine: given some prior and some sample observation data, we should be able to derive a posterior distribution for the random variable of interest. no big deal. or so it seems. if we take a look again at equation , we will realize that there is an evidence term that we have to calculate sitting in the denominator. the formula for evidence can be expressed as follows: computing this quantity is not as easy as it may appear. indeed, this is one of the reasons why the bayesian way of thinking was eschewed for so long by statisticians: prior to the advent of calculators and computers, mathematicians had trouble deriving the closed form expression for the evidence term with just pen and paper. we might consider options other than direct calculation, such as monte carlo approximation or deriving a proportionality experssion by assuming evidence to be a constant. indeed, the latter is the approach we took in this post on bayesian inference. using a beta prior and binomial likelihood, we used the property of conjugacy to derive a formula for the posterior. however, this raises yet another question: what if the prior and likelihood do not have a conjugate relationship? what if we have a very messy prior or complicated likelihood function, so convoluted to the point that calculating the posterior is near impossible? simple monte carlo approximation might not do because of a problem called the curse of dimensionality: the volume of the sample space increases exponentially with the number of dimensions. in high dimensions, the brute force monte carlo approach may not be the most appropriate. markov chain monte carlo seeks to solve this conundrum of posterior derivation in high dimensions sample space. and indeed, it does a pretty good job of solving it. how does markov chain monte carlo get around the problem outlined above? to see this, we need to understand the two components that comprise markov chain monte carlo: markov chains and monte carlo methods. we covered the topic of markov chains on two posts, one on pagerank and the other on the game of chutes and ladders. nonetheless, some recap would be of help. wikepedia defines markov chains as follows: a markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. in other words, a markov chain is a method of generating a sequence of random variables where the current value of that random variable probabilistically dpends on its prior value. by recursion, this means that the next value of that random variable only depends on its current state. to put this into context, we used markovian analysis to assess the probability that a user on the internet would move to one site to another in the context of analyzing google's pagerank algorithm. markov chains also popped up when we dealt with chutes and ladders, since the next position of the player in game only depends on their current position on the game board. these examples all demonstrate the markov property, also known as memorylessness. later on, we will see how markov chains come in handy when we decide to ""jump"" from one number to the next when sampling from the posterior distribution to derive an approximation of the parameter of interest. we also explored monte carlo in some detail here on this blog. for those of you who haven't already, i highly recommend reading the post, as we developed a good intuition of when monte carlo simulations can come in handy to deal with tasks of varying difficulty. to cut to the chase, monte carlo methods are used to solve intractable problems, or problems that require expensive computing. instead of systematically deriving a closed forrm solution, we can alternatively opt for a scheme of random sampling and hope that, with a sufficient sample size, we would eventually be able to derive an approximation of the parameter. although this seems stupid at first, it is an incredibly powerful approach to solving many problems, including the one presented here involving posterior calculation in bayesian inference. the metropolis hastings algorithm is one of the first markov chain monte carlo model that was developed in the late 20th century to simulate particle movement. more advanced mcmc models have been introduced since; however, the metrapolis hastings algorithm still deserves our attention as it demonstrates the basis of how many markov chain monte carlo models operate. let's get into the details of the model. at the heart of metrapolis hastings is the proposal distribution, which we use to simulate the markov chain random walk part of the model. setting the parameters for this proposal distribution can be done arbitrarily, i.e. we can set it to be any random numbers. theoretically, regardless of the parameters of the proposal distribution, the mcmc model would give us the same result after infinite iterations of sampling. in the metrapolis hastings model, the proposal distribution is assumed as normal. next, we have to decide if the current value of is a value to accept or not. accepting a randomly sampled value means adding it to our list of historic observations if we draw a histogram of the entries in this list, it is our hope that we would end up with a close approximation of the posterior distribution. accepting this value is often referred to as a ""jump,"" because we can visualize this process as a random walk in the posterior sample space from point to . the question is, how do we decide to jump or not? the answer lies in bayes' theorem: if, for example, we should accept the value and perform the jump, because this means that the new proposed parameter does a better job of explaining the data than does the current one. but recall the dilemma we discussed earlier: how do we compute the posterior? after all, the complexity of calcualting evidence was the very reason why scientists came up with mcmc in the first place. well, here's a clever trick that might be of use: rearrange in fractional form to get rid of the evidence term in the denominator. in other words, can be reexpressed as which means the evidence term nicely disappears, giving us an expression that we can easily evaluate! this is how metropolis hastings resolves the dilemma of evidence computation very simple yet some surprisingly effective algebra. but before we move into code, there is something that should be corrected before we move on. in , we derived an experssion for the jump condition. the jump condition is not wrong per se, yet a slight modification has to be made to fully capture the gist of metrapolis hastings. the precise jump condition for the sampler goes as follows: where this simply means that we accept the prorposed pararmeter if the quantity calculated in is larger than a random number between 0 and 1 sampled from a uniform distribution. this is why mcmc models involve a form of random walk while leaving room for somewhat unlikely parameters to be selected, the model samples relatively more from regions of high posterior probability. now that we have some understanding of how markov chain monte carlo and the metropolis hastings algorithm, let's implement the mcmc sampler in python. as per convention, listed below are the dependencies required for this demonstration. let's start by generating some toy data for our analysis. array it's always a good idea to plot the data to get a sense of its shape. the data looks roughly normal. this is because we created the toy data using the function, which generates random numbers from a normal distribution centered around 0. the task for this tutorial, given this data, is going to be estimating the mean of the posterior distribution, assuming we know its standard deviation to be 1. the benefit of working with this dumb example is that we can analytically derive a closed form exprerssion for the posterior distribution. this is because a normal prior is conjugate with a normal likelihood of known variance, meaning that the posterior distribution for the mean will also turn out to be normal. if you are wondering if this property of conjugacy is relevant to the notion of conjugacy discussed above with bayesian inference, you are exactly correct: statisticians have a laundry list of distributions with conjugate relationships, accessible on this wikipedia article. the bottom line is that we can calculate the posterior analytically, which essentially gives us an answer with which we can evaluate our implementation of the metropolis hastings algorithm. the equation for the posterior is presented below. this assumes that the data is normally distributed with known variance and that the prior is normal, representable as for a complete mathematical derivation of , refer to this document. as we will see later on, we use to calculate the likelihood and to calculate the prior. for now, however, let's focus on the analyticial derivation of the posterior. this can be achieved by translating the equation in into code as shown below. let's see what the posterior looks like given the prior . for sample observations, we use the toy data set we generated earlier. there we have it, the posterior distribution given sample observations and a normal prior. as expected, we see that the result is normal, a result due to the property of conjugacy. another observation we might make is that the posterior mean is seems to be slightly larger than 0. this is an expected result given that the mean of the numbers in our toy data set was larger than 0. 0.11441773195529023 because the posterior can be intuited as an ""average"" between the observed data set and the prior, we would expect the posterior to be centered around a value greater than zero, which is indeed the case. now that we have a full answer key to our problem, it's time to build the metropolis hastings sampler from scratch and compare the estimation generated by the sampler with the true analytical posterior we have just derived. let's go back to equation , which is the crux of the metropolis hastings sampler. to recap, the mcmc sampler works by assuming some value sampled from the proposal distribution, calculating the likelihood and posterior, and seeing if the new proposed value is worth accepting, i.e. if it is worth making a jump in the random walk. all of this sounds pretty abstract when written in words, but it is a simple idea encapsulated by . let's build the metropolis hastings sampler by implementing the algorithm described above in code as shown below. i looked at thomas wiecki's implementation for reference and modified it to suit the purposes of this post. although markov chain monte carlo sounds complicated, really it is achieved by this single block of code. of course, this code is limited in that is only applicable to a very specific situation, namely the task of deriving the posterior given a normal prior and a normal likelihood with known variance. nevertheless, we can glean so much insight from this fascinating function. let's quickly test the function by making it sample five estimations of the mean parameter. array as expected, the sampler starts from the 0, which is the default argument and took a jump at the second sample. after that jump, the sampler rejects the next three values sampled from the proposal distribution, as it stayed dormant at the value 0.17414333. however, with more iterations, we would expect the function to make more jumps, gradually painting a picture of what the posterior should look like. in fact, we can create what is called the trace plot to see which values were sampled by the metropolis hastings algorithm. trace plots are important because they tell us whether or not our model is well calibrated, i.e. a sufficient amount of state changes occur. the trace plot contains the trace of 15000 accepted values sampled from the proposal distribution. we see that there are some fluctuations, indicating that state transitions occurred, but also that there seems to be values that the sampler preferred over others. eyeballing the trace plot, the ""comfort zone"" seems to be slightly above 0, as we expect. to illustrate the importance of trace plots, let's see an example involving a bad setup involving a proposal distribution with too small a variance. the trace plot below shows that, although the mcmc model does manage to sample many values, it likes to stay too much in its current state, thus making taking much longer for the sampler to properly estimate the posterior by sampling a wide range of values. the bottom line is that setting the right proposal distribution is important, and that trace plots are a good place to start to check if the proposal distribution is set up properly. now, it's time to look at the answer key and see if our sampler has done well. let's plot sampled by our metropolis hastings sampler with the analytic posterior to see if they roughly match. fantastic! although the estimated posterior is not exactly equal to the analytic posterior, the two are quite similar to each other. we could quantify how similar or different they are by using metrics such as kl divergence, but for simplicity's sake, let's contain the post within the realm of bayes as we have done so far. this goes to show just how useful and powerful markov chain monte carlo can be: even if a complicated likelihood function in high dimensional space, we would be able to use a similar sampling sequence to estimate the posterior. what's even more fascinating about markov chain monte carlo is that, regardless of the value we start off with in the proposal distribution, we will eventually be able to approximate the posterior. this is due to the markov chain part of mcmc: one of the most interesting properties of markov chains is that, no matter where we start, we end up in the same stationary distribution. together, these properties makes mcmc models like metropolis hastings incredible useful for solving intractable problems. is a library made specifically for bayesian analysis. of course, it includes functions that implement markov chain monte carlo models. although building the metropolis hastings algorithm from scratch was a worthy challenge, we can't build models from scratch every time we want to conduct from bayesian analysis involving an intractable posterior, which is why packages like always come in handy. with just a few lines of code, we can perform the exact same operation we performed above. in fact, let's compare our metropolis hastings sampler with the built in function in the library. multiprocess sampling metropolis: mu sampling 4 chains: 100%|██████████| 62000/62000 00:05 pretty similar, wouldn't you say? markov chain monte carlo is a powerful method with which we can estimate intractable posterior distributions. it is undoubtedly one of the most important tools that a bayesian statistician should have under their belt. and even if you are frequentist, i still think mcmc models are worth looking at because it's cool to see just how easily we can estimate a distribution with little to no knowledge about the mathematics involved in calculating the posterior. it's also fascinating to see how the marriage of two seemingly unrelated concepts that arose out of different contexts monte carlo methods and markov chains can produce such a powerful algorithm. in the next post, we will continue our journey down the bayesian rabbit hole. perhaps we will start with another application of bayesian thinking in machine learning. if naive bayes is your jam, make sure to tune in some other time. this post: https://jaketae.github.io/study/bayes/ conjugacy: https://en.wikipedia.org/wiki/conjugate_prior curse of dimensionality: https://en.wikipedia.org/wiki/curse_of_dimensionality monte carlo: https://jaketae.github.io/study/monte carlo/ wikipedia: https://en.wikipedia.org/wiki/markov_chain one: https://jaketae.github.io/study/pagerank and markov/ the other: https://jaketae.github.io/study/markov chain/ markov property: https://en.wikipedia.org/wiki/markov_property here: https://jaketae.github.io/study/monte carlo/ metrapolis hastings algorithm: https://en.wikipedia.org/wiki/metropolis–hastings_algorithm this document: https://people.eecs.berkeley.edu/~jordan/courses/260 spring10/lectures/lecture5.pdf thomas wiecki's implementation: https://twiecki.io/blog/2015/11/10/mcmc sampling/",0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
2020-03-22-pca,"principal component analysis is one of those techniques that i've always heard about somewhere, but didn't have a chance to really dive into. pca would come up in papers on gans, tutorials on unsupervised machine learning, and of course, math textbooks, whether it be on statistics or linear algebra. i decided that it's about time that i devote a post to this topic, especially since i promised one after writing about singular value decomposition on this blog some time ago. so here it goes. what do we need principal component analysis for? or more importantly, what is a principal component to begin with? well, to cut to the chase, pca is a way of implementing dimensionality reduction, often referred to as lossy compression. this simply means that we want to transform some data living in high dimensional space into lower dimensions. imagine having a data with hundreds of thousands of feature columns. it would take a lot of computing power to apply a machine learning model to fit the data and generate predictions. this is when pca comes in: with pca, we can figure out which dimensions are the most important and apply a transformation to compress that data into lower dimensions, making it a lot more tractable and easier to work with. and in case you're still wondering, principal components refer to those new extracted dimensions used to newly represent data! let's derive pca with some good old linear algebra tricks. i used ian goodfellow's deep learning and a lecture slide from columbia references for this post. the setup of a classic pca problem might be summarized as follows. suppose we have a dataset of points, each living in dimensional space. in other words, where our goal is to find a way to compress the data into lower dimensional space where . we might imagine this as a transformation, i.e. the objective is to find a transformation so that applying will yield a new vector living in lower dimensional space. we can also imagine there being a reverse transformation or a decoding function that achieves because pca is in essence a linear transformation, it is most natural to express and understand it as a matrix. let's define this transformation as , and the matrix corresponding to the decoding . in other words, pca makes a number of assumptions to simplify this problem. the most important assumption is that each column of is orthogonal to each other. as we will see later in an alternate derivation with statistics, this has to do with the notion of covariance. another restriction is that the columns of must have a euclidean norm of one. this constraint is necessary for us to find a unique matrix that achieves compression otherwise, we could have any multiples, leading to an infinite number of such matrices. we make one more convenient assumption about the given data points, . that is, is assumed to have a mean of zero, i.e. . if this is not the case, we can easily perform standardization by subtracting the mean from the data. with this setup in mind, let's finally start the derivation. as said earlier, the goal of pca is to compress data with as little loss of information as possible. we don't want to compress data in a haphazard fashion; instead, we want the compression scheme to be able to preserve the structure of the data as much as possible in its lower dimensional representation. from this, we can come up with the following equation: in other words, the goal is to find that which minimizes the difference between the original data and the reconstructed data. note that finding this optimal amounts to finding that most effectively compresses given data. instead of the l2 norm, let's consider the squared l2 norm for convenience purposes. note that minimizing the l2 norm is equal to minimizing the squared l2 norm, so there is no semantic difference. by definition of vector transpose, we can now express the squared l2 norm versions of as follows: where the second to last equality is due to the fact that and are both constants that denote the same value. also, the argument of the minimum is with respect to , we can omit the first term, which is purely in terms of . it's time to take derivatives. but in order to do so, we need to unpack , since we have no idea how to take its derivative. using , we can reorganize as follows: the last equality is due to the fact that we constrained the columns of to be unit vectors that are orthogonal to each other. now we can take a derivative of the argument with respect to and set it equal to zero to find the minimum. this tells us that the optimal way of compressing is simply by multiplying it by the transpose of the decoding matrix. in other words, we have found the transformation in . for those of you who are confused about how gradients and matrix calculus work, here is a very short explanation. first, notice that is just a scalar, since is a column vector. taking a gradient with respect to this quantity would mean that we get another column vector of equal dimensions with with the following elements: and we know how to go from there. the same line of thinking can be applied to think about the second term, . we know that is a row vector since its dot product with should be possible dimensionally speaking. then, we know that the gradient with respect to should give each of the elements of , but in column vector format hence the need for a transpose. in general, the rule of thumb is that the gradient of a scalar with respect to a vector or a matrix should return a vector or matrix of the same dimension. recall from that reconstruction can be achieved by applying compression followed by a decoding operation: since we know that is just and is by definition, we can express in a different way. in retrospect, this is somewhat intuitive since can roughly be thought of as a pseudo orthonormal matrix pseudo since there is no guarantee that it is a square matrix. now, all that is left is to find the matrix . the way to go about this is to reconsider , the notion of minimizing data loss, given our findings in . in other words, instead of considering a single observation, here we consider the design matrix in its entirety. note that is a design matrix whose rows correspond to a single observation. and because we are dealing with matrices, the euclidean norm was replaced with its matrix equivalent, the frobenius norm. observe that the first term can safely be removed from the argument since it is a constant with respect to ; let's also change the argument of the minimum to the maximum given the negative sign. the frobenius norm of a real matrix can be calculated as therefore, the last equality is due to a useful property of trace, which is that we can cycle the order of matrices without changing its value. let's consider a single column in , denoted as . you might also imagine this as a situation where is one dimensional, meaning we want to compress data into a single scalar value. it is not difficult to see that the trace of , which is a scalar in the one dimensional case, is maximized when is an eigenvector of with the largest eigenvalue. generalizing this result back to , we see that is a matrix whose columns correspond to the eigenvectors of in descending order. if you had prior exposure to pca, you might know that the standard way of obtaining principal components is by calculating the covariance matrix of the data and finding its eigenvectors. here, i attempt to present an explanation of how and why the procedure outlined in the preceding section is essentially achieving the same tasks, albeit through a different frame of thought. the unbiased sample covariance matrix is given by of course, this is operating under the assumption that has already been standardized such that the mean of the data is zero. and certainly look different. however, under the hood, they express the same quantity. in , we assumed from the beginning that our data has a mean of zero. in , we make this assumption explicit by subtracting the mean from the data. in , we assumed a row based design matrix, where each data point is stored as a row vector. in , we assumed that each data points are stored as columns of the design matrix; hence the difference in the order of transpose. in , we are dealing with the unbiased sample covariance matrix, which is why we divide by a fraction of . in , we simply express this division as an expectation encapsulating the entire expression. so in a nutshell, the conclusion we arrived at in the preceding section with the minimization of residual sums ultimately amounts to finding the covariance matrix and its eigenvectors. i found this to be the more dominant interpretation of pca, since indeed it is highly intuitive: the goal of pca is to find the axes or the principal components that which maximize the variance seen in the data. setosa.io has some excellent visualizations on the notion of covariance and how it relates to pca, so i highly recommend that you go check it out. if were to derive pca from the gecko with the covariance approach, we would be using an iterative approach to find a single principal component at a time. specifically, our goal would be to find that which maximizes hence the problem is now framed as a constrained optimization problem. we use lagrangians to solve constrained optimization. the intuition for the lagrangian method is that the gradient of the constraint and the argument should be parallel to each other at the point of optimization. we go about this by taking the gradient of the argument with respect to : since 2 is just a constant, we can absorb it into to form a more concise expression. also, since the covariance matrix is by definition symmetric, we can simplify things further to end up with and once again, we have shown that the principal components are the eigenvectors of the covariance matrix. but the procedure outlined above can be used to find only one principal component, that is the eigenvector with the largest eigenvalue. how do we go about searching for multiple eigenvectors? this can be done, once again, with lagrangians, with the added caveat that we will have more trailing terms in the end. let's elaborate on this point further. here, we assume that we have already obtained the first component, , and our goal is to find the next component, . with induction, we can easily see how this analysis would apply to finding . simply put, the goal is to maximize under the constraint that is orthogonal to while also satisfying the constraint that it is a unit vector. therefore, using lagrangians, in the last equality, we make a trivial substitution to simplify and get rid of the constant. we also use the fact that the covariance matrix is symmetric. if we left multiply by , but since , the first two terms go to zero. also, the last term reduces to since . this necessarily means that . if we plug this result back into , we end up with the definition of the eigenvector again, but this time for . essentially, we iterate this process to find a specified number of principal components, which amounts to finding number of eigenvectors of the sample covariance matrix. a while back, we discussed both eigendecomposition as well as singular value decomposition, both of which are useful ways of decomposing matrices into discrete factors. in this section, we will see how pca is essentially a way of performing and applying these decomposition techniques under the hood. recall that eigendecomposition is a method of decomposing matrices as follows: where is a diagonal matrix of eigenvalues and is a matrix of eigenvectors. pca is closely related to eigendecomposition, and this should come as no surprise. essentially, by finding the eigenvalues and eigenvectors of , we are performing an eigendecomposition on the covariance matrix: notice that is a matrix of principal components. of course, in this case, is a square matrix of full rank; to apply dimension compression, we need to slice the first entries of . at any rate, it is clear that pca involves eigendecomposition of the covariance matrix. eigendecomposition can only be applied to matrices of full rank. however, there is a more generalized method for non square matrices, which is singular value decomposition. here is a blueprint of svd: where is a matrix containing the roots of the eigenvalues, with appropriate dimensional configurations to accommodate the shape of the original matrix. we cannot perform eigendecomposition on , which has no guarantee that it is square; however, svd is definitely an option. assume that can be decomposed into , , and . then the covariance matrix becomes and we end up in the same place as we did in . this is no surprise given that the derivation of svd involves eigendecomposition. in this post, we took a deep dive into the mathematics behind principal component analysis. pca is a very useful technique used in many areas of machine learning. one of the most common applications is to apply pca to a high dimensional dataset before applying a clustering algorithm. this makes it easier for the ml model to cluster data, since the data is now aligned in such a way that it shows the most variance. upon some more research, i also found an interesting paper that shows that there is a solid mathematical relationship between k means clustering and pca. i haven't read the paper from top to bottom, but instead glossed over a summary of the paper on this thread on stack overflow. it's certainly a lot of information to take in, and i have no intent of covering this topic in this already rather lengthy post on pca. so perhaps this discussion will be tabled for a later time, as interesting as it seems. i hope you enjoyed reading this post. amidst the chaos of the covid19 pandemic, let's try to stay strong and find peace ruminating over some matrices and formulas. trust me, it works better than you might think.",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
2020-02-25-gan,"generative models are fascinating. it is no wonder that gans, or general adversarial networks, are considered by many to be where future lies for deep learning and neural networks. in this post, we will attempt to create a very simple vanilla gan using tensorflow. specifically, our goal will be to train a neural network that is capable of generating compelling images of ships. although this is a pretty mundane task, it nonetheless sheds lights on the potential that gan models hold. let's jump right into it. below are the dependencies and settings we will be using throughout this tutorial. before we start building the gan model, it is probably a good idea to define some variables that we will be using to configure the parameters of convolutional layers, namely the dimensionality of the images we will be dealing with, as well as the number of color channels and the size of the latent dimension. similar to variational autoencoders, gans are composed of two parts: the generator and the discriminator. as ian goodfellow described in the paper where he first put out the notion of a gan, generators are best understood as counterfeiters of currency, whereas the discriminator is the police trying to distinguish the fake from the true. in other words, a gan is a two component model that involves an internal tug of war between two adversarial parties, each trying their best to accomplish their mission. as this competition progresses, the generator becomes increasingly better at creating fake images; the discriminator also starts to excel at determining the veracity of a presented image. enough of theoretical dwellings, let's begin by defining the generator model. the is a function that returns a generator model according to some set parameters. let's take a look at the structure of this network in more detail. warning:tensorflow:from /usr/local/lib/python3.6/dist packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling baseresourcevariable.__init__ with constraint is deprecated and will be removed in a future version. instructions for updating: if using keras pass _constraint arguments to layers. model: ""model"" _________________________________________________________________ ================================================================= input_1 0 _________________________________________________________________ dense 4227072 _________________________________________________________________ leaky_re_lu 0 _________________________________________________________________ reshape 0 _________________________________________________________________ conv2d 819456 _________________________________________________________________ leaky_re_lu_1 0 _________________________________________________________________ conv2d_transpose 1048832 _________________________________________________________________ leaky_re_lu_2 0 _________________________________________________________________ conv2d_1 1638656 _________________________________________________________________ leaky_re_lu_3 0 _________________________________________________________________ conv2d_2 1638656 _________________________________________________________________ leaky_re_lu_4 0 _________________________________________________________________ conv2d_3 37635 ================================================================= total params: 9,410,307 trainable params: 9,410,307 non trainable params: 0 _________________________________________________________________ none notice that the output of the generator is a batch image of dimensions . this is exactly the same as the , , and information we defined earlier, and that is no coincidence: in order to fool the discriminator, the generator has to generate images that are of the same dimensions as the training images from imagenet. now it's time to complete the gan by creating a corresponding discriminator, the discerning police officer. the discriminator is essentially a simple binary classier that ascertains whether a given image is true or fake. therefore, it is no surprise that the final output layer will have one neuron with a sigmoid activation function. let's take a more detailed look at the function as shown below. and again, a model summary for convenient reference: warning:tensorflow:from /usr/local/lib/python3.6/dist packages/tensorflow_core/python/ops/nn_impl.py:183: where is deprecated and will be removed in a future version. instructions for updating: use tf.where in 2.0, which has the same broadcast rule as np.where model: ""model_1"" _________________________________________________________________ ================================================================= input_2 0 _________________________________________________________________ conv2d_4 3584 _________________________________________________________________ leaky_re_lu_5 0 _________________________________________________________________ conv2d_5 262272 _________________________________________________________________ leaky_re_lu_6 0 _________________________________________________________________ conv2d_6 262272 _________________________________________________________________ leaky_re_lu_7 0 _________________________________________________________________ conv2d_7 262272 _________________________________________________________________ leaky_re_lu_8 0 _________________________________________________________________ flatten 0 _________________________________________________________________ dropout 0 _________________________________________________________________ dense_1 513 ================================================================= total params: 790,913 trainable params: 790,913 non trainable params: 0 _________________________________________________________________ none now we have both the discriminator and the generator, but the two are not really connected in the sense that they exist as discrete models lacking any connection between them. what we want to do, however, is to establish some relationship between the generator and the discriminator to complete a gan, and hence train them in conjunction. this process of putting the pieces together, or adjoining the models, is where i personally find the genius in gan design. the key takeaway here is that we define and . as you might imagine, the shape of the input is defined by we defined earlier. this is the latent space from which we will sample a random noise vector frame to feed into our gan. then, the connection between the generator and the discriminator is effectively established by the statement . all this is saying is that gan's output is the evaluation of the generator's fake image by the discriminator. if the generator does well, it will fool the discriminator and thus output 1; 0 vice versa. let's take a look at the code implementation of this logic. model: ""model_2"" _________________________________________________________________ ================================================================= input_3 0 _________________________________________________________________ model 9410307 _________________________________________________________________ model_1 790913 ================================================================= total params: 10,201,220 trainable params: 9,410,307 non trainable params: 790,913 _________________________________________________________________ none now it's time to train our model. let's first load our dataset. for this, we will be using the images. the dataset contains low resolutions images, so our output is also going to be very rough, but it is a good starting point nonetheless. one hacky thing we do is concatenating the training and testing data. this is because for a gan, we don't need to differentiate the two: on the contrary, the more data we have for training, the better. one might suggest that testing data is necessary for the discriminator, which is a valid point, but the end goal here is to build a high performing generator, not the discriminator, so we will gloss over that point for now. for this tutorial, we will be using images of ships, which are labeled as 8. so let's go ahead and specify that. we see that contains 6000 images, which is more than enough to start training our gan. to train the gan, we will define a function. essentially, this function creates binary labels for real and fake images. recall that the goal of the discriminator is to successfully discern generated images from real ones. also recall that to create generated images, the generator needs to sample from a latent dimension. in other words, training will consist of the following steps: sample a random vector to be fed into the generator create zero labels for the corresponding generated images create one labels for real images from the training dataset train the discriminator with the two labels train the gan by coercing a true label for all images these high level abstractions are what implements behind the scenes. there are several subtleties that deserve our attention. first, we fade out the labels ever so slightly to expedite the training process. these are little magic tricks that people have found to work well on gan training. while i'm not entirely sure about the underlying principle, it most likely comes from the fact that having a smooth manifold is conducive to the training of a neural network. second, coercing a true label on the gan essentially trains the generator. note that we never explicitly address the generator in the function; instead, we only train the discriminator. by coercing a true label on the gan, we are effectively forcing the generator to produce more compelling images, and penalizing it when it fails to do so. personally, i find this part to be the genius and beauty of training gans. now that we have an idea of what the function accomplishes, let's use it to start training. warning:tensorflow:discrepancy between trainable weights and collected trainable weights, did you set without calling after ? iteration 0/10000 ============================== d loss: 0.679, gan loss: 0.736 iteration 200/10000 ============================== d loss: 0.560, gan loss: 2.285 iteration 400/10000 ============================== d loss: 0.678, gan loss: 0.801 iteration 600/10000 ============================== d loss: 0.556, gan loss: 2.400 iteration 800/10000 ============================== d loss: 0.695, gan loss: 0.705 iteration 1000/10000 ============================== d loss: 0.699, gan loss: 0.652 iteration 1200/10000 ============================== d loss: 0.718, gan loss: 0.606 iteration 1400/10000 ============================== d loss: 0.706, gan loss: 0.679 iteration 1600/10000 ============================== d loss: 0.675, gan loss: 0.702 iteration 1800/10000 ============================== d loss: 0.651, gan loss: 0.668 iteration 2000/10000 ============================== d loss: 0.748, gan loss: 0.805 iteration 2200/10000 ============================== d loss: 0.682, gan loss: 0.729 iteration 2400/10000 ============================== d loss: 0.402, gan loss: 3.102 iteration 2600/10000 ============================== d loss: 0.672, gan loss: 0.665 iteration 2800/10000 ============================== d loss: 0.659, gan loss: 0.534 iteration 3000/10000 ============================== d loss: 0.686, gan loss: 0.679 iteration 3200/10000 ============================== d loss: 0.645, gan loss: 0.679 iteration 3400/10000 ============================== d loss: 0.681, gan loss: 0.728 iteration 3600/10000 ============================== d loss: 0.792, gan loss: 1.180 iteration 3800/10000 ============================== d loss: 0.687, gan loss: 0.897 iteration 4000/10000 ============================== d loss: 0.791, gan loss: 1.159 iteration 4200/10000 ============================== d loss: 0.695, gan loss: 0.680 iteration 4400/10000 ============================== d loss: 0.671, gan loss: 0.706 iteration 4600/10000 ============================== d loss: 0.702, gan loss: 0.811 iteration 4800/10000 ============================== d loss: 0.697, gan loss: 0.634 iteration 5000/10000 ============================== d loss: 0.759, gan loss: 0.802 iteration 5200/10000 ============================== d loss: 0.677, gan loss: 0.740 iteration 5400/10000 ============================== d loss: 0.701, gan loss: 0.663 iteration 5600/10000 ============================== d loss: 0.670, gan loss: 0.598 iteration 5800/10000 ============================== d loss: 0.615, gan loss: 0.756 iteration 6000/10000 ============================== d loss: 0.677, gan loss: 0.626 iteration 6200/10000 ============================== d loss: 0.669, gan loss: 0.767 iteration 6400/10000 ============================== d loss: 0.682, gan loss: 0.644 iteration 6600/10000 ============================== d loss: 0.742, gan loss: 0.955 iteration 6800/10000 ============================== d loss: 0.701, gan loss: 0.680 iteration 7000/10000 ============================== d loss: 0.303, gan loss: 7.814 iteration 7200/10000 ============================== d loss: 0.596, gan loss: 0.847 iteration 7400/10000 ============================== d loss: 0.717, gan loss: 0.770 iteration 7600/10000 ============================== d loss: 0.707, gan loss: 0.742 iteration 7800/10000 ============================== d loss: 0.697, gan loss: 0.795 iteration 8000/10000 ============================== d loss: 0.647, gan loss: 0.672 iteration 8200/10000 ============================== d loss: 0.676, gan loss: 0.725 iteration 8400/10000 ============================== d loss: 0.608, gan loss: 1.050 iteration 8600/10000 ============================== d loss: 0.757, gan loss: 0.824 iteration 8800/10000 ============================== d loss: 0.614, gan loss: 0.758 iteration 9000/10000 ============================== d loss: 0.660, gan loss: 0.647 iteration 9200/10000 ============================== d loss: 0.651, gan loss: 1.122 iteration 9400/10000 ============================== d loss: 0.710, gan loss: 0.991 iteration 9600/10000 ============================== d loss: 0.734, gan loss: 0.901 iteration 9800/10000 ============================== d loss: 0.681, gan loss: 0.899 the seems to fluctuate a bit, which is not necessarily a good sign but also quite a common phenomenon in gan training. gans are notoriously difficult to train, since it requires balancing the performance of the generator and the discriminator in such a way that one does not overpower the other. this is referred to as a min max game in game theory terms, and finding an equilibrium in such structures are known to be difficult. let's take a look at the results now that the iterations are over. the created images are admittedly fuzzy, pixelated, and some even somewhat alien looking. this point notwithstanding, i find it incredibly fascinating to see that at least some generated images actually resemble ships in the sea. of particular interest to me are the red ships that appear in and . given the simplicity of the structure of our network, i would say that this is a successful result. let's take a look at the learning curve of the gan. as you might expect, the loss is very spiky and erratic. this is why it is hard to determine when to stop training a gan. of course, there are obvious signs of failure: when the loss of one component starts to get exponentially larger or smaller than its competitor, for instance. however, this did not happen here, so i let the training continue until the specified number of interactions were over. the results, as shown above, suggest that we haven't failed in our task. in a future post, we will be taking a look at the mathematics behind gans to really understand what's happening behind the scenes when we pit the generator against its mortal enemy, the discriminator. see you in the next post!",0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2020-07-22-text-preprocessing,"in today's post, we will finally start modeling the auto tagger model that i wanted to build for more blog. as you may have noticed, every blog post is classified into a few different tags, which are essentially topic keywords that i have been manually assigning thus far. i have been getting increasingly lazier these past few weeks, which is ironically what compelled me into experimenting and studying more about the basics of nlp. as mentioned in previous posts, this is why i've been posting things like tf idf vectorization and word embeddings. while there are so many sota models out there, for the purposes of this mini project, i decided to go slow. in what may or may not become a little series of its own, i aspire to achieve the following: design a basic parsing algorithm to clean blog posts in markdown vectorize the cleaned string data build a target vector corresponding to each blog post construct and train a document classifier develop a pipeline to generate and display the model's predictions this is by no means a short, easy project for me. this is also the first time that i'm dealing with real data data created by no one other than myself so there is an interesting element of meta ness to it that i enjoy. after all, this very post that i'm writing will also be a valuable addition to the training set for the model. with that said, let's jump right into it. the first challenge is cleaning the textual data we have. by cleaning, i don't mean vectorizing; instead, we need to first retrieve the data, get rid of extraneous characters, code blocks, and mathjax expressions, and so on. after all, our simple model cannot be expected to understand code blocks or latex expressions, as awesome as that sounds. there were two routes i could take with parsing. the first was web scraping; the second, using directly parsing raw markdown files. i'll detail each attempts i've made in the following sections, then explain why i chose one approach over the other. because all my published posts are available on my blog website, i could crawl the blog and extract tags to construct my training dataset. here are some of the steps i took while experimenting with this approach. for demonstration purposes, i decided to use a recent post on gaussian process regression, as it contains a nice blend of both code and mathjax expressions. now we can take a look at the first tag in the web scraped list. in this post, we will explore the gaussian process in the context of regression. this is a topic i meant to study for a long time, yet was never able to due to the seemingly intimidating mathematics involved. however, after consulting some extremely well curated resources on this topic, such as kilian’s lecture notes and ubc lecture videos by nando de freitas, i think i’m finally starting to understand what gp is. i highly recommend that you check out these resources, as they are both very beginner friendly and build up each concept from the basics. with that out of the way, let’s get started. this is not a bad starting point, but obviously there is so much more work that has to be done. for one thing, we need to remove tags that are often wrapped around tags. we also have to remove inline latex expressions, which as written as . below is a function that that i wrote to clean the data with the following considerations in mind. for demonstration purposes, let's try scraping the post on gp regression i mentioned earlier. 'ution of the predicted data at a given test point. gaussian processes are similar to bayesian linear regression in that the final result is a distribution which we can sample from. the biggest point of difference between gp and bayesian regression, however, is that gp is a fundamentally non parametric approach, whereas the latter is a parametric one. i think this is the most fascinating part about gps—as we will see later on, gps do not require us to specify any function or model to fit the data. instead, all we need to do is to identify the mean and covariance of a multivariate gaussian that defines the posterior of the gp. all of this sounds too good be true—how can a single multivariate gaussian distribution be enough for what could potentially be a high dimensional, complicated regression problem? let’s discuss some mathematical ideas that enable gp to be so powerful. gaussians are essentially a black hole of distributions: once a gaussian, always a gaussian. for example, we ' we see that the text has indeed been parsed, which is great! so we have the basic tools to parse a post given a url. so naturally, the next step would be to figure out all the urls for the blog posts i have on my website. of course, i could do this manually, but that sort of defeats the point of building an auto tagger. so after some trial and error, here is another function i wrote that scrapes all blog post urls on my website. we start from the root url, then basically extract hrefs from the elements that each represent a single blog post. 'https://jaketae.github.io/development/tinkering docker/', 'https://jaketae.github.io/study/word2vec/', 'https://jaketae.github.io/study/complex fibonacci/', 'https://jaketae.github.io/study/tf idf/', 'https://jaketae.github.io/study/gaussian process/', 'https://jaketae.github.io/study/genetic algorithm/', 'https://jaketae.github.io/study/revisiting basel/', 'https://jaketae.github.io/study/zeta prime/', 'https://jaketae.github.io/study/bfs dfs/', 'https://jaketae.github.io/study/numerical methods/', 'https://jaketae.github.io/study/gibbs sampling/', 'https://jaketae.github.io/development/sklearn sprint/', 'https://jaketae.github.io/study/spark basics/', 'https://jaketae.github.io/study/dissecting lstm/', 'https://jaketae.github.io/study/sklearn pipeline/', 'https://jaketae.github.io/study/natural gradient/', 'https://jaketae.github.io/blog/workflow cleanup/', 'https://jaketae.github.io/study/r tutorial 4/', 'https://jaketae.github.io/study/sql basics/', 'https://jaketae.github.io/study/r tutorial 3/', 'https://jaketae.github.io/development/c/', 'https://jaketae.github.io/study/leibniz rule/', 'https://jaketae.github.io/study/r tutorial 2/', 'https://jaketae.github.io/study/r tutorial 1/', 'https://jaketae.github.io/study/fisher/', 'https://jaketae.github.io/study/stieltjes/', 'https://jaketae.github.io/study/stirling/', 'https://jaketae.github.io/study/pca/', 'https://jaketae.github.io/study/fourier/', 'https://jaketae.github.io/study/gan math/', 'https://jaketae.github.io/study/kl mle/', 'https://jaketae.github.io/study/development/open source/', 'https://jaketae.github.io/study/development/flask/', 'https://jaketae.github.io/study/gan/', 'https://jaketae.github.io/study/vae/', 'https://jaketae.github.io/study/autoencoder/', 'https://jaketae.github.io/study/auto complete/', 'https://jaketae.github.io/study/rnn/', 'https://jaketae.github.io/study/neural net/', 'https://jaketae.github.io/study/cnn/', 'https://jaketae.github.io/blog/typora/', 'https://jaketae.github.io/study/map convex/', 'https://jaketae.github.io/study/exponential family/', 'https://jaketae.github.io/study/bayesian regression/', 'https://jaketae.github.io/study/naive bayes/', 'https://jaketae.github.io/study/first keras/', 'https://jaketae.github.io/study/r tutorial/', 'https://jaketae.github.io/development/anaconda/', 'https://jaketae.github.io/study/mcmc/', 'https://jaketae.github.io/study/logistic regression/', 'https://jaketae.github.io/study/map mle/', 'https://jaketae.github.io/study/knn/', 'https://jaketae.github.io/study/information entropy/', 'https://jaketae.github.io/study/moment/', 'https://jaketae.github.io/study/gaussian distribution/', 'https://jaketae.github.io/study/svd/', 'https://jaketae.github.io/study/linear regression/', 'https://jaketae.github.io/study/monte carlo/', 'https://jaketae.github.io/study/likelihood/', 'https://jaketae.github.io/blog/jupyter automation/', 'https://jaketae.github.io/study/bayes/', 'https://jaketae.github.io/blog/test/', 'https://jaketae.github.io/study/basel zeta/', 'https://jaketae.github.io/study/gamma/', 'https://jaketae.github.io/study/poisson/', 'https://jaketae.github.io/study/eulers identity/', 'https://jaketae.github.io/study/markov chain/', 'https://jaketae.github.io/tech/new mbp/', 'https://jaketae.github.io/study/pagerank and markov/', 'https://jaketae.github.io/blog/studying deep learning/' i knew that i had been writing somewhat consistently for the past few months, but looking at this full list made me realize how fast time has flown by. continuing with our discussion on cleaning data, now we have all the basic tools we need to build our training data. in fact, we can simply build our raw strings training data simply by looping over all the urls and extracting text from each: at this point, you might be wondering why i even attempted a second approach, given that these methods all work fine. the answer is that, although web scraping works okay and we could certainly continue with this approach but we would have to build a text parser anyway. think about it: although we can build the training data through web scraping, to run the actual inference, we need to parse the draft, in markdown format, that has not been published yet. in other words, we have no choice but to deal with markdown files, since we have to parse and clean our draft to feed into the model. it is after this belated realization that i started building a parser. now, the interesting part is that i tried two different approachdes going down this road as well. so really, the accurate description would be that i tried three different methods. the first sub approach was the one i first thought about, and is thus naturally the more naive method of the two. this is simply an adaptation of the algorithm used in the function, involving a boolean variable that would switch on and off as we loop through the words, switching whenever we see a delimiter like restr.replace""""?\n \n_postsnltk.mdstop_wordsstem_tokenizer.ipynb.md.md` file. this shouldn't be too difficult, but nonetheless it is an important detail that would be very cool to implement once we have a finalized model. it would be even better if we could implement some mechanism to train our model with new data per each blog post upload; after all, we don't want to use an old model trained with old data. instead, we want to feed it with new data so that it is able to learn more. hopefully we'll find a way to tackle these considerations as we move forward. catch you up in the next post!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
2019-12-07-svd,"i've been using a music streaming service for the past few weeks, and it's been a great experience so far. i usually listen to some smoothing new age piano or jazz while i'm working, while i prefer k pop on my daily commutes and bass heavy house music during my workouts. having processed these information through repeated user input on my part, the streaming application now regularly generates playlists each reflective of the three different genres of music that i enjoy most. this got me wondering: what is the underlying algorithm beind content selection and recommendation? how do prominent streaming services such as netflix and spotify provide recommendations to their users that seem to reflect their personal preferences and tastes? from a business perspective, these questions carry extreme significance since the accuracy of a recommendation algorithm may directly impact sales revenue. in this post, we will dive into this question by developing an elementary recommendation engine. the mechanism we will use to achieve this objective is a technique in linear algebra known as singular value decomposition or svd for short. svd is an incredibly powerful way of processing data, and also ties in with other important techniques in applied statistics such as principal component analysis, which we might also take a look at in a future post. enough with the preface, let's dive right into developing our model. before we start coding away, let's first try to understand what singular value decomposition is. in a previous post on markov chains, we examined the clockwork behind eigendecomposition, a technique used to decompose non degenerate square matrices. singular value decomposition is similar to eigendecomposition in that it is a technique that can be used to factor matrices into distinct components. in fact, in deriving the svd formula, we will later inevitably run into eigenvalues and eigenvectors, which should remind us of eigendecomposition. however, svd is distinct from eigendecomposition in that it can be used to factor not only square matrices, but any matrices, whether square or rectangular, degenerate or non singular. this wide applicability is what makes singular decomposition such a useful method of processing matrices. now that we have a general idea of what svd entails, let's get down into the details. in this section, we take a look at the mathematical clockwork behind the svd formula. in doing so, we might run into some concepts of linear algebra that requie us to understand some basic the properties of symmetric matrices. the first section is devoted to explaining the formula using these properties; the second section provides explanations and simple proofs for some of the properties that we reference duirng derivation. we might as well start by presenting the formula for singular value decomposition. given some by matrix , singular value decomposition can be performed as follows: there are two important points to be made about formula . the first pertains to the dimensions of each factor: , , . in eigendecomposition, the factors were all square matrices whose dimension was identical to that of the matrix that we sought to decompose. in svd, however, since the target matrix can be rectangular, the factors are always of the same shape. the second point to note is that and are orthogonal matrices; , a diagonal matrix. this decomposition structure is similar to that of eigendecomposition, and this is no coincidence: in fact, formula can simply be shown by performing an eigendecomposition on and . let's begin by calculating the first case, , assuming formiula . this process looks as follows: the last equality stands since the inverse of an orthogonal matrix is equal to its transpose. substituting for , equation simplifies to and we finally have what we have seen with eigendecomposition: a matrix of independent vectors equal to the rank of the original matrix, a diagonal matrix, and an inverse. indeed, what we have in is an eigendecomposition of the matrix . intuitively speaking, because matrix is not necessarily square, we calculate to make it square, then perform the familiar eigendecomposition. note that we have orthogonal eigenvectors in this case because is a symmetric matrix more specifically, positive semi definite. we won't get into this subtopic too much, but we will explore a very simple proof for this property, so don't worry. for now, let's continue with our exploration of the svd formula by turning our attention from matrix a factor of eigendecomposition on to the matrix . much like we understood as a factor of eigendecomposition, can be seen as a factor of eigendecomposition, this time on the matrix . concretly, notice the parallel between and . it's not difficult to see that, by symmetry, is also going to be an orthogonal matrix containing the eigenvectors of . the most important difference between and concerns dimensionality: while is a by matrix, v is an by . this disparity originates from the fact that itself is a rectangular matrix, meaning that the dimensions of and are also different. another point that requires clarification pertains to . earlier, we made a substitution of for . this tells us that contains the square roots of the eigenvalues of and , which, it is important to note, has identical non zero eigenvalues. if this point brings confusion, i recommend that you peruse over the next subsection on linear algebra. let's conclude this section with the formula for singular value decomposition: hopefully, now it is clear what , , and are. singular value decomposition can intuitively be thought of as a square root version of eigendecomposition, since essentially and are all derivatives that come from the ""square"" of a matrix, the two transpose multiples. this intuition also aligns with the fact that is a diagonal matrix containing the square roots of eigenvalues of the transpose products. with these in mind, let's get ready to build the recommendation model. in this optional section, we take a look at two mathematical propositions we referenced while motivating the svd formula: first, that symmetric matrices have orthogonal eigenvectors; second, that and have identical non zero eigenvalues. the proof for both of these statements are simple, but feel free to gloss over this section if you just want to see svd at work instead of the mathematical details behind singular value decomposition. let be some symmetric matrix, i.e. . also assume that has two distinct eigenvectors, and with corresponding eigenvalues and . with this setup, we start from the definition of eigenvectors and eigenvalues: if we apply transpose on both sides, we can legally multiply both sides by , which results in the following: however, since , furthermore, we can use the fact that the eigenvalue corresponding to is . then, since , the only way for to make sense is if and this is exactly what we have been trying to show. since and are two distinct eigenvectors of the symmetric matrix , we have successfully shown that any two eigenvectors of will be orthogonal, i.e. their dot product is going to be zero. let's start by assuming that has some non zero eigenvector whose corresponding eigenvalue is . then, we have if we left multiply both sides by , we get by the definition of an eigenvector, it is not difficult to see that has an eigenvector whose corresponding eigenvalue is . in short, the reason why svd works is that the eigenvalue matrix can be obtained either way by performing an eigendecomposition of the matrix or . now that we have a mathematical understanding of how singular value decomposition, let's see how we can apply svd to build a simple recommendation algorithm. this section will continue as follows. first, we examine svd as a technique of data compression and dimensionality reduction. next, we generate some toy data of movie reviews and apply svd to see how we can build a simple function that gives movie recommendations to users given their movie ratings history. let's jump right in. why is singular value decomposition so important? sure, it should now be fairly clear that svd is a decomposition technique that can be applied to any matrix, whether square or not, which in and of itself makes it a very powerful tool in the statistician's arsenal. but the true beauty of singular value decomposition comes from the fact that we can perform data compression by extracting meaningful information from the given data. this process is otherwise known as dimensionality reduction, and it is one of the most common applications of singular value decomposition. let's see what this means with an example. here is , a target matrix for singuluar value decomposition. calculating , we get which is symmetric as we expect. we can calculate the eigenvalues of this matrix by finding the roots of the following characteristic polynomial: since in svd is the diagonal matrix that contains the square roots of the eigenvalues of , we can conclude that where denotes the value of the th diagonal entry in . therefore, given the dimensionality of , we can conclude that next, we find the eigenvalues of . this process can be performed by identifying the null space of the matrix . for instance, given , given the orientation of this matrix, we see that by doing the same for , we can construct the matrix : repeating the procedure for to obtain the factor , we can complete the singular value decomposition on a: the key to dimensionality reduction is that the first few columns of , its corresponding eigenvalues in , and the corresponding first few rows of contain the most amount of information on matrix . as we go down the diagonal entries of , we see that the eigenvalues get smaller. the rule of thumb is that the smaller the eigenvalue, the lesser contribution it has on expressing data on . in other words, we can obtain an approximation of by extracting the first few columns and rows of each factor. for example, this may seem like a very clumsy way of approximating . however, this is because the toy matrix we dealt with was a mere two by three matrix with only two non zero entries in the diagonal of . imagine performing the same analysis on a much larger matrix, from which we extract number of non trivial entries of . on scale, singular value decomposition becomes more powerful, as it allows large amounts of data to be processed in managable bites. this is more than enough theory on svd. now is finally the time to jump into building our recommendation model with singular value decomposition. in this section, we will generate some random data, namely the ratings matrix. the row of the ratings matrix can be interpreted as users; the columns, movies. in other words, denotes the ratings the th user gave for the th movie. the example we will use was borrowed from this post by zacharia miller. let's quickly build this ratings matrix using and as shown below. let's first see what this matrix looks like. we can do this simply by calling the function and saving it to some variable. for notational consistency, let's name this variable . great! now we have a matrix of binary numbers, where denotes the fact that the user liked the movie and the fact that they disliked it. we can make some cursory qualitative observations of this toy data. note, for instance, that users who like movie 2 also tend to like movie 3. also, user 6 and user 8 have identical prefernece for movies perhaps they both like a particular genre, or tend to like the movie starred by some actor or actress. we would expect singular value decomposition to capture these observations in some way, albeit approximately. now, let's actually perform singular value decomposition on the ratings matrix. we could try to do this manually by hand, but let's utilize the power of modern computing to save ourselves of the time and mental effort involved in calculating the eigenvalues and eigenvectors of a ten by ten matrix. luckily for us, the module contains some excellent functionality to help us with singular value decomposition. using this library, singular value decomposition can very simply be achieved with just a few lines of code. the parameters of the function are , the ratings matrix, and , the number of non trivial entries of to select for dimensionality reduction, as we have seen earlier. more technically speaking, corresponds to the number of ""concepts"" or dimensions that we will extract from the matrix. let's see what this means by actually running this function. great! this is what dimensionality reduction means in the loosest sense. instead of having 5 entries each row, as we had with the original ratings matrix , we now have 3 entries per row. in other words, the information on users has been compressed into three dimensions. unlike in , where each column corresponded to some movie, we don't really know what the columns of stand for. it might be some genre, actress, or any hidden patterns in the data set that we are not aware of. regardless, what's important here is that we can now understand data more easily in smaller dimensions. an impotant observation to make is that, as we have noted earlier, user 6 and user 8 have rows that are identical. while this should not be a surprise given that the two users had what seemed to be an identical taste in movies, it is still interesting to see how svd is able to extract this information and display it onto a new axis. next, let's see what looks like. shown above is the transpose of , which means that is just really . what's important here is that the five movies have also been reduced to three dimensions. we don't really know what the columns of this matrix means; all we know is that it is some distillation and amalgamation of information about the ten users on some unknown axis. at any rate, the previous ten dimensional vectors have now been reduced to three dimensions, which is great news for us as three dimensional beings, it's always easier to visualize and deal with three dimensions or less than 10d. movie 2 and movie 3 do not look as similar as they did before on the ratings matrix. however, perhaps this is due to the fact that all entries of this matrix have pretty small values, and it is difficult to see how the difference between movie 2 and 3 compares to, say, the distance between movies 1 and 4. perhaps we should scale this in terms of relative distances or plot it on a three dimensional space, which is exactly what we are going to in a moment. before we jump into visualizations, however, let's deal with the elephant in the room first: is it okay to simply chop off a few dimensions to reduce a high dimensional image to fit into three dimensional space? to answer this question, let's check the matrix for this particular instance of singular value decomposition. array note that we already have the first three values of in our hands given that in our instantiation of singular value decomposition. the information we lose pertains to the last two values, given by and . these values are smaller in order of magnitude compared to, for instance, the largest value of , which is . this supports the idea that the information we lose amid dimensionality reduction is minimal. alternatively, we can also see this by taking a look at the full, unreduced version of the matrix or . for example, the code snippet below displays the full version of the factor . it is not difficult to see that the last few columns of contain values so small that their contribution to data is going to be minimal at best. this is not the most mathematical way of presenting the concept of data doing so would require us to take a look at other metrics such as covariance but this basic analysis will suffice for our purposes for now. the takeaway is that dimensionality reduction is a meaningful way to extract important information from our data. now that we have performed svd on the ratings matrix, let's move onto the last step: crafting a model for our recommendation algorithm. my personal pet theory is that using any word in conjunction with ""algorithm"" makes the concept sound more complex than it actually is. this is exactly what we are doing here, because in reality, our so called algoithm for movie recommendations is going to be very simple. the intuition behind the recommendation system is distance calculation. simply put, if users have similar movie preferences, the points representing the two users will appear to be close when plotted on a graph. let's see what this means by plotting using . this can be achieved with the following code. we can pass as an argument for the function to see a three dimensional plot of users' movie preferences, as shown below. note that the points corresponding to user 6 and user 8 exactly overlap, which is why the points look darker despite being positioned near the corner of the plot. this is also why we can only count seven points in total despite having plotted eight data points. in short, this visualization shows how we might be able to use distance calculation to give movie recommendations to a new user. assume, for instance, that we get a new suscriber to our movie application. if we can plot onto the space above, we will be able to see to whom user 10's preference is most similar. this comparison is useful since user 10 will most likely like the movie that the other use also rated highly. we can also create a similar plot for movies instead of users. the plot is shown below: with some alteration of the viewing angle, now we see through visualization that movies 2 and 3 are close, as we had expected from the original ratings matrix . this is an interesting result, and it shows just how powerful singular value decomposition is at extracting important patterns from given data sets. now that we understand what svd does for us, it's time to code our recommender function that uses distance calculation to output movie recommendations. in this post, we will be using the dot product as a means of determining distance, although other metrics such as euclidean distance would suit our purposes as well. an advantage of using the dot product is that it is computationally less expensive and easy to achieve with code, as shown below. the function recommends an number of movies given that the user rated highly. for example, let's say some user really liked movie 2 and is looking for two more movies that are similar to movie 2. then, we can simply call the function above by passing in appropriate arguments as follows. 3, 4 this function tells us that our movie application should recommend to our user movies 3 and 4, in that order. this result is not surprising given the fact that we have already observed the closeness between movies 2 and 3 if a user likes movie 2, we should definitely recommend movie 3 to them. our algorithm also tells us that the distance between movie 2 and 4 is also pretty close, although not as close as the distance between movies 2 and 3. what is happening behind the scene here? our function simply calculates the distance between the vector representation of each movies as a dot product. if we were to print the local variable array defined within the function, for instance, we would see the following result. 0, 0.21039350295933443, 1, 0.033077064237217, 3, 0.4411602025458312, 4, 0.07975391765448048 this tells us how close movies 0, 1, 3, and 4 are with movie 2. the larger the dot product, the closer the movie; hence, the more compelling that recommendation. the function then sorts the array and outputs the first movies as a recommendation. of course, we could think of an alternate implementation of this algorithm that makes use of the matrix instead of , but that would be a slightly different recommendation system that uses past user's movie ratings as information to predict whether or not the particular individual would like a given movie. as we can see, svd can be used in countless ways in the domain of recommendation algorithms, which goes to show how powerful it is as a tool for data analysis. in today's post, we dealt primarily with singular value decomposition and its application in the context of recommendation systems. although the system we built in this post is extremely simple, especially in comparison to the complex models that companies use in real life situations, nonetheless our exploration of svd is valuable in that we started from the bare basics to build our own model. what is even more fascinating is that many recommendation systems involve singular value decomposition in one way or another, meaning that our exploration is not detached from the context of reality. hopefully this post has given you some intuition behind how recommendation systems work and what math is involved in those algorithms. on a tangential note, recently, i have begun to realize that linear algebra as a university subject is very different from linear algebra as a field of applied math. although i found interest in linear algebra last year when i took the course as a first year, studying math on my own has endowed me with a more holistic understanding of how the concepts and formulas we learned in linear algebra class can be used in real life contexts. while i am no expert in pedagogy or teaching methodology, this makes me believe that perhaps linear algebra could be taught better if students were exposed to applications with appropriate data to toy around with. just a passing thought. anyhow, that's enough blogging for today. catch you up in the next one. singular value decomposition: https://en.wikipedia.org/wiki/singular_value_decomposition previous post: https://jaketae.github.io/study/markov chain/ eigendecomposition: https://en.wikipedia.org/wiki/eigendecomposition_of_a_matrix eigenvalues and eigenvectors: https://en.wikipedia.org/wiki/eigenvalues_and_eigenvectors this post: http://zwmiller.com/projects/simple_recommender.html",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
2020-04-25-r-tutorial-2,"in this post, we will continue our journey with the r programming language. in the last post, we explored some basic plotting functions and how to use them to visualize data. in this post, we will take a break from commands relating to visualization and instead focus on some tools for statistical analysis on distributions. let’s jump right in. binomial distribution ===================== let’s consider the simple example of a fair coin toss, just because we are uncreative and like to recycle overused examples in textbooks. the function is used to calculate the probability desntiy function . for example, we might compute as follows: dbinom we can also use slicing to obtain , if that quantity is ever an interest to us. sum) notice that the function was used in order to aggregate all the values in the returned list. as you can see, this is one way one might go about calculating the cumulative mass function. cdfs cannot be computed in this fashion since slicing of integers from cannot be used for continuous random variables. there is a more direct way to calculate the cmf right away without using the command, and that is . here is a simple demonstration. pbinom as expected we get the same exact value. the argument tells r that we want values lesser or equal to 10, inclusive. quite similarly, we can also use as a quantile function, which can be considered as the inverse of in the sense that it gives us a value instead of a value. qbinom the and commands we have looked so far dealt with probability mass functions of the binomial distribution. but what if we want to sample a random variable from this distribution, say to perform some sort of monte carlo approximation? this can be achieved with the command. rbinom note that this is a simulation of the binomial random variable, not bernoulli. since we specified , we get five numbers. if we repeat this many times, it turns into a very primitive form of monte carlo simulation. hist, main='binomial distribution', las=1) normal distribution =================== one useful pattern to realize is that the designers of r were very systematic: they didn’t name functions out of arbitrary whim. instead, there is a set pattern, where it strictly adheres to the form , where is a single character, one of , , , or . here is a quick rundown of what each of these characters signify: : pdf or pmf : cdf or cmf : inverse cdf or cmf : random sampling from distribution given this piece of information, perhaps it’s unsurprising that the commands for the normal distribution are , , , and . let’s start with the first one on the list, . dnorm recall that the equation for a univariate standard normal distribution is given by if you plug in x = 0 into this equation, you will see that the value returned by the function, which is simply the normalizing constant, is indeed approximately 0.39. in short, represents the pdf of the normal distribution. the next on the list is , which we already know models the gaussian cdf. this can easily be verified by the fact that pnorm this is expected behavior, since corresponds to the exact mid point of the gaussian pdf. z_scores as a bite sized exercise, let’s try to take a look at the empirical rule of the normal distribution, namely that values within one standard deviation from the mean cover roughly 68% of the entire distribution. pnorm pnorm we could have also used the argument, which defines in which direction we calcalate the cdf. if is set to , then the function returns the integral from to infinity of the pdf of the normal distribution. 1 pnorm pnorm the is best understood as the inverse cdf function. this means that the function would receive as input the value of the area under the function, which can also be interpreted as the score. qnorm we can directly verify the fact that is an inverse of by pluggin in a value. pnorm) last but not least, the function can be used to sample values from the normal distribution with the specified parameters. let’s start by sampling 10 values from the standard normal distribution. rnorm we can also set the seed to make sure that results are replicable. the command does not return anything; it merely sets the seed for the current thread. set.seed poisson distribution ==================== recall that a poisson distribution is used to model the probability of having some number of events occuring within a window of unit time given some rate parameter λ. suppose that the phenomenon we’re modeling has an average rate of 7. if we want to know the probability , we can use the function to calculate the pmf: lambda it’s certainly not exactly a gaussian, but at least it does not unimodal and symmetric. if lln and clt is true, then we already know the mean and variance of this normal distribution: the mean is simply the rate parameter, 7, and the variance can be calculated as let’s see if these are indeed true. first, we can verify the mean of the sample means via mean that is sure enough close to 7, as we would expect. then, there’s variance: sd^2 lambda/n and indeed, it seems like the results match. note that the second calculation is based on the results in . note that in specifying , we didn’t have to specify a range by creating a or using slicing; instead, r is able to understand that is a variable. hist curve), add=t) the result seems to quite strongly vindicate clt and lln, as expected. conclusion ========== today’s post introduced some very useful functions relating to probability distributions. the more i dive into r, the more i’m amazed by how powerful r is as a statistical computing language. while i’m still trying to wrap my head around some of r’s quirky syntax , but this minor foible is quickly offset by the fact that it offers powerful vectorization, simulating, and sampling features. i love , but r just seems to do a bit better in some respects. in the next post, we will be taking a look at things like the t distribution and hypothesis testing. stay tuned for more!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
2020-04-11-fisher,"fisher's information is an interesting concept that connects many of the dots that we have explored so far: maximum likelihood estimation, gradient, jacobian, and the hessian, to name just a few. when i first came across fisher's matrix a few months ago, i lacked the mathematical foundation to fully comprehend what it was. i'm still far from reaching that level of knowledge, but i thought i'd take a jab at it nonetheless. after all, i realized that sitting down to write a blog post about some concept forces me to study more, so it is a positive, self reinforcing cycle. let's begin. fisher's score function is deeply related to maximum likelihood estimation. in fact, it's something that we already know we just haven't defined it explicitly as fisher's score before. first, we begin with the definition of the likelihood function. assume some dataset where each observation is identically and independently distributed according to a true underlying distribution parametrized by . given this probability density function , we can write the likelihood function as follows: while it is sometimes the convention that the likelihood function be denoted as , we opt for an alternative notation to reserve for the loss function. to continue, we know that the maximum likelihood estimate of the distribution's parameter is given by this is the standard drill we already know. the next step, as we all know, is to take the derivative of the term in the argument maxima, set it equal to zero, and voila! we have found the maximum likelihood estimate of the parameter. a quick aside that may become later is the fact that maximizing the likelihood amounts to minimizing the loss function. now here comes the definition of fisher's score function, which really is nothing more than what we've done above: it's just the gradient of the log likelihood function. in other words, we have already been implicitly using fisher's score to find the maximum of the likelihood function all along, just without explicitly using the term. fisher's score is simply the gradient or the derivative of the log likelihood function, which means that setting the score equal to zero gives us the maximum likelihood estimate of the parameter. an important characteristic to note about fisher's score is the fact that the score evaluated the true value of the parameter equals zero. concretely, this means that given a true parameter , this might seem deceptively obvious: after all, the whole point of fisher's score and maximum likelihood estimation is to find a parameter value that would set the gradient equal to zero. this is exactly what i had thought, but there are subtle intricacies taking place here that deserves our attention. so let's hash out exactly why the expectation of the score with respect to the true underlying distribution is zero. to begin, let's write out the full expression of the expectation in integral form. if we evaluate this integral at the true parameter, i.e. when , the key part of this derivation is the use of the leibniz rule, or sometimes known as feynman's technique or differentiation under the integral sign. i am most definitely going to write a post detailing in intuitive explanation behind why this operation makes sense in the future, but to prevent unnecessary divergence, for now it suffices to use that rule to show that the expected value of fisher's score is zero at the true parameter. things start to get a little more interesting as we move onto the discussion of fisher's information matrix. there are two sides of the coin that we will consider in this discussion: fisher's information as understood as the covariance matrix of the score function, and fisher's information as understood as a hessian of the negative log likelihood. the gist of it is that there are two different ways of understanding the same concept, and that they provide intriguing complementary views on the information matrix. before jumping into anything else, perhaps it's instructive to review variance, covariance, and the covariance matrix. here is a little cheat sheet to help you out . an intuitive way to think about variance is to consider it as a measure of how far samples are from the mean. we square that quantity to prevent negative values from canceling out positive ones. covariance is just an extension of this concept applied to a comparison of two random variables instead of one. here, we consider how two variables move in tandem. and the variance covariance matrix is simply a matrix that contains information on the covariance of multiple random variables in a neat, compact matrix form. a closed form expression for the covariance matrix given a random vector , which follows immediately from aforementioned definitions and some linear algebra, looks as follows: enough of the prologue and review, now we're ready to start talking about fisher. the information matrix is defined as the covariance matrix of the score function as a random vector. concretely, note that the 0's follow straight from the earlier observation that . intuitively, fisher's information gives us an estimate of how certain we are about the estimate of the parameter . this can be seen by recognizing the apparent similarity between the definition of the covariance matrix we have defined above and the definition of fisher's information. in fact, the variance of the parameter is explained by the inverse of fisher's information matrix, and this concept is known as the cramer rao lower bound. for the purposes of this post, i won't get deep into what crlb is, but there are interesting connections we can make between fisher's information, crlb, and the likelihood, which we will get into later. because fisher's information requires computing the expectation given some probability distribution, it is often intractable. therefore, given some dataset, often times we use the empirical fisher as a drop in substitute for fisher's information. the empirical fisher is defined quite simply as follows: in other words, it is simply an unweighted average of the covariance of the score function for each observed data point. although this is a subtlety, it helps to clarify nonetheless. something that may not be immediately apparent yet nonetheless true and very important about fisher's information is the fact that it is the negative expected value of the second derivative of the log likelihood. in our multivariate context where is a vector, the second derivative is effectively the hessian. in other words, you might be wondering how the information matrix can be defined in two says, the covariance and the hessian. indeed, this threw me off quite a bit as well, and i struggled to find and understand a good resource that explained why this was the case. thankfully, mark reid's blog and an mit lecture contained some very helpful pointers that got me a long way. the derivation is not the easiest, but i'll try to provide a concise version based on my admittedly limited understanding of this topic. let's start from some trivially obvious statements. first, from the definition of a pdf and the derivative operation, we know that therefore, both the first and second derivative of this function are going to be zero. in multivariate speak, both the gradient and the hessian are zero vectors and matrices, respectively. using the leibniz rule we saw earlier, we can interchange the derivative and come up with the following expressions. granted, these expressions somewhat muffle the shape of the quantity we are dealing with, namely vectors and matrices, but it is concise and intuitive enough for our purposes. with these statements in mind, let's now begin the derivation by first taking a look at the hessian of the score function. from the chain rule, we know that this does not look good at all. however, let's not fall into despair, since our goal is not to calculate the second derivative or the hessian itself, but rather its negative expected value. in calculating the expected value, we will be using integrals, which is where the seemingly trivial statements we established earlier come in handy. by linearity of expectation, we can split this expectation up into two pieces. let's use integrals to express the first expectation. the good news is that now we see terms canceling out each other. moreover, from the leibniz rule and the interchanging of the integral and the derivative, we have shown that the integral in fact evaluates to zero. this ultimately leaves us with therefore we have established that and we're done! in this post, we took a look at fisher's score and the information matrix. there are a lot of concepts that we can build on from here, such as cramer rao's lower bound or natural gradient descent, both of which are interesting concepts at the intersection of machine learning and statistics. although the derivation is by no means mathematically robust, it nonetheless vindicates a notion that is not necessary apparently obvious, yet makes a lot of intuitive sense in hindsight. i personally found this video by ben lambert to be particularly helpful in understanding the connection between likelihood and information. the gist of it is simple: if we consider the hessian or the second derivative to be indicative of the curvature of the likelihood function, the variance of our estimate of the optimal parameter would be larger if the curvature was smaller, and vice versa. in a sense, the larger the value of the information matrix, the more certain we are about the estimate, and thus the more information we know about the parameter. i hope you enjoyed reading this post. catch you up on another post, most likely on the leibniz rule, then natural gradient descent!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
2020-04-01-stirling,"it’s about time that we go back to the old themes again. when i first started this blog, i briefly dabbled in real analysis via euler, with a particular focus on factorials, interpolation, and the beta function. i decided to go a bit retro and revisit these motifs in today’s post, by introducing stirling’s approximation of the factorial. there are many variants of stirling’s approximation, but here we introduce the general form as shown: let’s begin the derivation by first recalling the poisson distribution. the poisson distribution is used to model the probability that a certain event occurs a specified number of times within a defined time interval given the rate at which these events occur. the formula looks as follows: one interesting fact about the poisson distribution is that, when the parameter is sufficiently large, the poisson approximates the gaussian distribution whose mean and variance are both . this happens when the random variable . we can easily simplify since the power of the exponent is zero. thus, we have by simply rearranging , we arrive at stirling’s approximation of the factorial: this is cool, but we still haven’t really shown why a poisson can be used to approximate a gaussian after all, this premise was the bulk of this demonstration. to see the intuition behind this approximation, it is constructive to consider what happens when we add independent poisson random variables. say we have and , both of which are independent poisson random variables with mean and . then, will be a new poisson random variable with mean . if we extend this idea to apply to independent random variables instead of just two, we can conclude that collection of independent random variables from to sampled from a population of mean will have mean . and by the nature of the poisson distribution, the same goes for variance . the central limit theorem then tells us that the distribution of the sum of these random variables will approximate a normal distribution. this concludes a rough proof of the stirling approximation. for those of you who are feeling rusty on the poisson distribution as i was, here is a simple explanation on the poisson specifically, its mean and variance. by the virtue of the definition of the parameter, it should be fairly clear why : is a rate parameter that indicates how many events occur within a window of unit time. the expected calculation can easily be shown using taylor expansion: next, we prove that the variance of a poisson random variable defined by parameter is equal to . let be a poisson random variable. then, then, using the definition of variance, we know that from this, we are once again reminded of the defining property of the poisson, which is that both the mean and variance of a poisson random variable is defined by the parameter . let's tie this back to our original discussion of the central limit theorem. clt states that, even if a distribution of a random variable is not normal, the distribution of the sums of these random variables will approximate a normal distribution. using this handy property on the independent and identically distributed poisson random variables of mean and variance , we can see how the sum of these random variables approximates a gaussian distribution . hence the stirling approximation!",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
2020-06-12-gibbs-sampling,"it's been a while since i have posted anything about math or statistics related, and i'll admit that i've been taking a brief break from these domains, instead working on some personal projects and uping my python coding skills. this post is going to be a fun, exciting mesh of some python and math. without further ado, let's get started. i remember struggling to understand metropolis hastings a while back. gibbs sampling, on the other hand, came somewhat very naturally and intuitively to me. this is not because i've suddenly grown intelligent over the past couple of months, but because gibbs sampling is conceptually simpler, at least in my humble opinion. all that is necessary to understand gibbs sampling is the notion of conditional probability distributions. we know the classic context in which mcmc comes into play in a bayesian setting: there is some intractable distribution that we wish to sample from. metropolis hastings was one simple way to go about this, and gibbs sampling provides another method. a feature that makes gibbs sampling unique is its restrictive context. in order to use gibbs sampling, we need to have access to information regarding the conditional probabilities of the distribution we seek to sample from. in other words, say we want to sample from some joint probability distribution number of random variables. let's denote this distribution as follows: turns out that the gibbs sampler is a more specific version of the metropolis hastings algorithm. we can only use the gibbs sampler in a restricted context: namely, that we have access to conditional probability distributions. you quickly see why the gibbs sampler can only be used in limited contexts. nonetheless, when these set of information are available, it is a powerful algorithm with which we can sample from intractable distributions. let's see how this works. the gist of the gibbs sampler is simple: sample from known conditional distributions, and use that resulting value to sample the next random variable from the following conditional probability distribution, ad infinitum. but this is just a lot of words and some needless latin for fun and flair, so let's hash out what the sentence really means. continuing on from our generic example, let's say we sampled a value from the first conditional probability distribution. we will use a superscript and subscript notation to each denote the iteration and the sequence of random variable. assume that we start from some random dimensional vector to start with. following our notation, this vector would be the superscripts are all 0 since this is the first ""sample"" we will start off with. theoretically, it doesn't matter what these random numbers are asymptotically speaking, we should still be able to approximate the final distribution, especially if given the fact that we take burn in into account. on the first iteration, we will begin by sampling from the first probability distribution. note that we simply used the initial random values for through to sample the first value from a conditional probability distribution. now, we do the same to sample . only this time, we can use the result from earlier, namely . we can see how this might help us yield a slightly more convincing result than simply using the random data. we still have to use random values for through since we haven't sampled from their relevant conditional distributions just yet. however, as we go through all the random variables in order, it becomes obvious that we will no longer be using randomly initialized values at one point. specifically, on the th iteration, we would expect something like this to happen: can be any number between 1 and , since it is used to represent the th random variable. as we repeat more iterations of sampling, we will eventually end up with a plausible representation of dimensional vectors, which is what we sought to sample from the intractable distribution! in this section, we will take a look at a very simple example, namely sampling from a bivariate gaussian distribution. although we have dealt with dimensional examples in the algorithm analysis above, for the sake of demonstration, let's work on a simple example that we can also easily visualize and intuit. for this reason, the bivariate gaussian distribution is a sensible choice. for this post, i'll be using , which is a data visualization library built on top of . i'll simply be using to display a bivariate gaussian. for reproducibility's sake, we will also set a random seed. the code for the gibbs sampler is simple, partially because the distribution we are dealing with is a bivariate gaussian, not some high dimensional intractable distribution. this point notwithstanding, the function shows the gist of how gibbs sampling works. here, we pass in parameters for the conditional distribution, and start sampling given an initial value corresponding to . as stated earlier, this random value can be chosen arbitrarily. of course, if we start from a value that is way off, it will take much longer for the algorithm to converge, i.e. we will have to discard a large portion of initially sampled values. this is known as burn in. in this case, however, we will apply a quick hack and start from a plausible value to begin with, reducing the need for burn in. we then take turns sampling from the conditional probability distributions using the sampled values, and append to a list to accumulate the result. note that the two functions are symmetrical, which is expected given that this is a bivariate distribution. these functions simulate a conditional distribution, where given a value of one random variable, we can sample the value of the other. this is the core mechanism by which we will be sampling from the joint probability distribution using the gibbs sampling algorithm. let's initialize the parameters for the distribution and test the sampler. 5.094375689072134, 5.058667902942396, 5.175861934887288, 5.447651414116084, 5.358397131507042, 5.278071396535993, 5.550314691828798, 5.641095821184971, 5.487786105738833, 5.542093903446283 great! this works as expected. for the purposes of demonstrating the implications of burn in, let's define discard the first 100 values that were sampled. below is the plot of the final resulting distribution based on sampled values using the gibbs sampler. the result is what we would expect: a bivariate gaussian. and this is what we end up with if we sample directly from the bivariate gaussian instead of using the gibbs sampler. note that we can do this only because we chose a deliberately simple example; in many other contexts, this would certainly not be the case . notice the similarity between the result achieved by sampling from the gibbs sampler and the result produced from direct sampling as shown below. so at this point, we have now empirically checked that gibbs sampling indeed works: even if we can't directly sample from the distribution, if we have access to conditional distributions, we can still achieve an asymptotically similar result. now comes the mathematics portion of deriving the conditional distribution of a multivariate gaussian, as promised earlier. in this section, we will derive an expression for the conditional distribution of the multivariate gaussian. this isn't really relevant to the gibbs sampling algorithm itself, since the sampler can be used in non gaussian contexts as long as we have access to conditional distributions. nonetheless, deriving this is a good mental exercise that merits some discussion. just for the sake of quick review, let's briefly revisit the familiar definition of a conditional probability: in the context of random vectors, we can rewrite this as of course, if and are scalars, we go back to the familiar bivariate context of our example. in short, deriving the expression for the conditional distribution simply amounts to simplifying the fraction whose denominator is the marginal distribution and the numerator is the joint distribution. let's clarify the setup and notation first. we define a dimensional random vector that follows a multivariate gaussian distribution, namely . this vector, denoted as , can be split into a dimensional vector and dimensional vector in the following fashion: it is apparent that . similarly, we can split up the covariance matrix in the following fashion where , , . also, given the symmetric property of the covariance matrix, . the goal is to derive is the conditional distribution, . this derivation was heavily adapted from this source and this thread on stack exchange. it is certainly a somewhat lengthy derivation, but there is nothing too conceptually difficult involved it's just a lot of algebra and simplifications. we begin from the formula for the multivariate gaussian: for convenience purposes, let then, let note that this is not a one to one correspondence, i.e. . the blocks are only one to one insofar as being dimensionally equivalent. then, using block matrix multiplication, notice that the final result should be a single scalar given the dimensions of each matrix. therefore, we can further simply the expression above using the fact that . specifically, the second and third terms are transposes of each other. although we simply resorted a convenient substitution in , we still need to derive an expression for the inverse of the covariance matrix. note that the inverse of the covariance matrix can intuitively be understood as the precision matrix. we won't derive the block matrix inversion formula here. the derivation is just a matter of simply plugging in and substituting one expression for another. for a detailed full derivation, checkout this link or this journal article. to cut to the chase, we end up with plugging these results back into , and with some elided simplification steps, we end up with note that we can more conveniently express the result in the following fashion: we're now almost done. heuristically, we know that the addition in will become a multiplication when plugged back into the original formula for the multivariate gaussian as shown in , using . therefore, if we divide the entire expression by , we will only end up with the term produced by . using this heuristic, we conclude that notice that this result is exactly what we have in the function which we used to sample from the conditional distribution. the gibbs sampler is another very interesting algorithm we can use to sample from complicated, intractable distributions. although the use case of the gibbs sampler is somewhat limited due to the fact that we need to be able to access the conditional distribution, it is a powerful algorithm nonetheless. we also discussed the notion of conditional distributions of the multivariate gaussian in this post. the derivation was not the simplest, and granted we omitted a lot of algebra along the way, but it was a good mental exercise nonetheless. if you are interested in a simpler proof, i highly recommend that you check out the stack exchange post i linked above. i hope you enjoyed reading this post. catch you up in the next one!",0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
2020-03-19-fourier,"taylor series is used in countless areas of mathematics and sciences. it is a handy little tool in the mathematicians arsenal that allows us to decompose any function into a series of polynomials, which are fairly easy to work with. today, we are going to take a brief look at another type of series expansion, known as fourier series. note that these concepts are my annotations of professor gilbert strang's amazing lecture, available on youtube. the biggest difference between taylor series and fourier series is that, unlike taylor series, whose basic fundamental unit is a polynomial term, the building block of a fourier series is a trigonometric function, namely one of either sine or cosine. concretely, a generic formula of a fourier expansion looks as follows: personally, i found this formula to be more difficult to intuit than the taylor series. however, once you understand the underlying mechanics, it’s fascinating to see how periodic wave functions can be decomposed as such. first, let’s begin with an analysis of orthogonality. commonly, we define to vectors and as being orthogonal if that is, if their dot product yields zero. this follows from the definition of a dot product, which has to do with cosines. with a stretch of imagination, we can extend this definition of orthogonality to the context of functions, not just vectors. for vectors, a dot product entails summing the element wise products of each component. functions don’t quite have a clearly defined, discrete component. therefore, instead of simply adding, we integrate over a given domain. for example, the same applies to cosines and sines: where and can be any integer. in other words, cosine functions of different frequencies are orthogonal to each other, as are cosines are with sines! now, why is orthogonality relevant at all for understanding the fourier series? it’s time to sit back and let the magic unfold when we multiply to and integrate the entire expression. if we divide both sides of by , you will realize that we have derived an expression for the constant corresponding to the expansion term: the key takeaway here is this: by exploiting orthogonality, we can knock out every term but one, the very term that we multiplied to the expansion. by the same token, therefore, we can deduce that we can do the same for the sine terms: the only small caveat is that the case is a bit more specific for . when , reduces to a constant of one, which is why we end up with instead of . in other words, hence, we end up with this exceptional term has a very intuitive interpretation: it is the average of the function over the domain of integration. indeed, if we were to perform some expansion, it makes intuitive sense that we start from an average. one observation to make about fourier expansion is the fact that it is a combination of sines and cosines and we have seen those before with, lo and behold, euler’s formula. recall that euler’s formula is a piece of magic that connects all the dots of mathematics. here is the familiar equation: using euler’s formula, we can formulate an alternative representation of fourier series: let’s unequivocally drive this concept home with a simple example involving the dirac delta function. the delta function is interesting function that looks like this: the delta function has two nice properties that make it great to work with. first, it integrates to one if the domain includes . this is the point where the graph peaks in the diagram. second, the delta function is even. this automatically tells us that when we perform a fourier expansion, we will have no sine functions sine functions are by nature odd. with this understanding in mind, let’s derive the fourier series of the dirac delta by starting with . the equality is due to the first property of the delta function outlined in the previous paragraph. the derivation of the rest of the constants can be done in a similar fashion. the trick is to use the fact that the delta function is zero in all domains but . therefore, the oscillations of will be nullified by the delta function in all but that one point, where is just one. therefore, simply reduces to integrating the delta function itself, which is also one! to sum up, we have the following: i find it fascinating to see how a function so singular and unusual as the dirac delta can be reduced to a summation of cosines, which are curvy, oscillating harmonics. this is the beauty of expansion techniques like the fourier and taylor: as counterintuitive as it may seem, these tools tell us that any function can be approximated through an infinite summation, even if the original function may not resemble the building block of the expansion technique at all at a glance.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
2020-05-22-r-tutorial-4,"in this post, we will continue our journey down the r road to take a deeper dive into data frames. r is great for data analysis and wranging when it comes to dealing with tabular data, especially thanks to the package, which is r’s equivalent of python’s . setup ===== let’s begin by loading . library we will also be using the package, which is a dataset documenting all flights departing new york city in 2013. library lets take a look at what this dataset looks like. head on a quick side note, i’ve recently realized that we can also use the pipe operator from . this is somewhat similar to how pipes work in unix. for example, instead of , we can also do flights %% head i’ve found that some people refer to use this pipe operator when dealing with , because doing so arguably improves code readability by separating out the dataset from the rest of the arguments of the function. with this note in mind, let’s jump into . basic operations ================ in this section, we will take a look at some basic operations we can perform on data frames, namely , , , , and . if you are familar with sql or , the semantics of some of this words might come a bit more naturally. but even if they don’t, worry not; we will go through each function one by one and get our hands dirty. filter as the name implies, literally filters the data frame according to some condition. this is similar to how filter works in other languages. let’s take a look at a python example. to run python code in r notebooks , we can import the package. library nums = list) odd_nums = list) odd_nums here, we have filtered the list according to the simple function defined in line. this is essentially how works in as well. let’s take a look at an example. say we want to only look at flights scheduled on january 1st. then, we can do flights %% filter %% head in a simple example like this, using pipe operators may not be necessary, but one advatnage is that we can avoid the use of nested functions. we can also avoid the use of creating intermediary local variables. let’s continue our discussion of the function. we can exploit the full powers of by using it in conjunction with various logical operators. for example, if we want to retrieve the list of flights that occurred in either january or feburary, we can do flights %% filter the translates to “or.” we also have things like , with stands for “and,” , which stands for “not,” and , which stands for the exclusive or. one refinement we can add to the statement above is the use of . flights %% filter) to check for ranges, we can use the function. for example, flights %% filter) %% head here, we only filter those flights whose departing time was between 6 and 7 in the morning. as one last example, let’s filter through the data frame and try to see which entries have missing values for . to do this, we can use . flights %% filter) %% nrow we see that there are a total of 8255 rows whose column is missing. select is another very useful function for retrieving information from a data frame. if a voice in your head starts whispering sql, well, that’s sort of the right idea. the function, as you might expect, literally selects columns from a data frame, much like the statement in sql does the same. of course, we can also add conditions for selection, similar to how works in sql. let’s get more concrete by taking a look at an example. flights %% select %% head here, we have selected three columns, , , and , from the data frame. we can also make use of slicing for selection, using the and the syntax. flights %% select) %% head here, we have selected every column except the ones between to , inclusive. the power of truly comes into light when we use in conjunction with other helper functions, such as , , or . this is somewhat similar to what sql offers with the keyword. for example, select col_a, col_b from some_table where col_c like 'a%' would give us data points from where the entry starts with the character . this directly corresponds to the helper function. mutate, transmute another useful function to have under our belt is the ability to create new columns using existing columns in the data frame. for example, we might want to calculate a new variable, , as follows: speed = distance / air_time 60 an easy way to achieve this is to use . let’s see this in action. flights %% mutate %% select %% head here, we first created a new column, named , using the formula delineated above, then selected that specific column and displayed the first five entries. note that is not an in place operation. instead, it creates a copy. therefore, in order to save the results, we must store it to a new data frame object. new_flight % mutate head what if we want to get only the newly created column, instead of appending it to the entire copy of the data frame? in other words, is there a more elegant way of doing things instead of applying after as we have done in the example above? well, this is exactly what is for. flights %% transmute %% head this gives us the same result as applying a after , and indeed it is much more concise and readable. while does not really add expressive power, it is a convenient function to have nonetheless. arrange in sql speak, is r’s way of ordering entries in ascending or descending order. in , we can achieve the same result using . let’s take a look. flights %% arrange %% head because we arranged, or sorted, the data frame in the order of , , and , the first entries we get are from january 1st of 2013. note that by default, sorts entries in ascending order. to do things in descending order, we need to wrap column variables around , as in , for instance. summarize is a very useful function that collapses contents on the data frame into a single row. quite aptly, it provides a nice way to summarize the data. for example, if we are interested the mean of the column, we might do flights %% summarize) here, we have calculated the mean of and labeled it as . we toggle since does contain null entries. aside from there are several functions can come in handy. here is a non exhaustive list: : mean absolute deviation for example, let’s see how works. flights %% count %% head this is functionally equivalent to flights %% group_by %% summarize) %% head this provides a nice segue into , which we have just seen in the example above. group by is useful, but it is pretty boring when used alone. instead, we might want to use it in conjunction with , which is another very powerful function in . if you come from a or sql background, you might already be familiar with what does: as the name implies, the operation groups the data frame according to some axis or column dimension. this is useful because now we can apply operations like calculating the mean on these groups individuall, then get an aggregated result. for instance, delay_by_month % group_by %% summarize) delay_by_month now we get a nice summary of the data set, namely the mean delay time for flights each month. here, we might proceed with some visualization. here is a quick review. ggplot + geom_bar, stat = ""identity"") it seems like the most delays happen in the summer and the winter. we can’t be sure with just this data, and we certainly shouldn’t be jumping to any conclusions, but one plausible hypothesis might be that people tend to go on vacation trips during these months, possibly leading to more delays as more flights are in operation. one useful note to mention is the fact that grouping by multiple variables, then applying a summary effectively peals off one layer of the axis by which the data frame is grouped. this is a mouthful, but let’s see what this means with an example. per_day % group_by %% summarize) head if we apply another summary on this data frame, we obtain per_month % summarize) head notice that the column is now gone, and instead we have a data frame grouped by and only. then, it won’t come as a surprise that re applying this step once more would result in : per_year % summarize) head in this case, the result is uninteresting because the dataset only pertained to flights that occurred in 2013 to begin with. but if we had more than one year, then we would expect the results to have shown up here. is not the only function that works well with . in fact, we can use it on any function we have learned so far. for example, here is an example with . flights %% group_by %% filter 200) this returns a data frame containing entries for only popular destinations. here is another example, this time in combination with , , and . flights %% filter %% mutate) %% select %% head as can be witnessed in these examples, offers a powerful way of organizing data, especially when combined with different operators. pipeline workflow ================= let’s take a look at an example from r4ds, which is the task of exploring the relationship between distance and the average delay for each flight destination. here is one way we might go about it using the pipeline operator and the functions we have learned so far. dist_delay % group_by %% summarize, mean_dist = mean, mean_delay = mean ) %% filter head now that we have the data ready, let’s try plotting it. ggplot) + geom_point, alpha = 1/3) + geom_smooth we’ve grouped the data set according to , then applied some summarize function to aggregate the data by mean, then filtered the results so that we only have destinations that had more than 20 flights in 2013.note that we chucked in a function we haven’t seen before, , which basically returns the number of rows in a data frame. because we applied a , the would give us the number of counts of entries for each destination. the aspect of the workflow should be familiar from the previous tutorial. we can also use in conjunction with by using pipe. for example, say we want to drill down on flight delay times. first, let’s begin by filtering entries with missing values. delays % filter, !is.na) %% group_by %% summarize, delay = mean ) head using this data frame, we can create a plot as follows: ggplot) + geom_point let’s get rid of some outliers with only a few counts, as they skew the y axis. delays %% filter %% ggplot) + geom_point see how we were able to use data frame manipulation with directly: we were able to elide the argument because the resulting data frame was directly piped into the function. conclusion ========== r4ds chapter 3 contained a lot of dense information, but the basics of using pipelines, selecting, mutating, fitering, and group by’s on data frames are no doubt useful skills that will come in handy in future tutorials. i personally don’t think it is necessary to digest everything that is in the book nor what is written here: instead, it’s important to see the philosopy of r and . it’s always fun to learn new syntax of a language, and i think that’s part of the reason why i’ve enjoyed writing this post despite the density of the material presented. i hope you enjoyed reading this post. in the next post, we’ll probably discuss the how to’s of exploratory data analysis, thus putting our skills to the test. see you in the next one!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
2020-07-05-tf-idf,"although i've been able to automate some portion of the blog workflow, there's always been a challenging part that i wanted to further automate myself using deep learning: automatic tagging and categorization. every post requires some form of yaml front matter, containing information such as the title, tag, and category of the blog post to be uploaded. although i sometimes create new tags or categories if existing ones seem unfit, i only deal with a limited number of topics on this blog, which is why i've always thought that some form of supervised learning be able to automate the process by at least generating some possible tags for me. i'm currently in the process of preparing the data and building a simple nlp document classification model for the job. however, nlp is a field that i'm admittedly not well acquainted with, not to mention the fact that i've not bee posting a lot about deep learning implementations for a while now. so in today's short post, i decided to write about tf idf vectorization, which is a very simple yet powerful technique that is often used in routine tasks like document classification, where sota models aren't really required. as always, this post is going to take a hands on approach by demonstrating a simple way of implementing tf idf vectorization from scratch. let's get started. tf idf stands for term frequency inverse document frequency. this is all there is to it in fact, the formula for tf idf can simply be expressed as where denotes a single term; , a singe document, and , a collection of documents. so simply put, tf idf is simply a product of the term frequency, denoted above as , and inverse document frequency, . all there is left, then, is to figure out what term frequency and inverse document frequency are. without much explanation, you can probably guess what term frequency is: it simply indicates how frequently a word appeared in a given document. for example, if there were a total of 3 distinct words in a document , then each of the three words would have a tf score of . put differently, the sum of the tf vector for each document should sum to one. the definition of a tf score might be thus expressed as where the denominator denotes the count of all occurrences of the term in document , and the numerator represents the total number of terms in the document. roughly speaking, inverse document frequency is simply the reciprocal of document frequency. therefore, it suffices to show what document frequency is, since idf would immediately follow from df. before getting into the formula, i think it's instructive to consider the motivation behind tf idf, and in particular what role idf plays in the final score. the motivation behind tf idf commences from a simple question: how do we determine the semantic importance of a word in a set of documents? on one hand, words the appear a lot are probably worth paying attention to. for example, in one of my posts on gaussian distributions, the word ""gaussian"" probably appears many times throughout the post. a keyword probably appears frequently in the document; hence the need to calculate tf. on the other hand, there might be words that appear a lot, but aren't really that important at all. for example, consider the word ""denote."" i know that i use this word a lot before writing down equations or formulas, just for the sake of notational clarity. however, the word itself carries little information on what the post is about. the same goes for other words, such as ""example,"" ""however,"" and so on. so term frequency only doesn't really tell us much; instead, we want to pay attention to words that occur frequently in a given document, but doesn't appear a lot in others such words are most likely to be unique keywords that potentially capture the gist of that document. given this analysis, it isn't difficult to see why tf idf is designed the way it is. although we give priority weight to words with high term frequency, we discount words that appear frequently across all documents by dividing tf by idf, or inverse document frequency. in short, document frequency tells us how frequently a given word appears throughout all documents; the inverse is the reciprocal of that quantity. however, this is a mere technically; the intuition we motivated earlier still applies regardless. with these ideas in mind, let's go implement tf idf vectorization in python! in this section, we will develop a simple set of methods to convert a set of raw documents to tf idf vectors, using a dummy dataset. below are four documents that we will be using throughout this tutorial. the first step is to preprocess and tokenize the data. although the specifics of preprocessing would probably differ from task to task, in this simple example, we simply remove all punctuations, change documents to lower case letters, and tokenize them by breaking down documents into a bag of words. other possible techniques not discussed here include stemming and lemmatization. the function accepts as input a set of documents and removes all the punctuation in each document. here is the result of applying our function to the dummy data. 'tom plays soccer', 'tom loves basketball', 'basketball is his hobby', 'sarah loves basketball' next, we need to tokenize the strings by splitting them into words. in this process, we will also convert all documents to lower case as well. note that works on each documents, not the entire collection. let's try calling the function with the first document in our dummy example. 'tom', 'plays', 'soccer' finally, as part of the preprocessing step, let's build the corpus. the corpus simply refers to the entire set of words in the dataset. specifically for our purposes, the corpus will be a dictionary whose keys are the words and values are an ordinal index. another way to think about the corpus in this context is to consider it as a word to index mapping. we will be using the indices to represent each word in the tf, idf, and tf idf vectors later on in the tutorial. because we have a very simple example, our corpus only contains 9 words. this also means that our tf idf vectors for each document will also be a list of length 9. thus it isn't difficult to see how tf idf vectorization can result in extremely high dimensional matrices, which is why we often apply techniques such as lemmatization or pca on the final result. also note that published modules use sparse representations to minimize computational load, as we will later see with scikit learn. now it's time to implement the first step: calculating term frequency. in python, this simply amounts to looping through each document, creating a tf vector per iteration, and making sure that they are normalized as frequencies at the very end. in creating tf vectors for each document, we will be referencing the word to index mapping in our corpus. let's see what we get for the four documents in our dummy example. 0.0, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.0 0.25, 0.0, 0.0, 0.25, 0.0, 0.25, 0.0, 0.0, 0.25 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0 due to floating point arithmetic, the decimals don't look the most pleasing to the eye, but it's clear that normalization has been performed as expected. also note that we get 4 vectors of length 9 each, as expected. next, it's time to implement the idf portion of the vectorization process. in order to calculate idf, we first need a total count of each number in the entire document collection. a module that is perfect for this job is , which accepts as input an iterable and outputs a dictionary like object whose values represent the count of each key. let's test in on our dummy dataset to see if we get the count of each tokenized word. counter this is precisely what we need to calculate idf. recall the formula for calculating idf as noted earlier, the intuition behind idf was that important keywords probably appear only in specific relevant documents, whereas generic words of comparatively lesser importance appear throughout all documents. we transcribe into code as follows: now, we have the idf vectors for the nine terms in the dummy dataset. 1.916290731874155, 1.916290731874155, 1.916290731874155, 1.2231435513142097, 1.916290731874155, 1.916290731874155, 1.5108256237659907, 1.5108256237659907, 1.916290731874155 at this point, all there is left to do is to multiply the term frequencies with their corresponding idf scores. this is extremely easy, since we are essentially performing a dot product of the tf and idf vectors for each document. as a final step, we normalize the result to ensure that longer documents do not overshadow shorter ones. normalizing is pretty simple, so we'll assume that we have a function that does the job for now. before we test the code, we obviously need to implement . this can simply done by obtaining the sum of the l2 norm of each vector, then dividing each element by that constant. here is an easy contrived example we can do in our heads: 0.6, 0.8 and now we're done! if let's print the tf idf vectors for each of the four documents in the dummy example. 0.0, 0.617614370975602, 0.0, 0.0, 0.617614370975602, 0.0, 0.0, 0.48693426407352264, 0.0 0.0, 0.0, 0.0, 0.496816117482646, 0.0, 0.0, 0.6136667440107332, 0.6136667440107332, 0.0 0.5417361046803605, 0.0, 0.0, 0.3457831381910465, 0.0, 0.5417361046803605, 0.0, 0.0, 0.5417361046803605 0.0, 0.0, 0.7020348194149619, 0.4480997313625987, 0.0, 0.0, 0.5534923152870045, 0.0, 0.0 it seems about right, as all the vectors appear normalized and are of the desired dimensions. however, to really verify the result, it's probably a good idea to pit our algorithm against scikit learn's implementation. in scikit learn, the does all the job. to transform the data to tf idf vectors, we need to create an instance of the and call its method, . and here are the results: 0. 0. 0. 0. 0. 0.61761437 0. 0.61761437 0.48693426 0.49681612 0. 0. 0. 0.61366674 0. 0. 0. 0.61366674 0.34578314 0.5417361 0.5417361 0.5417361 0. 0. 0. 0. 0. 0.44809973 0. 0. 0. 0.55349232 0. 0.70203482 0. 0. there are several observations to be made about this result. first, note that the default return type of is a sparse matrix. sparse matrices are a great choice since many of the entries of the matrix will be zero there is probably no document that contains every word in the corpus. therefore, sparse representations can save a lot of space and compute time. this is why we had to call on the result. second, you might be wondering why the order of elements are different. this is because the way we built ordinal indexing in corpus is probably different from how scikit learn implements it internally. this point notwithstanding, it's clear that the values of each vectors are identical, disregarding the fact that the result produced by our algorithm has more decimal points due to floating point arithmetic. this was a short introductory post on tf idf vectors. when i first heard about tf idf vectors from a friend studying computational linguistics, i was intimidated. however, now that i have a project i want to complete, namely an auto tagging and classification nlp model, i've mustered more courage and motivation to continue my study the basics of nlp. i hope you've enjoyed reading this post. catch you up in the next one!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0
2020-01-09-R-tutorial,"this is an experimental jupyter notebook written using . the purpose of this notebook is threefolds: first, to document my progress with self learning the r language; second, to test the functionality of the r kernel on jupyter; and third, to see if the shell script is capable of converting notebooks written in r to markdown format. the example codes in this notebook were borrowed from hands on programming with r by garrett grolemund. 2 'die' 1 6 6 5 6 1 0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 0.8 1 1 0.512 0.216 0.064 0.008 0 0.008 0.064 0.216 0.512 1 4 10 9 7 5 7 9 9 7 8 'integer' 21 true 4.44089209850063e 16 false 'logical' 'complex' null names 'face' 'suit' 'value' class 'data.frame' row.names 47 42 9 12 5 32 26 49 27 31 18 52 19 1 46 25 13 50 45 24 3 15 44 28 8 30 36 38 21 48 11 6 14 41 34 39 35 16 22 51 20 2 33 40 10 4 37 43 7 29 levels 'level 1' 'level 2' 'level 3' 1 ""b"" ""0"" ""0"" 0 0 b bb 0 0 var1var2 11 21 31 41 51 61 var1var2value 112 213 314 415 516 617 var1var2valueprob 1 1 2 0.015625 2 1 3 0.015625 3 1 4 0.015625 4 1 5 0.015625 5 1 6 0.015625 6 1 7 0.046875 8.25 var1var2var3 dd dd dd 7 dd dd bbbdd dd bb dd dd b dd dd c dd dd var1var2var3prob dd dd dd 0.000027 7 dd dd 0.000027 bbb dd dd 0.000054 bb dd dd 0.000090 b dd dd 0.000225 c dd dd 0.000009 1 1 ""my"" 1 ""first"" 1 ""for"" 1 ""loop"" 'loop' 'my' 'fourth' 'for' 'loop' 4 var1var2var3probprize dd dd dd 0.0000270.021600 7 dd dd 0.0000270.000000 bbb dd dd 0.0000540.000000 bb dd dd 0.0000900.000000 b dd dd 0.0002250.000000 c dd dd 0.0000090.000072 0.538014 0.934356 373 user system elapsed 0.599 0.017 0.620 user system elapsed 0.269 0.034 0.304 user system elapsed 0.518 0.035 0.555 user system elapsed 0.181 0.032 0.213 0 0 0 b 0 7 b 0 dd w1w2w3prize 7 b 0 0 b b bbb 5 dd bb b 10 user system elapsed 0.005 0.000 0.005",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
2020-02-08-rnn,"neural networks are powerful models that can be used to identify complex hidden patterns in data. there are many types of neural networks, two of which we have seen already on this blog: the vanilla, feed forward neural network and convolutional neural networks, often abbreviated as convnets. today, we will add a third kind to this exciting mix: recurrent neural networks, or rnns. let's take a brief conceptual look at how recurrent neural networks work, then implement a toy rnn to see how it compares to other models on the imdb movie reviews dataset. i heavily borrowed my examples from deep learning with python by françois chollet and the tutorial on text classification available from the official tensorflow documentation. recurrent neural networks, as the name implies, refer to neural network models that contain some sort of internal looping structure that simulates a flow of information. a good way to conceptualize this loop is to think of something like loops, where a certain operation is performed repeatedly for a specified number of cycles. given these pieces of information, we might ask ourselves two questions. firstly, what does this looping operation involve? secondly, what is the purpose of having this loop in the first place? let's try to answer both questions in the following subsections. one of the most salient features of a recurrent neural network is that it is capable of emulating some primitive form of memory. why might we want to do this? well, take the example of reading a text. when we read, we don't process a given text at once in its totality; instead, we break them down into pieces, such as a word or a bag of words, and build our understanding based on information obtained from the previous sequence of text. in other words, processing information through reading is at best understood as a process of continuously receiving new information while retaining information obtained from the previous sequence. this is why recurrent neural network models are frequently employed in the context of natural language processing. but the applications of rnns extends beyond the domain of nlp. for example, say we are given a dataset of temperature recording of a city district. obviously, the structural integrity of that dataset is very important, i.e. we should not shuffle the datasets because making weather predictions requires us to understand temporal patterns. in predicting the weather 24 hours from today, data points pertaining to the last two days will be obviously much more important that those recorded a month ago. in such instances of time series analysis, recurrent neural networks perform better than other models we have looked at thus far. to better understand how rnns work, let's try to build a very simple recurrent neural network from scratch with . we will only implement forward propagation for the sake of simplicity, but with some matrix calculus, one can imagine how deriving the formula for back propagation will be possible. let's cut to the chase: rnns emulate memory by using the output from the previous sequence as an input to the next. perhaps writing this down in matrix notation might give you a better idea of what the statement above means. here is one way we might implement a very simple recurrent neural network. if the word ""recursion"" pops up into your mind, then you are on the right track. notice that in calculating , the output of the current sequence, the output of the previous sequence, is used. by using the output of the previous sequence, the recurrent neural network is able to ""remember"" some of the information that was produced by the previous input. granted, this is not exactly what memory is or how it works in the strictest sense, but we can see how some information from the previous input is trickling down to affect the computation of the current sequence of input data. note that i used for the example, but we can just about use any other activation function. here is one way we might implement this in python. although not necessary, i decided to opt for a class based implementation to make things look tight and nicer. let's create a class object to see that everything works properly. 6400 there is really not much point in seeing the output because the calculations are going to be based off of randomly generated data and randomly created weights, but perhaps there is something to be learned from the dimensionality of input and output data. first, note that our model accepts input of size . the dimensionality of the output is a little more tricky because of the option. what concatenate does is that it basically flattens all number of outputs into a single list. in this case, because we set the option to , we get a flattened list containing , or 6400 elements. the main takeaway is that recurrent neural networks can be used to implement some sort of memory functionality, which is useful when dealing with datasets where there exists some sort of sequential structure. one way to implement memory is by using the output of the previous sequence to define a variable, which is used to compute the next output as we have done above. now let's get down to business with the api. implementing a recurrent neural network is not so much different from building a simple feed forward or convolutional neural network: we simply import a rnn specific layer and arrange these layers to construct a working model. before we proceed any further, let's first import all necessary dependencies for this tutorial. as with any tutorial, we need to start by loading and preprocessing data. luckily, there isn't going to be much preprocessing involved since we will be using a dataset available from the library, the imbd movie reviews dataset. the dataset contains 25,000 movie reviews from imdb, labeled as either positive or negative. each review is encoded as a sequence of integers according to a consistent encoding scheme. in other words, each integer corresponds to a unique word in the vocabulary of the dataset. more specifically, the integer to which a word is mapped corresponds to the frequency with which the word appears, i.e. the word encoded as 10 corresponds to the 10th most frequent word in the data. we will apply some very basic preprocessing on the dataset so that we can feed it into our model. specifically, we will preprocess the dataset such that only a number of most frequently occurring words are considered. this step will weed out words that occur very infrequently, thus decreasing the amount of noise from the network's perspective. next, we will apply padding to the dataset so that all reviews are of length . this means that longer reviews will be truncated, whereas shorter reviews will be padded with zero entries. below is a sample code implementation of this process. let's load the data using the function after specifying necessary parameters. now that the data is here and ready to go, it's time to build our neural network. to spice things up a bit, let's create four different models and compare their performance. before we jump into that, however, we first need to understand what an embedding layer is, as it is key to natural language processing. simply put, an embedding layer is a layer that transforms words or integers that encode words into dense vectors. this is a necessary transformation since neural networks are incapable of dealing with non quantitative variables. why dense vectors, then? can't we simply use one hot encoding? that is a valid point, since one hot encoding is how we mostly deal with categorical variables in a dataset. however, one downside of this approach is that we end up with many sparse vectors. in other words, a lot of resources are wasted because the model now has to process vectors of thousands or millions of dimensions, depending on the vocabulary size. instead of using sparse vectors to represent each word, we can simply use a denser vector of smaller dimensions to encode our data. another advantage of this approach is that dense vectors can be used to encode semantic information. you might have heard of the famous example that ""king minus male equals queen minus female."" if we were to represent the words king, queen, male, and female as vectors, we can add and subtract vectors to represent and distill meaningful information. this vector based computation is the key to natural language processing with deep neural networks: by back propagating and adjusting the weights of our embedding layer, our model can eventually be trained to ""understand"" the meaning of words and their relationship with other words in the form of dense vectors. enough talking, let's use the embedding layer to build our neural networks, starting with the simple feed forward model. the feed forward neural network model will first have an embedding layer that processes input. then, the output of this embedding layer will be flattened to be passed onto a dense layer with one output transformed by the sigmoid activation function. we use the sigmoid function since we want to conduct a sentiment analysis of determining whether a given movie review is positive or negative. that wasn't so difficult. let's initialize our model by defining the model parameters , , and , then plot the model to see the structure of the network alongside the input and output dimensions of each layer. note that we defined to be 16, which means that each word is transformed into dense vectors living in 16 dimensions. by plotting the model, we can get a better idea of the layers that compose the model. the next model we will build is a simple recurrent neural network. this neural network is going to have an embedding layer, just like the previous model. however, instead of a dense layer, it will have two consecutive layers stacked on top of each other. the layer is essentially the implementation of the model we built earlier. let's take a look. we instantiate the model and take plot the network, just as we have done above. the model we built is, as the name shamelessly puts out, pretty simple. there are a lot more advanced recurrent neural networks that have complicated internal cell structures to better emulate human memory, in a sense. the biggest difference between a simple recurrent neural network and an lstm is that lstms have a unique parameter known as the carrier that encodes an additional layer of information about the state of the cell. i might write a separate post devoted to the intricacies of the lstm, but if you're an avid reader who's itching to know more about it right away, i highly recommend this excellent post by christiopher olah. for now, let's just say that lstms represent a huge improvement over conventional rnns, and that we can implement them in by simply calling the layer as shown below: because lstm layers take a lot longer to train than others, and because the representational capacity of a single lstm layer is higher than that of others, i decided to use only one lstm layer instead of two. let's initialize this model to take a better look. the last model we will create is a convnet, which we explored on this previous post on image classification. convolutional neural networks are great at identifying spatial patterns in data, which is why they also perform reasonably well in natural language processing. another huge advantage of convents over recurrent networks is that they took a lot lesser time and resources to train. this is why it is often a good idea to build a convent to establish a baseline performance metric. let's initialize the model with identical parameters and take a look at its internal structure. let's train all four models using the training data. for control our experiment, we will train all four models over the same , , and . there isn't much exciting here to look at it terms of code; it's just a matter of patience, waiting for the models to hopefully converge to a global minimum. for future reference, all training history is dumped in the object where corresponds to the model number. train on 20000 samples, validate on 5000 samples epoch 1/10 20000/20000 ============================== 2s 119us/sample loss: 0.2706 acc: 0.8922 val_loss: 0.3372 val_acc: 0.8562 epoch 2/10 20000/20000 ============================== 2s 121us/sample loss: 0.2434 acc: 0.9073 val_loss: 0.3483 val_acc: 0.8476 epoch 3/10 20000/20000 ============================== 2s 118us/sample loss: 0.2173 acc: 0.9212 val_loss: 0.3585 val_acc: 0.8454 epoch 4/10 20000/20000 ============================== 2s 116us/sample loss: 0.1920 acc: 0.9364 val_loss: 0.3904 val_acc: 0.8320 epoch 5/10 20000/20000 ============================== 2s 112us/sample loss: 0.1689 acc: 0.9477 val_loss: 0.3901 val_acc: 0.8308 epoch 6/10 20000/20000 ============================== 2s 109us/sample loss: 0.1472 acc: 0.9589 val_loss: 0.4091 val_acc: 0.8268 epoch 7/10 20000/20000 ============================== 2s 115us/sample loss: 0.1272 acc: 0.9687 val_loss: 0.4306 val_acc: 0.8214 epoch 8/10 20000/20000 ============================== 2s 114us/sample loss: 0.1098 acc: 0.9771 val_loss: 0.4550 val_acc: 0.8220 epoch 9/10 20000/20000 ============================== 2s 111us/sample loss: 0.0943 acc: 0.9829 val_loss: 0.4867 val_acc: 0.8150 epoch 10/10 20000/20000 ============================== 2s 113us/sample loss: 0.0799 acc: 0.9890 val_loss: 0.5074 val_acc: 0.8134 train on 20000 samples, validate on 5000 samples epoch 1/10 20000/20000 ============================== 53s 3ms/sample loss: 0.6252 acc: 0.6412 val_loss: 0.6472 val_acc: 0.6328 epoch 2/10 20000/20000 ============================== 53s 3ms/sample loss: 0.6286 acc: 0.6467 val_loss: 0.6108 val_acc: 0.6628 epoch 3/10 20000/20000 ============================== 52s 3ms/sample loss: 0.5302 acc: 0.7362 val_loss: 0.4725 val_acc: 0.7834 epoch 4/10 20000/20000 ============================== 52s 3ms/sample loss: 0.4340 acc: 0.8036 val_loss: 0.4519 val_acc: 0.7890 epoch 5/10 20000/20000 ============================== 52s 3ms/sample loss: 0.4045 acc: 0.8242 val_loss: 0.4529 val_acc: 0.8108 epoch 6/10 20000/20000 ============================== 53s 3ms/sample loss: 0.3885 acc: 0.8338 val_loss: 0.4481 val_acc: 0.7916 epoch 7/10 20000/20000 ============================== 52s 3ms/sample loss: 0.3751 acc: 0.8381 val_loss: 0.4470 val_acc: 0.7882 epoch 8/10 20000/20000 ============================== 52s 3ms/sample loss: 0.3443 acc: 0.8566 val_loss: 0.4582 val_acc: 0.8054 epoch 9/10 20000/20000 ============================== 52s 3ms/sample loss: 0.3310 acc: 0.8615 val_loss: 0.4757 val_acc: 0.8048 epoch 10/10 20000/20000 ============================== 52s 3ms/sample loss: 0.3146 acc: 0.8703 val_loss: 0.5022 val_acc: 0.7892 train on 20000 samples, validate on 5000 samples epoch 1/10 20000/20000 ============================== 85s 4ms/sample loss: 0.4972 acc: 0.7459 val_loss: 0.4147 val_acc: 0.8120 epoch 2/10 20000/20000 ============================== 85s 4ms/sample loss: 0.3874 acc: 0.8345 val_loss: 0.3996 val_acc: 0.8206 epoch 3/10 20000/20000 ============================== 85s 4ms/sample loss: 0.3639 acc: 0.8399 val_loss: 0.3948 val_acc: 0.8164 epoch 4/10 20000/20000 ============================== 84s 4ms/sample loss: 0.3483 acc: 0.8497 val_loss: 0.3963 val_acc: 0.8160 epoch 5/10 20000/20000 ============================== 84s 4ms/sample loss: 0.3371 acc: 0.8551 val_loss: 0.3870 val_acc: 0.8190 epoch 6/10 20000/20000 ============================== 85s 4ms/sample loss: 0.3271 acc: 0.8566 val_loss: 0.4170 val_acc: 0.8064 epoch 7/10 20000/20000 ============================== 84s 4ms/sample loss: 0.3173 acc: 0.8607 val_loss: 0.3956 val_acc: 0.8126 epoch 8/10 20000/20000 ============================== 84s 4ms/sample loss: 0.3049 acc: 0.8689 val_loss: 0.4180 val_acc: 0.8250 epoch 9/10 20000/20000 ============================== 85s 4ms/sample loss: 0.2996 acc: 0.8698 val_loss: 0.4207 val_acc: 0.8186 epoch 10/10 20000/20000 ============================== 84s 4ms/sample loss: 0.2892 acc: 0.8764 val_loss: 0.4177 val_acc: 0.8238 train on 20000 samples, validate on 5000 samples epoch 1/10 20000/20000 ============================== 8s 418us/sample loss: 0.6473 acc: 0.6470 val_loss: 0.5372 val_acc: 0.7466 epoch 2/10 20000/20000 ============================== 2s 106us/sample loss: 0.5667 acc: 0.7542 val_loss: 0.7262 val_acc: 0.5218 epoch 3/10 20000/20000 ============================== 2s 108us/sample loss: 0.5669 acc: 0.7245 val_loss: 0.5634 val_acc: 0.7370 epoch 4/10 20000/20000 ============================== 2s 107us/sample loss: 0.5552 acc: 0.7383 val_loss: 0.5574 val_acc: 0.7634 epoch 5/10 20000/20000 ============================== 2s 106us/sample loss: 0.4822 acc: 0.7847 val_loss: 0.5535 val_acc: 0.7614 epoch 6/10 20000/20000 ============================== 2s 115us/sample loss: 0.4619 acc: 0.8069 val_loss: 0.5387 val_acc: 0.7730 epoch 7/10 20000/20000 ============================== 2s 106us/sample loss: 0.4385 acc: 0.8191 val_loss: 0.5487 val_acc: 0.7762 epoch 8/10 20000/20000 ============================== 2s 110us/sample loss: 0.4413 acc: 0.8173 val_loss: 0.5513 val_acc: 0.7748 epoch 9/10 20000/20000 ============================== 2s 107us/sample loss: 0.4005 acc: 0.8329 val_loss: 0.6434 val_acc: 0.7672 epoch 10/10 20000/20000 ============================== 2s 107us/sample loss: 0.3908 acc: 0.8400 val_loss: 0.6809 val_acc: 0.7844 after a long time of waiting, the training is finally complete! if you are following this tutorial on your local workstation, please note that the time required for training may vary depending on your hardware configurations or the specification of our instance if you are using a cloud based platform like aws. none of our models reached the threshold of ninety percent accuracy, but they all managed to converge to some reasonable number, hovering around the high seventies to low eighties. let's test the performance of our models by using the and data, both of which none of our models have seen before. 25000/25000 ============================== 1s 48us/sample loss: 0.5968 acc: 0.7931 0.5968295060539246, 0.79312 25000/25000 ============================== 14s 562us/sample loss: 0.4948 acc: 0.7911 0.4948168795013428, 0.79112 25000/25000 ============================== 24s 946us/sample loss: 0.4092 acc: 0.8321 0.4091824066162109, 0.83212 25000/25000 ============================== 1s 42us/sample loss: 0.6494 acc: 0.7915 0.6493830096054077, 0.79152 based on the results, it looks like the lstm model performed best, beating other models by a small margin. at this point, we cannot conclude as to whether or not this marginal boost in performance is significant. judging this would not only depend on the context, but also most likely require us to have a larger test dataset that captures the statistics of the population data. this point notwithstanding, it is certainly beneficial to know that lstm networks are good at detecting sequential patterns in data. last but not least, let's visualize the training scheme of all four models to take a identify any possible signs of convergence and overfitting, if any. to do that, we will be using the function shown below. the dense feed forward network seems to have a very linear pattern. one immediate pattern we see is that the model seems to be overfitting right away, since the testing accuracy decreases with each epoch while the training accuracy increases. this is certainly not a good sign; in the best case scenario, we want to see that training and testing labels moving in the same direction. perhaps this is the biggest indication that a simple feed forward network is a suboptimal model choice in the context of this problem. the graphs for the model seems a lot better. at the very least, we see the training and test labels moving in unison: the accuracy increases with each epoch, while the loss slowly decreases. however, we do see some overfitting happening at the last two epochs or so. specifically, note that cross entropy loss for the testing data seems to pick up an incrementing pattern past the seventh epoch. this observation suggests that we need to configure the model differently, presumably by decreasing the number of tunable parameters. next comes the winner of the day, the lstm network. an interesting point to note about the learning curve of this model is that the test data accuracy and loss seem to stay roughly stagnant despite the progression of epochs. to better understand this phenomena, we probably have to run more trials with more data over longer iterations than we have done in this tutorial. this point notwithstanding, it is interesting to see how a single layer lstm network can outperform a stacked rnn network. the last up on this list is the one dimensional convolutional neural network. the convent produced very remarkable results in this experiment, especially given its extremely short training time. recurrent neural networks typically take a lot of time to train even when they are not stacked because each neuron is defined by a rather complicated operation involving many parameters, such as states, carriage, and so on. convents, on the other hand, are relatively simpler, and thus take noticeably shorter to train and deploy. this tutorial demonstrates that convents can perform as well as simple recurrent networks to establish a baseline performance metric. in this post, we briefly introduced and explored the concept of recurrent neural networks, how they work, and how to build them using the functional api. recurrent neural networks are one of the hottest topics in the contemporary deep learning academia because it presents numerous possibilities for applications. hopefully this post gave you a better understanding of what all the hype is about, why rnns are effective at what they do, and how they can be used in the context of basic natural language processing. in the next post, we will take a look at another interesting natural language processing task. peace! my deepest condolences to those affected by the wuhan corona virus, as well as the families and fans of kobe bryant.",0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0
2020-09-14-pytorch-primer,"i've always been a fan of tensorflow, specifically , for its simplicity and ease of use in implementing algorithms and building models. today, i decided to give pytorch a try. it is my understanding that tensorflow is more often used in coporate production environments, whereas pytorch is favored by academics, especially those in the field of nlp. i thought it would be an interesting idea to give it a try, so here is my first go at it. note that the majority of the code shown here are either borrowed from or are adaptations of those available on the pytorch website, which is full of rich content and tutorials for beginners. of course, basic knowledge of dl and python would be helpful, but otherwise, it is a great place to start. let's dive right in! like tensorflow, pytorch is a scientific computing library that makes use of gpu computing power to acceleration calculations. and of course, it can be used to create neural networks. in this section, we will take a look at how automatic differentiation works in pytorch. note that differentiation is at the core of backpropagation, which is why demonstrating what might seem like a relatively low level portion of the api is valuable. let's begin our discussion by first importing the pytorch module. it isn't difficult to see that is a scientific computing library, much like . for instance, we can easily create a matrice of ones as follows: tensor the is a parameter we pass into the function to tell pytorch that this is something we want to keep track of later for something like backpropagation using gradient computation. in other words, it ""tags"" the object for pytorch. let's make up some dummy operations to see how this tagging and gradient calculation works. y = tensor z = tensor out = 27.0 note that performs element wise multiplication, otherwise known as the dot product for vectors and the hadamard product for matrics and tensors. let's look at how autograd works. to initiate gradient computation, we need to first call on the final result, in which case . then, we can simply call to tell pytorch to calculate the gradient. note that this works only because we ""tagged"" with the parameter. if we try to call on any of the other intermediate variables, such as or , pytorch will complain. tensor let's try to understand the result of this computation. let denote the final tensor. since we called , and since has a total of four elements, we can write out our dummy calculations mathematically in the following fashion: using partial differentiation to obtain the gradients, since , since is just an arbitrary, non specific index out of a total of four, we can easily see that the same applies for all other indices, and hence we will end up with a matrix whose all four entries take the value of 4.5, as pytorch has rightly computed. we can go even a step farther and declare custom operations. for example, here's a dummy implementation of the relu function. let's talk about the method first. note that it takes in two argument parameters: and . as you might have guessed, is simply the value that the function will be provided with. the can simply be thought of as a cache where we can store vectors or matrices to be used during backpropagation. in this case, we store the by calling method. during the backward pass, we compute the gradient. here, we need to retrieve the variable which was stored in the context. this is because the relu function takes the following form: thus, its derivative is during backpropagation, this means that gradients will flow down to the next layer only for those indices whose input elements to te relu function were greater than 0. thus, we need the input vector for reference purposes, and this is done via stashing it in the variable. we will see how we can incorporate into the model in the next section. in this example, we'll take a look at an extremely simple model to gain a better understanding of how everything comes into play in a more practical example. this is the method that i've mostly been using when implementing simple dense fully connected models in numpy. the idea is that we would mathematically derive the formula for the gradients ourselves, then backpropagate these values during the optimization process. of course, this can be done with pytorch. to build our simple model, let's first write out some variables to use, starting with the configuration of our model and its dimensions. we will also need some input and output tensors to be fed into the model for trainining and optimization. next, here are the weight matrices we will use. for now, we assume a simple two layered dense feed forward network. last but not least, let's define a simple squared error loss function to use during the training step. with this entire setup, we can now hash out what the entire training iteration is going to look like. wrapped in a loop, we perform one forward pass, then perform backpropagation to adjust the weights. epoch 99: 714.8318481445312 epoch 199: 3.586176633834839 epoch 299: 0.03582914546132088 epoch 399: 0.0007462671492248774 epoch 499: 8.23544614831917e 05 great! we see that the loss drops as more epochs elapse. while there is no problem with this approach, things can get a lot more unwieldy once we start building out more complicated models. in these cases, we will want to use the auto differentiation functionality we reviewed earlier. let's see this in action. also, let's make this more pytorch y by making use of classes. we will revisit why class based implementations are important in the next section. loss: 321.8311767578125 loss: 0.6376868486404419 loss: 0.0022135386243462563 loss: 7.520567305618897e 05 loss: 1.823174170567654e 05 notice we didn't have to explicitly specify the backpropagation formula with matrix derivatives: by simply calling properties for each of the weights matrices, we were able to perform gradient descent. one detail to note is that, unlike in the case above where we had to explicitly call in order to obtain the loss value which would be of type we leave the computed loss to remain as a tensor in order to call . we also make sure to reset the gradients per epoch by calling . we can also improve our implementation by making use of the class that we implemented earlier. this is simple as doing this might be a better way to implement the function for reasons of simplicity and readability. although ing works, it's more arguably cleaner to write a relu this way. also, this is a dummy example, and we can imagine a lot of situations where we might want to write custom functions to carry out specific tasks. much like tensorflow, pytorch offers to ways of declaring models: function based and class based methods. although i have just started getting into pytorch, my impression is that the later is more preferred by pytorch developers, whereas this is not necessarily the case with keras or . of course, this is a matter of preference and development setting, so perhaps such first impression generalizations do not carry much weight. nonetheless, in this section, we will take a look at both ways of building models. let's start with the function based method. the function based method reminds me a lot of keras's sequential method. let's remind ourselves of kera's basic sequential model api: now let's compare this method with pytorch's way of declaring sequential models: this model declaration is in fact exactly identical to the simple model we have declared above. you can easily see how similar this code snippet is to the keras example. the only difference is that the activation function is declared independently of the layer itself in pytorch, whereas keras combines them into one via argument. of course, you don't have to specify this argument, and we can import the relu function from tensorflow to make it explicit like the pytorch example. the point, however, is that the sequential model api for both libraries are pretty similar. another way to build models is by subclassing . the submodule in pytorch is the one that deals with neural networks; hence the . this subclassing might look as follows: this model is no different from the we defined earlier. the only notable difference is that we didn't define a separate type function. for the most part, the overall idea boils down to the weights are definited in the function the function deals with forward pass now let's take a look at what the training code looks like. epoch 99: 2.7017245292663574 epoch 199: 0.050356119871139526 epoch 299: 0.001569570624269545 epoch 399: 5.8641602663556114e 05 epoch 499: 2.4734026737860404e 06 although things might look a bit different, there's not much going on in this process, other than the fact that some of the functions and logic we wrote before are now abstracted away by pytorch. for example, we see , which is effectively the mean squared error loss, similar to how we defined above. another difference we see is , which, as the variable name makes apparent, is the optimizer that we use for backpropagation. in this specific instance, we use sgd. each backpropagation step is then performed simply via . in this tutorial, we took a very brief look at the pytorch model. this is by no means a comprehensive guide, and i could not even tell anyone that i ""know"" how to use pytorch. nonetheless, i'm glad that i was able to gain some exposure to the famed pytorch module. also, working with django has somewhat helped me grasp the idea of classes more easily, which certainly helped me take in class based concepts in pytorch more easily. i distinctively remember people saying that pytorch is more object oriented compared to tensorflow, and i might express agreement to that statement after having gone through the extreme basics of pytorch. in the upcoming articles, i hope to use pytorch to build more realistic models, preferrably in the domain of nlp, as that seems to be where pytorch's comparative advantage stands out the most compared to tensorflow. of course, this is not to say that i don't like tensorflow anymore, or that pytorch is not an appropriate module to use in non nlp contexts: i think each of them are powerful libraries of their own that provide a unique set of functionalities for the user. and being bilinguial or even a polyglot, if you can use things like caffe perhaps in the dl module landscape will certainly not hurt at all. i hope you've enjoyed this article. catch you up in the next one!",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2020-11-01-pytorch-vgg,"in today's post, we will be taking a quick look at the vgg model and how to implement one using pytorch. this is going to be a short post since the vgg architecture itself isn't too complicated: it's just a heavily stacked cnn. nonetheless, i thought it would be an interesting challenge. full disclosure that i wrote the code after having gone through aladdin persson's wonderful tutorial video. he also has a host of other pytorch related vidoes that i found really helpful and informative. having said that, let's jump right in. we first import the necessary modules. let's first take a look at what the vgg architecture looks like. shown below is a table from the vgg paper. we see that there are a number of different configurations. these configurations typically go by the name of vgg 11, vgg 13, vgg 16, and vgg 19, where the suffix numbers come from the number of layers. each value of the dictionary below encodes the architecture information for each model. the integer elements represents the out channel of each layer. represents a max pool layer. you will quickly see that the dictionary is just a simple representation of the tabular information above. now it's time to build the class that, given some architecture encoding as shown above, can produce a pytorch model. the basic idea behind this is that we can make use of iteration to loop through each element of the model architecture in list encoding and stack convolutional layers to form a sub unit of the network. whenever we encounter , we would append a max pool layer to that stack. this is probably the longest code block i've written on this blog, but as you can see, the meat of the code lies in two methods, and . these methods are where all the fun stacking and appending described above takes place. i actually added a little bit of customization to make this model a little more broadly applicable. first, i added batch normalization, which wasn't in the original paper. batch normalization is known to stabilize training and improve performance; it wasn't in the original vgg paper because the batch norm technique hadn't been introduced back when the paper was published. also, the model above can actually handle rectangular images, not just square ones. of course, there still is a constraint, which is that the and parameters must be multiples of 32. valueerror traceback in 3 in_height=200, 4 in_width=150, 5 architecture=vgg_types""vgg16"" 6 ) in __init__ 16 self.num_classes = num_classes 17 self.convs = self.init_convs 18 self.fcs = self.init_fcs 19 20 def forward: in init_fcs 29 if + != 0: 30 raise valueerror 33 out_height = self.in_height // factor valueerror: and must be multiples of 32 let's roll out the model architecture by taking a look at vgg19, which is the deepest architecture within the vgg family. if we print the model, we can see the deep structure of convolutions, batch norms, and max pool layers. vgg: sequential: conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : maxpool2d, stride=, padding=0, dilation=1, ceil_mode=false) : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : maxpool2d, stride=, padding=0, dilation=1, ceil_mode=false) : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : maxpool2d, stride=, padding=0, dilation=1, ceil_mode=false) : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : maxpool2d, stride=, padding=0, dilation=1, ceil_mode=false) : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : maxpool2d, stride=, padding=0, dilation=1, ceil_mode=false) ) : sequential: linear : relu : dropout : linear : relu : dropout : linear ) ) we can clearly see the two submodules of the network: the convolutional portion and the fully connected portion. now let's see if all the dimensions and tensor sizes match up. this quick sanity check can be done by passing in a dummy input. this input represents a 3 channel 224 by 224 image. passing in this dummy input and checking its shape, we can verify that forward propagation works as intended. torch.size and indeed, we get a batched output of size , which is expected given that the input was a batch containing two images. just for the fun of it, let's define and see if it is capable of processing rectangular images. again, we can pass in a dummy input. this time, each image is of size . and we see that the model is able to correctly output what would be a probability distribution after a softmax. torch.size",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2020-06-25-revisiting-basel,"in the last post, we revisited the riemann zeta function, which we had briefly introduced in another previous post on euler's take on the famous basel problem. it seems like math is my jam nowadays, so i decided to write another post on this topic but this time, with some slightly different takes. in this post, we will explore an alternative way of solving the basel problem using fourier series expansion, and also discuss a alternative representations of the basel problem in integral form. for the integral representations, i'm directly referencing flammable maths, a youtube channel that i found both entertaining and informative. let's get started. first, let's recall what the basel problem is. the problem is quite simple: the goal is to obtain the value of an infinite series, namely this seems like an innocuous, straightforward problem. one can easily prove, for instance, the fact that this series converges using integral approximation. however, to obtain the value of this series is a lot more difficult than it appears it is no coincidence that this problem remained unsolved for years until euler came along. while there are many ways to solve this problem euler's method, in particular, is one of the countless examples through which one can witness his amazing intuition and heuristic but we will be using fourier expansion to solve this problem, as it also provides a nice segue into the dirichlet eta function. we explored the topic of fourier expansion in this previous post. to recap, from a very high level, fourier expansion is a way of expressing some function in terms of trigonometric functions. if taylor expansion used polynomials as the building block, fourier expansion uses sines and cosines. a generic formula for the fourier transform can be expressed as follows: with some integration, it can be shown that where refers to the domain of integration. for instance, if we are integrating from to , . a classic interval that is most commonly used is , and this is no coincidence: notice that, when , the taylor series shown in simplifies into the following: and indeed this is the format and the interval we will be using when constructing a fourier series to tackle the basel problem. to continue, we can derive a very similar expression for , given the specified interval from . now that we have reviewed what fourier series is and how we can construct it, let's jump into the basel problem. just like the taylor series, we can use fourier expansion to represent any function continuous function. for our purposes, let's try to expand a simple polynomial function, , using fourier. we can begin with . let's continue with finding the even coefficients corresponding to the cosines. with some integration by parts, we can all agree that where the terms appear because we end up plugging into , a periodic function. and we can do the same for sine. or, even better, with the key insight that is an even function, we might intelligently deduce that there will be no sine terms at all, since sine functions are by nature odd. in other words, all . this can of course be shown through derivation as we have done above for the cosine coefficients. therefore, putting everything together, we end up with if we consider the case when , we have do you smell the basel problem in the air? the summation on the right hand side is a great sign that we are almost done in our derivation. moving the fractional term to the left hand side, we get: diding both sides by 4, and there you have it, the answer to the basel problem, solved using fourier series! we can also derive a convergence value of the dirichelt eta function from this fourier series as well. recall that the eta function looks as follows: now how can we get a dirichelt eta function out of the fourier series of ? well, let's get back to and think our way through. one noteworthy observation is that we already have in the summation, which looks awfully similar to the dirichlet eta function. since we want to get rid of the cosine term, we can simply set this will make all cosine terms evaluate to 1, effectively eliminating them from the expression. then, we get with a very small bit of algebra, we end up with and there we have it, the value of ! it's interesting to see how all this came out of the fourier series of . in this section, we will be taking a look at some interesting representations of the basel problem, mysteriously packaged in integrals. at a glance, it's somewhat unintuitive to think that an infinite summation problem can be stated as an integral in exact terms; however, the translation from summation to integrals are not out of the blue. using things like taylor series, it is in fact possible to show that the basel problem can be stated as an integral. for instance, consider this integral one thing i am starting to realize these past few days is that some of these integrals are extremely difficult despite being deceptively simple in their looks. this is a good example. to get started, we might consider making a quick change of variables, namely . this will effectively get rid of the rather messy looking denominator sitting in the fraction. to make further progress, at this point let's consider the taylor series expansion of . we can derive this by considering the following integral: since this integral evaluates to . one way to look at would be to consider it as a sum of some geometric series whose first term begins with 1 and has a constant ratio of . in other words, here is where a bit of complication comes in. turns out that under certain conditions, we can exchange the summation and the integral , using things like the dominating convergence theorem of fubini's theorem. however, these are topics for another post. for now, we will assume that this trick is legal and continue on. now we have now that we have a summation representation of , let's move onto . we use the same trick we used earlier to interchange the summation and the integral. this gives us since we have to terms with negative ones with the same exponent, we can safely remove both of them: and notice that we now have the basel problem! if you plug in and increment from there, it is immediately apparent that this is the case. so there we have it, the integral representation of the basel problem! let's look at another example, this time using a double integral representation. the motivation behind this approach is simple. this is a useful result, since it means that we can express the basel problem as an integral of two different variables. now, all we need is a summation expression before the integration. and now we are basically back to the basel problem. note that we can also use the interchange of integral and summation technique again to reexpress as shown below. notice that now we have a geometric series, which means that now we can also express this integral as like this, there are countless ways of using integrals to express the basel problem. this representation, in particular, could be understood as an integral over a unit square in the cartesian coordinate over a bivariate function, . in this post, we took a look at a new way of approaching the basel problem using fourier expansion. we also looked at some interesting integral representations of the basel problem. while a lot of this is just simple calculus and algebra, i nonetheless find it fascinating how the basel problem can be approached from so many different angles hence my renewed respect for euler and other mathematicians who wrestled with this problem hundreds of years ago. as simple as it appears, there are so many different techniques and modes of analysis we can use to approach the problem. it was nice exercise and review of some calculus techniques. i've been digging more into the curious interchange of integral and summation recently, and when this operation is allowed, if at all. turns out that this problem is slightly more complicated than it appears and requires some understanding of measure theory, which i had tried getting into a few months ago without much fruition. hopefully this time, i'll be able to figure something out, or at the very least gain some intuition on this operation. i hope you've enjoyed reading this post. catch you up in the next one.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
2020-05-01-leibniz-rule,"before i begin, i must say that this video by brian storey at olin college is the most intuitive explanation of the leibniz rule i have seen so far. granted, my greedy search over the internet space was by no means exhaustive, so i've probably missed some other hidden gems here and there. also, the video is intended as a visual explanation for beginners rather than a robust analytical proof of the leibniz rule. this point notwithstanding, i highly recommend that you check out the video. this post is going to provide a short, condensed summary of the proof presented in the video, minus the fancy visualization that pen and paper can afford. the leibniz rule, sometimes referred to as feynman's rule or differentiation under the integral sign rule, is an interesting, highly useful way of computing complicated integrals. a simple version of the leibniz rule might be stated as follows: as you can see, what this rule essentially tells us is that integrals and derivatives are interchangeable under mild conditions. we've used this rule many times in a previous post on fisher's information matrix when computing expected values that involved derivatives. why is this the case? it turns out that the leibniz rule can be proved by using the definition of derivatives and some taylor expansion. recall that the definition of a derivative can be written as this is something that we'd see straight out of a calculus textbook. as simple as it seems, we can in fact analyze leibniz's rule by applying this definition, as shown below: thus we have shown that, if the limits of integration are constants, we can switch the order of integration and differentiation. but because our quench for knowledge is insatiable, let's consider the more general case as well: when the limits are not bounded by constant, but rather functions. specifically, the case we will consider looks as follows. in this case, we see that and are each functions of variable . with some thinking, it is not difficult to convince ourselves that this will indeed introduce some complications that require modifications to our original analysis. now, not only are we slightly moving the graph of in the axis, we are also shifting the limits of integration such that there is a horizontal shift of the area box in the axis. but fear not, let's apply the same approach to answer this question. this may appear to be a lot of computation, but all we've done is just separating out the integrals while paying attention to the domains of integration. let's continue by doing the same for the remaining terms. the first two terms in the limit go away since goes to zero. while the same applies to the fractional terms, one difference is that they are also divided by , which is why they remain. we have simplified quite a bit, but we still have two terms in the limit expression that we'd like to remove. we can do this by applying the definition of the integral. and we're done! there are other ways of seeing the leibniz rule, such as by interpreting it as a corollary of the fundamental theorem of calculus and the chain rule, as outlined in here , but i find the geometrically motivated interpretation presented in this article to be the most intuitive. i hope you enjoyed reading this post. catch you up in the next one!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
2019-11-07-studying-deep-learning,"so here goes my first post! when i first heard about deep learning and google's alphago, i was skeptical about the capabilities of artificial intelligence. the idea that computers could learn without being explicitly programmed seemed too unreal to be true. this was especially the case given that i was taking a computer science course in high school, where students were expected to write if else statements and the variants. if a human programmer doesn't instruct the computer to perform a for loop ten times, how would a computer ""know"" to perform this operation in the first place? this question has nagged me for a very long time. it has nagged be quite a bit, actually, to the point that i have decided to study the inner workings of deep learning on my own. this little project, inspired by daniel bourke's post, ""my self created artificial intelligence masters degree,"" encapsulates my journey towards uncovering a mystery that is deep learning. i would not call my spare time self studying to be equivalent to working towards a masters degree, but the idea of developing a curriculum and outlining a clear set of goals and timelines seems like a great way for self motivation. what, then, would i be studying in particular? based on cursory research and my current level of superficial understanding, deep learning might be described as a burgeoning interdisciplinary field that is a marriage of computer science, statistics, linguistics, and infinitely many more. at its core, however, deep learning algorithms are undeniably buttressed by the field of mathematics, specifically linear algebra, calculus, and probabilistic theory. built on top of these theoretical foundations, deep learning utilizes the power of modern computing to process big data and identify patterns that help make accurate predictions. of course, the picture that i have just presented is by no means extensive, yet one point remains crystal clear: as a student with very shallow programming background and knowledge of just introductory college level math, self studying deep learning is going to be a challenge, and a fun one indeed. which naturally leads to the question: from where do i begin? the first book to be added to my curriculum is the hundred page machine learning book, written by andriy burkov. as the title suggests, this book provides a highly condensed guide on the clockwork of machine learning. whether the contents of this book will be a significant challenge given my current level of knowledge is yet unclear, yet it will surely show me an overall glimpse of what studying machine learning will entail for me. there are other books and online resources that i would like to study, but introducing a comprehensive list of such resources, alongside a justification behind each selection, will be tabled for separate post. come back in a few days for a hot, freshly baked book review of one of the best sellers on the art of machine learning! my self created artificial intelligence masters degree: https://hackernoon.com/my self created ai masters degree ddc7aae92d0e the hundred page machine learning book: http://themlbook.com",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
2019-11-28-bayes,"so far on this blog, we have looked the mathematics behind distributions, most notably binomial, poisson, and gamma, with a little bit of exponential. these distributions are interesting in and of themselves, but their true beauty shines through when we analyze them under the light of bayesian inference. in today's post, we first develop an intuition for conditional probabilities to derive bayes' theorem. from there, we motivate the method of bayesian inference as a means of understanding probability. suppose a man believes he may have been affected with a flu after days of fever and coughing. at the nearest hospital, he is offered to undergo a clinical examination that is known to have an accuracy of 90 percent, i.e. it will return positive results to positive cases 90 percent of the time. however, it is also known that the test produces false positives 50 percent of the time. in other words, a healthy, unaffected individual will test positive with a probability of 50 percent. in cases like these, conditional probability is a great way to package and represent information. conditional probability refers to a measure of the probability of an event occurring, given that another event has occurred. mathematically, we can define the conditional probability of event given as follows: this equation simple states that the conditional probability of given is the fraction of the marginal probability and the area of intersection between those two events, . this is a highly intuitive restatement of the definition of conditional probability introduced above: given that event has already occurred, conditional probability tells us the probability that event occurs, which is then synonymous to that statement that has occurred. by the same token, we can also define the reverse conditional probability of given through symmetry and substitution. notice that the numerator stays unchanged since the operation of intersection is commutative. now let's develop an intuition for conditional probabilities by applying it to our example of clinical trials and the potentially affected patient. the purported accuracy of the clinical test is 90 percent, which we might express as follows, using the conditional probability notation: by the same token, we can also express the information on false positives as shown below. this conditional probability statement espouses that, given an individual who is not sick, the test returns a false positive 50 percent of the time. conditional probability provides us with an interesting way to analyze given information. for instance, let be the event that it rains tomorrow, and be the event that it is cloudy at the present moment. although we are no experts in climatology and weather forecast, common sense tells us that since with the additional piece of information that current weather conditions are cloudy, we are inclined to believe that it will likely rain tomorrow, or in the near future. like this, conditional probability allows us to update our beliefs on uncertainty given new information, and we will see in the later sections that this is the core idea behind bayesian inference. let's return back to the example of the potential patient with a flu. shortly afterwards at the hospital, the the man was convinced by the doctor and decided to take the clinical test, the result of which was positive. we cannot assume that the man is sick, however, since the test has a rather high rate of false positives as we saw earlier. in this situation, the parameter that is of interest to us can be expressed as in other words, given a positive test result, what is the probability that the man is actually sick? however, we have no means as of yet to directly answer this question; the two pieces of information we have are that , and that . to calculate the value of , we need bayes's theorem to do its trick. let's quickly derive bayes' theorem using the definition of conditional probabilities delineated earlier. recall that multiply and on both sides of and respectively to obtain the following result: notice that the two equations describe the same quantity, namely . we can use equivalence to put these two equations together in the following form. equation can be manipulated in the following manner to finally produce a simple form of bayes' theorem: we can motivate a more intricate version this rule by modifying the denominator. given that and are discrete events, we can break down as a union of intersections between and , where represents subsets within event . in concrete form, we can rewrite this as additionally, we can rewrite the conditional probability in terms of and according to the definition of conditional probability we observed earlier. applying these alterations to to rewrite produces equation : this is the equation of bayes' theorem. in simple language, bayes' theorem tells us that the conditional probability of some subset given is equal to its relevant fraction within a weighted summation of the conditional probabilities given . although this equation may seem complicated at a glance, we can develop an intuition for this formula by reminding ourselves of the definition of conditional probabilities, as well as the fact that independent events can be expressed as a union of intersections. at the end of the day, bayes' theorem provides a powerful tool through which we can calculate a conditional probability in terms of its reverse, i.e. calculate by utilizing . why is this important at all? let's return back to our example of the potential patient. recall that the conditional probability of our interest was while the pieces of information we were provided were this is where bayes' theorem comes in handy. notice that we have expressed in terms of and . from a statistics point of view, all we have to do now is conduct a random survey of the population to see the percentage of the demographic infected with the flu. let's say that 15 percent of the population has been affected with this flu. plugging in the relevant value yields using bayes' theorem, we are able to conclude that there is roughly a 24 percent chance that the man who tests positive on this examination is affected by the flu. that seems pretty low given the 90 percent accuracy of the test, doesn't it? this ostensible discrepancy originates from the fact that the test has a substantial false positive of 50 percent, and also that the vast majority of the population is unaffected by the disease. this means that, if the entire population were to conduct this test, there would be more false positives than there would be true positives; hence the distortion in the value of the conditional probability. but what if the man were to take the same test again? intuition tells us that the more test he takes, the more confident we can be on whether the man is or is not affected by the disease. for instance, if the man repeats the exam once and receives a positive report, the conditional probability that he is sick given two consecutive positive test results should be higher than the 24 percent we calculated above. we can see this in practice by reapplying bayes' theorem with updated information, as shown below: we see that the value of the conditional probability has indeed increased, lending credence to the idea that the man is sick. like this, like this, bayes' theorem is a powerful tool that can be used to calculate conditional probabilities and to update them continuously through repeated trials. from a bayesian perspective, we begin with some expectation, or prior probability, that an event will occur. we then update this prior probability by computing conditional probabilities with new information obtained for each trial, the result of which yields a posterior probability. this posterior probability can then be used as a new prior probability for subsequent analysis. in this light, bayesian statistics offers a new way to compute new information and update our beliefs about an event in probabilistic terms. bayesian inference is nothing more than an extension of bayes' theorem. the biggest difference between the two is that bayesian inference mainly deals with probability distributions instead of point probabilities. the case of the potential patient we analyzed above was a simple yet illuminating example, but it was limiting in that we assumed all parameters to be simple constants, such as for test accuracy and for false positive frequency. in reality, most statistical estimates exist as probability distributions since there are limitations to our ability to measure and survey data from the population. for example, a simple random sampling of the population might reveal that 15 percent of the sample population is affected with the flu, but this would most likely produce a normal distribution with mean centered around 0.15 instead of a point probability. from a bayesian standpoint, we would then replace the point probability in our example above with an equation for the distribution, from which we can proceed with the bayesian analysis of updating our prior with the posterior through repeated testing and computation. bayes' theorem, specifically in the context of statistical inference, can be expressed as where stands for observed or measured data, stands for parameters, and stands for some probability distribution. in the language of bayesian inference, is the posterior distribution for the parameter , is the likelihood function that expresses the likelihood of having parameter given some observed data , is the prior distribution for the parameter , and is evidence, the marginal probability of seeing the data, which is determined by summing or integrating across all possible values of the parameter, weighted by how strongly we believe in those particular values of . concretely, notice that this is not so different from the expansion of the denominator we saw with bayes' theorem, specifically equation . the only difference here is that the integral takes continuous probability density functions into account, as opposed to discrete point probabilities we dealt with earlier. if we temporarily disregard the constants that show up in , we can conveniently trim down the equation for bayesian inference as follows: this idea is not totally alien to us indeed, this is precisely the insight we gleaned from the example of the potential patient. this statement is also highly intuitive as well. the posterior probability would be some mix of our initial belief, expressed as a prior, and the data newly presented, the likelihood. bayesian inference, then, can be understood as a procedure for incorporating prior beliefs with evidence in order to derive an updated posterior. what makes bayesian inference such a powerful technique is that the derived posterior can themselves be used as a prior for subsequent inference conducted with new data. to see bayesian inference in action, let's dive into the most classic, beaten to death yet nonetheless useful example in probability and statistics: the coin flip. this example was borrowed from the following post. assume that we have a coin whose fairness is unknown. to be fair, most coins are approximately fair given the physics of metallurgy and center of mass, but for now let's assume that we are ignorant of coin's fairness, or the lack thereof. by employing bayesian inference, we can update our beliefs on the fairness of the coin as we accumulate more data through repeated coin flips. for the purposes of this post, we will assume that each coin flip is independent of others, i.e. the coin flips are independent and identically distributed. let's start by coming up with a model representation of the likelihood function, which we might recall is the probability of having a parameter value of given some data . it is not difficult to see that the best distribution for the likelihood function given the setup of the problem is the binary distribution since each coin flip is a bernoulli trial. let denote a random variable that represents the number of tails in coin flips. for convenience purposes, we define 1 to be heads and 0 to be tails. then, the conditional probability of obtaining heads given a fairness parameter can be expressed as we can perform a quick sanity check on this formula by observing that, when , the probability of observing heads diminishes to 0, unless , in which case the probability becomes 1. this behavior is expected since represents a perfectly biased coin that always shows tails. by symmetry, the same logic applies to a hypothetical coin that always shows heads, and represents a fairness parameter of 1. now that we have derived a likelihood function, we move onto the next component necessary for bayesian analysis: the prior. determining a probability distribution for the prior is a bit more challenging than coming up with the likelihood function, but we do have certain clues as to what characteristics our prior should look possess. first, the domain of the prior probability distribution should be contained within . this is because the range of the fairness parameter is also defined within this range. this constraint immediately tells us that where is represents the probability density function that represents the prior. recall that some of the other functions we have looked at, namely binomial, poisson, gamma, or exponential are all defined within the unclosed interval , making it unsuitable for our purposes. the beta distribution nicely satisfies this criterion. the beta distribution is somewhat similar to the gamma distribution we analyzed earlier in that it is defined by two shape parameters, and . concretely, the probability density function of the beta distribution goes as follows: the coefficient, expressed in terms of a fraction of gamma functions, provides a definition for the beta function. the derivation of the beta distribution and its apparent relationship with the gamma function deserves an entirely separate post devoted specifically to the said topic. for the purpose of this post, an intuitive understanding of this distribution and function will suffice. a salient feature of the beta distribution that is domain is contained within . this means that, application wise, the beta distribution is most often used to model a distribution of probabilities, say the batting average of a baseball player as shown in this post. it is also worth noting that the beta function, which serves as a coefficient in the equation for the beta pdf, serves as a normalization constant to ensure that integrating the function over the domain would yield 1 as per the definition of a pdf. to see this, one needs to prove this is left as an exercise for the keen reader. we will revisit this problem in a separate post. another reason why the beta distribution is an excellent choice for our prior representation is that it is a conjugate prior to the binomial distribution. simply put, this means that using the beta distribution as our prior, combined with a binomial likelihood function, will produce a posterior that also follows a beta distribution. this fact is crucial for bayesian analysis. recall that the beauty of bayesian inference originates from repeated applicability: a posterior we obtain after a single round of calculation can be used as a prior to perform the next iteration of inference. in order to ensure the ease of this procedure, intuitively it is necessary for the prior and the posterior to take the same form of distribution. conjugate priors streamline the bayesian process of updating our priors with posteriors by ensuring that this condition is satisfied. in simple language, mathematicians have found that certain priors go well with certain likelihoods. for instance, a normal prior goes along with a normal likelihood; gamma prior, poisson likelihood; gamma prior, normal likelihood, and so on. our current combination, beta prior and binomial likelihood, is also up on this list. to develop some intuition, here is a graphical representation of the beta function for different values of and . this code block produces the following diagram. figure 1: beta distribution for different parameters graphically speaking, the larger the value of and , the more bell shaped it becomes. also notice that a larger corresponds to a rightward shift, i.e. a head biased coin; a larger , a tail oriented one. when and take the same value, the local extrema of the beta distribution is established at , when the coin is perfectly fair. now that we have established the usability of the beta function as a conjugate prior to the binomial likelihood function, let's finally see bayesian inference at work. recall the simplified version of bayes' theorem for inference, given as follows: for the prior and the likelihood, we can now plug in the equations corresponding to each distribution to generate a new posterior. notice that , which stands for data, is now given in the form where denotes the number of heads; , the total number of coin flips. notice also that constants, such as the combinatorial expression or the reciprocal of the beta function, can be dropped since we are only establishing a proportional relationship between the left and right hand sides. further simplifications can be applied: but notice that this expression for the posterior can be encapsulated as a beta distribution since therefore, we started from a prior of to end up with a posterior of . this is an incredibly powerful mechanism of updating our beliefs based on presented data. this process also proves that, as purported earlier, the beta distribution is indeed a conjugate prior of a binomial likelihood function. now, it's time to put our theory to the test with concrete numbers. suppose we start our experiment with completely no expectation as to the fairness of the coin. in other words, the prior would appear to be a uniform distribution, which is really a specific instance of a beta distribution with . presented below is a code snippet that simulates 500 coin flips, throughout which we perform five calculations to update our posterior. executing this code block produces the following figure. figure 2: updating posterior probability this plot shows us the change in our posterior distribution that occurs due to bayesian update with the processing of each data chunk. specifically, we perform this bayesian update after trials. when no coin flips are performed, as shown in the first subplot, the prior follows a uniform distribution as detailed above. as more coin tosses are performed, however, we start to develop an understanding of the fairness of the coin. when we only have a few data points, the more probability there is that we obtain skewed data, which is why the mean estimate of our posterior seems skewed as well. however, with a larger number of trials, the law of large numbers guarantees that we will eventually be able to identify the value of our parameter , which is indeed the case. the key takeaway from this code block is the line . this is all the bayesian method there is in this updating procedure. notice that this line of code directly corresponds to the formula for the updated beta posterior distribution we found earlier, which is refers to , corresponds to , and both and are set to in order to take into account the initial prior which tends to a uniform distribution. an interesting observation we can make about this result is that the variance of the beta posterior decreases with more trials, i.e. the narrower the distribution gets. this is directly reflective of the fact that we grow increasingly confident about our estimate of the parameter with more tosses of the coin. at the end of the 500th trial, we can conclude that the coin is fair indeed, which is expected given that we simulated the coin flip using the command . if we were to alter the argument for this method, say , then we would expect the final result of the update to reflect the coin's bias. bayes' theorem is a powerful tool that is the basis of bayesian statistical analysis. although our example was just a simple coin toss, the sample principle and mechanism can be extended to countless other situations, which is why baye's theorem remains highly relevant to this day, especially in the field of machine learning and statistical analysis. bayesian statistics presents us with an interesting way of understanding probability. the classical way of understanding probability is the frequentist approach, which purports that a probability for an event is the limit of its frequency in infinite trials. in other words, to say that a coin is fair is to say that, theoretically, performing an infinite number of coin flips would result in 50 percent heads and 50 percent tails. however, the bayesian approach we explored today presents a drastically different picture. in bayesian statistics, probability is an embodiment of our subjective beliefs about a parameter, such as the fairness of a coin. by performing trials, infinite or not, we gain more information about the parameter of our interest, which affects the posterior probability. both interpretations of probability are valid, and they help complement each other to help us gain a broader understanding of what the notion of probability entails. i hope this post gave you a better understanding as to why distributions are important specifically in the context of conjugate priors. in a future post, we will continue our exploration of the beta distribution introduced today, and connect the dots between beta, gamma, and many more distributions in the context of bayesian statistics. see you in the next one. poisson: https://jaketae.github.io/study/poisson/ gamma: https://jaketae.github.io/study/gamma/ conditional probability: https://en.wikipedia.org/wiki/conditional_probability likelihood function: https://en.wikipedia.org/wiki/likelihood_function beta distribution: https://en.wikipedia.org/wiki/beta_distribution this post: https://stats.stackexchange.com/questions/47771/what is the intuition behind beta distribution bayes' theorem: https://en.wikipedia.org/wiki/bayes%27_theorem prior probability: https://en.wikipedia.org/wiki/prior_probability posterior probability: https://en.wikipedia.org/wiki/posterior_probability frequentist: https://en.wikipedia.org/wiki/frequentist_probability law of large numbers: https://en.wikipedia.org/wiki/law_of_large_numbers following post: https://www.quantstart.com/articles/bayesian statistics a beginners guide",0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
2019-11-20-poisson,"at the yongsan provost marshall office, i receive a wide variety of calls during my shift. some of them are part of routine communications, such as gate checks or facility operation checks. others are more spontaneous; fire alarm reports come in from time to time, along with calls from the korean national police about intoxicated soldiers who get involved in mutual assault or misdemeanors of the likes. once, i got a call from the american red cross about a suicidal attempt of a soldier off post. all combined, i typically find myself answering about ten to fifteen calls per shift. but yesterday was a special day, a good one indeed, because i received only five calls in total. this not only meant that usag yongsan was safe and sound, but also that i had a relatively light workload. on other days when lawlessness prevails over order, the pmo quickly descends into chaos patrols get dispatched, the desk sergeant files mountains of paperwork, and i find myself responding to countless phone calls while relaying relevant information to senior officials, first sergeants, and the korean national police. so yesterday got me thinking: what is the probability that i get only five calls within a time frame of eight hours, given some estimate of the average number of calls received by the pmo, say 12? how lucky was i? one way we might represent this situation is through a binomial distribution. simply put, a binomial distribution simulates multiple bernoulli trials, which are experiments with only two discrete results, such as heads and tails, or more generally, successes and failures. a binomial random variable can be defined as the number of success in repeated trials with probability of success . for example, if we perform ten tosses of a fair coin, the random variable would be the number of heads; would be , and would be . mathematically, the probability distribution function of a binomial distribution can be written as follows: we can derive this equation by running a simple thought experiment. let's say we are tossing a coin ten times. how can we obtain the probability of getting one head and nine tails? to begin with, here is the list of all possible arrangements: notice that all we had to do was to choose one number that specifies the index of the trial in which a coin toss produced a head. because there are ten ways of choosing a number from integers to , we got ten different arrangements of the situation satisfying the condition . you might recall that this combinatoric condition can be expressed as , which is the coefficient of the binomial distribution equation. now that we know that there are ten different cases, we have to evaluate the probability that each of these cases occur, since the total probability , where . calculating this probability is simple: take the first case, as an example. assuming independence on each coin toss, we can use multiplication to calculate this probability: notice that because we assumed the coin was fair. had it not been fair, we would have different probabilities for and , explained by the relationship that . this is what the binomial pmf is implying: calculating the probability that we get successes in trials requires that we multiply the probability of success by times and the probability of failure by times, because those are the numbers of successful and unsuccessful trials respectively. now that we have reviewed the concept of binomial distribution, it is time to apply it to our example of phone calls at the yongsan pmo. although i do not have an data sheet on me, let's assume for the sake of convenience that, on average, 12 calls come to the pmo per shift, which is eight hours. given this information, how can we simulate the situation as a binomial distribution? first, we have to define what constitutes a success in this situation. while there might be other ways to go about this agenda, the most straightforward approach would be to define a phone call as a success. this brings us to the next question: how many trials do we have? here is where things get a bit more complicated we don't really have trials! notice that this situation is somewhat distinct from coin tosses, as we do not have a clearly defined ""trial"" or an experiment. nonetheless, we can approximate the distribution of this random variable by considering each ten minute blocks as a unit for a single trial, i.e. if a call is received by the pmo between 22:00 and 22:10, then the trial is a success; if not, a failure. blocking eight hours by ten minutes gives us a total of 48 trials. because we assumed the average number of phone calls on a single shift to be 12, the probability of success . let's simulate this experiment times. we can model this binomial distribution as follows: this code block produces the following output: figure 1: modeling phone calls as a binomial distribution under this assumption, we can also calculate how lucky i was yesterday when i only received five calls by plugging in the appropriate values into the binomial pmf function: from a frequentist's point of view, i would have lazy days like these only 7 times for every thousand days, which is nearly three years! given that my military service will last only 1.5 years from now, i won't every have such a lucky day again, at least according to the binomial distribution. but a few glaring problem exists with this mode of analysis. for one, we operated under a rather shaky definition of a trial by arbitrarily segmenting eight hours into ten minute blocks. if we modify this definition, say, by saying that a single minute comprises an experiment, hence a total of 480 trials, we get a different values for and , which would clearly impact our calculation of . in fact, some weird things happen if we block time into large units, such as an hour notice how the value of becomes , which is a probabilistic impossibility as should always take values between . another issue with this model is that a bernoulli trial does not allow for simultaneous successes. say, for instance, that within one ten minute block, we got two calls. however, because the result of a bernoulli trial is binary, i.e. either a success or a failure, it cannot contain more than one success in unit time. therefore, binary distribution cannot encode higher dimensions of information, such as two or three simultaneous successes in one trial. these set of complications motivate a new way of modeling phone calls. in the next section, we look at an alternate approach to the problem: the poisson distribution. here is some food for thought: what if we divide up unit time into infinitesimally small segments instead of the original ten, such that ? this idea is precisely the motivation behind the poisson distribution. if we divide our time frame of interest into infinite segments, smaller even than microseconds, we can theoretically model multiple successful events, which is something that the binomial distribution could not account for. intuitively speaking, this approach is akin to modeling a continuous function as infinitely many stepwise functions such that two ""adjacent"" dots on the graph could be considered as identical points or, in probabilistic terms, a simultaneous event. and because we have infinitely many trials and only a fixed number of success, this necessarily means that would approach 0. although this value may seem odd, the argument that the probability of receiving a call at this very instant is 0, since ""instant"" as a unit of time is infinitely short to have a clearly defined probability. from an algebraic standpoint, is necessary to ensure that , the expected number of success, converges to a real value. now let's derive the poisson formula by tweaking the pmf for a binomial distribution. we commence from this expression: we can substitute for from the definition: using the definition of combinatorics, recall that can alternately be defined as . from this definition, it flows that: but then the last term converges to 1 as goes to : we can further simplify the rest of the terms in the limit expression as well. specifically, collapses to . these terms can be coupled with in the denominator as follows: putting this all together yields: and we have derived the pmf for the poisson distribution! we can perform a crude sanity check on this function by graphing it and checking that its maximum occurs at . in this example, we use the numbers we assumed in the pmo phone call example, in which . the code produces the following graph. figure 2: modeling phone calls as a poisson distribution as expected, the graph peaks at . at a glance, this distribution resembles the binomial distribution we looked at earlier, and indeed that is no coincidence: the poisson distribution is essentially a special case of binomial distributions whereby the number of trials is literally pushed to the limit. as stated earlier, the binomial distribution can be considered as a very rough approximation of the poisson distribution, and the accuracy of approximation would be expected to increase as increases. so let me ask the question again: how lucky was i yesterday? the probability distribution function of the poisson distribution tells us that can be calculated through the following equation: the result given by the poisson distribution is somewhat larger than that derived from the binomial distribution, which was . this discrepancy notwithstanding, the fact that i had a very lucky day yesterday does not change: i would have days like these once every 100 days, and those days surely don't come often. but to really calculate how lucky i get for the next 18 months of my life in the military, we need to do a bit more: we need to also take into account the fact that receiving lesser than 5 calls on a shift also constitutes a lucky day. in other words, we need to calculate , as shown below: this calculation can be done rather straightforwardly by plugging in numbers into the poisson distribution function as demonstrated above. of course, this is not the most elegant way to solve the problem. we could, for instance, tweak the poisson distribution function and perform integration. the following is a code block that produces a visualization of what this integral would look like on a graph. here is the figure produced by executing the code block above. figure 3: poisson probability mass function you might notice from the code block that the integrand is not quite the poisson distribution instead of a factorial, we have an unfamiliar face, the function. why was this modification necessary? recall that integrations can only be performed over smooth and continuous functions, hence the classic example of the absolute value as a non integrable function. factorials, unfortunately, also fall into this category of non integrable functions, because the factorial operation is only defined for integers, not all real numbers. to remedy this deficiency of the factorial, we resort to the gamma function, which is essentially a continuous version of the factorial. mathematically speaking, the gamma function satisfies the recursive definition of the factorial: using the gamma distribution function, we can then calculate the area of the shaded region on the figure above. although i do not present the full calculation process here, the value is approximately equal to that we obtained above, . so to answer the title of this post: about 2 in every 100 days, i will have a chill shift where i get lesser than five calls in eight hours. but all of this aside, i should make it abundantly clear in this concluding section that i like my job, and that i love answering calls on the phone. i can assure you that no sarcasm is involved. if you insist on calculating this integral by hand, i leave that for a mental exercise for the keen reader. or even better, you can tune back into this blog a few days later to check out my post on the gamma function, where we explore the world of distributions beyond the binomial and the poisson. binomial distribution: https://en.wikipedia.org/wiki/binomial_distribution bernoulli trials: https://en.wikipedia.org/wiki/bernoulli_distribution frequentist's point of view: https://en.wikipedia.org/wiki/frequentist_probability poisson distribution: https://en.wikipedia.org/wiki/poisson_distribution gamma distribution: https://en.wikipedia.org/wiki/gamma_distribution",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0
2020-02-01-cnn,"recently, a friend recommended me a book, deep learning with python by francois chollet. as an eager learner just starting to fiddle with the keras api, i decided it was a good starting point. i have just finished the first section of part 2 on convolutional neural networks and image processing. my impression so far is that the book is more focused on code than math. the apparent advantage of this approach is that it shows readers how to build neural networks very transparently. it's also a good introduction to many neural network models, such as cnns or lstms. on the flip side, it might leave some readers wondering why these models work, concretely and mathematically. this point notwithstanding, i've been enjoying the book very much so far, and this post is a reflection of just that. today, we will use tensorflow's module to build a convolutional neural network for image detection. this code is based on what i have learned from the book, so much credit goes to deep learning with python. i have also looked at machine learning mastery blog for additional reference. let's begin! below are the modules that we will need to import for this demonstration. note that this jupyter notebook was written on google colaboratory. the default version of tensorflow in colab will soon switch to tensorflow 2.x. we recommend you upgrade now or ensure your notebook will continue to use tensorflow 1.x via the %tensorflow_version 1.x magic: more info. the function loads the cifar10 data set from module, then applies some basic preprocessing to make it more usable for deep learning purposes. the cifar10 data set contains 50000 training images, each labeled with 1 of 10 classes, ranging from cats, horses, and trucks to airplanes. this is a classic classification task. the preprocessing occurs on two levels: first, the images are normalized so that its pixels takes values between 0 and 1. the training labels are transformed from an integer class label to a one hot encoded vector. let's load the data to proceed with our analysis. let's see what the dimensions of the data are. because the cifar10 data set include color images for its training set, we would expect three channels, each corresponding to red, green, and blue . as expected, it appears that the training data is a tensor of four dimensions, while the target labels are 10 dimensional vectors. to get a better idea of what the cifar10 data looks like, here is a basic function that will display the images in the data set for us using . we can see that, although the images are very pixelated, it is somewhat possible to make out what each image is showing. of course, this task is going to be a lot more difficult for our neural network. now is finally the time to buid the neural network. this network pretty much follows the standard vanilla convolutional neural network model, which typically involves stacking convolution, batch normalization, and pooling layers on top of each other. the dropout layers were also added so as to minimize any potential overfitting. the argument was configured as , named after kaiming he who found the optimal weight initialization kernel for convolutional layers. the argument ensures that the the feature maps are not downsized too quickly due to repeated applications of convolution and pooling. the layer simply normalizes the tensor returned from the previous layer. there are ongoing research as to what effect batch normalization has on neural networks, but the general consensus is that it helps the model learn more quickly and efficiently. the function returns the predefined sequential model, compiled using the configurations as shown below. let's take a look at the summary of the model. the summary shows that this model has 551,466 trainable parameters. the memory capacity of this model is not big, but it is definitely larger than the network we built in the previous post using the keras api. warning:tensorflow:from /usr/local/lib/python3.6/dist packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling baseresourcevariable.__init__ with constraint is deprecated and will be removed in a future version. instructions for updating: if using keras pass _constraint arguments to layers. model: ""sequential"" _________________________________________________________________ ================================================================= conv2d 896 _________________________________________________________________ batch_normalization 128 _________________________________________________________________ conv2d_1 9248 _________________________________________________________________ batch_normalization_1 128 _________________________________________________________________ max_pooling2d 0 _________________________________________________________________ dropout 0 _________________________________________________________________ conv2d_2 18496 _________________________________________________________________ batch_normalization_2 256 _________________________________________________________________ conv2d_3 36928 _________________________________________________________________ batch_normalization_3 256 _________________________________________________________________ max_pooling2d_1 0 _________________________________________________________________ dropout_1 0 _________________________________________________________________ conv2d_4 73856 _________________________________________________________________ batch_normalization_4 512 _________________________________________________________________ conv2d_5 147584 _________________________________________________________________ batch_normalization_5 512 _________________________________________________________________ max_pooling2d_2 0 _________________________________________________________________ dropout_2 0 _________________________________________________________________ flatten 0 _________________________________________________________________ dense 262272 _________________________________________________________________ dropout_3 0 _________________________________________________________________ dense_1 1290 ================================================================= total params: 552,362 trainable params: 551,466 non trainable params: 896 _________________________________________________________________ now that the model is ready to be deployed, we need to train and test the model. but before that, let's quickly define a function that will provide us with a visualization of how the model is learning. this function is very similar to the one used in the previous post all it does it that it plots the model's accuracy and cross entropy loss with each epoch. this visualization will help us see whether our model is actually learning with each epoch, and whether or not overfitting is occurring at any point in training. the last piece of the puzzle we need is the function. this function is essentially wraps all the functions we have created previously, first by loading the data, training using that data, building a model, training a model, and calling on the function to provide a visualization of the model's learning curve. one minor tweak i used to spice up this function is the , which basically creates more images for the neural network to train on by slightly modifying existing images in the training set. these modifications involve shifting, zooming, and flipping. when we don't have enough data to train our model on, using the can be useful. finally, let's see how well our model performs! epoch 1/50 epoch 1/50 5000/704 1s loss: 1.4018 acc: 0.4452 704/704 24s loss: 1.9197 acc: 0.3393 val_loss: 1.5981 val_acc: 0.4452 epoch 2/50 epoch 1/50 5000/704 1s loss: 1.3273 acc: 0.5178 704/704 22s loss: 1.4458 acc: 0.4765 val_loss: 1.4521 val_acc: 0.5178 epoch 3/50 epoch 1/50 5000/704 1s loss: 0.9988 acc: 0.5868 704/704 22s loss: 1.2782 acc: 0.5444 val_loss: 1.1961 val_acc: 0.5868 epoch 4/50 epoch 1/50 5000/704 1s loss: 1.2566 acc: 0.5264 704/704 21s loss: 1.1569 acc: 0.5904 val_loss: 1.5175 val_acc: 0.5264 epoch 5/50 epoch 1/50 5000/704 1s loss: 1.0085 acc: 0.6482 704/704 22s loss: 1.0686 acc: 0.6256 val_loss: 1.0519 val_acc: 0.6482 epoch 6/50 epoch 1/50 5000/704 1s loss: 0.9098 acc: 0.6940 704/704 22s loss: 0.9966 acc: 0.6506 val_loss: 0.8696 val_acc: 0.6940 epoch 7/50 epoch 1/50 5000/704 1s loss: 1.0636 acc: 0.6506 704/704 22s loss: 0.9380 acc: 0.6744 val_loss: 1.0681 val_acc: 0.6506 epoch 8/50 epoch 1/50 5000/704 1s loss: 0.8172 acc: 0.7074 704/704 22s loss: 0.9034 acc: 0.6847 val_loss: 0.8733 val_acc: 0.7074 epoch 9/50 epoch 1/50 5000/704 1s loss: 0.6724 acc: 0.7406 704/704 22s loss: 0.8547 acc: 0.7062 val_loss: 0.7682 val_acc: 0.7406 epoch 10/50 epoch 1/50 5000/704 1s loss: 0.7381 acc: 0.7498 704/704 22s loss: 0.8273 acc: 0.7144 val_loss: 0.7370 val_acc: 0.7498 epoch 11/50 epoch 1/50 5000/704 1s loss: 0.8509 acc: 0.7356 704/704 22s loss: 0.7928 acc: 0.7277 val_loss: 0.7625 val_acc: 0.7356 epoch 12/50 epoch 1/50 5000/704 1s loss: 0.5065 acc: 0.8012 704/704 22s loss: 0.7791 acc: 0.7338 val_loss: 0.5763 val_acc: 0.8012 epoch 13/50 epoch 1/50 5000/704 1s loss: 0.6216 acc: 0.7698 704/704 22s loss: 0.7530 acc: 0.7410 val_loss: 0.7289 val_acc: 0.7698 epoch 14/50 epoch 1/50 5000/704 1s loss: 0.5690 acc: 0.7570 704/704 22s loss: 0.7303 acc: 0.7490 val_loss: 0.6940 val_acc: 0.7570 epoch 15/50 epoch 1/50 5000/704 1s loss: 0.4754 acc: 0.7940 704/704 22s loss: 0.7092 acc: 0.7604 val_loss: 0.6025 val_acc: 0.7940 epoch 16/50 epoch 1/50 5000/704 1s loss: 0.7061 acc: 0.7704 704/704 22s loss: 0.6983 acc: 0.7622 val_loss: 0.6904 val_acc: 0.7704 epoch 17/50 epoch 1/50 5000/704 1s loss: 0.4567 acc: 0.8052 704/704 21s loss: 0.6852 acc: 0.7658 val_loss: 0.5671 val_acc: 0.8052 epoch 18/50 epoch 1/50 5000/704 1s loss: 0.3820 acc: 0.8304 704/704 21s loss: 0.6692 acc: 0.7736 val_loss: 0.4989 val_acc: 0.8304 epoch 19/50 epoch 1/50 5000/704 1s loss: 0.5878 acc: 0.7718 704/704 22s loss: 0.6491 acc: 0.7782 val_loss: 0.6974 val_acc: 0.7718 epoch 20/50 epoch 1/50 5000/704 1s loss: 0.4602 acc: 0.7876 704/704 22s loss: 0.6468 acc: 0.7807 val_loss: 0.6501 val_acc: 0.7876 epoch 21/50 epoch 1/50 5000/704 1s loss: 0.4576 acc: 0.8152 704/704 21s loss: 0.6296 acc: 0.7876 val_loss: 0.5577 val_acc: 0.8152 epoch 22/50 epoch 1/50 5000/704 1s loss: 0.4969 acc: 0.8178 704/704 21s loss: 0.6297 acc: 0.7870 val_loss: 0.5341 val_acc: 0.8178 epoch 23/50 epoch 1/50 5000/704 1s loss: 0.4315 acc: 0.7962 704/704 21s loss: 0.6142 acc: 0.7916 val_loss: 0.5970 val_acc: 0.7962 accuracy: 78.76999974250793% the result shows that the model has learned decently well, with testing accuracy of approximately 80 percent. this is not the best result, but it is certainly not bad, especially given the fact that the images in the dataset, as we have seen above, are very pixelated and sometimes difficult for even humans to decipher and categorize. that's all for today. it's fascinating to see how cnns are capable of perceiving images and categorizing them after appropriate training. but as we all know, the potential of neural networks far extends beyond image classificaiton. in a future post, we might look at more complicated models, such as rnn or lstms, to achieve even cooler tasks! deep learning with python: https://www.manning.com/books/deep learning with python machine learning mastery blog: https://machinelearningmastery.com/how to develop a cnn from scratch for cifar 10 photo classification/ previous post: https://jaketae.github.io/study/first keras/",0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0
2019-11-13-pagerank-and-markov,"google is the most popular search engine in the world. it is so popular that the word ""google"" has been added to the oxford english dictionary as a proper verb, denoting the act of searching on google. while google's success as an internet search engine might be attributed to a plethora of factors, the company's famous pagerank algorithm is undoubtedly a contributing factor behind the stage. the pagerank algorithm is a method by which google ranks different pages on the world wide web, displaying the most relevant and important pages on the top of the search result when a user inputs an entry. simply put, pagerank determines which websites are most likely to contain the information the user is looking for and returns the most optimal search result. while the nuts and bolts of this algorithm may appear complicated and indeed they are the underlying concept is surprisingly intuitive: the relevance or importance of a page is determined by the number of hyperlinks going to and from the website. let's hash out this proposition by creating a miniature version of the internet. in our microcosm, there are only five websites, represented as nodes on a network graph. below is a simple representation created using python and the networkx package. running this block results in the following graph: figure 1: representation of a miniature world wide web how is this a model of the internet? well, as simple as it seems, the network graph contains all the pertinent information necessary for our preliminary analysis: namely, hyperlinks going from one page to another. let's take node d as an example. the pointed edges indicate that page e contains a link to page d, and that page d contains another link that redirects the user to page a. interpreted in this fashion, the graph indicates which pages have a lot of incoming and outgoing reference links. but all this aside, why are hyperlinks important for the pagerank algorithm in the first place? a useful intuition might be that websites with a lot of incoming references are likely to be influential sources, often written by prominent individuals. this analysis is certainly the case in the field of academics, where works of literature that are frequently cited quickly gain clout and attain an established position in the given discipline. another reasoning is that hyperlinks tell us where a user is most likely to wound up in after browsing through returned search results. take the extreme example of an isolated node, where there are zero outgoing and ingoing links to the website. it is unlikely that a user will end up on that webpage, as opposed to a popular site with a spiderweb of edges on a network graph. then, it would make sense for the pagerank algorithm to display that website on top; the isolated node, the bottom. suppose we want to know where a user is most likely to end up in after a given search. this process is often referred to as a random walk because, as the name suggests, it describes a path after a succession of random steps on some mathematical space. while it is highly unlikely that a user visits a website, randomly selects one of the hyperlinks on the given page, and repeats the two steps above repeatedly, the assumption on randomness is what allows us to simulate a user's navigation of the internet from the point of view of markov chains, a stochastic model that describes a sequence of possible events, or states, in which the probability of each event is contingent only upon the previous state attained in the previous event. one good example of a markov chain is the famous chutes and ladders game, in which the player's next position is dependent only upon their present position on the game board. for this reason, markov chains are said to be memoryless: in the chutes and ladders game, whether the player ended up in their current position by taking a ladder or a normal dice roll is irrelevant to the progress of the game. figure 2: chutes and ladders game a salient characteristic of a markov chain is that the probabilities of each event can be represented and calculated by simple matrix multiplication. the specifics of this mechanism will be a topic for another post, but intuitively speaking, there would be some stochastic matrix that represents probabilities, and some vector that denotes the th state in the markov chain. then, multiplying this state vector by the stochastic matrix would yield , where the new vector denotes the probability distribution in the th state in the markov chain. the beauty behind the markov chain is that the result of this multiplication operation, when iterated many times, converges to a stationary distribution vector regardless of where we started from, i.e. . to make all of this more concrete, let's return back to our example of the internet microcosm and the five websites. in order to apply a stochastic analysis on our model, it is first necessary to translate the network graph presented above into a stochastic matrix whose individual entries are nonnegative real numbers that denote some probability of change from one state to another. here is the matrix representation of the network graph in our example: the column vector can be defined as where denotes the probability that the user is browsing website x at state . for instance, would be an appropriate vector representation of a state distribution in a markov chain where a user began their random walk at website a. this will be our example. from these, how can we learn more more about ? would give us the answer: notice that the result is just the first column of the stochastic matrix, with all entries 0 except for the second one! why is this the case? besides the algebraic argument that matrix multiplications can be performed on a column by entry basis, the network graph contains the most intuitive answer to our question: there is only one link from page a to page b, which is why takes the absolute probability of 1. simply put, the user clicks on the one and only link on page a to move to page b, as the highlighted path shows. figure 3: network graph with path highlight once the user reaches page b, however, they now have two choices instead of one: either go back to page a or visit page c. this increase in uncertainty is reflected in the entries of the next vector, : as our intuition suggests, , and since . although no formal proof has been presented, it is now fairly clear that performing this calculation times would yield the vector that contains information about the probability of the user being at websites a, b, c, d, and e, respectively. let's quickly calculate the values of this state vector over some loops. the output of this program is . as the markov process rightly predicts, these numbers each converge to certain values with more repetitions. also notice that this result is independent of the initial vector we started out with! suppose a new initial vector xppppp\lim\limits_ x_n 29\%39\%22\%2\%7\%p is the smallest out of the five entries, which aligns with the fact that node d held the most insular position in the network graph. all in all, analyzed from the dimension of time, the notion of stationary distribution is coherent with our intuition that, no matter where the user starts, the average time they spend on each website should be the same given the memoryless nature of the markov chain. so we commenced from the seemingly simple question of what pagerank entails. the markov chain madness may have appeared a bit like a rabbit hole, but it is highly germane to the clockwork behind google's search algorithm. although we used only one parameter hyperlinks as the basis of our analysis, in reality pagerank performs batch calculations on a much larger sum of data to ultimately derive the equivalent of our stationary distribution vector. the website that the user is most likely to spend the most time on, i.e. the website that is most likely important and relevant to the user's search entry, is placed on the top of the list. other entries follow in sorted order. so there you have it: the pagerank algorithm demystified. now the question is, will google place this post on the top of the search result when a user types ""pagerank""? probably not given the lack of active hyperlinks to and from this webpage. but we'll see. added to the oxford english dictionary: https://www.theatlantic.com/technology/archive/2014/06/the first use of the verb to google on television buffy the vampire slayer/373599/ the pagerank algorithm: https://en.wikipedia.org/wiki/pagerank networkx package: https://networkx.github.io random walk: https://en.wikipedia.org/wiki/random_walk memoryless: https://en.wikipedia.org/wiki/markov_property",0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2020-06-05-spark-basics,"i've stumbled across the word ""apache spark"" on the internet so many times, yet i never had the chance to really get to know what it was. for one thing, it seemed rather intimidating, full of buzzwords like ""cloud computing"", ""data streaming,"" or ""scalability,"" just to name a few among many others. however, a few days ago, i decided to give it a shot and try to at least get a glimpse of what it was all about. so here i report my findings after binge watching online tutorials on apache spark. if you're into data science or even just software development, you might have heard some other apache variants like apache kafka, apache cassandra, and many more. when i first heard about these, i began wondering: is apache some sort of umbrella software, with spark, kafka, and other variants being different spinoffs from this parent entity? i was slightly more confused because the apache i had heard of, at least as far as i recalled, had to do with web servers and hosting. turns out that there is an organization called the apache software foundation, which is the world's largest open source foundation. this foundation, of course, has to do with the apache http server project, which was the web server side of things that i had ever so faintly heard about. then what is apache spark? spark was originally developed at uc berkeley at the amp lab. later, its code base was open sourced and eventually donated to the apache software foundation; hence its current name, apache spark. for this tutorial, we will be loading apache spark on jupypter notebook. there are many tutorials on how to install apache spark, and they are easy to follow along. however, i'll also share a quick synopsis of my own just for reference. installing apache spark is pretty straight forward if you are comfortable dealing with on macos or on linux. the executive summary is that you need to add apache spark binaries to the variable of your system. what is a ? basically, the variable is where all your little unix programs live. for example, when we run simple commands like or , we are essentially invoking built in mini programs in our posix system. the variable tells the computer where these mini programs reside in, namely , which is by default part of the variable. can the variable be modified? the answer is a huge yes. say we have our own little mini program, and we want to be able to run it from the command line prompt. then, we would simply modify so that the computer knows where our custom mini program is located and know what to do whenever we type some command in the terminal. this is why we enter the python shell in interactive mode when we type on the terminal. here is the little setup i have on my own : here, i prepended to the default then appended at the end. appending and prepending result in different behaviors: by default, the computer searches for commands in the variable in order. in other words, in the current setup, the computer will first search the directory, then search the default directory, and look at the very last, at least in my current setup. note that spark has specific java requirements that may or may not align with the default java installation on your workstation. in my case, i had to install a different version of java and apply certain configurations. the path variable is a result of this minor modification. the contents in the directory simply contains the result of unzipping the file available for download on the apache spark website. once the variable has been configured, run , and you should be ready to run apache spark on your local workstation! to see if the installation and configuration has been done correctly, type on the terminal: to use jupyter with spark, we need to do a little more work. there are two ways to do this, but i will introduce the method that i found not only fairly simple, but also more applicable and generalizable. all we need is to install package via . then, on jupyter, we can do: then simply import apache spark via that is literally all we need! we can of course still use apache spark on the terminal simply by typing if we want, but it's always good to have more options on the table. the rdd api is the most basic way of dealing with data in spark. rdd stands for ""resilient distributed dataset."" although more abstracted, higher level apis such as spark sql or spark dataframes are becoming increasingly popular, thus challenging rdd's standing as a means of accessing and transforming data, it is a useful structure to learn nonetheless. one salient feature of rdds is that computation in an rdd is parallelized across the cluster. to run spark, we need to initialize a spark context. a spark context is the entry point to spark that is needed to deal with rdds. we can initialize one simply as follows: strictly speaking, the more proper way to do this would be to follow the syntax guideline on the official website. however, we use a more simplified approach without initializing different apps for each example, simply for convenience purposes. let's begin with a simple dummy example. here, we turn a normal python list into an rdd, then print out its contents after applying a squaring function. we use to turn the rdd into an iterable, specifically a list. 1 4 9 16 another useful function is and . as you might have easily guessed, these functions are literally used to count the number of elements itself or their number of occurrences. this is perhaps best demonstrated by an example. 7 works in a similar fashion, but it creates a dictionary of key value pairs, where the key is an element and the value is the count of that element in the input list. defaultdict as the name implies, is a way of reducing a rdd into something like a single value. in the example below, we reduce an rdd created with a list of numbers into a product of all the numbers in that original input list. rdd: 1, 2, 3, 4 24 this was a pretty simple example. let's take a look at a marginally more realistic of an example, albeit still extremely simple. i'm using the files from this spark tutorial on github. after cloning the repo, we establish a base file path and retrieve the file we will be using in this example. let's see what this rdd looks like. we can do this via , much like in pandas. '1,""goroka"",""goroka"",""papua new guinea"",""gka"",""ayga"", 6.081689,145.391881,5282,10,""u"",""pacific/port_moresby""', '2,""madang"",""madang"",""papua new guinea"",""mag"",""aymd"", 5.207083,145.7887,20,10,""u"",""pacific/port_moresby""', '3,""mount hagen"",""mount hagen"",""papua new guinea"",""hgu"",""aymh"", 5.826789,144.295861,5388,10,""u"",""pacific/port_moresby""', '4,""nadzab"",""nadzab"",""papua new guinea"",""lae"",""aynz"", 6.569828,146.726242,239,10,""u"",""pacific/port_moresby""', '5,""port moresby jacksons intl"",""port moresby"",""papua new guinea"",""pom"",""aypy"", 9.443383,147.22005,146,10,""u"",""pacific/port_moresby""' if you look carefully, you will realize that each element is a long string, not multiple elements separated by a comma as we would like. let's define a helper function to split up each elements as we would like. let's test out this function with the first element in the rdd. '1', 'goroka', 'goroka', 'papua new guinea', 'gka', 'ayga', ' 6.081689', '145.391881', '5282', '10', 'u', 'pacific/port_moresby' great! it worked as expected. now let's say that we want to retrieve only those rows whose entries deal with airports in the united states. specifically, we want the city and the name of the airport. how would we go about this task? well, one simple idea would be to filter the data for airports in the united states, then only displaying the relevant information, namely the name of the airport and the city in which it is located. let's begin by defining the function. and we test it on the first element to verify that it works as expected: 'goroka, goroka' as stated earlier, we first filter the data set so that we only have entries that pertain to airports in the united states. then, we the rdd using the function we defined above. this will transform all elements into the form we want: the name of the airport and the city. we actually used above when we were dealing with square numbers. it's pretty similar to how map works in python or other functional programming languages. 'putnam county airport, greencastle', 'dowagiac municipal airport, dowagiac', 'cambridge municipal airport, cambridge', 'door county cherryland airport, sturgeon bay', 'shoestring aviation airfield, stewartstown' now let's take a look at another commonly used operation: . for this example, we load a text file containing prime numbers and create a rdd. ' 2\t 3\t 5\t 7\t 11\t 13\t 17\t 19\t 23\t 29', ' 31\t 37\t 41\t 43\t 47\t 53\t 59\t 61\t 67\t 71', ' 73\t 79\t 83\t 89\t 97\t101\t103\t107\t109\t113', '127\t131\t137\t139\t149\t151\t157\t163\t167\t173', '179\t181\t191\t193\t197\t199\t211\t223\t227\t229' , as the name implies, maps a certain operation over the elements of a rdd. the difference between and is that the latter flattens the output. in this case, we split the numbers along . normally, this would create a separate list for each line. however, since we also flatten that output, there is no distinction between one line and another. 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71 just for a simple recap, let's try adding all the numbers in that rdd using . 24133 we can also consider rdds to be likes sets, in the pythonic or the mathematical sense, whichever you conceptually prefer. the idea is that we can use set operators, such as intersection or unions, to extract data we want from the rdd to create a new rdd. below is an example using nasa records, each from july and august of 1995. 'host\tlogname\ttime\tmethod\turl\tresponse\tbytes', '199.72.81.55\t \t804571201\tget\t/history/apollo/\t200\t6245\t\t', 'unicomp6.unicomp.net\t \t804571206\tget\t/shuttle/countdown/\t200\t3985\t\t', '199.120.110.21\t \t804571209\tget\t/shuttle/missions/sts 73/mission sts 73.html\t200\t4085\t\t', 'burger.letters.com\t \t804571211\tget\t/shuttle/countdown/liftoff.html\t304\t0\t\t' the task is to obtain hosts that are both in the july and august logs. we might want to break this task up into several discrete components. the first step would be to extract the host information from the original logs. we can do this simply by splitting and obtaining the first element of each resulting list. 'host', 'in24.inetnebr.com', 'uplherc.upl.com', 'uplherc.upl.com', 'uplherc.upl.com' then, all we have to do is to apply the intersection operation, then filter out the first column header . 'www a1.proxy.aol.com', 'www d3.proxy.aol.com', 'piweba1y.prodigy.com', 'www d4.proxy.aol.com', 'piweba2y.prodigy.com' lastly, if we wanted to save the rdd to some output file, we would use the function. note that the output file would be split up into multiple files, since computation is distributed in spark. also, it is generally considered standard good practice to split up huge datasets into separate files, rather than coalescing all of them into a single one. so far, we have looked at rdds. now, let's turn our attention to another type of widely used rdds: pair rdds. pair rdds are widely used because they are, in a way, like dictionaries with key value pairs. this key value structure is very useful, since we can imagine there being a lot of operations where, for instance, a value is reduced according to their keys, or some elements are grouped by their keys, and et cetera. let's take a look at what we can do with pair rdds. both and operations work the same way as you would expect with normal rdds. let's first create a toy example using the rdd we looked at earlier. to remind ourselves of what this data looked like, we list the first five elements in the rdd. '1,""goroka"",""goroka"",""papua new guinea"",""gka"",""ayga"", 6.081689,145.391881,5282,10,""u"",""pacific/port_moresby""', '2,""madang"",""madang"",""papua new guinea"",""mag"",""aymd"", 5.207083,145.7887,20,10,""u"",""pacific/port_moresby""', '3,""mount hagen"",""mount hagen"",""papua new guinea"",""hgu"",""aymh"", 5.826789,144.295861,5388,10,""u"",""pacific/port_moresby""', '4,""nadzab"",""nadzab"",""papua new guinea"",""lae"",""aynz"", 6.569828,146.726242,239,10,""u"",""pacific/port_moresby""', '5,""port moresby jacksons intl"",""port moresby"",""papua new guinea"",""pom"",""aypy"", 9.443383,147.22005,146,10,""u"",""pacific/port_moresby""' next, we define a function with which we will map the rdd. the biggest difference between this function and the ones we have defined previously is that this tuple returns a tuple instead of a single value. therefore, this tuple functionally takes the structure of a pair. in this case, we have the name of the airport and the country in which it is located in. for demonstration and sanity check purposes, let's apply the function to an example: works as expected! if we map the entire rdd with , then we would end up with an rdd containing tuples as each of its elements. in a nutshell, this is what a pair rdd looks like. , , , , if we want to use filter, we can simply access keys or values as appropriate using list indexing with brackets. for example, if we want to obtain a list of airports in the united states, we might execute the following statement. , , , , as stated earlier, one of the advantages of using pair rdds is the ability to perform key or value specific operations. for example, we might want to apply some map function on the values of the rdd while leaving the keys unchanged. let;s say we want to change country names to uppercase. this might be achieved as follows: , , , , however, this statement is rather verbose, since it requires us to specify that we want to leave the key unchanged by declaring the tuple as . instead, we can use to achieve the same result with much less boilerplate. , , , , note that we didn't have to tell spark what to do with the keys: it already knew that the keys should be left unchanged, and that mapping should only be applied to the values of each pair element in the rdd. earlier, we took a look at the operation, which was used to calculate things like sums or products. the equivalent for pair rdds is . let's take a look at an example of a simple word frequency counting using a dummy text file. ""the history of new york begins around 10,000 bc, when the first native americans arrived. by 1100 ad, new york's main native cultures, the iroquoian and algonquian, had developed. european discovery of new york was led by the french in 1524 and the first land claim came in 1609 by the dutch. as part of new netherland, the colony was important in the fur trade and eventually became an agricultural resource thanks to the patroon system. in 1626 the dutch bought the island of manhattan from native americans.1 in 1664, england renamed the colony new york, after the duke of york new york city gained prominence in the 18th century as a major trading port in the thirteen colonies."", '' to count the occurrences of words, we first need to split the strings into words. note that we want to use since we don't want to establish a distinction between different sentences; instead, we want to flatten the output. 'the', 'history', 'of', 'new', 'york' a hacky way to go about this task is to first transform the rdd into a pair rdd where each key is a word and the value is 1. then, we can add up the values according to each key. let's accomplish this step by step. , , , , as you might have guessed, this is where comes into play. applying the simple addition lambda function with produces a pair rdd that contains the total count for each word. , , , , one natural extension we might want to go from the word counting example is sorting. one can easily imagine situations where we might want to sort a pair rdd in some ascending or descending order according to value. this can be achieved via the function. here, we set so that the most frequent words would come at the top. , , , , another common operation with pair rdds is . however, before we get into the details, it should be noted that this method is strongly discouraged for performance reasons, especially on large datasets. for more information, i highly recommend that you take a look at this notebook by databricks although it is written in scala, you will understand most of what is going on based on your knowledge of pyspark. with this caveat out of the way, let's take a look at what we can do with . we first use the rdd we've used before in other examples. '1,""goroka"",""goroka"",""papua new guinea"",""gka"",""ayga"", 6.081689,145.391881,5282,10,""u"",""pacific/port_moresby""', '2,""madang"",""madang"",""papua new guinea"",""mag"",""aymd"", 5.207083,145.7887,20,10,""u"",""pacific/port_moresby""', '3,""mount hagen"",""mount hagen"",""papua new guinea"",""hgu"",""aymh"", 5.826789,144.295861,5388,10,""u"",""pacific/port_moresby""', '4,""nadzab"",""nadzab"",""papua new guinea"",""lae"",""aynz"", 6.569828,146.726242,239,10,""u"",""pacific/port_moresby""', '5,""port moresby jacksons intl"",""port moresby"",""papua new guinea"",""pom"",""aypy"", 9.443383,147.22005,146,10,""u"",""pacific/port_moresby""' this time, we use a mapping function that returns a key value pair in the form of . as you may have guessed, we want to group by country keys to build a new pair rdd. first, we check that the function works as expected, thus producing a pair rdd of country airport pair elements. , , , , if we apply to this rdd, we get a pair rdd whose values are objects in pyspark speak. this is somewhat like a list but offered through the spark interface and arguably less tractable than normal python lists in that they can't simply be indexed with brackets. , , , , to get a sneak peak into what objects look like, we can convert them into a list. note that we normally wouldn't enforce list conversion on large datasets. iceland 'akureyri', 'egilsstadir', 'hornafjordur', 'husavik', 'isafjordur', 'keflavik international airport', 'patreksfjordur', 'reykjavik', 'siglufjordur', 'vestmannaeyjar', 'reykjahlid airport', 'bakki airport', 'vopnafjörður airport', 'thorshofn airport', 'grímsey airport', 'bildudalur airport', 'gjogur airport', 'saudarkrokur', 'selfoss airport', 'reykjahlid', 'seydisfjordur', 'nordfjordur airport' join, which comes from relational algebra, is a very common operation that comes from relational algebra. it is commonly used in sql to bring two or more tables into the same picture. for a quick visual representation of what joins are, here is an image that might be of help. we can perform joins on pair rdds as well. we can consider pair rdds to be somewhat like sql tables with just a primary key and a single column to go with it. let's quickly create a toy dataset to illustrate the join operators in pyspark. here, we have a list of names, ages, and their countries of origin. to best demonstrate the join operation, we intentionally create a mismatch of keys in the and . here is a little helper function to help us take a look at what the keys and values are in a pair rdd. key: sarah, value: 16 key: john, value: 20 key: tom, value: 16 key: clara, value: 15 key: tom, value: uk key: clara, value: fr key: ellie, value: prc key: jake, value: can key: demir, value: bel as we can see, the and each have some overlapping keys, but not all keys are in both rdds. for instance, tom and clara are in both rdds, but john is only in the . this intentional mismatch is going to be useful later when we discuss the difference between left and right joins. first, let's take a look at , which in pyspark refers to an inner join. since this an inner join, we only get results pertaining to keys that are present in both rdds, namely clara and tom. key: clara, value: key: tom, value: note that if we flip the order of joins, the order of elements in the values of each key value pairs also changes. key: clara, value: key: tom, value: things get slightly more interesting with other joins like . i personally find it intuitive to image two van diagrams, with the left one being completely filled in the case of a left join. in other words, we keep all the keys of the left table while joining with the table on the right. in this case, since there exists a key mismatch, only those keys that are present in both tables will end up with a full joined tuple; others are s in their joined values. key: john, value: key: clara, value: key: tom, value: key: sarah, value: the same happens with . this is exactly identical to what the left join is, except with the very obvious caveat that the right diagram is filled, not the left. key: ellie, value: key: clara, value: key: tom, value: key: jake, value: key: demir, value: lastly, let's take a look at . this is a combination of the left and right join we fill up the entire van diagram, both left and right. notice that this is exactly what happens when we add the results from and , with duplicates removed. key: john, value: key: ellie, value: key: clara, value: key: tom, value: key: jake, value: key: sarah, value: key: demir, value: in this post, we explored the various aspects of apache spark: what it is, how to set it up, and what we can do with it via the rdd api. there is a lot more to spark that we haven't discussed, such as spark sql or mllib. i will most definitely be writing a post on these as i become more familiar with the various apis and functionalities that spark has to offer. i doubt i'll be using spark for any personal project, since spark is used for processing large datasets across different clusters, not on a single computer as we have done here. however, it was an interesting journey and one that was definitely worth the time and effort, since i feel like i've at least gained some glimpse of what all the hype behind the spark keyword is. i hope you've enjoyed reading this post. see you in the next one!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
2020-01-25-map-convex,"in a previous post, we briefly explored the notion of maximum a posteriori and how it relates to maximum likelihood estimation. specifically, we derived a generic formula for map and explored how it compares to that for mle. today's post is going to be an interesting sequel to that story: by performing map on the univariate gaussian, we will show how map can be interpreted as a convex combination, thus motivating a more intuitive understanding of what map actually entails under the hood. let's jump right into it. the univariate gaussian is a good example to work with because it is simple and intuitive yet also complex enough for meaningful analysis. after all, it is one of the most widely used probability distributions and also one that models many natural phenomena. with that justification firmly in mind, let's take a look at the setup of the map of the mean for the univariate gaussian. as always, we begin with some dataset of independent observations. in this case, because we are dealing with the univariate gaussian, each observations will simply be a scalar instead of a vector. in other words, let's assume that the random variable is normally distributed according to some parameter . we will assume that the standard deviation of the random variable is given as . we could have considered standard deviation to be a parameter, but since the goal of this demonstration is to conduct map estimation on the mean of the univariate gaussian, we assume that the standard deviation is known. next, we have to define a prior distribution for the parameter. let's say that is also normally distributed around some mean with a standard deviation of 1, as shown below. recall that the goal of map is, as the same suggests, to maximize the posterior distribution. to derive the posterior, we need two ingredients: a prior and a likelihood function. we already have the first ingredient, the prior, as we have just defined it above. the last piece of the puzzle, then, is the likelihood function. since we have assumed our data to be independently distributed, we can easily calculate the likelihood as follows: all that is left is to compute the posterior according to baye's formula for bayesian inference. we can thus calculate the map estimate of as shown below. the second equality is due to proportionality, whereby is independent of and thus can be removed from the argmax operation. the fourth equality is due to the monotonically increasing nature of the logarithmic function. we always love using logarithms to convert products to sums, because sums are almost always easier to work with than products, especially when it comes to integration or differentiation. if any of these points sounds confusing or unfamiliar, i highly recommend that you check out my articles on map and mle. to proceed, we have to derive concrete mathematical expressions for the log likelihood and the log prior. recall the formula for the univariate gaussian that describes our data: then, from , we know that the likelihood function is simply going to be a product of the univariate gaussian distribution. more specifically, the log likelihood is going to be the sum of the logs of the gaussian probability distribution function. there is the log likelihood function! all we need now is the log prior. recall that the prior is a normal distribution centered around mean with standard deviation of 1. in pdf terms, this translates to the log prior can simply be derived by casting the logarithmic function to the probability distribution function. now we are ready to enter the maximization step of the sequence. to calculate the maximum of the posterior distribution, we need to derive the posterior and set the gradient equal to zero. for a more robust analysis, it would be required to show that the second derivative is smaller than 0, which is indeed true in this case. however, for the sake of simplicity of demonstration, we skip that process and move directly to calculating the gradient. let's rearrange the final equality in . from , we can finally derive an expression for . this value of the parameter is one that which maximizes the posterior distribution. and we have derived the map estimate for the mean of the univariate gaussian! maximum a posteriori analysis is great and all, but what does the final result exactly tell us? while there might be many ways to interpret understand the result as derived in , one particular useful intuition to have relates to the concept of convex combinations. simply put, a convex combination is a linear combination of different points or quantities in which the coefficients of the linear combinations add up to one. more concretely, we can also imagine that and are each dimensional vectors, and that a convex combination is simply a dot product of these two vectors given that the elements of sum up to one. why did i suddenly bring up convex combinations out of no where? well, it turns out that the result in in fact an instance of a convex combination of two points satisfying the form indeed, it is not difficult to see that the coefficient of and add up to 1, which is precisely the condition for a linear combination to be considered convex. now here is the important part: the implication of this observation is that we can consider the map estimate of parameter as an interpolation, or more simply, some weighted average between and . this interpretation also aligns with the whole notion of bayesian inference: our knowledge of the parameter is partially defined by the prior, but updated as more data is introduced. and as we obtain larger quantities of data, the relative importance of the prior distribution starts to diminish. imagine that we have an infinite number of data points. then, will soley be determined by the likelihood function, as the weight ascribed to the prior will decrease to zero. in other words, conversely, we can imagine how having no data points at all would cause the weight values to shift in favor of the prior such that no importance is ascribed to the mle estimate of the parameter. in this short article, we reviewed the concept of maximum a posteriori and developed a useful intuition about its result from the perspective of convex combinations. maximum a posteriori, alongside its close relative maximum likelihood estimation, is an interesting topic that deserives our attention. hopefully through this post, you gained a better understanding of what the result of an map estimation actually means from a bayesian point of view: a weighted average between the prior mean and the mle estimate, where the weight is determined by the number of data points at our disposal. also, i just thought that the name ""convex conbinations"" is pretty cool. the fancier the name, the more desire to learn it is a natural human instinct indeed. i hope you enjoyed reading this article. in the next post, we will continue our discussion by exploring the concept of conjugate priors, which is something that we have always assumed as true and glossed over, yet is arguably what remains at the very core of bayesian analysis.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
2020-07-10-complex-fibonacci,"a few days ago, a video popped up in my youtube suggestions. we all know how disturbingly powerful the youtube recommendation algorithm is: more than 90 percent of the times, i thoroughly enjoy all suggestions put forth by the mastermind algorithm. this time was no exception: in fact, i enjoyed it so much that i decided to write a short blog post about it. also a quick plug: if you haven't checked out matt parker's channel, i highly recommend that you do. let's dive right into today's topic: extending the fibonacci sequence to complex numbers. we all know what the fibonacci sequence looks like, but for formality and notational clarity's sake, here is what the fibonacci sequence looks like: there are some different conventions as to where the sequence starts. i personally prefer the one that starts from zero, with zero indexing. here is what i mean: implementing fibonacci numbers in code is one of the most common exercises that are used to teach concepts such as recursion, memoization, and dynamic programming. this is certainly not the point of today's post, but here is an obligatory code snippet nonetheless. the code above is the standard fibonacci function as we know it, implemented with simple bare bone recursion. while the code works perfectly fine, there is a fatal problem with this code: it recalculates so many values. namely, in calling , the program goes all the way up to the th fibonacci number. then, in the next call of , the program recalculates the same values calculated earlier, up until the th fibonacci number, just one short of the previous one. the classic way to deal with this problem is to use a technique known as memoization. the idea is simple: we keep some sort of memo or cache of values that have already been calculated and store them in some data structure that we don't have to recalculate values that have already been computed prior. here is a simple implementation. to see how effective memoization is compared to vanilla recursion, let's use the module. 3.82 s ± 91 ms per loop comparing this to the test on with memoization, the benefits of caching becomes immediately clear: 152 ns ± 4.33 ns per loop 3 second and 150 nanoseconds are different by orders of magnitude, and we only asked the functions to calculate the 35th fibonacci number. we can get a sense of how quickly this disparity would grow if we try to calculate something like the 1000th fibonacci number in the sequence. another perk of using caching as above is that we can now get the full sequence up to the 35th fibonacci number. although memoization is interesting, it is not the main topic of today's post. instead, i want to discuss binet's formula, a formula with which we can calculate the th fibonacci number. binet's formula states that we can trivially verify that and that . for more robust empirical verification, we will resort to code later. it is worth noting that the quantity in the parenthesis, namely is otherwise referred to as the golden ratio. also observe that the other quantity is the negative inverse of the golden ratio. let's take a closer look at why the binet's formula makes sense. this is not going to be a rigorous proof or a derivation, but rather an attempt at cursory analysis to provide food for thought. this process was heavily referenced from this quora post. intuitively, binet's formula has to do with the well known fact that the ratio between two consecutive fibonacci numbers approaches the golden ratio as goes to infinity. in this light, we might understand the fibonacci sequence as a geometric sequence with a constant ratio. the goal, then, is to show that the ratio is in fact the golden ratio. then, we have the following recurrence relation between consecutive terms. dividing both sides by , we get this is a simple quadratic equation that we can easily solve. with some precalculus algebra, we get and indeed we start to see the golden ratio and its negative inverse as solutions to the quadratic. this means that we can express the fibonacci sequence as a linear combinations of these two solutions: much like solving any difference equations, we have two initial conditions, namely that , . we also trivially know that , but only two conditions suffice to ascertain the value of the coefficients, and . with some algebra, one can verify that putting these together, we finally get binet's formula: an interesting point to note about binet's formula is that doesn't necessarily have to be a non negative integer as we had previously assumed. in fact, it can be any number: rational, irrational, real, or even complex. the fact that the fibonacci numbers can extend to real number indexing becomes more apparent once we code out the formula. nothing special at all, this is just a literal transcription of the formula presented above. but now, watch what happens when we try to get the 1.1th fibonacci number, for instance: lo and behold, we get a complex fibonacci number! i thought this was so fascinating, almost like seeing a magic of some sort. although i had known about the fibonacci sequence for as long as i can remember, i had never thought about it in continuous terms: in my mind, the fibonacci sequence was, after all, a sequence a discrete set of numbers adhering to the simple rule that the next number in line is the sum of the previous two. the intriguing part is that, even in this complex fibonacci madness, the simple rule still holds. for instance, you might be wondering why we don't compare things exactly by means of this is because this equality doesn't hold due to floating point arithmetic. therefore, we simply verify equivalence by comparing their magnitude with an arbitrarily small number, . the takeaway from the code snippet is that holds, regardless of whether or not is a non negative integer. indeed, binet's formula gives us what we might refer to as the interpolation of the fibonacci sequence, in this case extended along the real number line. a corollary of the real number interpolation of the fibonacci sequence via binet's formula is that now we can effectively plot the complex fibonacci numbers on the cartesian plane. because can be continuous, we would expect some graph to appear, where the axis represents real numbers, and , the imaginary. this requires a bit of a hack though; note that the result of binet's formula is a complex number, or a two dimensional data point. the input to the function is just a one dimensional real number. therefore, we need a way of representing a map from a one dimensional real number line to a two dimensional complex plane. this is sort of tricky if you think about it: the normal two dimensional plane as we know it can only represent a mapping from the axis to the axis in other words, a transformation from one dimensional space to another one dimensional space. a three dimensional coordinate system, on the other hand, represents a transformation from a two dimensional space, represented by and , to another one dimensional space, namely . we aren't used to going to other way around, where a one dimensional space is mapped to a two dimensional space, as is the case here. a simple hack that nonetheless makes a lot of sense in this case is to use the real number line for two purposes: representing the input dimension, namely the real number line, and one component of the output dimension the real number portion of the output to binet's formula. this admittedly results in a loss of information, since finding the point where won't give us the th fibonacci number; instead, it will only tell us what the fibonacci number is whose real number component equals . nonetheless, this is an approach that makes sense since the real number line is a common dimension in both the input and output data. with this in mind, let's go ahead and try to plot the interpolation of the fibonacci sequence on the complex plane. first, we import the modules we will need. then, we simply specify the domain on the real number line and generate the fibonacci numbers, separating out the real and imaginary components. note that is not going to be used for plotting; instead, we use as the axis, and this is where the loss of temporal information comes in, as mentioned earlier. now, let's go ahead and plot it out! and there it is, the full fibonacci sequence, interpolated across the real numbers. when i first saw this pattern in matt parker's video, i was simply in awe, a loss of words. there's something inexplicably beautiful and wonderful at this pattern, almost as if it was some part of god's plan. okay, maybe i'm being too melodramatic about a graph, but there is no denying that this pattern is geometrically interesting and pleasing to the eye. everything looks so intentional and deliberate. the comments on the aesthetics of the snail shell aside, one point that deserves our attention is what appears to be a straight line. well, turns out that this is, in fact, not a straight line. the only reason why it appears straight is that the snail pattern overshadows the little vibrations on this portion of the graph. indeed, zooming in, we see that there is an interesting damping motion going on. this is what the fibonacci sequence would have looked like had we plotted only the positive domain of the real number line. in this post, we took a look at the fibonacci sequence and its interpolation across the real number line. we could go even crazier, as did matt parker in his own video, by attempting to interpolate the sequence on the complex number plane, at which point we would now have a mapping from two dimensions to two dimensions, effectively forcing us to think in terms of four dimensions. there is no fast, handy way of drawing or visualizing four dimensions, as we are creatures that are naturally accustomed to three dimensions. there are interesting observations to be made with the full fledged complex interpolation of the sequence, but i thought this is already interesting as it is nonetheless. nowadays, i'm reminded of just how many things that i thought i knew well like the fibonacci sequence are rife with things to study and rejoice in wonder. more so than the value of understanding something brand new, perhaps the value of intellectual exploration lies in realizing just how ignorant one is, as ironic as it sounds. i didn't want to end on such a philosophical note, but things have already precipitated contrary to my intentions. anyhow, i hope you've enjoyed reading this post. catch you up in the next one.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
2019-12-16-moment,"the word “moment” has many meanings. most commonly, it connotes a slice of time. in the realm of physics, moment refers to the rotational tendency of some object, similar to how torque measures the change in an object’s angular momentum. as statisticians, however, what we are interested in is what moment means in math and statistics. in this post, we will attempt to shed new light on the topic of probability distributions through moment generating functions, or mgf for short. the mathematical definition of moments is actually quite simple. first moment: second moment: third moment: fourth moment: and of course, we can imagine how the list would continue: the th moment of a random variable would be . it is worth noting that the first moment corresponds to the mean of the distribution, . the second moment is related to variance, as . the third moment relates to the symmetry of the distribution, or the lack thereof, a quality which goes by the name of skewness. the fourth moment relates to kurtosis, which is a measure of how heavy the tail of a distribution is. higher kurtosis corresponds to many outliers, while the converse would signify that the distribution contains little deviations. as you can see, the common theme is that the moment contains information about the defining features of a distribution, which is why it is such a convenient way to present information about a distribution. as the name suggests, mgf is a function that generates the moments of a distribution. more specifically, we can calculate the th moment of a distribution simply by taking the th derivative of a moment generating function, then plugging in 0 for parameter . we will see what is in a moment when we look at the default formula for mgf. this sounds good and all, but why do we want an mgf in the first place, one might ask. well, given that moments convey defining properties of a distribution, a moment generating function is basically an all in one package that contains every bit of information about the distribution in question. enough of the abstract, let’s get more specific by taking a look at the mathematical formula for an mgf. if is a continuous random variable, we would take an integral instead. now, you might be wondering how taking the th derivative of gives us the th moment of the distribution for the random variable . to convince ourselves of this statement, we need to start by looking at the taylor polynomial for the exponential. it’s not difficult to see the coherency of this expression by taking its derivative the derivative of the polynomial is equal to itself, as we would expect for . from here, we can sufficiently deduce that the coherency of can simply be seen by making the substitution . to continue, now that we have an expression for , we can now calculate , which we might recall is the definition of a moment generating function. where the second equality stands due to linearity of expectation. all the magic happens when we derive this function with respect to . at , all terms in except for the very first one go to zero, leaving us with in other words, deriving the mgf once and plugging in 0 to leaves us with the first moment, as expected. if we derive the function again and do the same, and by induction, we can see how the th derivative of the mgf at would give us the th moment of the distribution, . the easiest way to demonstrate the usefulness of mgf is with an example. for fun, let's revisit a distribution we examined a long time ago on this blog: the poisson distribution. to briefly recap, the poisson distribution can be considered as an variation of the binomial distribution where the number of trials, , diverges to infinity, with rate of success defined as . this is why the poisson distribution is frequently used to model how many random events are likely in a given time frame. here is the probability distribution of the poisson distribution. note that denotes the number of occurrences of the random event in question. the task here is to obtain the mean of the distribution, i.e. to calculate the first moment, . the traditional, no brainer way of doing this would be to refer to the definition of expected values to compute the sum computing this sum is not difficult, but it requires some clever manipulations and substitutions. let's start by simplifying the factorial in the denominator, and pulling out some expressions out of the sigma. where the third equality stands due to the variant of the taylor series for the exponential function we looked at earlier: therefore, we have confirmed that the mean of a poisson distribution is equal to , which aligns with what we know about the distribution. another way we can calculate the first moment of the poisson is by deriving its mgf. this might sound a lot more complicated than just computing the expected value the familiar way demonstrated above, but in fact, mgfs are surprisingly easy to calculate, sometimes even easier than using the definition expectation. let's begin by presenting a statement of the mgf. let's factor out terms that contain lambda, which is not affected by the summation. again, we refer to equation to realize that the sigma expression simplifies into an exponential. in other words, from this observation, we can simplify equation as follows: and there is the mgf of the poisson distribution! all we have to do to obtain the first moment of the poisson distribution, then, is to derive the mgf once and set to 0. using the chain rule, at , so we have confirmed again that the mean of a poisson distribution is equal to . let's take another distribution as an example, this time the exponential distribution. we have not looked specifically at the exponential distribution in depth previously, but it is a distribution closely related to the gamma distribution, which we derived in this post. specifically, when parameter in a gamma distribution, it is in effect an exponential distribution. perhaps we will explore these relationships, along with the erlang distribution, in a future post. for now, all we have to know is the probability density function of the exponential distribution, which is this time, the task is to obtain the third moment of the distribution, i.e. . but the fundamental approach remains identical: we can either use the definition of expected values to calculate the third moment, or compute the mgf and derive it three times. at a glance, the latter seems a lot more complicated. however, it won't take long for us to see that sometimes, calculating the mgf is sometimes as easy as, if not easier than, taking the expected values approach. let's twist up the order and try the mgf method first. we can pull out the lambda and combine the exponential terms to get this is an easy integral. let's proceed with the integration and evaluation sequence: now, all we have to do is to derive the result in three time and plug in . although calculating the third order derivative may sound intimidating, it may seem easier in comparison to evaluating the integral which would require us to use integration by parts. in the end, both and are pointing at the same quantity, namely the third moment of the exponential distribution. perhaps the complexity of calculating either quantity is similar, and the question might just boil down to a matter of preference. however, this example shows that the mgf is a robust method of calculating moments of a distribution, and even more, potentially less computationally expensive than using the brute force method to directly calculate expected values. this was a short post on moments and moment generating functions. moments was one of these terms that i had come across on wikipedia or math stackexchange posts, but never had a chance to figure out. hopefully, this post gave you some intuition behind the notion of moments, as well as how moment generating functions can be used to compute useful properties that explain a distribution. in the next post, we will take a brief break from the world of distributions and discuss some topics in information theory that i personally found interesting. if you would like to dwell on the question like ""how do we quantify randomness,"" don't hesitate to tune in again in a few days! moments: https://en.wikipedia.org/wiki/moment_ kurtosis: https://en.wikipedia.org/wiki/kurtosis taylor polynomial: https://en.wikipedia.org/wiki/taylor_series this blog: https://jaketae.github.io/study/poisson/ exponential distribution: https://en.wikipedia.org/wiki/exponential_distribution this post: https://jaketae.github.io/study/gamma/",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
2019-12-31-logistic-regression,"this tutorial is a continuation of the ""from scratch"" series we started last time with the blog post demonstrating the implementation of a simple k nearest neighbors algorithm. the machine learning model we will be looking at today is logistic regression. if the ""regression"" part sounds familiar, yes, that is because logistic regression is a close cousin of linear regression both models are employed in the context of regression problems. linear regression is used when the estimation parameter is a continuous variable; logistic regression is best suited to tackle binary classification problems. implementing the logistic regression model is slightly more challenging due to the mathematics involved in gradient descent, but we will make every step explicit throughout the way. without further ado, let's get into it. to understand the clockwork behind logistic regression, it is necessary to understand the logistic function. simply put, the logistic function is a s shaped curve the squishes real values between positive and negative infinity into the range . this property is convenient from a machine learning perspective because it allows us to perform binary classification. binary classification is a type of classification problem where we are assigned the task of categorizing data into two groups. for instance, given the dimensions of a patient's tumor, determine whether the tumor is malignant or benign. another problem might involve classifying emails as either spam or not spam. we can label spam emails as 1 and non spam emails as 0, feed the data into a predefined machine learning algorithm, and generate predictions using that model. if the output of an algorithm given some data point is larger than 0.5, it is likely that the given input is a spam; if it is smaller than the 0.5 threshold, chances are the email is not spam. let's take a look at the shape of the sigmoid function, which is a special case of the logistic function that we will use throughout this post. to plot the sigmoid function, we need to import some libraries. the sigmoid function is defined as follows: we can express this as a python function, as demonstrated in the code snippet below. let's quickly plot the graph to see what the sigmoid function looks like. as we can see, the sigmoid is a smooth, differentiable function that is bounded between 0 and 1. it is also symmetrical around the point , which is why we can use 0.5 as a threshold for determining the class of a given data point. the logistic regression model uses the sigmoid function to generate predictions, but how exactly does it work? recall that, in the case of linear regression, our goal was to determine the coefficients of some linear function, specifically logistic regression is not so different from linear regression. in fact, we can borrow the same notation we used for linear regression to frame logistic regression as follows: in other words, logistic regression can be understood as a process in which our goal is to find the weight coefficients in the equation above the best describe the given data set. unlike in linear regression, where the predicted value is computed simply by passing the data as arguments into a linear function, logistic regression outputs numbers between 0 and 1, making binary classification possible. however, there is certainly an element of linearity involved, which is part of the reason why both linear and logistic regression models fall under a larger family of models called generalized linear models. now that we know the basic maths behind logistic regression using the sigmoid function, it's time to implement it via code. welcome to the next part of the tutorial, where we start building the actual model from scratch. as always, it's a good idea to have some dummy data ready for disposal so that we can develop some basic intuition about dimensionality of our data when handling inputs and outputs of our functions. here is the data we used in the last post on k nearest neighbors algorithm, slightly modified for the purposes of this post. let's start by translating equation into executable code. the idea is that we want to get a dot product of the weight vector and the data vector, then plug the resulting value into the sigmoid function to get some value between 0 and 1. the function shown below exactly performs this task, with the added caveat that it returns a label prediction when the boolean argument is set to ; a raw sigmoid output when the same is set to . let's perform a quick sanity check by using some dummy weight vector. using the coefficients in the list, we can generate predictions for each observation in the . 0, 0, 1, 0, 0, 0, 0, 0, 0, 0 the actual class information is stored in the list. 0, 0, 0, 0, 0, 1, 1, 1, 1, 1 the dummy coefficients are poorly optimized, which is why the predicted class labels do not align well with the actual class labels. this tells us that more tuning is required to update the coefficients and build a robust logistic regression model. but how exactly can we tune our model? simply eyeballing the actual and predicted labels of our data is probably not going to help us much. to optimize the coefficients to best fit our data, we need to construct some loss function that is, a function that describes how badly our model is performing. then, we can optimize the weights of our model by minimizing that loss function, which would mean that our model gradually makes better predictions with each round of optimization. if you recall the previous post on entropy, you will remember that we discussed a concept called cross entropy. in that post, we derived the formula for cross entropy and intuitively understood it as a way of measuring the ""distance"" between two distributions. this is exactly what we need: a way of quantifying how different the actual and predicted class labels are! recall the formula for cross entropy: we can consider class labels as a bernoulli distribution where data that belongs to class 1 has probability 1 of belonging to that class 1 and probability 0 of belonging to class 0, and vice versa for observations in class 0. the logistic regression model will output a bernoulli distribution, such as , which means that the given input has a 60 percent chance of belonging to class 1; 40 percent to class 0. applying this to , we get: and that is the loss function we will use for logistic regression! the reason why we have two terms, one involving just and another involving is due to the structure of the bernoulli distribution, which by definition can be written as now that we have a loss function to work with, let's build a function that computes cross entropy loss given and using . the function returns the average cross entropy over all input data. we use average cross entropy instead of total cross entropy, because it doesn't make sense to penalize the model for high cross entropy when the input data set was large to begin with. now what's next? since we have a loss function, we need to build an algorithm that will allow us to minimize this cost function. one of the most common methods used to achieve cost minimization is gradient descent. as you might be able to tell, this algorithm has a lot to do with gradients, which can loosely be understood as a fancy way of saying derivatives. below is an illustration of the gradient descent algorithm in action, sourced from this blog. basically, what gradient descent does is that it takes the derivative of the loss function with respect to the weight vector every epoch, or iteration, and takes a small step in the opposite direction of that derivative. if you think of this in the context of two dimensions as shown in the illustration, the gradient descent algorithm ends up moving down the parabola, taking little steps each time, until it eventually reaches the global minimum. in mathematical notation, we might express this process as follows: if we were to perform this in vectorized format, where represents a vector containing the weight coefficients of the logistic regression model: the notation is used to denote gradients, an important operation in matrix calculus which we explored in when deriving the normal equation solution to linear regression on this blog. the denotes a hyperparameter known as the learning rate, which essentially determines how big of a step the gradient descent model takes with each iteration. the main takeaway here is the the gradient descent method allows us to find the local minimum of any convex function, no matter how multidimensional or complex. this is an incredibly powerful statement, and it is one that lies at the core of many machine learning algorithms. to implement gradient descent with code, we have to figure out what the gradient descent equation is in the case of logistic regression. to do this, we need a bit of calculus work using the chain rule. recall that our goal is to compute since we want to calculate the slope of the cross entropy function with respect to the weights vector. for notational convenience, let's denote the gradient as a derivative: the gradient in can be broken down into distinct components via the chain rule: where so the task of calculating the gradient now boils down to finding the derivative for each of the terms shown in . let's start with the easiet one, the last term, which is the derivative of with respect to . from , we can conclude that the next in line is the derivative of the sigmoid function, which goes as follows: now we are almost there. the last piece of the puzzle is computing the first term in , the derivative of the cross entropy function. putting this all together into , we get: voila! we have derived an expression for the gradient of the cross entropy loss function. there is one more tiny little step we have to make to concretize this equation, and that is to consider the average of the total gradient, since as it stands applies to only one data observation. granted, this derivation is not meant to be a rigorous demonstration of mathematical proof, because we glossed over some details concerning matrix transpose, dot products, and dimensionality. still, it provides a solid basis for the construction of the gradient descent algorithm in code, as shown below. to avoid compensating code readability, i made a stylistic choice of using and to denote the vector of coefficients instead of using for notational consistency. other than adding some switch optional parameters such as or , the code simply follows the gradient descent algorithm outlined above. note that equation is expressed via ; equation is expressed by the line . let's quickly check that the function works as expected using the dummy data we created earlier. iteration 0, cost: 1.182528373826317 iteration 50, cost: 0.1306481850308255 iteration 100, cost: 0.07491036607639494 iteration 150, cost: 0.053585058580496114 array great! we see that the average cross entropy decreases with more iterations. the returned array contains the coefficients of the logistic regression model, which we can use to now make predictions. we can stop here, but just like we did in the post on k nearest neighbors, let's wrap all the functions we have created so far into a single function that represents the logistic regression model. our model is ready. time for testing with some real world data. let's import some data from the web. the data we will be looking at is the banknote authentification data set, publicly available on the uci machine learning repository. this data set contains 1372 observations of bank notes, classified as either authentic or counterfeit. the five features columns of this data set are: variance of wavelet wavelet transformed image skewness of wavelet transformed image kurtosis of wavelet transformed image entropy of image class let's use some modules to import this data set onto our notebook, as shown below. the imported data set was slightly modified in two ways to fit our model. first, i separated the class label data from the data set and stored it as a separate array. second, i appended s to each observation to create a new column that accounts for intercept approximation. all this means is that we consider our linear model to be where for all available sample observations. this is something that we have been assuming all along throughout the gradient descent derivation process, but had not been stated explicitly to reduce confusion. just consider it a strategic choice on our part to simplify the model while allowing for the logistic regression model to consider bias. let's check the shape of the imported data set to check that the data has been partitioned correctly. now, it's time to split the data into training and testing data. to do this, i recycled a function we built earlier in the previous post on k nearest neighbors algorithm. using , we can partition the data set into training and testing data. let's make 20 percent of observations as testing data and allocate the rest for training. it's time for some training and prediction generation. because we did all the work in the previous section, training and predicting can be achieved with just a single line of command. to see how quickly average cross entropy is decreasing, i turned on the as true. this way, we can see how quickly the loss is declining over every 50 epochs. iteration 0, cost: 1.8645873915204194 iteration 50, cost: 0.12724295853835318 iteration 100, cost: 0.09500780070378237 iteration 150, cost: 0.08051200520236851 iteration 200, cost: 0.07152813537793914 iteration 250, cost: 0.06521190357551303 iteration 300, cost: 0.06046029372003293 iteration 350, cost: 0.0567287055259228 iteration 400, cost: 0.05370806680847458 iteration 450, cost: 0.05120649109874576 iteration 500, cost: 0.04909714872119953 iteration 550, cost: 0.047292239538382304 iteration 600, cost: 0.045728769034644075 iteration 650, cost: 0.04436022796651144 iteration 700, cost: 0.04315146064694956 iteration 750, cost: 0.042075362704003874 iteration 800, cost: 0.041110680842644444 iteration 850, cost: 0.04024050353492443 iteration 900, cost: 0.0394511996919855 iteration 950, cost: 0.038731656216495214 it's now time to see how well our model has done. let's compare , the list that contains the model's predictions, with , which is essentially the answer key. this is great news. the result shows us that we have correctly predicted 272 values while making wrong predictions in only 2 cases. let's systematize this quantity by creating a function that returns how accurate our model is given and . let's use this function to test how well our model performed. 0.9927007299270073 99 percent is not a bad estimate at all. one interesting question to consider is how much boost in accuracy we see with each epoch, i.e. what is the bang per buck of each iteration cycle? this is an important question to consider because gradient descent is computationally expensive; if we can train our model in just 10 epochs instead of 1000, why not choose the former? to answer this question, let's plot accuracy against epoch. for fun, i added the learning parameter as an argument to the function as well. let's create a plot to see how accuracy changes over 200 epochs, given a learning rate of 0.1. we see that accuracy spikes up on the first 20 epochs or so and quite quickly converges to about 90 percent. past a certain threshold, the model seems to hover consistently at around the high 90s range, but accuracy still continues to increase ever so slightly with each epoch, though not as quickly as before. if we set the learning rate to a smaller number, we would expect the model to take a lot longer to tune, and indeed this seems to be true: with a much smaller learning rate, the model seems to struggle to achieve high accuracy. however, although there are a lot of uneven spikes, the model still manages to reach a pretty high accuracy score by 200 epochs. this tells us that the success of model training depends a lot on how we set the learning rate; setting an excessively high value for the learning rate might result in overshooting, while a low learning rate might prevent the model from quickly learning from the data and making meaningful progress. accuracy helps us intuitively understand how well our model is doing, but recall that the main objective of gradient descent is not to maximize accuracy, but to minimize the cross entropy loss function. therefore, perhaps it makes more sense to evaluate the performance of our logistic regression model by plotting cross entropy. presented below is a simple function that plots epoch versus cross entropy given a list of learning rates, . let's plot cross entropy loss for three different values of : 0.05, 0.1, and 0.5. just like before, we cap the number of iterations to 200 epochs. the graph shows that the larger the learning rate, the quicker the decrease in cross entropy loss. this result is coherent with what the previous visualizations on accuracy suggested: the higher the learning rate, the quicker the model learns from the training data. in this post, we built the logistic regression model from scratch by deriving an equation for gradient descent on cross entropy given a sigmoid function. in the process, we brought together many useful concepts we explored on this blog previously, such as matrix calculus, cross entropy, and more. it's always exciting to see when seemingly unrelated concepts come together to form beautiful pictures in unexpected ways, and that is what motivates me to continue my journey down this road. the logistic regression model is simple yet incredibly powerful in the context of binary classification. as we saw earlier with the application of the model to the task of bank notes authentication, the logistic regression model can, when tweaked with the appropriate parameters, make surprisingly accurate predictions given sufficient amount of training data. of course, the processing of training and tweaking is not always easy because we have to determine some hyperparameters, most notably the learning rate of the gradient descent algorithm, but the fact that logistic regression is a robust model is unchanged nonetheless. hopefully this post gave you some idea of what happens behind the scene in a regression based machine learning model. thanks for reading. see you in the next post, and happy new year! this blog: https://scipython.com/blog/visualizing the gradient descent method/ the blog post: https://jaketae.github.io/study/knn/ logistic function: https://en.wikipedia.org/wiki/logistic_function linear regression: https://jaketae.github.io/study/linear regression/ binary classification: https://en.wikipedia.org/wiki/binary_classification generalized linear models: https://en.wikipedia.org/wiki/generalized_linear_model previous post: https://jaketae.github.io/study/information entropy/ cross entropy: https://en.wikipedia.org/wiki/cross_entropy uci machine learning repository: https://archive.ics.uci.edu/ml/datasets/banknote+authentication gradient descent: https://en.wikipedia.org/wiki/gradient_descent",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0
2020-01-07-anaconda,"as a novice who just started learning python just three months ago, i was clueless about what virtual environments were. all i knew was that anaconda was purportedly a good way to download and use python, in particular because it came with many scientific packages pre installed. i faintly remember reading somewhere that anaconda came with conda, a package manager, but i didn't really dig much into it because i was busy learning the python language to begin with. i wasn't interested in the complicated details i just wanted to learn how to use this language to start building and graphing and calculating. and because i had on idea about how to use anaconda or conda for that matter, i used to install packages left and right, using the pip installer through the command or through the conda installer, with some optional arguments and channel specifications that google and stack exchange search would tell me to do. for a package available on , i might have triggered a command something like this: little did i know back then that, my abusing these installation methods, my system root directory was being clogged up with so countless python packages and modules. the reason why i never bothered to dig into the intricacies of these installation options was that i never felt the need to: they almost always worked. after installing a package, i had no trouble importing it on pycharm or sublime. of course, there were some packages that simply didn't work, such as , but those were extremely rare, and i was always able to find an alternative package with similar functionality. admittedly, this in large part because the packages i use most often are extremely standard, such as , , and occasionally and , all of which are shipped with anaconda by default. problems started to arise only recently when i decided to starting learning r to expand my knowledge in the domain of statistical computing. after some quick research, i found that there were two ways of installing r on a local system. the most popular and easiest way was to download it straight from cran, the comprehensive r archive network. this was the course i followed first, and it was great until i realized that the better way to go would be to manage everything with conda through virtual environments. instead of a system wide installation of r, why not create a designated space for r within conda, and use jupyter notebooks from there? this question led me down a rabbit hole of stack exchange questions and medium posts. i summarize my findings here. installing r with conda could quite easily be achieved with a single command line. this command installs both r and the package, which includes approximately 80 most popular scientific packages for the r programming language. great! it took a few minutes for my laptop to complete the installation, but it eventually got through. now, i should be good to go, right? only, i noticed a few problems. first, when i opened a new terminal window, i realized that it took noticeably longer for the terminal to load. the bash shell seemed to look up multiple languages, such as , , and before fully loading, which i figured was a problem even to the uneducated eye, this meant that the system was unnecessarily clogged. it didn't take me long to realize that the cause of the problem was that all packages were being installed in the root directory of my system as mentioned earlier. this is not the recommended way of installing packages: in fact, the reason why we have package managers like conda is precisely to prevent users from installing everything in the root. conda allows users to create what are called ""virtual environments."" the simplest way to think about virtual environments that they are isolated universes where different packages live. having these isolated worlds is convenient because it means that users can install different versions of the same package. for instance, in a virtual environment called , we might install version 1.1; in , numpy versiosn 1.15. why might we want to do this? well, certain libraries that use as a dependency might require a specific version of . if we didn't have virtual environments, we would have to downgrade installed system wide on the root directory, and who knows what other compatability issues might pop up after the forced downgrade. virtual environments is essentially a very convenient way of managing where packages are installed while leaving the root directory clean and uncluttered. so the solution to my problem was simple: perform a conda reset to clean the root directory, which was then cluttered with both python and r modules. then, create virtual environments each for python and r and reinstall modules as necessary. through this, what i wanted to achieve was a clean system root environment, with all packages installed in specific virtual environments that i could toggle on or off depending on my workflow. resetting the conda root environment can simply achieved with a single line of command. on the terminal, type this will delete all pacakages installed in the root directory and revert the system back to where it was when we first installed anaconda. note that this includes the package we installed earlier. before running this command, i obtained a list of all libraries installed on the root just to see exactly what packages i had installed would be erased. to see the list of all available modules, type and the terminal will display the list of all packages installed in the current environment. after the resetting was complete, i ran a quick conda update command to make sure that all libraries werre up to date. now that the root directory has been unclogged, it's time to create virtual environments and reinstall packages onto our system! creating virtual environments is very easy. to create a python conda virtual environment, just type if you want to create a conda environment with all anaconda packages pre installed, add as an optional argument. notice that we can specify the python version on which the environment is going to run. this is another convenient perk of using virtual environments: we can use multiple python versions on the same machine simply by creating multiple virtual environments corresponding to each version. we can also create a virtual environment for r. to achieve this, simply type and now we are done! to make sure that all the virtual environments have been properly created, we can run a quick check command. in my case, i have created a total of 4 virtual environments, excluding the default root directory. to activate a virtual environment, we can use the command. for example, if i want to activate the virtual environment i created specifically for r, i would type and i would be good to go! objective achieved: clean the root directory and create a designated conda virtual environment for the r programming language. if you have been following along, you will realize that we still haven't figured out a way to use jupyter notebooks with r. in fact, if you have been following this post so far, you will realize that you only have one kernel available on jupyter notebooks, and that is python installed on the system root. to allow jupyter notebook to recognize each conda virtual environment, we need some tweaking to do: namely, installing kernel packages. let's say i want to use jupyter notebook with the virtual environment. first, let's activate the environment. next, let's install , which will allow jupyter notebook to recognize the conda virtual environment. then, we have to link the to jupyter notebooks with the following command: that's it! now, all we have to do is to repeat this process for the rest of the python conda virtual environments. for the virtual environment, a slightly different alteration has to be made to this procedure: instead of , we need to install through the following command. of course, we should execute this command after activating the r conda virtual environment. now, if we boot up jupyter notebook, we should be able to see that there is a kernel available for every conda virtual environment! now, we have an integrated system of virtual environments sall managed under conda, each conveniently accessible as kernels within jupyter notebooks! anaconda offers the option of installing rstudio through the anaconda distribution. however, much like the vast majority of r users i saw online, i do not recommend downloading rstudio through anaconda: the rstudio application available on anaconda is not up to date, nor is it supported by cran. instead, we can install rstudio through cran, but configure rstudio to locate the r profile in the anaconda directory instead of the standard directory that would be created had we installed the r language through cran. this sounds a needlessly complicated, so let's hash it out step by step. follow this link to install the rstudio application, which is the most widely used default ide for the r programming language. the good thing about rstudio is that it is free and open source, which is something that everybody likes. the installation should be very straight forward, so i'll jump straight into the problem. when you complete the installation and run the application, you will run into this error message. this message means that rstudio was unable to locate the directory where r is installed. the problem arises because we installed r through conda, not through cran. therefore, we have to manually specify the anaconda directory where r is installed for rstudio. in other words, we have to force rstudio to use a specific version of r installed on our system. this can be achieved by typing this bash command in the terminal: but how do we figure out where the anaconda r directory is located? first, let's activate the conda virtual environment where r is installed. next, we invoke the command to see where r is installed. great! now we know where the r profile is. all we have to do is use the command to configure the r directory for rstudio. in my case, this would look as follows: and now we are done! well, almost. if you try to open rstudio, it will still show you the same error. instead of opening the app via the launchpad, we have to open rstudio through the same terminal we used to configure rstudio. in other words, in the same terminal window, type now, rstudio should boot up and you should be able to use it without any problems! this approach works, but it's also inconvenient because we have to go through this process every time when we want to open up rstudio. not really productive, is it? the way to go about this problem is to create a shell alias. an alias is basically a personalized command on the terminal that we can invoke. using an alias, we can also chain multiple bash commands into a sweet, single line. pretty convenient, isn't it? it also suits our purpose because essentially the steps we have to take to launch r is type multiple commands on the shell interface before finally launching rstudio from the terminal. to create a system wide alias, we have to add an alias to our bash profile. first, let's locate the bash profile on our system in the root directory this is what shows up in my terminal. it tells me that there are 320 files in the root directory, one of which is the file that we have to edit to make our alias system wide and permanent. let's open the . a text editor will pop up, allowing us to view and edit the contents of the . after the last line of the profile, add the following lines. the alias i created is . essentially, what it does is that it chains multiple linux command lines into one convenient command that i can invoke in any terminal. it will automatically configure rstudio to look for the specific anacdona directory where r in installed in and open the rstudio application by browsing into the appropriate directory. in other words, instead of going through all the hassle, we can now simply type , and rstudio will fire up, ready for use! as someone who was unfamiliar with virtual environments, bash, and command lines, this little rabbit hole down the quest of using r with conda was an interesting experience. although all of what was dealt in this post was pretty basic and elementary tweaking, i am glad i was able to produce a working system all under conda. now that my r environment setup is complete, it's time to go back to studying the r programming language. i've only taken a cursory look at r, but the syntax seems interesting, and there are both parallels and differences i see in comparison to other langauges i know like python or java. hopefully i can get the hang of r and start using it for data analysis. catch you up in the next one! r: https://www.r project.org anaconda: https://anaconda.org conda: https://conda.io/en/ cran: http://cran.r project.org/mirrors.html this link: https://rstudio.com shell alias: https://www.geeksforgeeks.org/alias command in linux with examples/",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2020-08-01-gaussian-mixture-models,"we've discussed gaussians a few times on this blog. in particular, recently we explored gaussian process regression, which is personally a post i really enjoyed writing because i learned so much while studying and writing about it. today, we will continue our exploration of the gaussian world with yet another machine learning model that bears the name of gauss: gaussian mixture models. after watching yet another inspiring video by mathematicalmonk on youtube, i meant to write about gaussian mixture models for quite some time, and finally here it is. i would also like to thank ritvikmath for a great beginner friendly explanation on gmms and expectation maximization, as well as fiveminutestats for a wonderful exposition on the intuition behind the em algorithm. without further ado, let's jump right into it. the motivating idea behind gmms is that we can model seemingly complicated distributions as a convex combination of gaussians each defined by different parameters. one visual analogy i found particularly useful is imagining gaussians as some sort of hill or mountain on a contour map. if we have multiple hills adjacent to one another, we can essentially model the topography of the region as a combination of gaussians. at peaks, we would see circular contour lines, but where the hills meet, we might see different patterns, most likely circular patterns overlapping with each other. the key point here is that the combination is convex; in other words, the mixing coefficient for each gaussian should add up to one. if we consider gmm to be a generative model, then we can imagine the generating process as follows: sample a class index from a categorical distribution to determine the class in which the data point will belong sample a data point from the gaussian distribution that corresponds to the sampled class index at this point, for clarity's sake, let's introduce some notations and concretize what has been elaborated above. first, we define a categorical distribution that will represent the mixing coefficients described above. let be an dimensional vector that parametrizes this categorical distribution. in other words, this means that we assume the data to have different clusters. each is then a mixing coefficient that establishes the convexness of the linear combinations of the underlying gaussians. now we can sample the cluster index, denoted as , from the categorical distribution as shown below. note that we use for the cluster index, since it is considered a latent variable we don't observe it directly, yet it is deeply involved in the data generation process. quite simply, the probability that a data point belongs to the th cluster is represented by . the last step to data generation, as outlined in the bullet points above, is sampling a point from the corresponding gaussian. where and are the mean and covariance matrices that parameterize the th gaussian in the mixture model. now that we have an idea of how gmms can serve as a generative model that describes the underlying data generation process, let's think about the marginal probability that is, the probability that some point in the sample space was generated by the gmm. after all, what we observe is the final output, not the latent variables. therefore, it would be convenient to be able to come up with some expression for the marginal distribution. we can come up with a simple expression using the law of total probability and marginalization. where can take values between 1 and . notice that we can thus simplify , as this is simply the categorical distribution for the mixing coefficients. in other words, we can also simplify the condition probability expression, since we already know that it follows a normal distribution. therefore, we end up with and there we have it, the density function of the gaussian mixture model! we have a convex combination of different gaussian pdfs that compose the gaussian mixture model. in the context of machine learning, the goal is to find the parameters of the model that best describe some given set of data. in the case of gmms, this is no different. the parameters that we have to estimate are of course, and are not single vectors or matrices, but collection of such objects. but for notational simplicity, i opted to write them as such as shown above. given a collection of data points, denoted as , we can now come up with the following expression: denotes the likelihood function, and is a collection of all parameters that define the mixture model. in other words, all this is doing is that we are using the marginal distribution expression we derived earlier and applying that to a situation in which we have data points instead of just one, which is the context we have been operating in so far. we multiply the probabilities given the assumption of independence. from the perspective of maximum likelihood estimation, the goal then would be to maximize this likelihood to find the optimal parameters for and . before we attempt mle with the likelihood function, let's first try to calculate the log likelihood, as this often makes mle much easier by decoupling products as summations. let . and now we see a problem: the log is not applied to the inside of the function due to the summation. this is bad news since deriving this expression by will become very tedious. the result is for sure not going to look pretty, let's try deriving the log likelihood by and set it equal to zero. at least one good news is that, for now, we can safely ignore the outer summation given the linearity of derivatives. we ultimately end up with the expression below: this is not pretty at all, and it seems extremely difficult, if not impossible, to solve analytically for . this is why we can't approach mle the way we usually do, by directly calculating derivatives and setting them equal to zero. this is where expectation maximization, or em algorithm comes in. before we get into the details of what the em algorithm is, it's perhaps best to provide a very brief overview of how the em algorithm works. a very simple way to understand em is to think of the gibbs sampler. simply put, gibbs sampling is a way of approximating some joint distribution given conditional distributions. the underlying idea was to sample from one distribution and use that sampled result to in turn generate another sample from the next conditional distribution. one might visualize this as a chain of circular dependence: if we obtain a sample from past samples, the new sample can then be used to generate the next sample. repeat this process until convergence, and we are done. turns out that the gibbs sampler can be considered a specific flavor of the em method. although i am still far away from fully understanding the inner workings of the em algorithm, the underlying idea is clear: given some sort of dependence relationship, much like we saw in the case of gibbs sampling above, we can generate some sample in one iteration and use that sample in the next. as we will see in this section, such a relationship can clearly be established, which is why em is so commonly used in the context of gaussian mixture models. let's begin by defining some quantities, the first being the posterior distribution. in other words, given some data, what is the probability that it will belong to a certain class? using the definition of conditional probability, we can arrive at the following conclusion: this represents the probability that, given some point , the point belongs in the th cluster. with this result, we can now rewrite the mle calculation that we were performing earlier. using the new notation, we can thus simplify the result down to we can then simplify this expression to derive an expression for . an important trick is here to use the fact that the covariance matrix is positive semi definite. therefore, the covariance matrix plays no role in driving the value down to zero. with some algebraic manipulations, we arrive at let's introduce another notational device to simplify the expresison even further. let recall that was defined to be the posterior probability that a given point belongs to the th cluster. then, since we are essentially summing up this quantity across the entire data points in the dataset , we can interpret to effectively be the number of points in the dataset that are assinged to the th cluster. then, we can now simplify the mle estimate of the mean as but we can now observe something interesting. notice that a depend on . in turn, is defined in terms of . this is the very circular dependency that we discussed earlier as we were introducing the em algorithm and comparing it with the gibbs sampler. now it becomes increasingly apparent why the em algorithm is needed to find a converging solution for the mle estimates. we can take a similar approach to calculate the mle of the other two remaining paramters, namely and . the derivation is more complicated since is a matrix; is subject to constraints that apply to any categorical distribution: all elements must be positive and must sum up to one. for my own personal reference and the curious minded, here is a link to a resource that contains the full derivation. but the fundamental idea is that we would commence from the log likelihood function and derive our way to the solution. the solutions are presented below: so the full picture is now complete: given the inter dependence of derived quantities, we seek to optimize them using the expectation maximization algorithm. specifically, the em method works as follows: initialize the parameters with some value e step: estimate the posterior probability using the other two parameters m step: estimate and using calculated in the previous step repeat until convergence in today's post, we took a deep dive into gaussian mixture models. i find gmms to be conceptually very intuitive and interesting at the same time. i'm also personally satisfied and glad that i have learned yet another ml/mathematical concept that starts with the word gaussian. much like how i felt when learning about gaussian process regression, now i have an even greater respect for the gaussian distribution, although i should probably be calling it normal instead, just like everybody else. i'm also happy that i was able to write this blog post in just a single day. of course, this is in large part due to the fact that i had spent some time a few weeks ago studying this material, but nonetheless i think i'm starting to find the optimal balance between intern dev work and self studying of math and machine learning. i hope you've enjoyed reading this post. in a future post, i will be implementing gaussian mixture models in python from scrach. stay tuned for more!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0
2020-06-02-dissecting-lstm,"in this post, we will revisit the topic of recurrent neural networks, or rnns. although we have used rnns before in a previous post on character based text prediction, we glossed over lstm and assumed it as a black box that just worked. today, we will take a detailed look at how lstms work by dissecting its components. note that this post was inspired by this article by kristiadi. i also heavily referenced this post by christopher olah. if you find any part of this article intriguing and intellectually captivating, you will surely enjoy reading their blogs as well. with this in mind, let's jump right into it. long short term memory networks, or lstms for short, are one of the most widely used building blocks of recurrent neural networks, or rnns. this is because lstms overcame many of the limitations of basic vanilla rnns: while simple rnn gates are bad at retaining long term information and only remember input information that were fed into it relatively recently, lstms do a great job of retaining important information, even if they were fed into the cell long time ago. in other words, they are able to somewhat mimic the function of the brain, which involves both long and short term memory. the structure of an lstm cell might be summarized as follows: note that represents the hadamard product), which is nothing more than just the element wise multiplication of matrices. this long list of equations surely looks like a lot, but each of them has a specific purpose. let's take a look. the first component of lstm is the forget gate. this corresponds to these two set of equations: is nothing more than just the good old forward pass. we concatenate and , then multiply it with some weights, add a bias, and apply a sigmoid activation. at this point, you might be wondering why we use a sigmoid activation instead of something like relu. the reason behind this choice of activation function becomes apparent once we look at , which is how lstm imitates forgetting. for now, we will only focus on the first term in . recall that the output of a sigmoid activation is between 0 and 1. say the output of applying a sigmoid activation results in some value that is very close to 0. in that case, calculating the hadamard product will also result in a value of an entry very close to 0. given the interpretation that , also known as the cell state, is an artificial way of simulating long term memory, we can see how having zeros is similar to forgetfulness: a zero entry effectively means that the network deemed a particular piece of information as obsolete and decided to forget it in favor of accepting new information. in short, the sigmoid activation and the hadamard product form the basis of lstm's forget gate. by now, it should be apparent why we use sigmoid activations: instead of causing divergence with something like relu, we want to deliberately saturate and cause the network to produce some ""vanishing"" values. but if our lstm network only keeps forgetting, obviously this is going to be problematic. instead, we also want to update the cell state using the new input values. let's take a look at the cell state equation again: previously when discussing the forget gate, we focused only on the first term. taking a look at the second term, we note that the term is adding some value to the cell state that has been updated to forget information. it only makes sense, then, for the second term to perform the information update sequence. but to understand the second term, we need to take a look at two other equations: is another forward pass involving concatenation, much like we saw in with . the only difference is that, instead of forgetting, is meant to simulate an update of the cell state. in some lstm variants, is simply replaced with , in which case the cell state update would be rewritten as however, we will stick to the convention that uses instead of the simpler variant as shown in . the best way to think of or is a filter: is a filter that determines which information to be updated and passed onto the cell state. now all we need are the raw materials to pass into that filter. the raw material is , defined in . notice that we use a activation instead of a sigmoid, since the aim of is not to produce a filter with sparse entries, but rather to generate substance, or potential information to be stored in memory. this is more in line with the classic vanilla neural network architecture we are familiar with. now, we can finally glue the pieces together to understand : we enforce forgetfulness, then supply the cell state with new information. this is now the updated cell state, which gets passed onto the next sequence as new inputs are fed into the lstm. so far, we have only looked at the recurrent features of lstm; in other words, how it uses information from the past to update its knowledge in the present at time . however, we haven't yet discussed the most important part of any neural network: generating output. obviously, all that hassle of forgetting and updating the cell state would be utterly meaningless if the cell state is not used to generate output. the whole purpose of maintaining a cell state, therefore, is to imitate long and short term memory of the brain to generate some output. thus, it is no surprise that the following two equations are structured the way they are: first, we see the familiar forward pass, a familiar structure we have seen earlier. borrowing the analogy we established in the previous post, is a filter that decides which information to use and drop. the raw material that we pass into this filter is in fact the cell state, processed by a activation function. this is also a familiar structure we saw earlier in the information update sequence of the network. only this time, we use the cell state to generate output. this makes sense somewhat intuitively, since the cell state is essentially the memory of the network, and hence to generate output would require the use of this memory. of course, this should not be construed so literally since what ultimately happens during backpropagation is entirely up to the network, and at that point we simply lay back and hope for the network to learn the best. this point notwithstanding, i find this admittedly coarse heuristic to be nonetheless useful in intuiting the clockwork behind lstms. at this point, we're not quite done yet; is not a vector of probabilities indicating which letter is the most likely in a one hot encoded representation. therefore, we will need to pass it through another affine layer, than apply a softmax activation. hence, is the final output of an lstm layer. here comes the tricky part: backprop. thankfully, backprop is somewhat simple in the case of lstms due to the use of hadamard products. the routine is not so much different from a vanilla neural network, so let's try to hash out the equations. as we already know, backpropagation in neural networks is merely an extended application of the chain rule, with some minor caveats that matrix calculus entails. first, let's begin slow and easy by deriving the expressions for the derivative of the sigmoid and the functions. first, below is the derivative of the sigmoid with respect to , the input. recall that the sigmoid function is defined as . let's do the same for . one useful fact about is the fact that it is in fact nothing more than just a rescaled sigmoid. this relationship becomes a bit more apparent when we graph the two functions side by side. let's refer to the definition of to derive an expression for its derivative . we know that note that is denoted as in the code segment. since we have this information, now it's just a matter of back propagating the gradients to the lower segments of the acyclic graph that defines the neural network. given , it only makes sense to continue with the next parameter, . the transpose or the order in which the terms are multiplied may be confusing, but with just some scratch work on paper, it isn't difficult to verify these gradients by checking their dimensions. the equation for is even simpler, since there is no matrix multiplication involved. thus, the gradient flows backwards without any modification. moving down a layer, we come across : let's begin by trying to find the gradient for . you might be wondering what the term is doing in that equation. after all, isn't that quantity precisely what we are trying to calculate? this is the one tricky yet also interesting part about rnn backpropagation. recall that the whole point of a recurrent neural network is its use of variables from the previous forward pass. for example, we know that in the next forward pass, will be concatenated with the input . in the backpropagation step corresponding to that forward pass, we would have computed ; thus, this gradient flows into the current backpropagation as well. although this diagram applies to a standard rnn instead of an lstm, the recurrent nature of backprop still stands. i present it here because i find this diagram to be very intuitive. if you look at the right, the star represents the gradient from the last pass. if you look to the left, you will see that there is going to be a gradient for that will eventually be passed over to the next backpropgation scheme. since the forward pass is recurrent, so is the backward pass. since we have , now it's time to move further. let's derive the expression for the gradient for . let's do the same for the other term, . to make things easier, let's make a quick substitution with an intermediate variable, i.e. let . then, but was just an intermediate variable. how can we get the gradient for itself? well, since the only transformation was just a , chain rule tells us that all we need is to multiply the antiderivative of , which we already derived above. also keep in mind that since is a recurrent variable, we have to apply the gradient from the next call as well, just like . note that all we had to do is to multiply the function we derived above, then add the backpropgation from the next iteration to account for the recurrent nature of the network. we still have a decent amount of work to do, but the fortunate news is that once we derive an expression for one parameter, the rest can also be obtained in an identical fashion. therefore, for the sake of demonstration, we will only deal with and . let's start with the easier of the two, . recall that as we have done earlier, let's introduce an intermediate variable, , and try deriving the gradient for that variable. note that with this substitution, . now we can move onto deriving the expressions for the gradient of the actual parameters, starting with . this is extremely simple since and are defined by a linear relationship. the next in line is . this is also very simple, since all we need to do is to consider one instance of matrix multiplication. where, given a concatenation operator , now we are done! the gradient for the rest of the parameters, such as or look almost exactly the same as and respectively, and not without reason: as we have noted above, what i conveniently called the filter and raw material structure of lstm gates remain consistent across the forget, input, and output gates. therefore, we can apply the same chain rule to arrive at the same expressions. however, there is one more caveat that requires our last bit of attention, and that is the gradient for . note that had been concatenated to the input in the form of throughout the forward pass. because this was a variable that was used during computation, we need to calculate its gradient as well. this might appear rather confusing since we are currently looking at time , and it seems as if the gradient for variables should be happening in the next iteration of backpropagation. while this is certainly true for the most part, due to the recurrent nature of lstms, we need to compute these gradients for in this step as well. this is precisely what we were talking about earlier when discussing the recurrent nature of backprop; the we compute here will be used in the next iteration of backpropagation, just like we added in the current backprop to calculate . becaue was used in many different places during the forward pass, we need to collect the gradients. given an intermediate variable we can express the gradient in the following fashion: then, we can obtain by un concatenation: where denotes the number of neurons in the lstm layer. we can do the same for . this is a lot simpler: these gradients, of course, will be passed onto the next iteration of backpropagation, just like we had assumed that the values of and were given from the previous sequence of backpropagation. because dl libraries make it extremely easy to declare and train lstm networks, it's often easy to gloss over what actually happens under the hood. however, there is certainly merit to dissecting and trying to understand the inner working of dl models like lstm cells, which offer a fascinating way of understanding the notion of memory. this is also important since rnns are the basis of other more complicated models such as attention based models or transformers, which is arguably the hottest topic these days in the field of nlp with the introduction of gpt 3 by openai. i hope you have enjoyed reading this post. catch you up in the next one!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2020-05-19-sql-basics,"recently, i was compelled by my own curiosity to study sql, a language i have heard about quite a lot but never had a chance to study. at first, sql sounded difficult and foreign largely because it was a language fundamentally different from other programming languages i had studied, such as java, python, or r. however, after watching this fantastic video tutorial on youtube, and completing a relevant course on datacamp, i think i now finally have a somewhat concrete understanding of what sql is and how to use it. of course, i'm still so far away from being fluent in sql, and the queries i can write are still pretty basic. much like the blog post on r, this post will serve as a reference for myself. note: this notebook was drafted in january of 2020, yet i never had a chance to finish it. finally, while working on some r tutorial notebooks on the package, i was reminded of this draft and hence decided to publish it. hopefully this editorial discontinuity does not affect the quality of writing and content of this article. there are many different ways of using and accessing sql from jupyter notebooks. here, i introduce two simple ways of practicing sql without much complicated setup. the first on the list is , which allows us to use magic commands in jupyter notebooks. to install, simply type the following line in the terminal, assuming that you have activated the conda virtual environment of choice. we can now use the magic command in jupyter to connect to a local database. in my case i had a mysql database initialized at localhost, and was able to connect to it as a root user. note that you should replace in the example command below according to your own configuration. 'connected: root@test' now that we have successfully connected to the data, we can use sql commands in jupyter! mysql+pymysql://root:@localhost:3306/test 5 rows affected. emp_id first_name last_name birth_day sex salary super_id branch_id 103 angela martin 1971 06 25 f 63000 102 2 101 jan levinson 1961 05 11 f 110000 100 1 104 kelly kapoor 1980 02 05 f 55000 102 2 107 andy bernard 1973 07 22 m 65000 106 3 100 david wallace 1967 11 17 m 250000 none 1 this method works, but it requires that you set up a mysql server on your local workstation. while this is not particularly difficult, this method is somewhat made less compelling by the fact that it does not work right out of the box. the method i prefer, therefore, is the one that i would like to introduce next. is an incredibly widely used python module for deailng with tabular data. it some similarities with sql in that they both deal with tables at the highest level. of course, the two serve very different purposes: sql is intended as a backend exclusive language, powering huge database servers and allowing developers to quickly query through large amounts of data. , on the other hand, is a must have in the python data scientist's toolbox, allowing them to extract new insight from organized tabular data. is a python module that allows us to query s using sql syntax. in other words, it is a great way to learn sql. the benefit of this approach is that no database setup is necessary: as long as there is some tabular data to work with, say some file, we are ready to go. for the purposes of this post, we will thus be using this latter approach. with all that said, let's get started. in this section, we will go over some basic core sql statements to get our feet wet. it would be utterly impossible for me to cover sql syntax in any level of detail in a single blog post, but this is a start nonetheless. at the minimum, i hope to continue this series as i start learning more sql. the main references used to write this post were this excellent medium article and the official documentation on the website. let's begin by loading a library to import some sample toy datasets at our disposal. let's first see a simple example of in action, alongwith . one of the perks of sql is that it somewhat reads like plain english instead of complicated computer code. of course, sql statements can get quite complex, in which case this rule starts to break down. however, it isn't too difficult to see what the statement above is doing: it is selecting the column and from the dataset which we loaded, and showing the top five results only in accordance with the . we can also replicate the output of by doing the following. the is essentially a wild card argument that tells sql that we want information pulled from every column instead of a specified few. this can be handy when we want to take a glimpse of the contents of the database. is not the only addition we can make to a statement. for instance, consider the keyword , which does exactly what you think it does: as you can see, allows us to select only unique values in the table. note that offers a simliar function, , with which we can somewhat recreate a similar result. array another useful fact to remember is that most often goes along with . we can imagine many instances where we would want to retrieve only those data entries that satisfy a certain condition. in the example below, we retrieve only those data entries whose species are labeled as . in speak, we would have done the following: 0 1.4 1 1.4 2 1.3 3 1.5 4 1.4 name: petal_length, dtype: float64 the version is not too difficult just yet, buti prefer sql's resemblance to plain human language. just for the sake of it, let's take a look at a slightly more complicated conditioning we can perform with , namely by linking multiple conditions on top of each other. in this example, we select and for only those entries whose species is setosa and is smaller than 3.2 . all we did there was join the two conditions via the keyword. in , this is made slighty more confusing by the fact that we use slicing to make multi column selections. and by the same token, the sql keyword translates into in . instead of sticking in the end, we could have used as we have been doing so far. it isn't difficult to see that introducing more conditionals can easily result in somewhat more longer statements in python, whereas that is not necessarily the case with sql. this is not to say that is inferior or poorly optimized; instead, it simply goes to show that the two platforms have their own comaprative advantages and that they mainly serve different purposes. often time when sorting through some tabular data, we want to sort the entries in ascending or descending order according to some axis. for example, we might want to rearrange the entries so that one with the largest comes first. let's see how we can achieve this with sql. by default, the keyword in sql lists values in asending order. to reverse this, we can explicitly add the keyword. we see that the entries with the largets is indeed at the top of the selected query result. we can also achieve a similar result in . in this case, i think also offers a simple, clean interface to access data. one point to note is that both sql and have the same default settings when it comes to ordering or sorting entries: by default, . also, it is interesting to see that sql does not have references to row values or ids because we did not set them up, whereas automatically keeps track of the original location of each row and displays them in the queried result. i decided to jam pack this last section with a bunch of somewhat similar commands: namely, , , and . these commands are loosely related to each other, which is why they are all grouped under this section. speaking of groups, we will continue our discussion of sql and in another post, starting with things lilke and . anyhow, let's begin by taking a look at . in sql, we can make selections based on whether an entry falls into a certain category. for instance, we might want to select data points only for setosas and virginicas. in that case, we might use the following sql statement. to demonstrate the fact that we have both setosas and virginicas, i decided to avoid the use of . the resulting table is has 100 rows and five columns. let's see if we can replicate this result in using . as expected, we also get a 100 by 5 table containing only setosas and virginicas. it is worth noting that this was not the smartest way to go about the problem; we could have used negative boolean indexing: namely, we could have told to pull every data point but those pertaining to versicolors. for example, in development settings, we would of course use the negative boolean indexing approach shown immediately above, but for demonstration purposes, it helps to see how can be used to model . in sql, empty values are encoded as . we can perform selection based on whether or not there is a entry in a row. this functionality is particularly important to preprocess data, which might be fed into some machine learning model. first, observe that the current data does not have any values. 0 therefore, let's add two dummy rows for the purposes of this demonstration. there are many ways to go about adding a row. for example, we might want to assign a new row by saying , or use . let's try the first approach for simplicity. now that we have this dummy row, let's see what we can do with sql. in fact, the syntax is not so much different from what we've been doing so far. the only new part is , which specifies that a certain attribute or column is . again, this is one of those instances that show that sql statements somewhat read like normal english statements. , on the other hand, obviously doesn't flow as easily, but its syntax is not so much complicated either: again, this shows that boolean indexing is a huge component of sifting through data frames. a lot of the inspiration behind this api obviously comes from r and treatment of its own data frames. the function does the exact opposite of . without running the function, we already know that substituting with will simply give us the rest of all the rows in the dataset. this post was intended as an introduction to sql, but somehow it digressed into a comparison of sql and syntax. nonetheless, for those who are already familiar with one framework, reading this cross comparison will help you glean a more intuitive sense of what the other side of the world looks like. as mentioned earlier, sql and each have their strenghts and weaknesses, and so it definitely helps to have both tools under the belt. as you might notice, my goal is to eventually gain some level of proficienchy in both python, sql, and r; hence the posts on r lately. it's interesting to see how different tools can be used to approach the same problem. better illuminated in that process are the philosophies behind each frameworks: where they each borrowed inspiration from, what values or ux aspects they prioritize, and et cetera. i hope you enjoyed reading this post. catch you up in the next one!",0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2020-10-25-pytorch-rnn,"in this post, we'll take a look at rnns, or recurrent neural networks, and attempt to implement parts of it in scratch through pytorch. yes, it's not entirely from scratch in the sense that we're still relying on pytorch autograd to compute gradients and implement backprop, but i still think there are valuable insights we can glean from this implementation as well. for a brief introductory overview of rnns, i recommend that you check out this previous post, where we explored not only what rnns are and how they work, but also how one can go about implementing an rnn model using keras. this time, we will be using pytorch, but take a more hands on approach to build a simple rnn from scratch. full disclaimer that this post was largely adapted from this pytorch tutorial this pytorch tutorial. i modified and changed some of the steps involved in preprocessing and training. i still recommend that you check it out as a supplementary material. with that in mind, let's get started. the task is to build a simple classification model that can correctly determine the nationality of a person given their name. put more simply, we want to be able to tell where a particular name is from. we will be using some labeled data from the pytorch tutorial. we can download it simply by typing this command will download and unzip the files into the current directory, under the folder name of . now that we have downloaded the data we need, let's take a look at the data in more detail. first, here are the dependencies we will need. we first specify a directory, then try to print out all the labels there are. we can then construct a dictionary that maps a language to a numerical label. we see that there are a total of 18 languages. i wrapped each label as a tensor so that we can use them directly during training. let's store the number of languages in some variable so that we can use it later in our model declaration, specifically when we specify the size of the final output layer. now, let's preprocess the names. we first want to use to standardize all names and remove any acute symbols or the likes. for example, 'slusarski' once we have a decoded string, we then need to convert it to a tensor so that the model can process it. this can first be done by constructing a mapping, as shown below. 59 we see that there are a total of 59 tokens in our character vocabulary. this includes spaces and punctuations, such as . this also means that each name will now be expressed as a tensor of size ; in other words, each character will be a tensor of size . we can now build a function that accomplishes this task, as shown below: if you read the code carefully, you'll realize that the output tensor is of size , which is different from the explanation above. well, the reason for that extra dimension is that we are using a batch size of 1 in this case. in pytorch, rnn layers expect the input tensor to be of size . since every name is going to have a different length, we don't batch the inputs for simplicity purposes and simply use each input as a single batch. for a more detailed discussion, check out this forum discussion. let's quickly verify the output of the function with a dummy input. tensor now we need to build a our dataset with all the preprocessing steps. let's collect all the decoded and converted tensors in a list, with accompanying labels. the labels can be obtained easily from the file name, for example . we could wrap this in a pytorch class, but for simplicity sake let's just use a good old loop to feed this data into our model. since we are dealing with normal lists, we can easily use 's to separate the training data from the testing data. let's see how many training and testing data we have. note that we used a of 0.1. train: 18063 test: 2007 we will be building two models: a simple rnn, which is going to be built from scratch, and a gru based model using pytorch's layers. now we can build our model. this is a very simple rnn that takes a single character tensor representation as input and produces some prediction and a hidden state, which can be used in the next iteration. notice that it is just some fully connected layers with a sigmoid non linearity applied during the hidden state computation. we call at the start of every new batch. for easier training and learning, i decided to use to initialize these hidden states. we can now build our model and start training it. i realized that training this model is very unstable, and as you can see the loss jumps up and down quite a bit. nonetheless, i didn't want to cook my 13 inch macbook pro so i decided to stop at two epochs. epoch 1/2, step 3000/18063, loss: 0.0390 epoch 1/2, step 6000/18063, loss: 1.0368 epoch 1/2, step 9000/18063, loss: 0.6718 epoch 1/2, step 12000/18063, loss: 0.0003 epoch 1/2, step 15000/18063, loss: 1.0658 epoch 1/2, step 18000/18063, loss: 1.0021 epoch 2/2, step 3000/18063, loss: 0.0021 epoch 2/2, step 6000/18063, loss: 0.0131 epoch 2/2, step 9000/18063, loss: 0.3842 epoch 2/2, step 12000/18063, loss: 0.0002 epoch 2/2, step 15000/18063, loss: 2.5420 epoch 2/2, step 18000/18063, loss: 0.0172 now we can test our model. we could look at other metrics, but accuracy is by far the simplest, so let's go with that. accuracy: 72.2471% the model records a 72 percent accuracy rate. this is very bad, but given how simple the models is and the fact that we only trained the model for two epochs, we can lay back and indulge in momentary happiness knowing that the simple rnn model was at least able to learn something. let's see how well our model does with some concrete examples. below is a function that accepts a string as input and outputs a decoded prediction. i don't know if any of these names were actually in the training or testing set; these are just some random names i came up with that i thought would be pretty reasonable. and voila, the results are promising. 'english' 'chinese' 'russian' the model seems to have classified all the names into correct categories! this is cool and all, and i could probably stop here, but i wanted to see how this custom model fares in comparison to, say, a model using pytorch layers. gru is probably not fair game for our simple rnn, but let's see how well it does. let's declare the model and an optimizer to go with it. notice that we are using a two layer gru, which is already one more than our current rnn implementation. epoch 1/2, step 3000/18063, loss: 1.8497 epoch 1/2, step 6000/18063, loss: 0.4908 epoch 1/2, step 9000/18063, loss: 1.0299 epoch 1/2, step 12000/18063, loss: 0.0855 epoch 1/2, step 15000/18063, loss: 0.0053 epoch 1/2, step 18000/18063, loss: 2.6417 epoch 2/2, step 3000/18063, loss: 0.0004 epoch 2/2, step 6000/18063, loss: 0.0008 epoch 2/2, step 9000/18063, loss: 0.1446 epoch 2/2, step 12000/18063, loss: 0.2125 epoch 2/2, step 15000/18063, loss: 3.7883 epoch 2/2, step 18000/18063, loss: 0.4862 the training appeared somewhat more stable at first, but we do see a weird jump near the end of the second epoch. this is partially because i didn't use gradient clipping for this gru model, and we might see better results with clipping applied. let's see the accuracy of this model. accuracy: 81.4150% and we get an accuracy of around 80 percent for this model. this is better than our simple rnn model, which is somewhat expected given that it had one additional layer and was using a more complicated rnn cell model. let's see how this model predicts given some raw name string. 'english' 'chinese' 'spanish' 'russian' the last one is interesting, because it is the name of a close turkish friend of mine. the model obviously isn't able to tell us that the name is turkish since it didn't see any data points that were labeled as turkish, but it tells us what nationality the name might fall under among the 18 labels it has been trained on. it's obviously wrong, but perhaps not too far off in some regards; at least it didn't say japanese, for instance. it's also not entirely fair game for the model since there are many names that might be described as multi national: perhaps there is a russian person with the name of demirkan. i learned quite a bit about rnns by implementing this rnn. it is admittedly simple, and it is somewhat different from the pytorch layer based approach in that it requires us to loop through each character manually, but the low level nature of it forced me to think more about tensor dimensions and the purpose of having a division between the hidden state and output. it was also a healthy reminder of how rnns can be difficult to train. in the coming posts, we will be looking at sequence to sequence models, or seq2seq for short. ever since i heard about seq2seq, i was fascinated by tthe power of transforming one form of data to another. although these models cannot be realistically trained on a cpu given the constraints of my local machine, i think implementing them themselves will be an exciting challenge. catch you up in the next one!",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0
2020-02-28-flask,"these past few days, i've been taking a hiatus from the spree of neural networks and machine learning to explore an entirely separate realm of technology: web development. let me tell you why. i tried learning html, css, and javascript about a month ago. however, that didn't go as well as i had expected, mainly due to lack of momentum. html was easy: it was just a way of doing markups with tags. and as many people might agree, i didn't really think learning html was akin to learning a programming language. the process felt more similar to studying something like or markdown. then came css, which turned out to be a whole different animal compared to html. css seemed simple on paper: it was just a way of stylizing html code so that it looks more presentable. however, the whole madness of padding, margins, classes, and ids got me rethinking about my original assessment of css. i ultimately ended up glossing over css tutorials, thinking that knowing the barebone basics what it does and how it does it will suffice for now. as expected, learning javascript was more comparable to leanring java or python. this was mainly because javascript is, by most standards, a full fledged programming language, featuring loops, conditional statements, variable declarations, and so on. getting the basics was pretty easy for this reason: i was able to orient myself into programming mode and focus on learning the syntax unique to javascript. in many ways, javascript made me reminisce about java, although there were several notable differences. this part was fun and easy. however, when the time came to pull everything back together, i was overwhelmed. the interaction between html, css, and javascript was interesting and all, but it was all too much for me to pick up in just a few days. tired, i decided to return back to my comfort zone of python and jupyter notebooks. until two days ago. i was reading about generative adversarial networks from the book generative deep learning by david foster when an idea struck me: it's great that we can build and train a neural network, but how do we present and deploy it in actuality? shouldn't there be some means of delivering the model, preferably through something like an application or a website? i was able to find my answer in these questions in just a few minutes: either deploy a model by integrating it into a web application or use frameworks such as tensorflow.js. at this point, i realized that running into front end work is going to be unavoidable for me. although i'd imagine machine learning engineers or data scientists typically don't work with web development to deploy models themselves, it is undeniable that proficiency in the front end stack to some capacity is going to be helpful asset to have at one's disposal. i thus garnered the courage and motivation to start my journey again with web development. this time, i had a clear sense of goal and purpose: build a web application capable of using a trained neural network or machine learning model to generate predictions and present them in a visually effective way. flask is a popular python based web framework used for web application development. i had heard about flask here and there from different sources, yet i never had a true idea of what it was, what it was capable of, and why it was so popular. after taking a look at some crash course videos and tutorials, i quickly understood why flask was so popular: it was based on python. this meant that anyone who was literate in python could at least try flask. this was a huge benefit since the entry barrier for flask was considerably lower for me than it might have been had i dabbled in other web frameworks that used a different language, such as ruby. of course, flask had its own perks, and i had to get at least a basic understand of what things like get and post requests meant in the context of configuring a web backend. however, with some tweaking and experimentation, i realized that flask was incredibly easy to use, at least for my purposes of realizing a simple application, mostly because it was simple, intuitive, and used python at its core. but python was not the only thing i needed. because my goal was to try to build an elementary web application, i inevitably had to learn how to write and understand html. i had tried learning html a few months ago, but it was a short lived endeavor that didn't last for long. this time, however, i was determined to learn html, because it was something that i didn't have a workaround for. luckily, html is not a difficult language to learn; it is, at its core, just a markdown language. this is a core difference between html and say, python, because python is what many would consider to be a full fledged programming and scripting language, whereas html is simply a tool that can be configured to build a bare skeleton for a web page to be displayed on screen. to implement fancy features, one would need other languages and frameworks such as javascript or react. in short, html is just a way of presenting information. again, with the help of online tutorials and crash course videos, i was able to achieve some progress with my html literachy. however, anyone knows that it is impossible to build a decent website with just html. to improve general aesthetics, i would have to learn css, which is a whole different animal. on top of that, building dynamic features that allows users to interact with contents on the webpage requires javascript as mentioned earlier. i had the choice of learning them, and indeed i did pick up a few very simple javascript syntax along the way. however, i ended up resorting to bootstrap, a convenient yet comprehensive open source front end framework that implements so many useful features under the hood. with the help of bootstrap, i was able to quickly prototype a decent looking website in no time something that would have taken weeks for me to do with vanilla javascript and css. i was also able to try out some basic sql query statements in my first flask mini project. i had learned very basic sql a few months back, yet i had never tried it out on a real project setting. although i am still a novice in sql, it was nice being able to see how querying could take place within a python setting using sql alchemy and other modules. i used sqlite for two simple web applications: a chat logging service and a to do application. i found sql to be useful in building these applications because i needed to implement a way of storing information coming from users, then displaying that information back whenever necessary, which is exactly what sql is for. for the purpose of my application, the query statements i used were incredibly simple and elementary, yet it was exciting to see that all the pieces were coming together and working well according to my expectation. after having completed this project, i was ready to deploy my first machine learning algorithm in the form of a web application. i thought deploying a deep learning model would be no different from building another simple web application, just like the ones i had built previously to familiarize myself with flask. to an extent, this expectation held true; however, there were definitely some challenges i struggled with along the way. the objective of the project was simple: assuming that there is a fully trained neural network, deploy it as a web application so that anyone could use it. for the purpose of this project, i decided to simply use an out of the box model, , which is available on . this is somewhat of a limited approach because it does not allow for the user to retrain the model; instead, the model is set in stone, and the user can only interact with it by using it. despite this simplified setup, however, i faced some difficulty implementing this feature mostly because of my lack of knowledge with web frameworks and the front end in general. the first challenge i faced originated from the fact that i had no knowledge of javascript. in order to build this web application, i needed to find a way of receiving some user input in this particular case, an image so that the model could receive that data as input and generate a prediction. this feature itself could simply be implemented with bootstrap html. however, i also wanted the name of the selected image file to be displayed on screen for the convenience of the user. to achieve this, i ended up combing through stack overflow for some time, an endeavor that was ultimately rewarded with a working code snippet that just did what i wanted. in the end, i was able to get a decent looking home page where the user could click on a button to select an image from their local storage, upload it, view the image, and ultimately pass it to the model to generate a prediction. the next notable challenge mainly had to do with displaying the predictions generated by the model. i expected this to be a fairly straightforward process: after all, generating a prediction itself could simply be achieved with the function available in keras. i expected using to display this result to be simple, but unfortunately, html webpages are not jupyter notebooks; i could not simply use to display a plot on the screen. in the end, after some trial and error and performing countless searches on stack overflow, i managed to get a working function that achieved my goal: to this day, i do not quite understand all of what is going on in this code snippet. however, i do know that there is some encoding and decoding taking place so that the created image could be delivered to flask's function as a parameter. i also created some fail safes to ensure that the image that was provided by the user was of supported format. this could simply be achieved with the following function: while this is not necessary, having this function helped me deal with problems that might occur if the user were to input some odd unsupported image type, such as a .pdf file. finally after much tweaking and coding, this is what i had when i uploaded an image of a cute dog to the web application! the result: you know how happy i was when all of this was done after a week of intensive flask ing, html ing, and boostrap ing. this was one of the first projects i had worked on independently aside from the small jupyter notebook based programming i did to write some of the posts on this blog. honestly, it was a fantastic and much needed experience because i had not done anything this project based for a long time. in fact, the last time i had worked on a project was in high school for my ap computer science final. nostalgic reflections aside, i feel like i was able to add a valuable kit to my available technology stack, and that is flask. flask is incredibly easy to pick up, especially if you come from a python background. setting up a flask application is also mind blowingly simple, and this was one of the reasons i decided to use flask instead of other frameworks, say django. don't get me wrong: i have heard so much great things about django that i'm considering learning it on top of flask, but for a quick, small project as this one, flask was the perfect fit. now that i know how to deploy models, albeit in an elementary fashion, i guess i can continue with my quest on deep learning with the reassurance that i know how to deliver ml models to individual users through an established pipeline.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
2019-11-14-new-mbp,"apple officially announced the new 16 inch macbook pro. this product has been a long awaited release for many tech enthusiasts particularly given the negative reviews of its previous generation 15 inch model, which has been criticized for overheating issues. while the 15 inch macbook pro has long been the machine of choice for many professional designers and developers, the prevalence of cpu throttlling issues with the 2018 i9 variant of the laptop has led many to believe that its design is incapable of supporting its modern internals. the new 16 inch macbook pro, which effectively replaces the 15 inch model, reportedly resolves this issue all together. with a redesigned chassis with improved fans and wider heat sinks, the renewed macbook pro is capable of supporting its power hungry internals. on its official website, apple reports as much as 28 percent increased airflow and 35 percent larger heat sinks compared to previous models. if these numbers are true, this is certainly great news for heavy users whose workflow requires sustained, high level performance on their machines. the 16 inch macbook pro is also equipped with what apple calls the ""magic keyboard,"" essentially a refined scissor switch mechanism keyboard. in 2016 when apple released the 12 inch macbook, it also announced a new keyboard switch mechanism called the butterfly keyboard, which apple claimed was a more stable, reliable typing system specifically engineered to fit into the thinner, smaller frame of the macbook. however, countless users have found butterfly keyboards to be unreliable, to the point that lawsuits were filed by unsatisfied consumers. aware of such negative feedback, apple not only implemented incremental updates to the design of the butterfly keys each year in their new product releases, but it also announced a free keyboard replacement program for eligible macbook users. however, the issue was not resolved until today, when apple finally decided to abandon the butterfly keyboard in favor of the original scissor mechanism. the 16 inch macbook pro's keyboard features improved key travel of 1 mm, twice of what the butterfly keyboard originally featured. minor updates have also been made to the keyboard layout, such as a physical escape key and a separate power button featuring touchid. other new features include: wider resolution screen that supports p3 color gamut amd's new 7 mm rdna gpu architecture 100 watt hour battery, rated to 11 hours of wireless web surfing adjustable screen refresh rates incremental hardware updates up to 64 gigabytes of 2666mhz ddr4 memory a six speaker system instead of the previous two as a user of the 2018 macbook pro 13 inch, i am most excited to see changes and updates made to the keyboard. although low key travel has not been a problem for me, typing accuracy sure has. pressing a key once would sometimes give me two or three repeated characters instead of the expected one, which has led me to resort to software fixes such as unshaky. however, this is merely a temporary solution at best, which is why i plan to schedule an appointment at the genius bar to get my keyboard replaced. it is not an exaggeration to say that the success of the 16 inch macbook pro is largely contingent upon the reliability of the magic keyboard. i am also pleasantly surprised at the fact that apple decided to make the 16 inch model slightly thicker than the previous generation. empirically, apple has always been the champion of thin and light laptops, even with their pro lineup. this generation of macbook pros, however, represents a deviation from this principle. as phil schiller said in this video with youtuber jonathan morrison, the design of the macbook pro embodies apple's attempt to balance various competing resources, such as performance, portability, aesthetics, and so on. if apple of the past took the artist's route of designing slim, visually appealing laptops while accepting compromises in areas of battery and performance, the apple of today seems to value more utilitarian concerns by seeking to provide customers the extra juice they need on the go. now i'm even more excited to see how the 13 inch macbook pro will be revamped next year. unshaky: https://github.com/aahung/unshaky 16 inch macbook pro: https://www.apple.com/macbook pro 16/ official website: https://www.apple.com/macbook pro 16/ overheating issues: https://www.youtube.com/watch?v=dx8j125s4cg this video: https://www.youtube.com/watch?v=kjfxcl1s8dc&t=833s 12 inch macbook: https://9to5mac.com/2016/04/19/apple releases new 12 inch retina macbook line new processors rose gold better battery life/ free keyboard replacement program: https://support.apple.com/keyboard service program for mac notebooks",0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
2020-02-10-auto-complete,"you might remember back in the old days when autocomplete was just terrible. the suggestions provided by autocomplete would be useless if not downright stupid i remember that one day when i intended to type ""gimme a sec,"" only to see my message get edited into ""gimme a sex"" by the divine touches of autocomplete. on the same day, the feature was turned off on my phone for the betterment of the world. now, times have changed. recently, i decided to give autocorrect a chance on my iphone. surprisingly, i find myself liking autocomplete more than hating it, especially now that the weather is getting colder by each day: when my frost numbed finger tips touch on the wrong places of the phone screen to produce words that aren't really words, iphone's autocomplete somehow magically reads my mind to rearrange all that inscrutable alphabet soup into words that make actual, coherent sense. sometimes, it's so good at correcting my typos that i intentionnally make careless mistakes on the keyboard just to see how far it can go. one of the obvious reasons behind such drastic improvements in autocomplete functionality is the development of deep neural networks. as we know, neural networks are great at learning hidden patterns as long as we feed it with enough data. in this post, we will implement a very simple version of a generative deep neural network that can easily form the backbone of some character based autocomplete algorithm. let's begin! let's first go ahead and import all dependencies for this tutorial. as always, we will be using the functional api to build our neural network. we will be training our neural network to speak like the great german philosopher friedrich nietzsche . first, let's build a function that retrieves the necessary text file document from the web to return a python string. let's take a look at the text data by examining its length. character length: 600893 just to make sure that the data has been loaded successfully, let's take a look at the first 100 characters of the string. preface supposing that truth is a woman what then? is there not ground for suspecting that all ph it's time to preprocess the text data to make it feedable to our neural network. as introduced in this previous post on recurrent neural networks, the smart way to deal with text preprocessing is typically to use an embedding layer that translates words into vectors. however, text embedding is insuitable for this task since our goal is to build a character level text generation model. in other words, our model is not going to generate word predictions; instead, it will spit out a character each prediction cycle. therefore, we will use an alternative technique, namely mapping each character to an integer value. this isn't as elegant as text embedding or even one hot encoding but for a character level analysis, it should work fine. the function takes a string text data as input and returns a list of training data, each of length , sampled every characters. it also returns the training labels and a hash table mapping characters to their respective integer encodings. let's perform a quick sanity check to see if the function works as expected. specifying to 60 means that each instance in the training data will be 60 consecutive characters sampled from the text data every characters. number of sequences: 200278 number of unique characters: 57 the result tells us that we have a total of 200278 training instances, which is probably plenty to train, test, and validate our model. the result also tells us that there are 57 unique characters in the text data. note that these unique characters not only include alphabets but also and other miscellaneous white spacing characters and punctuations. let's now design our model. because there is obviously going to be sequential, temporal structure underlying the training data, we will use an lstm layer, a type of advanced recurrent neural network we saw in the previous post. in fact, this is all we need, unless we want to create a deep neural network spanning multiple layers. however, training such a model would cost a lot of time and computational resource. for the sake of simplicity, we will build a simple model with a single lstm layer. the output layer is going to be a dense layer with number of neurons, activated with a softmax function. we can thus interpret the index of the biggest value of the final array to correspond to the most likely character. below is a full plot of the model that shows the dimensions of the input and output tensors of all layers. now, all we have to do is to train the model with the data. let's run this for 50 epochs, just to give our model enough time to explore the loss function and settle on a good minimum. train on 200278 samples epoch 1/50 200278/200278 ============================== 164s 817us/sample loss: 2.5568 epoch 2/50 200278/200278 ============================== 163s 813us/sample loss: 2.1656 epoch 3/50 200278/200278 ============================== 162s 810us/sample loss: 2.0227 epoch 4/50 200278/200278 ============================== 162s 809us/sample loss: 1.9278 epoch 5/50 200278/200278 ============================== 161s 805us/sample loss: 1.8586 epoch 6/50 200278/200278 ============================== 162s 811us/sample loss: 1.8032 epoch 7/50 200278/200278 ============================== 163s 815us/sample loss: 1.7582 epoch 8/50 200278/200278 ============================== 165s 825us/sample loss: 1.7197 epoch 9/50 200278/200278 ============================== 167s 833us/sample loss: 1.6866 epoch 10/50 200278/200278 ============================== 166s 830us/sample loss: 1.6577 epoch 11/50 200278/200278 ============================== 165s 823us/sample loss: 1.6312 epoch 12/50 200278/200278 ============================== 162s 810us/sample loss: 1.6074 epoch 13/50 200278/200278 ============================== 162s 811us/sample loss: 1.5862 epoch 14/50 200278/200278 ============================== 161s 805us/sample loss: 1.5668 epoch 15/50 200278/200278 ============================== 165s 822us/sample loss: 1.5492 epoch 16/50 200278/200278 ============================== 166s 829us/sample loss: 1.5333 epoch 17/50 200278/200278 ============================== 167s 832us/sample loss: 1.5182 epoch 18/50 200278/200278 ============================== 166s 828us/sample loss: 1.5051 epoch 19/50 200278/200278 ============================== 166s 827us/sample loss: 1.4922 epoch 20/50 200278/200278 ============================== 164s 819us/sample loss: 1.4801 epoch 21/50 200278/200278 ============================== 165s 826us/sample loss: 1.4688 epoch 22/50 200278/200278 ============================== 165s 826us/sample loss: 1.4582 epoch 23/50 200278/200278 ============================== 165s 822us/sample loss: 1.4488 epoch 24/50 200278/200278 ============================== 163s 813us/sample loss: 1.4386 epoch 25/50 200278/200278 ============================== 167s 832us/sample loss: 1.4305 epoch 26/50 200278/200278 ============================== 166s 830us/sample loss: 1.4220 epoch 27/50 200278/200278 ============================== 167s 832us/sample loss: 1.4137 epoch 28/50 200278/200278 ============================== 167s 833us/sample loss: 1.4060 epoch 29/50 200278/200278 ============================== 166s 827us/sample loss: 1.3989 epoch 30/50 200278/200278 ============================== 164s 820us/sample loss: 1.3910 epoch 31/50 200278/200278 ============================== 163s 815us/sample loss: 1.3846 epoch 32/50 200278/200278 ============================== 162s 810us/sample loss: 1.3777 epoch 33/50 200278/200278 ============================== 162s 809us/sample loss: 1.3720 epoch 34/50 200278/200278 ============================== 160s 798us/sample loss: 1.3649 epoch 35/50 200278/200278 ============================== 163s 815us/sample loss: 1.3599 epoch 36/50 200278/200278 ============================== 162s 807us/sample loss: 1.3538 epoch 37/50 200278/200278 ============================== 162s 808us/sample loss: 1.3482 epoch 38/50 200278/200278 ============================== 162s 809us/sample loss: 1.3423 epoch 39/50 200278/200278 ============================== 163s 813us/sample loss: 1.3371 epoch 40/50 200278/200278 ============================== 164s 820us/sample loss: 1.3319 epoch 41/50 200278/200278 ============================== 163s 814us/sample loss: 1.3268 epoch 42/50 200278/200278 ============================== 165s 825us/sample loss: 1.3223 epoch 43/50 200278/200278 ============================== 164s 820us/sample loss: 1.3171 epoch 44/50 200278/200278 ============================== 164s 819us/sample loss: 1.3127 epoch 45/50 200278/200278 ============================== 165s 822us/sample loss: 1.3080 epoch 46/50 200278/200278 ============================== 163s 813us/sample loss: 1.3034 epoch 47/50 200278/200278 ============================== 163s 813us/sample loss: 1.2987 epoch 48/50 200278/200278 ============================== 162s 809us/sample loss: 1.2955 epoch 49/50 200278/200278 ============================== 161s 804us/sample loss: 1.2905 epoch 50/50 200278/200278 ============================== 162s 811us/sample loss: 1.2865 as i was training this model on google colab, i noticed that training even this simple model took a lot of time. therefore, i decided that it is a good idea to probably save the trained model in the worst case scenario that poor network connection suddenly caused the jupyter kernel to die, saving a saved model file would be of huge help since i can continue training again from there. saving the model on google colab requires us to import a simple module, . the process is very simple. to load the model, we can simply call the command below. let's take a look at the loss curve of the model. we can simply look at the value of the loss function as printed throughout the training scheme, but why not visualize it if we can? as expected, the loss decreases throughout each epoch. the reason i was not paticularly worried about overfitting was that we had so much data to work with, especially in comparison with the relatively constrained memory capacity of our one layered model. one of the objectives of this tutorial was to demonstrate the fun we can have with generative models, namely neural networks that can be used to generate data themselves, not just classify or predict data points. to put this into perspective, let's compare the objectives of a generative model with that of a discriminative model. simply put, the goal of a discriminative model is to model and calculate where is a label and is some input vector. as you can see, discriminative models arise most commonly from the context of supervised machine learning, such as regression or classification. in contrast, the goal of a generative model is to approximate the distribution which we might construe to be the probability of observing evidence or data. by modeling this distribution, the goal is that we might be able to generate samples that appear to have been sampled from this distribution. in other words, we want our model to generate likely data points based on an approximation of the true distribution from which these observations came from. in the context of this tutorial, our neural network should be able to somewhat immitate the speech of the famous german philosopher based on the training it went through with text data, although we would not expect the content generated by our neural network to have the same level of depth and profoundity as those of his original writings. as mentioned above, the objective of a generative model is to model the distribution of the latent space from which observed data points came from. at this point, our trained model should be able to model this distribution, and thus generate predictions given some input vector. ... god is dead god is dead god is dead... we don't want this to happen. instead, we want to introduce some noise so that the model faces subtle obstructions, thereby making it get more ""creative"" with its output instead of getting trapped in an infinite loop of some likely sequence. below is a sample implementation of adding noise to the output using log and exponential transformations to the output vector of our model. the transformation might be expressed as follows: where denotes a transformation, denotes a prediction as a vector, denotes temperature as a measure of randomness, and is a normalizing constant. although this might appear complicated, all it's doing is that it is adding some perturbation or disturbance to the output data so that it is possible for less likely characters to be chosen as the final prediction. below is a sample implementation of this process in code. note that due to the algebraic quality of the vector transformation above, randomness is increased for large values of . now it's finally time to put our nietzsche model to the test. how we will do this is pretty simple. first, we will feed a 60 character excerpt from the text to our model. then, the model will output a prediction vector, which is then passed onto given a specified . we will finally have a prediction that is 1 character. then, we incorporate that one character prediction into the original 60 character data we started with. we slice the new augmented data set from to end up with another prediction. we would then slice the data set from, you guessed it, and repeat the process as outlined above. when we iterate through this cycle many times, we would eventually end up with some generated text. below is the function that implements the iteration process. we're almost done! to get a better sense of what impact temperature has on the generation of text, let's quickly write up a function that will allow us to generate text for differing values of . the time has come: let's test our model for four different temperature values from 0.3 to 1.2, evenly spaced. we will make our model go through 1000 iterations to make sure that we have a long enough text to read, analyze, and evaluate. for the sake of readability, i have reformatted the output result in markdown quotations. generated text at temperature 0.3: is a woman what then? is there not ground for suspecting that the experience and present strange of the soul is also as the stand of the most profound that the present the art and possible to the present spore as a man and the morality and present self instinct, and the subject that the presence of the surcessize, and also it is an action which the philosophers and the spirit has the consider the action to the philosopher and possess and the spirit is not be who can something the predicess of the constinate the same and self interpatence, the disconsises what is not to be more profound, as if it is a man as a distance of the same art and ther strict to the presing to the result the problem of the present the spirit what is the consequences and the development of the same art of philosophers and security and spirit and for the subjective in the disturce, as in the contrary and present stronger and present could not be an inclination and desires of the same and distinguished that is the discoverty in such a person itself influence and ethers as generated text at temperature 0.6: is a woman what then? is there not ground for suspecting to and the world will had to a such that the basis of the incussions of the spirit as the does not because actian free spirits of intellect of the commstical purtious expression of men are so much he is not unnor experiences of self conturity, and as anifegently religious in the man would not consciously, his action is not be actian at in accombs life for the such all procees of great and the heart of this conduct the spirity of the man can provate for in any once in any of the suriticular conduct that which own needs, when they are therefore, as such action and some difficulty that the strength, it, himself which has to its fine term of pricismans the exacte in its self recuphing and every strength and man to wist the action something man as the worst, that the was of a longent that the whole not be all the very subjectical proves the stronger extent he is necessary to metaphysical figure of the faith in the bolity in the pure belief as ""the such a successes of the values that is he ​ generated text at temperature 0.9: is a woman what then? is there not ground for suspecting that they grasutes, and so farmeduition of the does not only with this constrbicapity have honour and who distical seclles are denie'n, is one samiles are no luttrainess, and ethic and matficulty, concudes of morality to rost were presence of lighters caseful has prescally here at last not and servicatity, leads falled for child real appreparetess of worths the resticians when one to persans as a what a mean of that is as to the same heart tending noble stimptically and particious, we pach yought for that mankind, that the same take frights a contrady has howevers of a surplurating or in fact a sort, without present superite fimatical matterm of our being interlunally men who cal scornce. the shrinking's proglish, and traints he way to demitable pure explised and place can deterely by the compulse in whom is phypociative cinceous, and the higher and will bounthen in itsiluariant upon find the ""first the whore we man will simple condection and some than us a valuasly refiges who feel generated text at temperature 1.2: is a woman what then? is there not ground for suspecting that he therefore when shre, mun, a schopenhehtor abold gevert. 120 =as in find that is _know believinally bad, euser of view. bithic iftel canly in any knowitumentially. the charm surpose again, in swret feathryst, form of kinne of the world bejud age implaasoun ever? but that the is any appearance has clenge: the? a plexable gen preducl=s than condugebleines and aligh to advirenta nasure; findiminal it as, not take. the ideved towards upavanizing, would be thenion, in all pespres: it is of a concidenary, which, well founly con utbacte udwerlly upon mansing frauble of ""arrey been can the pritarnated from their christian often think prestation of mocives."" legt, lenge: this deps telows, plenhance of decessaticrances). hyrk an interlusally"" tone under good haggy,"" is have we leamness of conschous should it, of sicking ummenfeckinal zerturm erienweron of noble of himself clonizing there is conctumendable prefersy exaitunia states,"" whether they deve oves any of hispyssesss. int the results are fascinating. granted, our model is still bad at immitating nietzsche's style of writing, but i think the performance is impressive given that this was a character based text generation model. think about it for a second: to write even a single word, say ""present,"" the model has to correctly predict ""p"", ""r"", ""e"", ""s"", ""e"", ""n"", and ""t,"" all in tandem. imagine doing this for extended cycles, long enough to generate text that is comfortably a paragraph long. it's amazing how the text it generates even makes some sense at all. then, as temperature rises, we see more randomness and ""creativity"" at work. we start to see more words that aren't really words . at temperature 1.2, the model is basically going crazy with randomness, adding white spaces where there shouldn't be and sounding more and more like a speaker of old english or german, something that one might expect to see in english scripts written in pre shakesperean times. at any rate, it is simply fascinating to see how a neural network can be trained to immitate some style of writing. hopefully this tutorial gave you some intuition of how autocomplete works, although i presume business grade autocomplete functions on our phones are based on much more complicated algorithms. thanks for reading this post. in the next post, we might look at another example of a generative model known as generative adversarial networks, or gan for short. this is a burgeoning field in deep learning with a lot of prospect and attention, so i'm already excited to put out that post once it's done. see you in the next post. peace!",0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0
2019-11-29-test,"so far on this blog, all posts were written using markdown. markdown is very easy and learnable even for novices like me, but an issue i had was the inconvenience of integrating texts, code, and figures organically in one coherent file. that is why i decided to experiment with jupyter notebook, which i had meant to use and learn for a very long time. in this post, we will test the functionality of jupyter notebook by trying out various visualization tools available in python. in the next post, i will introduce the method i utilized toconvert this file into format to display it on our static website. hello world we can also use libraries to create visualizations. shown below is a simple representation of a sine graph created using and . let's step up the game a bit. here is a simple experimentation with subplots with the package again. next, we use to see if basic spreadsheet functionalities can be displayed on jupyter. 0 1 2 3 4 0 0.119558 0.632469 2.176383 0.310280 0.480731 1 0.285325 0.184601 0.808425 0.191247 0.562904 2 0.305665 2.057085 0.191773 0.217347 0.713348 3 0.608312 0.028068 0.222626 0.760257 1.193710 4 0.627122 1.325584 0.504316 0.079908 0.051234 we can visualize this toy data using the function, much like we created visualizations with above. is another powerful data visualization framework. it is based off of , but also contains graphing functionalities that its parent does not, such as heatmaps. pair plots are used to visualize multidimensional data. we can see pair plots in action by using the function. below are visualizations created using one of 's default data set. also contains a joint plot graphing functionality that allows us to group multiple graphs into one compact figure, as demonstrated below. the last example we will take a look at is the in . s are used when there is a lot of noise in the data. this is a quick check to see if equations can properly be displayed on jupyter using mathjax. here is the cross product formula . this is enough jupyter for today! once i fully familiarize myself with jupyter, perhaps i will write more posts using this platform.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2020-05-30-sklearn-pipeline,"in today's post, we will explore ways to build machine learning pipelines with scikit learn. a pipeline might sound like a big word, but it's just a way of chaining different operations together in a convenient object, almost like a wrapper. this abstracts out a lot of individual operations that may otherwise appear fragmented across the script. i also personally think that scikit learn's ml pipeline is very well designed. so here is a brief introduction to ml pipelines is scikit learn. for the purposes of this tutorial, we will be using the classic titanic dataset, otherwise known as the course material for kaggle 101. i'm still trying to get my feet into kaggle, so it is my hope that this tutorial will also help those trying to break into data science competitions. first, let's import the modules and datasets needed for this tutorial. scikit learn is the go to library for machine learning in python. it contains not only data loading utilities, but also imputers, encoders, pipelines, transformers, and search tools we will need to find the optimum model for the task. let's load the dataset using . let's observe the data by calling . by default, this shows us the first five rows and as many columns as it can fit within the notebook. let's take a look at the data in more depth to build a foundation for our analysis. this step typically involves the following steps: checking for null entries identifying covariance feature engineering let's proceed in order. before proceeding with any data analysis, it's always a good idea to pay attention to missing values how many of them there are, where they occur, et cetera. let's take a look. pclass false name false sex false age true sibsp false parch false ticket false fare true cabin true embarked false dtype: bool the is useful, but is doesn't really show us how many values are missing for each column. to probe into this issue in more detail, we need to use instead. pclass 0 name 0 sex 0 age 209 sibsp 0 parch 0 ticket 0 fare 1 cabin 822 embarked 0 dtype: int64 i recently realized that there is also a very cool data visualization library called for observing missing data. this visualization gives us a more intuitive sense of where the values are missing. in this case, the missing values seem to be distributed somewhat evenly or randomly. however, we can also imagine cases were missing values might have something to do with an inherent attribute in the dataset . in such cases, using this library to visualize where missing values occur is a good idea, as this is an additional dimension of information that calling wouldn't be able to reveal. now that we have a rough sense of where missing values occur, we need to decide from one of few choices: drop entries with missing values drop columns with too many missing value use imputation to fill missing values with alternate values indeed, in this case, we will go ahead and drop the attribute. this choice becomes more obvious when we compute the percentage of null values. pclass 0.000000 name 0.000000 sex 0.000000 age 19.961796 sibsp 0.000000 parch 0.000000 ticket 0.000000 fare 0.095511 cabin 78.510029 embarked 0.000000 dtype: float64 this shows us that 77 percent of the rows have missing attribute values. given this information, it's probably a bad idea to try and impute these values. we opt to drop it instead. correlation is a metric that we always care about, since ultimately the goal of ml engineering is to use a set of input features to generate a prediction. given this context, we don't want to feed our model useless information that lacks value; instead, we only want to feed into the model highly correlated, relevant, and informative features. if certain features in the raw data are deemed useless, we need to either drop it or engage in some sort of feature engineering to produce a new set of more correlated features. from this preliminary analysis, it seems like there are some very weekly correlated features, namely and . the week correlation suggests that perhaps we need to engage in some feature engineering to extract more meaningful information out of the dataset. let's use the findings from the previous section to engineer some more informative features. one popular approach is to make use of names to derive a feature. intuitively, this makes sense: mr. and mrs, cpt. and dr. might be of interest for our model. another popular approach is to combine the less important features and into something like . implementing these should fairly be simple, so let's try it here. note that in an actual setting, there will be no answer to reference; we will have to rely on our own domain knowledge and more extensive eda to figure out which features matter, and what new features we will need. sometimes, weakly correlated features can be combined together to form a new feature, which might exhibit higher correlation with respect to the target. we can combine and into a new feature, called . strictly speaking, we would have to add 1, but adding all values by one corresponds to shifting everything by a constant value, which will not affect modeling since such constant adjustments will be taken care of by the preprocessing step anyway. note that feature engineering is also applied to both the training and test set simultaneously. we have created two new features, namely and . let's go ahead and perform feature engineering on the column as well to squeeze out more information. now we have some data that seems a lot more workable. however, we still have a problem with the column: it seems like there are many titles, so we should probably perform some binning or grouping. for men, the most common title is ; for women, and . let's see if there is a difference in the survival rate between the two most common title for females it seems like the the difference is insignificant, so we will simply group them together in one. mr false miss false mrs false master false dr true rev true col true mlle true ms true major true mme true the countess true don true dona true jonkheer true sir true lady true capt true name: title, dtype: bool imputation refers to a technique used to replace missing values. there are many techniques we can use for imputation. from the analysis above, we know that the columns that require imputation are as follows: age fare embarked let's first take a look at the data types for each column. pclass float64 sex category age float64 fare float64 embarked category family_size float64 is_alone int64 title object dtype: object checking data types is necessary both for imputation and general data preprocessing. specifically, we need to pay attention as to whether a given column encodes categorical or numerical variables. for example, we can't use the mean to impute categorical variables; instead, something like the mode would make much more sense. the best way to determine whether a variable is categorical or not is simply to use domain knowledge and actually observe the data. of course, one might use hacky methods like the one below: although you might think that this is a working hack, this approach is in fact highly dangerous, even in this toy example. for example, consider , which is supposedly a numerical variable of type . however, earlier with , we saw that is in fact a ordinal variable taking discrete values, one of 1.0, 2.0, and 3.0. so hacky methods must not be used in isolation; at the very least, they need to be complemented with some form of human input. let's try to use a simple pipeline to deal with missing values in some categorical variables. this is going to be our first sneak peak at how pipelines are declared and used. here, we have declared a three step pipeline: an imputer, one hot encoder, and principal component analysis. how this works is fairly simple: the imputer looks for missing values and fills them according to the strategy specified. there are many strategies to choose from, such as most constant or most frequent. then, we one hot encode the categorical variables since most machine learning models cannot accept non numerical values as input. the last pca step might seem extraneous. however, as discussed in this stack overflow thread, the judicious combination of one hot plus pca can seldom be beat by other encoding schemes. pca finds the linear overlap, so will naturally tend to group similar features into the same feature. i don't have enough experience to attest to the veracity of this claim, but mathematically or statistically speaking, this proposition seems valid. the idea is that one hot encoding all categorical variables may very well lead to an unmanageable number of columns, thus causing one to flounder in the curse of dimensionality. a quick fix, then, is to apply pca or some other dimensionality reduction technique onto the results of one hot encoding. back to the implementation, note that we can look inside the individual components of by simply treating it as an iterable, much like a list or tuple. for example, simpleimputer next, we need to do something similar for numerical variables. only this time, we wouldn't be one hot encoding the data; instead, what we want to do is to apply some scaling, such as normalization or standardization. recently in one of andreas mueller's lectures on youtube, i learned about the , which uses median and iqr instead of mean and standard deviation as does the . this makes the a superior choice in the presence of outliers. let's try using it here. now that we have the two pipelines for numeric and categorical columns, now it's time to put them together into one nice package, then apply the process over the entire dataframe. this packaging can nicely be abstracted via the , which is the magic glue to put all the pieces together. we simply have to tell which transformer applies to which column, along with the name for each process. is the complete package that we will use to transform our data. note that allows us to specify which pipeline will be applied to which column. this is useful, since by default, imputers or transformers apply to the entire dataset. more often or not, this is not what we want; instead, we want to be able to micro manage categorical and numerical columns. the combination of and is thus a very powerful one. now all that is left is to build a final pipeline that includes the classifier model. let's see how well our model performs on a stratified 5 fold cross validation. note that this is without any hyperparameter tuning. 0.7630849851902483 and just like that, we can evaluate the performance of our model. when it comes to general fitting and testing, a useful tip i found on kaggle is the following rule of thumb: if a pipeline ends with a transformer, call then . if a pipeline ends with a model, call then . calling will cause all steps prior to the model to undergo , and the final step the model will run . if you think about it for a second, this configurations makes a lot of sense: if the pipeline contains a model, it means that it is the full package. all the steps prior to the model would involve wrangling the data; the last step would have the model use the data to make a prediction. therefore, calling should apply only to the last model after is called on all the preprocessing steps. if the pipeline itself is just a bundle of preprocessors, on the other hand, we should only be able to call . scikit learn's models are great, but in a sense they are too great. this is because there are a lot of hypterparameters to tune. fortunately for us, we can somewhat resort to a quasi brute force approach to deal with this: train models on a number of different combinations of hyperparameters and find the one that performs best! well, this is what does. is not quite as bad in that it doesn't create and test all possible models that distinct combinations of hyperparameters can yield: instead, it relies on a randomized algorithm to perform a search of the hyperparameter space. this is why is a lot quicker than , with marginal sacrifices in model performance. let's see how we might be able to perform hyperparameter search given a pipeline like the one we have built above. the parameter space we are searching for here is by no means exhaustive, but it covers a fair amount of ground. of course, we can go crazy with randomized search, basically shoving scikit learn with every possible configuration and even running a grid search instead. however, that would take an extreme amount of time and computing resources. therefore, it is important to consider which features are potentially the most important and zoom into these deciding parameters for hypterparameter optimization. randomizedsearchcv the search took a good five to ten minutes, which is a fair amount of time. let's take a look at its results. 0.8022875370243792 we can also take a look at the best parameters that were found. it's worth noting that the algorithm decided that the is superior to , which in my opinion is no surprise. however, it is interesting to see our intuition being vindicated in this fashion nonetheless. now it's time for us to evaluate the model. while there are many different metrics we can use, in binary classification, we can look at things like accuracy, precision, recall, and the f1 score. let's take a look. array the pipeline seems to be working correctly as expected, preprocessing and imputing the data as it was fit on the training data, then generating predictions using the model with optimized parameters. let's see how well our model is doing. one useful function in is the function, which, as the name implies, gives us a comprehensive report of many widely used metrics, such as precision, recall, and the f1 score. precision recall f1 score support 0 0.85 0.91 0.88 162 1 0.84 0.73 0.78 100 accuracy 0.84 262 macro avg 0.84 0.82 0.83 262 weighted avg 0.84 0.84 0.84 262 the report suggests that the accuracy of our model on the test dataset is about 84 percent. we can manually verify this claim by calculating the accuracy ourselves using boolean indexing. 0.8435114503816794 let's top this discussion off with a look at the confusion matrix, which is another way of compactly encoding various pieces of information for model evaluation, namely true positives, true negatives, false positives, and false negatives. note that precision and recall are all metrics that are computed using tp, tn, fp and fn as parameters. the confusion matrix shows that our model performs well at determining the death and survival of those passengers who actually died, but performs rather poorly on those who lived. analyses like these cannot be obtained simply by looking at accuracy, which is why plotting the confusion matrix is always a good idea to get a sense of the model's performance. although the titanic dataset is considered trite, much like mnist is in the context of dl, i still think there is a lot to be learned. even simple ml projects like these have infinite spaces and options for exploration and experimentation. i hope to go through these classic datasets and competitions to glean insight from excellent public kernels, just like this kaggle kernel which i referenced extensively to write this tutorial. in its inception, this post was conceived of as a simple introduction to 's pipelines, but it eventually ballooned up into a somewhat comprehensive rundown of a little kaggle project. i hope to do a lot more of these in the coming days, just because i think there is immense value in mastering ml, although dl sounds a lot cooler. i hope you enjoyed reading this post. catch you up in the next one!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
2020-06-16-numerical-methods,"recently, i ran into an interesting video on youtube on numerical methods . it was a channel called numericalmethodsguy, run by a professor of mechanical engineering at the university of florida. while the videos themselves were recorded a while back in 2009 at just 240p, i found the contents of the video to be very intriguing and easily digestable. his videos did not seem to assume much mathematical knowledge beyond basic high school calculus. after watching a few of his videos, i decided to implement some numerical methods algorithms in python. specifically, this post will deal with mainly two methods of solving non linear equations: the newton raphson method and the secant method. let's dive right into it. before we move on, it's first necessary to come up with a way of representing equations in python. for the sake of simplicity, let's first just consider polynomials. the most obvious, simplest way of representing polynomials in python is to simply use functions. for example, we can express as however, a downside of this approach is the fact that it's difficult to directly derive this equation despite the fact that it is nothing more than just a simple polynomial. so instead, we will use a list index based representation. namely, the th element of a list represents the coefficient of the th power in a polynomial equation. in other words, would translate into . the is a function that returns a python function given a list that conforms to this list index representation. let's see if this works as expected. 7 , so the function passes our quick sanity test. one useful helper function that i also implemented for the sake of convenience is a array to equation parser that translates a list representation into a mathematical expression in python. this is best demonstrated than explained, so i'll defer myself to an example. '1 x 3 20' below is the full definition of the function. at this point, i also thought that it would be useful and interesting to compose a function that translates the string output of into a proper python function we can use to calculate values. below is the function that receives as input some parsed output string and returns a corresponding python function. now, we can do something like this: f = 1 x 3 20 f = 105 now that we have more than enough tools we can use relating to the list index representation we decided to use to represent polynomials, it's time to exploit the convenience that this representation affords us to calculate derivatives. calculating derivatives using the list index representation is extremely easy and convenient: in fact, it can be achieved in just a single line. let's test this function with the example we have been using previously. let's also use the function to make the final result for human readable. f = 1 x 3 20 f' = 3 x 2 seems like the derivative calculation works as expected. in the process, i got a little bit extra and also wrote a function that integrates a function in list index representation format. 27 if we integrate , we end up with , where is the integration constant. excluding the integration constant, we get a result that is consistent with the function. f = 0.25 x 4 20.0 x 1 while it's great that we can calculate derivatives and integrals, one very obvious drawback of this direct approach is that we cannot deal with non polynomial functions, such as exponentials or logarithms. moreover, the list index representation is unable to represent polynomials that include terms whose powers are not positive integers. for these reasons, we will need some other methods of calculating derivatives as well. hence the motivation for approximation methods, outlined in the section below. if you probe the deepest depths of your memory, somewhere you will recall the following equation, which i'm sure all of us saw in some high school calculus class: this equation, commonly known as the definition of a derivative, is also known as the forward divided difference formula of calculating derivatives. there is another variant, known as the backward divided difference formula: and are almost nearly identical, but the difference lies in which term is subtracted from who. in , we go an infinitesimal step forward hence the and subtract the value at the point of approximation, . in , we go backwards, which is why we get . as approaches 0, and asymptotically gives us identical results. below is a python variant of the backward divided difference formula. some tweaks have been made to the formula for use in the section that follows, but at its core, it's clear that the function uses the approximation logic we've discussed so far. another variant of the forward and backward divided difference formula is the center divided difference. by now, you might have some intuition as to what this formula is as the name implies, we somehow use the center point, instead of going forward or backwards from the point of approximation. here is the formula: heuristically, this formula also makes sense. we can imagine going both a step forward and backward, then dividing the results by the total of two steps we've taken, one in each direction. shown below is the python implementation of the center divided difference formula. according to conventional mathematical wisdom, the center divided method normally provides a more robust way of approximating first order derivatives. in this subsection, we discuss why this is the case. using taylor expansion, we can approximate the value of as follows, given that goes to 0 under the limit. notice that we can manipulate to derive the forward divided difference equation in . if we move the term to the lhs, then divide both sides by , we end up with here, we used big o notation to denote the order of magnitude of the trailing terms. the trailing terms are significant since they are directly related to the accuracy of our approximation. an error term of means that, if we halve the step size, we will also halve the error. this is best understood as a linear relationship between error and the step size. we can conduct a similar mode of analysis with backward divided difference. by symmetry, we can express as if we rearrange , we end up with . again, we see that backward divided difference yields linear error, or a trailing term of . here's where things get more interesting: in the case of center divided difference, the magnitude of the error term is , meaning that halving the step size decreases the error by four folds. this is why center divided difference yields much more accurate approximations than forward or backward divided difference. to see this, we subtract from , then move some terms, and divide both sides by . notice that subtracting these two expression results in a lot of term cancellations. dividing both sides by yields from this result, we now know why the central divided difference method provides more accurate approximations of the derivative than do forward and backward divided difference. this is somewhat related to what we will be doing in the next section, so it's a good intuition to have throughout when reading the rest of this article. now that we have these tools for differential calculus, now comes the exciting part: solving non linear equations. specifically, we will be taking a look at two numerical methods: the newton raphson method and the secant method. it's time to put the methods we developed in the preceding sections to use for solving non linear equations. specifically, we'll begin by taking look at a classic algorithm, the newton raphson method. the newton raphson method is one of the many ways of solving non linear equations. the intuition behind the newton raphson method is pretty straightforward: we can use tangent lines to approximate the x intercept, which is effectively the root of the equation . specifically, we begin on some point on the graph, then obtain the tangent line on that point. then, we obtain the intercept of that tangent line, and repeat the process we've just completed by starting on a point on the graph whose value is equal to that intercept. the following image from wikipedia illustrates this process quite well. mathematically, the newton raphson method can be expressed recursively as follows: deriving this formula is quite simple. say we start at a point on the graph, . the tangent line from that point will have a slope of . therefore, the equation of the tangent line can be expressed as then, the intercept can simpy be obtained by finding an value that which makes . let denote that point. then, we arrive at the following update rule. since we will be using as the value for the next iteration, , and now we have the update rule as delineated in . below is an implementation of the newton raphson method in python. i've added some parameters to the function for functionality and customization. is simply some small value we use to decide when to stop the update; if the change in the value of the root is so small that it is not worth the extra compute, we should stop. determines how many iterations we want to continue. if the algorithm is unable to find the root within iterations, it likely means that the function provided does not have a root, or at the very least, the root is not discoverable via the algorithm. lastly, is a flag that determines whether we return the full update history or simply the last value in the iteration as a single value. one peculiarity that deserves attention is the exception, which occurs in this case if the number of arguments passed into the function does not match. i added this block to take into account the fact that the method and other approximate derivative calculation methods such as have differing numbers of parameters. let's see if this actually works by using the example we've been reusing thus far, , or and , both of which we have already defined and initialized above. 2.7144176165949068 the root seems to be around 2.7. and indeed, if we cube it, we end up with a value extremely close to 20. in other words, we have successfully found the root to . 20.000000000000004 instead of the direct derivative, , we can also use approximation methods. in the example below, we show that using results in a very similar value . 2.7144176165949068 this result aligns with the earlier observation that center divided difference provides very accurate approximations of the first derivative. note that the advantage of using is that we can now apply newton raphson to non polynomial equations that cannot be formulated in list index representation format. for instance, let's try something like . 1.0986122886724257 to verify that this is indeed correct, we can plug back into . also, given that , we can use some heuristics to realize that the answer must be only ever so slightly larger than 1. 1.2947865002388426e 11 notice that the result is extremely close to zero, suggesting that we have found the correct root. now that we have seen the robustness of the newton raphson method, let's take a look at another similar numerical method that uses backward divided difference for derivative approximation. in this section, we will look at the secant method, which is another method for identifying the roots of non linear equations. before we get into a description of how this method works, here's a quick graphic, again from wikipedia, on how the secant method works. as the name implies, the secant function works by drawing secant lines that cross the function at each iteration. then, much like the newton raphson method, we find the intercept of that secant line, find a new point on the graph whose coordinate corresponds to that intercept, and use the point from the previous iteration to find a new secant line. this process is very intuitively outlined in this video by numericalmethodsguy. the update rule for the secant method can be expressed as we can derive simply by slightly modifying the update rule we saw for newton raphson. recall that the newton raphson update rule was written as the only modification we need to make to this update rule is to replace with an approximation using the backward divided difference formula. here, we make a slight modification to , specifically by using values from previous iterations. if we plug back into , with some algebraic simplifications, we land on , the update rule for the secant method. this is left as an exercise for the reader. now let's take a look at how we might be able to implement this numerical method in code. presented below is the method, which follows the same general structure as the function we looked at earlier. the only part that differs is the part where we use backward divided difference, which requires that we look up not only the immediately previous root approximation, but the value prior to that as well. in other words, we need both and to calculate via an iterative update. and here is an obligatory sanity check using our previous example. 2.714417616613744 2.7 is a familiar value, and indeed it is what was returned by the newton raphson method as well. we confirm that this is indeed the root of the equation. 20.00000000041639 now that we have looked at both methods, it's time to make a quick comparison. we will be comparing three different methods: newton raphson method with direct polynomial derivatives newton raphson method with center divided difference secant method with backward divided difference by setting to be , we can obtain a full list of root updates as the three methods begin their quest for the root of the function. we can then see which method converges the quickest. let's see how this little experiment turns out. we first begin by importing some dependencies to plot the history of values. then, we obtain the history for each of the three approaches and plot them as a scatter plot. the result is shown below. you might have to squint your eye to see that and almost coincide exactly at the same points. i was honestly somewhat surprised by the result: although we had verified the advantage of using center divided difference over forward or backward divided difference via some simple big o analysis with trailing error terms, i did not expect the two to coincide with such exactitude. another interesting observation is that the secant method seems to take slightly longer than the newton raphson method. this is probably due to the fact that the secant method uses backward divided difference, and also the fact that it requires two previous at each iteration instead of one. the reason why the first update seems rather ineffective is that the two initial guesses that we fed into the model was probably not such a good starting point. the topic of today's post was somewhat different from what we had previously dealt with in this blog, but it was an interesting topic for me nonetheless. i had encountered the newton raphson method previously when going down my typical wikipedia rabbit holes, but it is only today that i feel like i've finally got a grasp of the concept. i consider this post to be a start of many more posts on numerical methods to come. i hope you've enjoyed reading this post. see you in the next one.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0
2020-05-23-workflow-cleanup,"these past few days, i've been writing posts on r while reading hadley wickham's r for data science. r is no python, but i'm definitely starting to see what makes r such an attractive language for data analysis. in particular, writing texts and code blocks on rstudio has been a good experience so far, keeping me focused and productive in my quest to r mastery . however, this newfound interest of mine has created a bit of a turbulence in my blog workflow. previously, the vast majority of my work fell into one of two categories: a math oriented post that only required mathjax on markdown a code oriented post written on jupyter notebooks, mostly with python therefore, the automation workflow that i had set up using worked perfectly fine, since i could simply write a jupyter notebook, then convert it into markdown format and do make whatever minor modifications are necessary prior to publishing. however, this certainly did not work for r notebooks, or r markdown documents, which go by the extension . although and files are functionally somewhat similar in that they both allow people to write both texts and execute code blocks all in one document, they are distinct file formats, and hence the based workflow had to be modified. is a functionality in rstudio that allows people to export files into other file formats, such as html, pdf, or even markdown documents. obviously, given the current setup of my blog, which uses jekyll, i need to export the notebooks into markdown format. so this was great news. here is the setup i use for knitting my document. the part that matters most is the section of the yaml front matter. here, we specify that we want to export the r notebook as a , which stands for markdown document. i also realized that setting to saves me a bit of hassle since the , , and can be pre configured within the r notebook. with this yaml header, clicking on the green knit button will make rstudio do its thing and churn out a file, as well as an image folder that contains the plots and graphs created from executing the code blocks in the notebook. however, the knit button is certainly not a magic button. one problem with the knit function is that, by default, it creates a directory of image files in the same location where the notebook is located. in my case, this was the directory. in other words, knitting would result in the creation of a directory like . moreover, the knitted file would also be in the directory. however, this was not how i had organized my files. here is a heavily truncated summary of how the blog is currently organized. a lot of irrelevant directories containing other customizations were omitted in this summary. in case you're wondering how to create this summary, run here is the result i got with my blog directory. as you can see, the images are located in the directory. also, the notebooks and the posts are in separate locations: while the notebooks reside in , the posts live in . therefore, knitting would mean that i would have to move the knitted file to move the knitted images to fortunately, it's pretty easy to achieve these tasks using the linux shell. in fact, this is very similar to what we had done previously with jupyter notebooks: only this time, r notebooks were added into the mix. therefore, i had to update the existing shell script to account for both and file separately. being lazy programmers, we always want to automate things as much as possible. my end goal was to achieve all the knitting, converting, and moving in just a single line on the command prompt. the first thing i had to do was to update the python and shell scripts that i had previously for handling files. let's first take a look at the previous shell script i had, introduced in this post. what this script does is pretty simple: it uses to create a file from a file. then, it uses a python script to make some edits. lastly, we move the image and markdown files to appropriate locations. here is the script after the revamp, with added functionality to account for files as well. the function can be seen as the driver program that fires different functions depending on the extension of the input file, specified as an argument on the command line. the function is nearly identical to what we had before. the only change is that the function now performs a preliminary check of whether or not an image directory exists before attempting to move it. the addition lies in , which is a function that performs both the knitting, editing, and the moving. due to the anaconda based r setup i've described in this post, using the command in the terminal requires the activation of the relevant virtual environment. after some searching, i realized that this can be done with . after activating the environment, knitting is performed via . then, it's the same drill all over again: edit the file with a python script, then move the files to their respective locations in the blog directory. when all of this is done, we open the newly created markdown file in the directory using our favorite markdown editor, typora. the reason why we need a python script is that the image hyperlinks are most often broken, leading to rendering issues. this issue was documented in the previous post for the case with ; hence the regular expressions with find and replace you see in the function. knitting on r does not engender the same issue. in fact, it works without issues. the issue arises, however, when we move the image file directory to . when the directory is moved, the hyperlinks start to break down. therefore, we have to make some adjustments. this improved python script accounts for both cases of and files, much like the shell script is able to handle both cases. while going about this task, i ran into a number of issues. among them, the most important one was to configure the python script to be able to handle and based markdown files. if the knitted or converted file are both markdown files, how would the python script know which markdown file was created from a r markdown or a ipython notebook? i decided to resolve this by passing an additional argument to the python script; hence the , which is the or flag. but in the end, this flag is hidden to the user it is a hidden api and thus does not introduce additional complexities to the usability of the script. shell scripting turned out to be more interesting than i had thought. i'm satisfied because a lot of manual work is now the computer's job; as a writer, i can now focus only on publishing quality content without having to fiddle with files or minor formatting issues with hyperlinks to images. this works with both ipython and r notebooks, which was the end goal of this little endeavor. shell scripting is something i hope to continue doing. if there is one thing i learned, it is that python + shell is an incredibly powerful combination that never goes wrong.",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
2019-12-28-map-mle,"in a previous post on likelihood, we explored the concept of maximum likelihood estimation, a technique used to optimize parameters of a distribution. in today’s post, we will take a look at another technique, known as maximum a posteriori estimation, or map for short. mle and map are distinct methods, but they are more similar than different. we will explore the similar mathematical underpinnings behind the methods to gain a better understanding of how distributions can be tweaked to best fit some given data. let’s begin! before we jump right into comparing map and mle, let’s refresh our memory on how maximum likelihood estimation worked. recall that likelihood is defined as in other words, the likelihood of some model parameter given data observations is equal to the probability of seeing given . thus, likelihood and probability are inevitably related concepts that describe the same landscape, only from different angles. the objective of maximum likelihood estimation, then, is to determine the values for a distribution’s parameters such that the likelihood of observing some given data is maximized under that distribution. in the example in the previous post on likelihoods, we showed that mle for a normal distribution is equivalent to setting as the sample mean; , sample variance. but this convenient case was specific only to the gaussian distribution. more generally, maximum likelihood estimation can be expressed as: it is not difficult to see why trying to compute this quantity may not be as easy as it seems: because we are dealing with probabilities, which are by definition smaller than 1, their product will quickly diverge to 0, which might cause arithmetic underflow. therefore, we typically use log likelihoods instead. maximizing the log likelihood amounts to maximizing the likelihood function since log is a monotonically increasing function. finding the maximum could be achieved multiple ways, such as through derivation or gradient descent. as the name suggests, maximum a posteriori is an optimization method that seeks to maximize the posterior distribution in a bayesian context, which we dealt with in this post. recall the bayesian analysis commences from a number of components, namely the prior, likelihood, evidence, and posterior. concretely, the objective of bayesian inference is to estimate the posterior distribution, whose probability distribution is often intractable, by computing the product of likelihood and the prior. this process could be repeated multiple times as more data flows in, which is how posterior update can be performed. we saw this mechanism in action with the example of a coin flip, given a binomial likelihood function and a beta prior, which are conjugate distribution pairs. then what does maximizing the posterior mean in the context of map? with some thinking, we can convince ourselves that maximizing the posterior distribution amounts to finding the optimal parameters of a distribution that best describe the given data set. this can be seen by simply interpreting the posterior from a conditional probability point of view: the posterior denotes the probability of the value of the model parameter is given data . put differently, the value of that maximizes the posterior is the optimal parameter value that best explains the sample observations. this is why at its heart, map is not so much different from mle: although mle is frequentist while map is bayesian, the underlying objective of the two methods are fundamentally identical. and indeed, this similarity can also be seen through math. and we see that is almost identical to , the formula for mle! the only part where differs is the inclusion of an additional term in the end, the log prior. what does this difference intuitively mean? simply put, if we specify a prior distribution for the model parameter, the likelihood is no longer just determined by the likelihood of each data point, but also weighted by the specified prior. consider the prior as an additional “constraint”, construed in a loose sense. the optimal parameter not only has to conform to the given data, but also not deviate too much from the established prior. to get a more intuitive hold of the role that a bayesian prior plays in map, let’s assume the simplest, most uninformative prior we can consider: the uniform distribution. a uniform prior conveys zero beliefs about the distribution of the parameter, i.e. all values of are equally probable. the implication of this decision is that the prior collapses to a constant. given the nature of the derived map formula in , constants can safely be ignored as it will not contribute to argument maximization in any way. concretely, therefore, in the case of a uniform prior, we see that map essentially boils down to mle! this is an informative result that tells us that, at their core, mle and map seek to perform the same operation. however, map, being a bayesian approach, takes a specified prior into account, whereas the frequenting mle simply seeks to dabble in data only, as probabilities are considered objective results of repeated infinite trials instead of subjective beliefs as a bayesian statistician would purport. i hope you enjoyed reading this post. see you in the next one! previous post: https://jaketae.github.io/study/likelihood/ this post: https://jaketae.github.io/study/bayes/ likelihood: https://en.wikipedia.org/wiki/likelihood_function maximum likelihood estimation: https://en.wikipedia.org/wiki/maximum_likelihood_estimation arithmetic underflow: https://en.wikipedia.org/wiki/arithmetic_underflow gradient descent: https://en.wikipedia.org/wiki/gradient_descent maximum a posteriori: https://en.wikipedia.org/wiki/maximum_a_posteriori_estimation posterior distribution: https://en.wikipedia.org/wiki/posterior_probability prior: https://en.wikipedia.org/wiki/prior_probability beta: https://en.wikipedia.org/wiki/beta_distribution conjugate distribution pairs: https://en.wikipedia.org/wiki/conjugate_prior conditional probability: https://en.wikipedia.org/wiki/conditional_probability",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
2019-12-01-likelihood,"""i think that's very unlikely."" ""no, you're probably right."" these are just some of the many remarks we use in every day conversations to express our beliefs. linguistically, words such as ""probably"" or ""likely"" serve to qualify the strength of our professed belief, that is, we express a degree of uncertainty involved with a given statement. in today's post, i suggest that we scrutinize the concept of likelihood what it is, how we calculate it, and most importantly, how different it is from probability. although the vast majority of us tend to conflate likelihood and probability in daily conversations, mathematically speaking, these two are distinct concepts, though closely related. after concretizing this difference, we then move onto a discussion of maximum likelihood, which is a useful tool frequently employed in bayesian statistics. without further ado, let's jump right in. as we have seen in an earlier post on bayesian analysis, likelihood tells us and pardon the circular definition here how likely a certain parameter is given some data. in other words, the likelihood function answers the question: provided some list of observed or sampled data , what is the likelihood that our parameter of interest takes on a certain value ? one measurement we can use to answer this question is simply the probability density of the observed value of the random variable at that distribution. in mathematical notation, this idea might be transcribed as: at a glance, likelihood seems to equal probability after all, that is what the equation seems to suggest. but first, let's clarify the fact that is probability density, not probability. moreover, the interpretation of probability density in the context of likelihood is different from that which arises when we discuss probability; likelihood attempts to explain the fit of observed data by altering the distribution parameter. probability, in contrast, primarily deals with the question of how probable the observed data is given some parameter . likelihood and probability, therefore, seem to ask similar questions, but in fact they approach the same phenomenon from opposite angles, one with a focus on the parameter and the other on data. let's develop more intuition by analyzing the difference between likelihood and probability from a graphical standpoint. to get started, recall the that this is the good old definition of probability as defined for a continuous random varriable , given some probability density function with parameter . graphically speaking, we can consider probability as the area or volume under the probability density function, which may be a curve, plane, or a hyperplane depending on the dimensionality of our context. figure 1: representation of probability as area unlike probability, likelihood is best understood as a point estimate on the pdf. imagine having two disparate distributions with distinct parameters. likelihood is an estimate we can use to see which of these two distributions better explain the data we have in our hands. intuitively, the closer the mean of the distribution is to the observed data point, the more likely the parameters for the distribution would be. we can see this in action with a simple line of code. this code block creates two distributions of different parameters, and . then, we assume that a sample of value 1 is observed. then, we can compare the likelihood of the two parameters given this data by comparing the probability density of the data for each of the two distributions. figure 2: representation of likelihood as height in this case, seems more likely, i.e. it better explains the data since , which is larger than . to sum up, likelihood is something that we can say about a distribution, specifically the parameter of the distribution. on the other hand, probabilities are quantities that we ascribe to individual data. although these two concepts are easy to conflate, and indeed there exists an important relationship between them explained by bayes' theorem, yet they should not be conflated in the world of mathematics. at the end of the day, both of them provide interesting ways to analyze the organic relationship between data and distributions. maximum likelihood estimation, or mle in short, is an important technique used in many subfields of statistics, most notably bayesian statistics. as the name suggests, the goal of maximum likelihood estimation is to find the parameters of a distribution that maximizes the probability of observing some given data . in other words, we want to find the optimal way to fit a distribution to the data. as our intuition suggests, mle quickly reduces into an optimization problem, the solution of which can be obtained through various means, such as newton's method or gradient descent. for the purposes of this post, we look at the simplest way that involves just a bit of calculus. the best way to demonstrate how mle works is through examples. in this post, we look at simple examples of maximum likelihood estimation in the context of normal distributions. we have never formally discussed normal distributions on this blog yet, but it is such a widely used, commonly referenced distribution that i decided to jump into mle with this example. but don't worry we will derive the normal distribution in a future post, so if any of this seems overwhelming, you can always come back to this post for reference. the probability density function for the normal distribution, with parameters and , can be written as follows: assume we have a list of observations that correspond to the random variable of interest, . for each in the sample data, we can calculate the likelihood of a distribution with parameters by calculating the probability densities at each point of the pdf where . we can then make the following statement about these probabilities: in other words, to maximize the likelihood simply means to find the value of a parameter that which maximizes the product of probabilities of observing each data point. the assumption of independence allows us to use multiplication to calculate the likelihood in this manner. applied in the context of normal distributions with observations, the likelihood function can therefore be calculated as follows: but finding the maximum of this function can quickly turn into a nightmare. recall that we are dealing with distributions here, whose pdfs are not always the simplest and the most elegant looking. if we multiply terms of the normal pdf, for instance, we would end up with a giant exponential term. to prevent this fiasco, we can introduce a simple transformation: logarithms. log is a monotonically increasing function, which is why maximizing some function is equivalent to maximizing the log of that function, . moreover, the log transformation expedites calculation since logarithms restructure multiplication as sums. with that in mind, we can construct a log equation for mle from as shown below. because we are dealing with euler’s number, , the natural log is our preferred base. using the property in , we can simplify the equation above: to find the maximum of this function, we can use a bit of calculus. specifically, our goal is to find a parameter that which makes the first derivative of the log likelihood function to equal 0. to find the optimal mean parameter , we derive the log likelihood function with respect to while considering all other variables as constants. from this, it follows that rearranging this equation, we are able to obtain the final expression for the optimal parameter that which maximizes the likelihood function: as part 2 of the trilogy, we can also do the same for the other parameter of interest in the normal distribution, namely the standard deviation denoted by . we can simplify this equation by multiplying both sides by . after a little bit of rearranging, we end up with finally, we have obtained the parameter values for the mean and variance of a normal distribution that maximizes the likelihood of our data. notice that, in the context of normal distributions, the ml parameters are simply the mean and standard deviation of the given data point, which closely aligns with our intuition: the normal distribution that best explains given data would have the sample mean and variance as its parameters, which is exactly what our result suggests. beyond the specific context of normal distributions, however, mle is generally very useful when trying to reconstruct or approximate the population distribution using observed data. let’s wrap this up by performing a quick verification of our formula for maximum likelihood estimation for normal distributions. first, we need to prepare some random numbers that will serve as our supposed observed data. we then calculate the optimum parameters and by using the formulas we have derived in and . we then generate two subplots of the log likelihood function as expressed in , where we vary while keeping at in one and flip this in the other. this can be achieved in the following manner. executing this code block produces the figure below. figure: log likelihood for mean and standard deviation from the graph, we can see that the maximum occurs at the mean and standard deviation of the distribution as we expect. combining these two results, we would expect the maximum likelihood distribution to follow where = and = in our code. and that concludes today’s article on likelihood. this post was motivated from a rather simple thought that came to my mind while overhearing a conversation that happened at the pmo office. despite the conceptual difference between probability and likelihood, people will continue to use employ these terms interchangeably in daily conversations. from a mathematician’s point of view, this might be unwelcome, but the vernacular rarely strictly aligns with academic lingua. in fact, it’s most often the reverse; when jargon or scholarly terms get diffused with everyday language, they often transform in meaning and usage. i presume words such as “likelihood” or “likely” fall into this criteria. all of this notwithstanding, i hope this post provided you with a better understanding of what likelihood is, and how it relates to other useful statistical concepts such as maximum likelihood estimation. the topic for our next post is going to be monte carlo simulations and methods. if “monte carlo” just sounds cool to you, as it did to me when i first came across it, tune in again next week. catch you up in the next one. earlier post: https://jaketae.github.io/study/bayes/ likelihood function: https://en.wikipedia.org/wiki/likelihood_function maximum likelihood estimation: https://en.wikipedia.org/wiki/maximum_likelihood_estimation bayesian statistics: https://en.wikipedia.org/wiki/bayesian_statistics monotonically increasing function: https://en.wikipedia.org/wiki/monotonic_function",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
2019-11-30-jupyter-automation,"in the last post, i tested out the functionality of jupyter notebook, a platform that i am just starting to get acquainted with. i'm pleased with how that experimental post turned out, although there are still things to modify and improve. in this post, i explain how i was able to automate the process of converting jupyter notebook into markdown using shell script and python commands. this blog runs on github pages using jekyll, a static website generator engine. when i conceived of the idea of starting a personal blog about a month ago, i considered various hosting options, such as wordpress or wix, but i eventually settled on jekyll, convinced by its utility as well as customizability. i also considered other options such as hugo or gatsby, but decided that jekyll was more starter friendly and offered to do things right out of the box. jekyll makes use of files to display posts. for the most part, uploading posts is a painless process as markdown files are very easy to create. however, one drawback of using this file format with jekyll is that embedding images in posts requires me to download and place images in the correct path, namely in the github repository. because most of the images i use are graphs and plots generated from executing blocks of code on python, this manual reference protocol requires that i follow a rather roundabout routine: write python code on sublime or pycharm call the method to save generated figures on local drive move image file to directory copy and paste relevant code manually onto the file delete the line reference image file in in file while this is not a complicated process, it is a time consuming one indeed, especially if the post gets longer. another downside of this workflow is that it is prone to human error because most of the steps are performed manually by me. copy and pasting, writing links, and moving files across different directories are all tasks that require human input. for those reasons, i found the my workflow to be inefficient. now enters automation and jupyter notebooks into the picture. the biggest advantage of using jupyter notebooks is that plots and figures are generated on the document directly. in other words, there is no need to write code in one location and copy and paste it to another, neither do i have to save a figure as an image file and link it in the post separately code, figures, and texts all stay nicely in one package. after all, the whole idea behind jupyter notebook is to present information in a readable, accessible fashion. this is why i found jupyter notebook to be a very convenient option: it streamlined the process of generating figures and writing explanations for my blog posts, specifically into the following three steps: write jupyter notebook run each code block save file by generating a checkpoint and now i'm done! notice how the complicated, inefficient workflow above has shrunk into three, nice and simple steps. the jupyter notebook ui is also highly intuitive, which is why it did not take a lot of time for me to start writing code and generating figures after fidgeting around with some options for a bit. however, jupyter notebook is not without its problems. for one, i had to find a method to integrate jupyter notebooks into jekyll. jupyter notebook comes in file format, which is not presentable on github pages by default. this meant that i had to convert jupyter notebook into something like or , the two most commonly used file types in jekyll. fortunately, i found a command line conversion tool, named that performed this task quite painlessly. after some experimentation, i was able to generate a file of the jupyter notebook. so i should be done, right? well, unfortunately not. examining the end product, i realized two glaring problems. first, the converted file did not include any yaml front matter. basically, all posts on jekyll include some header that contains information on their title, tag, category, and date information. the converted file, however, did not include this header because it was converted from jupyter notebooks which inherently does not have the notion of yaml front matter. this meant that i had to manually type in these information again, which is, to be fair, not such a time consuming process, but nonetheless additional work on my part. a more serious issue with this conversion was that all the image links were broken. this was certainly bad news: when i compiled and built the website on github, i found that none of the images were properly displayed on the web page. upon examination, i found that all the dysfunctional links took the following form: proper image links, on the other hand, look like on a quick side note, those of you who know will easily recognize that this conforms to syntax. anyhow, to make this work, i realized that i would have to walk through every image link in the converted file and restructure them correctly into standard tag format. this sounded intimidating, but i decided to try it nonetheless. after performing this menial task for some time, however, i concluded that this manual handwork was not the way to go. this is where the fun part comes in. i decided to implement a simple automation system that would solve all of my problems and worries. specifically, this script would have to perform the following list of tasks: convert jupyter notebook to markdown add yaml front matter to the converted file fix broken image links move the converted file to directory move image files to directory the first line of the function invokes a command, the result of which is the creation of a converted file. then, the script runs another a python script, but i will get more into that in a second. after python does its portion of the work, i use two commands to move the file to the directory and the associated image files to the directory, as specified above. then, the script echoes a short message to tell the user that the process is complete. pretty simple, yet it does its trick quite nicely. now onto the python part. if you take a look back at the task list delineated above, you will see that there are two bullet point that has not been ticked off by the script: adding yaml front matter and fixing broken image links and that is exactly what our script will do! let's take a look at the code: in a nutshell, the script opens and reads the converted file, finds and replaces broken image links with correct ones, and adds the default yaml front matter, defined as the string in the code above. when all the necessary edits are performed, the script then rewrites the file with the edited content. this script works fine, but i feel like all of this might have been implemented a lot more efficiently if i were proficient with regular expressions. the task of locating and editing broken image links into tags is a classic problem, but my knowledge of python and regular expressions were not quite enough to implement the most efficient mechanism to tackle this problem. nonetheless, my solution still produces the desired result, which is why i am very content with my first attempt at automation with python and script. now, i have a completely revised workflow that suits my purposes the best: jupyter notebooks to write code and generate figures alongside text explanations, and an automation system that converts these jupyter notebooks to markdown efficiently. all i have to do is to edit the default yaml front matter, such as coming up with good titles and categorizing my post to appropriate categories. this automation script is by no means perfect, which is why i plan on incremental updates as i go. specifically, i might add additional features, such as the ability to auto generate figure captions. i might also streamline the process of find and replace that occurs within the python script after learning more about regular expressions. but for now, the project seems solid as it is, and i am happy with what i have. having completed this little script, i am thinking of starting another automation project that will help me with my work at the pmo. this is going to be a much tougher challenge that involves reading in files and outputting files, but hopefully i can streamline my workflow at the office with python's help. i am excited to see where this project will go, and i will definitely keep this posted. in the meanwhile, happy thanksgiving! last post: https://jaketae.github.io/blog/test/ jupyter notebook: https://jupyter.org wordpress: https://wordpress.com wix: https://www.wix.com hugo: https://gohugo.io gatsby: https://www.gatsbyjs.org yaml front matter: https://jekyllrb.com/docs/front matter/ jekyll: https://jekyllrb.com github pages: https://pages.github.com",0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2020-07-02-gaussian-process,"in this post, we will explore the gaussian process in the context of regression. this is a topic i meant to study for a long time, yet was never able to due to the seemingly intimidating mathematics involved. however, after consulting some extremely well curated resources on this topic, such as kilian's lecture notes and ubc lecture videos by nando de freitas, i think i'm finally starting to understand what gp is. i highly recommend that you check out these resources, as they are both very beginner friendly and build up each concept from the basics. with that out of the way, let's get started. let's begin by considering the classic setup of a regression problem. the goal of regression is to predict some values given a set of observations, otherwise referred to as a training set. there are of course many variants of the regression problem. for instance, in a previous post we took a look at bayesian linear regression, where instead of a single point estimate, we tried to derive a distribution of the predicted data at a given test point. gaussian processes are similar to bayesian linear regression in that the final result is a distribution which we can sample from. the biggest point of difference between gp and bayesian regression, however, is that gp is a fundamentally non parametric approach, whereas the latter is a parametric one. i think this is the most fascinating part about gps as we will see later on, gps do not require us to specify any function or model to fit the data. instead, all we need to do is to identify the mean and covariance of a multivariate gaussian that defines the posterior of the gp. all of this sounds too good be true how can a single multivariate gaussian distribution be enough for what could potentially be a high dimensional, complicated regression problem? let's discuss some mathematical ideas that enable gp to be so powerful. given the setup that and and of course, the marginal of a multivariate gaussian also produces another gaussian. this marginalization property can be understood both intuitively by thinking about the implications of viewing the mean and covariance as vectors and matrices, or by taking a direct integral: lastly and most importantly, we also saw in the post on bayesian linear regression that the product of two gaussians is also gaussian. like this, a distribution that is gaussian most likely stays gaussian, withstanding such operations as marginalization, multiplication, or conditioning. this is a powerful property that we can use to motivate the ""gaussian ness"" behind gp. as stated earlier, gp is non parametric. simply put, this means that we don't have to consider things like the typical in the context of linear regression. normally, we would start off with something like this is sometimes also written in terms of a weight vector or a function . here, we also have some gaussian noise, denoted by : however, since gps are non parametric, we do not have to specify anything about the model. how do we remove this consideration? the short answer is that we marginalize out the model from the integral. let denote the model, the data, the predictions. normally, an integral like the one above would be intractable without a solution in closed form. hence, we would have to rely on random sampling methods such as mcmc. however, in this case, we do have a closed form solution, and we know what it looks like: a gaussian! this means that we uniquely identify the final posterior distribution through gp regression all we need is the mean and covariance. let's start with the easy one first: the mean. the mean is a trivial parameter because we can always normalize the mean to zero by subtracting the mean from the data. therefore, for simplicity purposes, we assume a zero mean throughout this post. the interesting part lies in the covariance. recall that the covariance matrix is defined as follows: roughly speaking, covariance tells us how correlated two entries of the random vector are. this is where i think gps get really interesting: the key point of gp is to realize that we want to model some smooth function that best fits our data. what does this ""smoothness"" mean in terms of covariance? the answer is that values that are close to each other must be highly correlated, whereas those that are far apart would have low covariance. in other words, knowing the value of tells us a lot about the value of , whereas it tells us very little about the value of . in short, the closer the values, the higher the covariance. so at the end of the day, all there is to gp regression is to construct this covariance matrix using some distance function. in gps, these covariance matrices are referred to as kernels. kernels can be understood as some sort of prior that we impose upon the regression problem. the idea of smoothness noted earlier is one such example of a prior. but it is a general prior that makes a lot of sense, since we normally don't want stepwise or non differential functions as the result of regression. but there are hundreds and thousands of kernels out there that each suit different purposes. for the sake of simplicity, however, we will only take a look at one such kernel that relates to smoothness: the squared exponential kernel, often referred to as the rbf kernel. the distance function of the squared exponential kernel looks as follows: we can apply distortions to the rbf function by adding things like coefficients, but for simplicity sake we omit them here. the key takeaway is that the rbf kernel function functions as a distance metric between two points. as an extreme example, let's consider the case when , the diagonal entries of the covariance matrix, which is effectively the variance along those components. then, conversely, when and are extremely different points, we can thus deduce that the distance function returns a value between 0 and 1 that indicates the similarity or closeness between two points. this is exactly the sort of behavior we want for the covariance matrix. in short, the multivariate gaussian that we will be using for gp regression can simply be summarized as where covariance denotes the kernel matrix. in the rbf kernel function above, we were assuming a function without any noise, namely that . however, once we add in gaussian noise as we saw above with where , then we need to make adjustments to the kerne to account for added variations. with some thinking, we can persuade ourselves that the only modification that is needed pertains to the diagonal entries of the covariance matrix. this is because only affects variance that exists within the univariate gaussian for each point on the axis without affecting the non diagonal entries, which otherwise pertain to covariance between two points. in other words, the new kernel matrix now becomes this can be seen as a minor correction to the kernel matrix to account for added gaussian noise. before we jump straight into code implementation, it's necessary to discuss the cholesky decomposition to get some technicality out of the way. the cholesky decomposition is a specialization of the ldu decomposition, applied to symmetric matrices. the idea is that we can factor a symmetric matrix as let's begin by considering the ldu decomposition of the matrix. we know that for symmetric matrices, . this can easily be shown by comparing the ldu decomposition of and respectively: therefore, we can rewrite the ldu decomposition of a as a nice property of diagonal matrices is that we can easily identify its square, namely, where is a matrix whose diagonal entries are each square root of the corresponding originals in . the tranpose is not necessary since is a diagonal matrix, but we do so for convenience purposes later on in the derivation. note the trivial case of the identity matrix, whose square root is equal to itself since all diagonal elements take the value of 1 . given this piece of information, what we can now do is to rewrite the factorization of as where . this is the cholesky decomposition of symmetric matrices to be more exact, positive semi definite matrices. the reason why the cholesky decomposition can only be performed on positive semi definite matrices becomes apparent when we think about the definition of positive semi definiteness. given any non zero vector , the key takeaway is that, given some positive semi definite matrix, we can easily factor it into what we might consider to be its square root in the form of . the cholesky decomposition is extremely useful in the context of sampling. recall that, in a univariate setting, we can model any normal distribution by simply sampling from a standard normal distribution with zero mean and unit variance: we can extend this simplle idea to the context of multivariate gaussians. one natural complication, however, is that variance is a matrix in a multivariate setting. therefore, we would somehow have to find the standard deviation of the gaussian, or effectively its square root. this is precisely where the cholesky decomposition comes in handy. we will be using this means of sampling when implementing gp regression in the next section. let's put all the pieces together. the crux of gp regression is conditioning. recall that here, the setup was that we have some multivariate gaussian vector . given some values for a portion of this random vector, namely , we can then derive another multivariate gaussian for using conditioning. this is exactly what we are trying to do with gp regression. assuming that the data is normally distributed, given a number of training points and their corresponding values, how can we make predictions at test points? in other words, are the test points; , the training points. then, we can now establish the following: where denotes the observed values in the training set and the s are each components of the kernel matrix for the entire dataset, including both the training and test sets: this partition also means that is the kernel for the training set; , the kernel for the test set. you might be wondering how the generic formula for the conditional distribution morphed into . while the notation might obscure their similarity, immediately follows from . first, because we assumed zero mean, the term simply collapses into . the same line of reasoning applies to ' hence, the first term disappears from the mean. as for the covariance, a simple comparison is enough to show that the two equations are identical. in a nutshell, gp regression simply amounts to generating a prediction given some training data through conditioning, under the assumption that the underlying function is a infinite dimensional vector that follows some gaussian distribution with a kernel acting as its prior. given this broad conceptual understanding, let's move onto more concrete implementations. these are the setting we will be using for this post. we set a random seed for reproducibility purposes. recall that, depsite its beautiful underlying complexity, all there is to gp regression is to identify some conditional gaussian with a kernel as its covariance. then, we can simply sample from this conditional distribution to obtain possible models that fit the data. as the first step, let's implement the rbf kernel. here, we modify to have an added parameter, , which is a multiplicative constant to the exponent. the function simply uses double iteration to fill each entry of the covariance matrix. note that and do not have to be identical in length; if their lengths are different, the resulting kernel matrix will simply be rectangular. this is expected given that the components of , namely in , will never be square unless the number of test and training points are equal. now let's generate some dummy data. in theory, the final function sampled through gp is considered an infinite dimensional vector, but for practical reasons of implementation, the vector in this case will be at most 60 dimensions: ten training points and 50 test points, appended together as one vector. next, let's build the kernel with the test points and draw random samples to see what our prior looks like. recall that sampling can be easily achieved by performing the cholesky decomposition on the kernel. let's plot the ten random samples drawn from the prior. note that at this point, we have not seen any training data at all. the only stricture imposed on gp vis a vis the kernel is the fact that the function must be smooth, i.e. points that are close to each other in must be highly correlated. indeed, the sampled data seems to present somewhat smooth curves, although the smoothness is somewhat mitigated by the fact that the model are only vectors of 50 dimensions. however, we would expect them to look even smoother had we augmented the dimensions of the test data to 100 dimensions or more. next, we need a function from which we generate dummy train data. for the purposes of demonstration, let's choose a simple sine function. let's generate 15 training data points from this function. note that we are performing a noiseless gp regression, since we did not add any gaussian noise to . however, we already know how to perform gp with noise, as we discussed earlier how noise only affects the diagonal entries of the kernel. now it's time to model the posterior. recall that the posterior distribution can be expressed as if we use or functions to calculate the inverse of the kernel matrix components, out life would admittedly be easy. however, using inversion is not only typically costly, but also prone to inaccuracy. therefore, we instead opt for a safer method, namely using . in doing so, we will also be introducing some intermediate variables for clarity. let's begin with the expression for the posterior mean , which is . the underlying idea is that we can apply cholesky decomposition on , and use that as a way to circumvent the need for direct inversion. let , then let's make another substitution, . then, this means that we can calculate the mean as similarly, for the covariance , we can introduce an intermediate variable from which we obtain notice that the final expressions for mean and covariance do not require any form of inversion, which was our end goal for efficient and accurate computation. let's transcribe everything back to code. let refer to . then, just to be safe, let's check that is of the desired shape, namely a vector with 50 entries. continuing with our computation of the posterior covariance, as expected, is a 50 by 50 matrix. we are now almost done. since we have computed the mean and covariance , all there is left is to generate samples from this distribution. for that, we resort to cholesky decomposition again, recalling the idea discussed earlier in . let's sample a total of 50 samples. now contains 50 samples generated from the posterior. it's important to keep in mind that these samples are each 50 dimensional vectors in a sense, they can be considered as ""functions"", which is why the gaussian process is often referred to as sampling functions from a multivariate gaussian. let's plot the final result, alongside the actual function . in red, i've also plotted the average of all the 50 samples to see how accurate the result holds up. the model behaves exactly as we would expect: where there is data, we are confident; where there is no data, we are uncertain. therefore, we see little variation on test points near the data. in sparse regions where there is no training data, the model reflects our uncertainty, which is why we observe variation within the sampled functions. comparing the region where there is a lot of training data, and where there is little data, this point becomes apparent. overall, the average of the fifty samples seems to somewhat capture the overall sinusoidal trend present in the training data, notwithstanding the extraneous curvature observed in some regions. my first attempt at understanding gaussian processes probably dates back to earlier this year, when i obtained an electronic copy of rasmussen's gaussian process for machine learning. i gave up on chapter 1. the book is still far beyond my current level of mathematics, but nonetheless i am glad that i was able to gain at least a cursory understanding of gp regression. i hope you've enjoyed reading this post. in a future post, i hope to dive into another topic i've not been able to understand back then: gaussian mixture models. see you in the next one!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0
2020-06-20-bfs-dfs,"in this post, we will be taking a look at a very simple yet popular search algorithm, namely breadth first search and depth first search methods. to give you some context, i've been solving some simple algorithms problems these days in my free time. i found that thees puzzles are very brain stimulating, not to mention the satisfying sense of reward when my code works. bfs and dfs search methods are widely applicable coding problems, so i decided to write a short post on this topic. on a separate note, i'm writing this notebook in jupyter labs instead of jupyter notebooks. jupyter labs feels very similar to the former, but the interface is arguably more modern looking. i'm not sure how i feel jupyter labs yet, but hopefully as i get to discover more features on this platform, i'll get a better idea of which interface better suits my workflow. let's get started! bfs and dfs are graph search methods. this means that we will need to have a way of representing a graph with code. of course, the most intuitive way of representing a graph would be to draw it; however, this would be meaningless to a computer. instead, we will use what is called an adjacency list, which is basically some data structure, mostly a hash map, whose key represents a node and values, the number of connected vertices on the graph. this is one way we can represent information on the vertices and edges of a graph. this example was borrowed from itholic's blog. here is an accompanying image to go along with the hash map. note that our mode of representation can only express unweighted directed or undirected graphs that is, the edges between each edges are weightless. to express weighted graphs, we can use something like symmetric matrices, also known as adjacency matrices, that contain information on the distance between each node. given an adjacency matrix , we would denote the distance between nodes and via . graph representation is an interesting topic, but it is beyond the scope of this post. for now, let's stick with the dictionary representation as shown above and continue probing the world of bfs and dfs. as the name implies, the bfs algorithm is a breadth based approach. what this means is that we explore each possibility layer by layer: when traversing the graph, we look at the entire picture hence the breadth then move onto the next step. here is a crude analogy for visual thinkers: imagine you have a frappuccino. if bfs were a person, they would drink the first layer the whipping cream, for instance then move onto the next. in contrast, mr. dfs would stick a straw into the cup and drink a little bit of each layer before moving the straw to some other location within the cup and taking a sip again. dfs goes deep, whereas bfs goes wide. enough of the coffee analogy, let's see how we can use bfs to traverse the graph. and here is the result we get when we perform a bfs on the graph. the returned list shows the order in which the nodes in the graph were visited. 'a', 'b', 'c', 'h', 'd', 'i', 'j', 'm', 'e', 'g', 'k', 'f', 'l' if you look at the visual illustration of the graph above and follow the path that was returned by the function call, you will quickly see that the order in which the nodes were visited can be understood as level traversal. recall the frappuccino analogy, where we said that mr. bfs would drink the coffee layer by layer. the result aligns with our earlier characterization and analogy. while there is nothing wrong with the function as it is, we can perform some slight optimization by using . notice that in the original function, we used to obtain the first element in the queue. this is a costly operation that takes time due to the nature of array data structures. the operation itself, which pops the last element of the list, takes only time complexity, but popping the first element or the head of the list takes linear time. we can thus use 's built in to micro optimize the algorithm. there is no change in the output path, since the underlying logic remains unchanged. 'a', 'b', 'c', 'h', 'd', 'i', 'j', 'm', 'e', 'g', 'k', 'f', 'l' now let's turn out attention to dfs. as you might be able to tell from the name, dfs first goes deep into the graph until it reaches a leaf node. then, it traverses the graph back up to its root, then taking another node to deeply search again. if you look at the function, you will notice that nothing much has changed from the earlier function. in fact, the only difference is that we now perform a normal on the list, thus obtaining its last element, instead of doing something like or as we had done above. while this may seem like a very minor difference, the implications of this design choice is substantial: dfs traverses the graph very differently from bfs. the key difference is that dfs goes down the graph, then comes back up, repeating this up and down motion until all nodes are visited. this vertical movement can also be understood, from a level's perspective, as depth hence the name, dfs. 'a', 'b', 'h', 'm', 'j', 'k', 'l', 'i', 'c', 'd', 'g', 'e', 'f' the implementation of the dfs traversing algorithm above used iteration with a loop. however, we can also use recursion with an inner helper function to populate the results list, then return the populated result. this uses the convenient fact that we can have nested functions in python with an internal local variable whose scope is effectively global for the inner function. if you think about the order in which the recursive calls are being made in the function, it will become obvious that the we had in the iterative version of the dfs algorithm was basically simulating an actual stack frame on the computer with recursion. in other words, the two methods achieve the same functionality, albeit in seemingly different ways. 'a', 'b', 'h', 'm', 'j', 'k', 'l', 'i', 'c', 'd', 'g', 'e', 'f' an interesting application of the dfs and bfs algorithms is in the context of path finding. in the examples above, we simply traversed the entire graph in order determined by the algorithm, given a starting node. however, what if we want to find ways to get from node x to node y? the traversing algorithm does not answer this question directly. so let's use these algorithms to answer the question in a more direct manner. note that finding the longest or shortest path from one node to another is considered an np hard problem, which means that it is a very difficult problem to which computer scientists and mathematicians have yet to find an answer for. the dfs or bfs approach outlined below is a very crude way of going about this problem and can hardly be called as a solution given the exponential amount of computation that is needed to perform on much complicated graphs. with this bearing in mind, let's take a look. in the example below, we use dfs to find one possible path from to . note that this may not be the quickest path, since dfs is unable to look at the graph level by level for that, we will need bfs. check that the function works as expected. 'a', 'b', 'h', 'j', 'k', 'l' next, here is the same path finding algorithm using bfs. the code is nearly identical to the dfs model we've seen above, but because this algorithm uses level order traversal, we can say with more confidence that the returned result is the shortest path from to . we also apply the micro optimization method we reviewed earlier with . because we had a very simple example, it turns out that the method we found with dfs was in fact the shortest path. 'a', 'b', 'h', 'j', 'k', 'l' the methods above return an efficient path from to . but what if we want to know all possible paths that are possible, even if some of them might be elongated or inefficient? the idea is that we specify a set number of iterations we want the algorithm to run for, denoted in the function parameter as . during that , we find all paths that are possible. when the iterations are over, we return the result. let's try running this for a hundred iterations and see what we get. 'a', 'b', 'h', 'j', 'k', 'l', 'a', 'b', 'a', 'b', 'h', 'j', 'k', 'l', 'a', 'b', 'c', 'b', 'h', 'j', 'k', 'l' great! note that the path we get all starts with and ends with , which is what we had specified. one problem, however, is the fact that besides the first path, the second and third paths include somewhat ineffective paths, where the walker presumably goes from to , then back to , and so on. this problem becomes even more apparent when we run it for more iterations. 'a', 'b', 'h', 'j', 'k', 'l', 'a', 'b', 'a', 'b', 'h', 'j', 'k', 'l', 'a', 'b', 'c', 'b', 'h', 'j', 'k', 'l', 'a', 'b', 'h', 'b', 'h', 'j', 'k', 'l', 'a', 'b', 'h', 'i', 'h', 'j', 'k', 'l', 'a', 'b', 'h', 'j', 'h', 'j', 'k', 'l', 'a', 'b', 'h', 'j', 'k', 'j', 'k', 'l', 'a', 'b', 'h', 'm', 'h', 'j', 'k', 'l', 'a', 'b', 'a', 'b', 'a', 'b', 'h', 'j', 'k', 'l', 'a', 'b', 'a', 'b', 'c', 'b', 'h', 'j', 'k', 'l', 'a', 'b', 'a', 'b', 'h', 'b', 'h', 'j', 'k', 'l', 'a', 'b', 'a', 'b', 'h', 'i', 'h', 'j', 'k', 'l', 'a', 'b', 'a', 'b', 'h', 'j', 'h', 'j', 'k', 'l', 'a', 'b', 'a', 'b', 'h', 'j', 'k', 'j', 'k', 'l', 'a', 'b', 'a', 'b', 'h', 'm', 'h', 'j', 'k', 'l', 'a', 'b', 'c', 'b', 'a', 'b', 'h', 'j', 'k', 'l', 'a', 'b', 'c', 'b', 'c', 'b', 'h', 'j', 'k', 'l' if we want to obtain only those results that are non overlapping, we can add a manual check, i.e. add only those results that have unique nodes. we add this check by adding a control statement, namely . this ensures that the node that we are considering is not a node that the path had already visited before. and if we run this on the graph, we obtain the result that we had expected. 'a', 'b', 'h', 'j', 'k', 'l' however, this is a rather boring example. let's alter the graph a bit by adding a bidirectional connection between nodes and . notice that this creates an internal loop within the graph. in graph theory language, this is known as a strongly connected component, and we might explore this topic in a future post. for now, we can understand them as a loop, the implication of which is that there are now multiple ways to get from one node to another via that loop. this time, when we run the path finding algorithm, we find that there are two ways of going from node to node . 'a', 'b', 'h', 'i', 'a', 'b', 'c', 'd', 'g', 'i' in this last section, we will take a look at two leetcode bfs related problems. one thing that i have slowly realized is that, solving a coding puzzle does not mean that i have understood the problem. in fact, more often or not, i've found myself struggling to solve problems that i somehow managed to solve weeks or months ago. i've therefore decided that it is good practice to come back to previous problems once in a while for review. the first problem is problem 102. this involves level order traversal of a binary search tree. upon reading this question, a small voice in you should be yelling ""bfs, bfs!"", because we have repeated many times that bfs is reminiscent of level order traversal, where we search a tree or a graph layer by layer. here is my solution. the idea is pretty simple: in , we keep track of nodes to visit. these nodes obviously live in the same level. then, we loop through this list of nodes. if the node is not , we add its value to a temporary list within the loop, denoted as . when we are done traversing the nodes in that level, we append the accumulated results of to . we also keep track of the next layer via . this contains the nodes to visit in the next iteration, so it becomes the new . in the next iteration, we repeat what we have done on the list until there is no more nodes to traverse, i.e. we are at the leaf nodes. this is problem 101 on leetcode. the problem is simple: we want to check if a binary tree is symmetric. an example of a symmetric tree is visualized below: how might we go about this? my initial thought was that we could use level order traversal and check if, at each level, the values of the node are symmetric. this check of symmetry can be done simply by doing something like a palindrome check, i.e. . however, upon more research and thinking, i've realized that there are smarter ways to do this by using queues. all we have to do is to make sure that the value of the left and right nodes are identical. if they are, now we need to check if the value of the left child of the left node is equal to that of the right child of the right node. this is the outer layer comparison, represented as . for an inner layer comparison, we need to check if the value of the right child of the left node is equal to that of the left child of the right node. this corresponds to . in the example above, for instance, we need to check that and are equal, and also that and are equal in the outer most leaf nodes. we add these to the queue so that comparisons can be made in the iterations that follow. in this post, we looked at bfs and dfs algorithms. these algorithms are very useful for traversing a tree structure. a generalization of dfs, for example, is the backtracking algorithm, which is often used to solve many problems. although there is nothing special about dfs and bfs in that they are essentially brute force methods of search, they are nonetheless powerful tools that can be used to tackle countless tasks. it's also just good exercise with python. i hope you've enjoyed reading this blog. catch you up in the next one.",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2020-03-15-gan-math,"generative adversarial networks refer to a family of generative models that seek to discover the underlying distribution behind a certain data generating process. this distribution is discovered through an adversarial competition between a generator and a discriminator. as we saw in an earlier introductory post on gans, the two models are trained such that the discriminator strives to distinguish between generated and true examples, while the generator seeks to confuse the discriminator by producing data that are as realistic and compelling as possible. in this post, we'll take a deep dive into the math behind gans. my primary source of reference is generative adversarial nets by ian goodfellow, et al. it is in this paper that goodfellow first outlined the concept of a gan, which is why it only makes sense that we commence from the analysis of this paper. let's begin! gan can be seen as an interplay between two different models: the generator and the discriminator. therefore, each model will have its own loss function. in this section, let's try to motivate an intuitive understanding of the loss function for each. to minimize confusion, let's define some notation that we will be using throughout this post. the goal of the discriminator is to correctly label generated images as false and empirical data points as true. therefore, we might consider the following to be the loss function of the discriminator: here, we are using a very generic, unspecific notation for to refer to some function that tells us the distance or the difference between the two functional parameters. we can go ahead and do the same for the generator. the goal of the generator is to confuse the discriminator as much as possible such that it mislabels generated images as being true. the key here is to remember that a loss function is something that we wish to minimize. in the case of the generator, it should strive to minimize the difference between 1, the label for true data, and the discriminator's evaluation of the generated fake data. a common loss function that is used in binary classification problems is binary cross entropy. as a quick review, let's remind ourselves of what the formula for cross entropy looks like: in classification tasks, the random variable is discrete. hence, the expectation can be expressed as a summation. we can simplify this expression even further in the case of binary cross entropy, since there are only two labels: zero and one. this is the function that we have been loosely using in the sections above. binary cross entropy fulfills our objective in that it measures how different two distributions are in the context of binary classification of determining whether an input data point is true or false. applying this to the loss functions in , we can do the same for : now we have two loss functions with which to train the generator and the discriminator! note that, for the loss function of the generator, the loss is small if is close to 1, since . this is exactly the sort of behavior we want from a loss function for the generator. it isn't difficult to see the cogency of with a similar approach. the original paper by goodfellow presents a slightly different version of the two loss functions derived above. essentially, the difference between and is the difference in sign, and whether we want to minimize or maximize a given quantity. in , we framed the function as a loss function to be minimized, whereas the original formulation presents it as a maximization problem, with the sign obviously flipped. then, goodfellow proceeds by framing as a min max game, where the discriminator seeks to maximize the given quantity whereas the generator seeks to achieve the reverse. in other words, the min max formulation is a concise one liner that intuitively demonstrates the adversarial nature of thecompetition between the generator and the discriminator. however, in practice, we define separate loss functions for the generator and the discriminator as we have done above. this is because the gradient of the function is steeper near than that of the function , meaning that trying to maximize , or equivalently, minimizing is going to lead to quicker, more substantial improvements to the performance of the generator than trying to minimize . now that we have defined the loss functions for the generator and the discriminator, it's time to leverage some math to solve the optimization problem, i.e. finding the parameters for the generator and the discriminator such that the loss functions are optimized. this corresponds to training the model in practical terms. when training a gan, we typically train one model at a time. in other words, when training the discriminator, the generator is assumed as fixed. we saw this in action in the previous post on how to build a basic gan. let's return back to the min max game. the quantity of interest can be defined as a function of and . let's call this the value function: in reality, we are more interested in the distribution modeled by the generator than . therefore, let's create a new variable, , and use this substitution to rewrite the value function: the goal of the discriminator is to maximize this value function. through a partial derivative of with respect to , we see that the optimal discriminator, denoted as , occurs when rearranging , we get and this is the condition for the optimal discriminator! note that the formula makes intuitive sense: if some sample is highly genuine, we would expect to be close to one and to be converge to zero, in which case the optimal discriminator would assign 1 to that sample. on the other hand, for a generated sample , we expect the optimal discriminator to assign a label of zero, since should be close to zero. to train the generator, we assume the discriminator to be fixed and proceed with the analysis of the value function. let's first plug in the result we found above, namely , into the value function to see what turns out. to proceed from here, we need a little bit of inspiration. little clever tricks like these are always a joy to look at. if you are confused, don't worry, you aren't the only one. basically, what is happening is that we are exploiting the properties of logarithms to pull out a that previously did not exist. in pulling out this number, we inevitably apply changes to the terms in the expectation, specifically by dividing the denominator by two. why was this necessary? the magic here is that we can now interpret the expectations as kullback leibler divergence: and it is here that we reencounter the jensen shannon divergence, which is defined as where . this means that the expression in can be expressed as a js divergence: the conclusion of this analysis is simple: the goal of training the generator, which is to minimize the value function , we want the js divergence between the distribution of the data and the distribution of generated examples to be as small as possible. this conclusion certainly aligns with our intuition: we want the generator to be able to learn the underlying distribution of the data from sampled training examples. in other words, and should be as close to each other as possible. the optimal generator is thus one that which is able to mimic to model a compelling model distribution . in this post, we took a brief tour of the math behind general adversarial networks. since the publication of goodfellow's work, more gan models have been introduced and studied by different scholars, such as the wasserstein gan or cyclegan to name just a few. the underlying mathematics for these models are obviously going to be different from what we have seen today, but this is a good starting point nonetheless. i hope you enjoyed reading this post. in the next post, i plan to explore the concept of fisher information and the fisher matrix. it is going to be another math heavy ride with gradients and hessians, so keep you belts fastened!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0
2020-04-05-stieltjes,"expectation is a core concept in statistics, and it is no surprise that any student interested in probability and statistics may have seen some expression like this: in the continuous case, the expression is most commonly presented in textbooks as follows: however, this variant might throw you off, which happened to me when i first came across it a few weeks ago: i mean, my calculus is rusty, but it kind of makes sense: the probably density function is, after all, a derivative of the cumulative density function, and so notationally there is some degree of coherency here. but still, this definition of the expected value threw me off quite a bit. what does it mean to integrate over a distribution function instead of a variable? after some research, however, the math gurus at stack exchange provided me with an answer. so here is a brief summary of my findings. the integral that we all know of is called the riemann integral. the confusing integral is in fact a generalization of the riemann integral, known as the riemann stieltjes integral . there is an even more general interpretation of integrals called the lebesgue integral, but we won't get into that here. first, let's take a look at the definition. the definition of the integral is actually a lot simpler than what one might imagine. here, is a value that falls within the interval . in short, we divide the interval of integration into infinitesimal pieces. imagine this process as being similar to what we learn in calculus 101, where integrals are visualized as an infinite sum of skinny rectangles as the limit approaches zero. essentially, we are doing the same thing, except that now, the base of each rectangle is defined as the difference between and instead of and as is the case with the riemann integral. another way to look at this is to consider the integral as calculating the area beneath the curve represented by the parameterization . this connection becomes a bit more apparent if we consider the fact that the riemann integral is calculating the area beneath the curve represented by . in other words, the riemann stieltjes integral can be seen as dealing with a change of variables. you might be wondering why the riemann stieltjes integral is necessary in the first place. after all, the definition of expectation we already know by heart should be enough, shouldn't it? to answer this question, consider the following function: this cumulative mass function is obviously discontinuous since it is a step wise function. this also means that it is not differentiable; hence, we cannot use the definition of expectation that we already know. however, this does not mean that the random variable does not have an expected value. in fact, it is possible to calculate the expectation using the riemann stieltjes integral quite easily, despite the discontinuity! the integral we wish to calculate is the following: therefore, we should immediately start visualizing splitting up the domain of integration, the real number line, into infinitesimal pieces. each box will be of height and width . in the context of the contrived example, this definition makes the calculation extremely easy, since equals zero in all locations but the jumps where the discontinuities occur. in other words, we can easily extend this idea to calculating things like variance or other higher moments. a more realistic example might be the dirac delta function. consider a constant random variable . in this case, we can imagine the probability density function as a literal spike in the sense that the pdf will peak at and be zero otherwise. the cumulative density function will thus exhibit a discontinuous jump from zero to 1 at . and by the same line of logic, it is easy to see that the expected value of this random variable is , as expected. although this is a rather boring example in that the expectation of a constant is of course the constant itself, it nonetheless demonstrates the potential applications of riemann stieltjes. i hope you enjoyed reading this post. lately, i have been busy working on some interesting projects. there is a lot of blogging and catching up to do, so stay posted for exciting updates to come!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
2020-07-13-word2vec,"in a previous post, we discussed how we can use tf idf vectorization to encode documents into vectors. while probing more into this topic and geting a taste of what nlp is like, i decided to take a jab at another closely related, classic topic in nlp: word2vec. word2vec is a technique introduced by google engineers in 2013, popularized by statements such as ""king man + woman = queen."" the gist of it, as you may know, is that we can express words as vectors that encode their semantics in a meaningful way. when i was just getting starting to learn tensorflow, i came across the embedding layer, which performed exactly this operation: transforming words into vectors. while i thought this process was extremely interesting, i didn't know about the internals of this structure until today, particularly after reading this wonderful tutorial by chris mccornick. in this post, we will be implementing word2vec, a popular embedding technique, from scratch with numpy. let's get started! instead of going over the concepts and implementations separately, let's jump straight into the whole implementation process and elaborate on what is necessary along the way. in order to create word embeddings, we need some sort of data. here is a text on machine learning from wikipedia. i've removed some parentheses and citation brackets to make things slightly easier. since we can't feed raw string texts into our model, we will need to preprocess this text. the first step, as is the approach taken in many nlp tasks, is to tokenize the text, i.e. splitting the text up into smaller units like words, getting rid of punctuations, and so on. here is a function that does this trick using regular expressions. let's create tokens using the wikipedia excerpt shown above. the returned object will be a list containing all the tokens in . another useful operation is to create a map between tokens and indices, and vice versa. in a sense, we are creating a lookup table that allows us to easily convert from words to indices, and indices to words. this will be particularly useful later on when we perform operations such as one hot encoding. let's check if the word to index and index to word maps have successfully been created. as we can see, the lookup table is a dictionary object containing the relationship between words and ids. note that each entry in this lookup table is a token created using the function we defined earlier. now that we have tokenized the text and created lookup tables, we can now proceed to generating the actual training data, which are going to take the form of matrices. since tokens are still in the form of strings, we need to encode them numerically using one hot vectorization. we also need to generate a bundle of input and target values, as this is a supervised learning technique. this then begs the question of what the input and target values are going to look like. what is the value that we are trying to approximate, and what sort of input will we be feeding into the model to generate predictions? the answer to these questions and how they tie into word2vec is at the heart of understanding word embeddings as you may be able to tell, word2vec is not some sort of blackbox magic, but a result of careful training with input and output values, just like any other machine learning task. so here comes the crux of word2vec: we loop through each word in the sentence. in each loop, we look at words to the left and right of the input word, as shown below. this illustration was taken from this article by ramzi karam. in the particular example as shown above, we would generate the following input and prediction pairs part of the training data. note that the window size is two, which is why we look up to two words to the left and right of the input word. so in a way, we can understand this as forcing the model to understand a rough sense of context the ability to see which words tend to stick together. in our own example, for instance, we would see a lot of , meaning that the model should be able to capture the close contextual affinity between these two words. below is the code that generates training data using the algorithm described above. we basically iterate over the tokenized data and generate pairs. one technicality here is that, for the first and last few tokens, it may not be possible to obtain words to the left or right of that input token. in those cases, we simply don't consider these word pairs and look at only what is feasible without causing s. also note that we create and separately instead of putting them in tuple form as demonstrated above. this is just for convenience with other matrix operations later on in the post. below is the definition for , an auxiliary function we used above to combine two objects. also, here is the code we use to one hot vectorize tokens. this process is necessary in order to represent each token as a vector, which can then be stacked to create the matrices and . finally, let's generate some training data with a window size of two. let's quickly check the dimensionality of the data to get a sense of what matrices we are working with. this intuition will become important in particular when training and writing equations for backpropagation in the next section. both and are matrices with 330 rows and 60 columns. here, 330 is the number of training examples we have. we would expect this number to have been larger had we used a larger window. 60 is the size of our corpus, or the number of unique tokens we have in the original text. since we have one hot encoded both the input and output as 60 dimensional sparse vectors, this is expected. now, we are finally ready to build and train our embedding network. at this point, you might be wondering how it is that training a neural network that predicts some nearby context word given an input token can be used to embed words into vectors. after all, the output of the network is going to be some probability vector that passed through a softmax layer, not an embedding vector. this is entirely correct, and this is a question that came to my mind as well. however, this is the part that gets the most interesting: the rows of the intermediate weight matrix is the embedding we are looking for! this becomes much more apparent once we consider the dimensions of the weight matrices that compose the model. for simplicity purposes, say we have a total of 5 words in the corpus, and that we want to embed these words as three dimensional vectors. more specifically, here is the first weight layer of the model: a crucial observation to make is that, because the input is a sparse vector containing one hot encoded vectors, the weight matrix effectively acts as a lookup table that moves one hot encoded vectors to dense vectors in a different dimension more precisely, the row space of the weight matrix. in this particular example, the weight matrix was a transformation of . this is exactly what we want to achieve with embedding: representing words as dense vectors, a step up from simple one hot encoding. this process is exactly what embedding is: as we start training this model with the training data generated above, we would expect the row space of this weight matrix to encode meaningful semantic information from the training data. continuing onwards, here is the second layer that receives as input the embeddings, then uses them to generate a set of outputs. we are almost done. all we now need in the last layer is a softmax layer. when the output is passed into this layer, it is converted into probability vectors whose elements sum up to one. this final output can be considered as context predictions, i.e. which words are likely to be in the window vicinity of the input word. in training specifically error calculation and backpropagation we would be comparing this prediction of probability vectors with its true one hot encoded targets. the error function that we use with softmax is cross entropy, defined as i like to think of this as a dot product of the target vector and the log of the prediction, because that is essentially what the summation is doing. in this alternate formulation, the cross entropy formula can be rewritten as because a one hot encoded vector in this case, all the elements in whose entry is zero will have no effect on the final outcome. indeed, we simply end up taking the negative log of the prediction. notice that the closer the value of the prediction is to 1, the smaller the cross entropy, and vice versa. this aligns with the behavior we want, since we want the predicted probability to be as close to 1 as possible. so let's summarize the entire process a little bit. first, embeddings are simply the rows of the first weight matrix, denoted as . through training and backpropgation, we adjust the weights of , along with the weight matrix in the second layer, denoted as , using cross entropy loss. overall, our model takes on the following structure: where is the matrix contains the prediction probability vectors. with this in mind, let's actual start building and train our model. let's start implement this model in code. the implementation we took here is extremely similar to the approach we took in this post. for an in depth review of backpropagation derivation with matrix calculus, i highly recommend that you check out the linked post. the representation we will use for the model is a python dictionary, whose values are the weight matrices and keys, the name with which we will refer to the weight matrices. in accordance with the nomenclature established earlier, we stick with and to refer to these weights. let's specify our model to create ten dimensional embeddings. in other words, each token will be represented as vectors living in ten dimensional space. note that actual models tend to use much higher dimensions, most commonly 300, but for our purposes this is not necessary. let's begin with forward propagation. coding the forward propagation process simply amounts to transcribing the three matrix multiplication equations in into numpy code. for backpropagation, we will need all the intermediate variables, so we hold them in a dictionary called . however, if we simply want the final prediction vectors only, not the cache, we set to . this is just a little auxiliary feature to make things slightly easier later. we also have to implement the function we used above. note that this function receives a matrix as input, not a vector, so we will need to slightly tune things up a bit using a simple loop. at this point, we are done with implementing the forward pass. however, before we move on, it's always a good idea to check the dimensionality of the matrices, as this will provide us with some useful intuition while coding backward propagation later on. the dimensionality of the matrix after passing the first layer, or the embedding layer, is as follows: this is expected, since we want all the 330 tokens in the text to be converted into ten dimensional vectors. next, let's check the dimensionality after passing through the second layer. this time, it is a 330 by 60 matrix. this also makes sense, since we want the output to be sixty dimensional, back to the original dimensions following one hot encoding. this result can then be passed onto the softmax layer, the result of which will be a bunch probability vectors. implementing backward propagation is slightly more difficult than forward propagation. however, the good news is that we have already derived the equation for backpropagation given a softmax layer with cross entropy loss in this post, where we built a neural network from scratch. the conclusion of the lengthy derivation was ultimately that given our model since we know the error, we can now backpropagate it throughout the entire network, recalling basic principles of matrix calculus. if backprop is still confusing to you due to all the tranposes going on, one pro tip is to think in terms of dimensions. after all, the dimension of the gradient must equal to the dimension of the original matrix. with that in mind, let's implement the backpropagation function. to keep a log of the value of the error throughout the backpropagation process, i decided to make the final return value of to be the cross entropy loss between the prediction and the target labels. the cross entropy loss function can easily be implemented as follows. now we're ready to train and test the model! as we only have a small number of training data coupled with the fact that the backpropagation algorithm is simple batch gradient descent let's just iterate for 50 epochs. while training, we will be caching the value of the cross entropy error function in a list. we can then plot this result to get a better sense of whether the training worked properly. and indeed it seems like we did well! we can thus say with some degree of confidence that the embedding layer has been trained as well. an obvious sanity check we can perform is to see which token our model predicts given the word ""learning."" if the model was trained properly, the most likely word should understandably be ""machine."" and indeed, when that is the result we get: notice that ""machine"" is at the top of the list of tokens, sorted by degree of affinity with ""learning."" machine intelligence the is so build are computer perform it learning conventional a improve subset automatically model algorithms do based artificial through that known experience vision wide programmed data tasks infeasible develop applications used seen on explicitly of study predictions such filtering where needed decisions mathematical email variety or order training and being without in sample to make difficult as building and training was fun and all, but our end goal was not to build a neural network; we wanted to get word embeddings. as stated earlier in this post, the key behind word embeddings is that the rows of the first weight matrix is effectively a dense representation of one hot encoded vectors each corresponding to various tokens in the text dataset. in our example, therefore, the embedding can simply be obtained by array but of course, this is not a user friendly way of displaying the embeddings. in particular, what we want is to be able to input a word through a function and receive as output the embedding vector for that given word. below is a function that implements this feature. when we test out the word ""machine,"" we get a dense ten dimensional vector as expected. array and of course, this vector is not a collection of some randomly initialized numbers, but a result of training with context data generated through the sliding window algorithm described above. in other words, these vectors encode meaningful semantic information that tells us which words tend to go along with each other. while this is a relatively simple, basic implementation of word2vec, the underlying principle remains the same nonetheless. the idea is that, we can train a neural network to generate word embeddings in the form of a weight matrix. this is why embedding layers can be trained to generate custom embeddings in popular neural network libraries like tensorflow or pytorch. if you end up training word embeddings on large datasets like wikipedia, you end up with things like word2vec and glove, another extremely popular alternative to word2vec. in general, it's fascinating to think that, with enough data, we can encode enough semantics into these embedding vectors to see relationships such as ""king man + woman = queen."" i hope you've enjoyed reading this post. see you in the next one.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,0
2020-04-16-r-tutorial-1,"it’s been a while since we last took a look at the r programming language. while i don’t see r becoming my main programming language , i decided it would still be nice to have r in my arsenal for statistical computing. also, it’s always a fun challenge to learn and try to slowly master a new language.this post will serve as my personal source of reference. it’s also worth mentioning that this document was written in r markdown, which seems to be a mix of markdown and jupyter notebook. it is very similar to jupyter in that it allows users to interweave text with bits of code—perfect for blogging purposes. i’m still getting used to rstudio and r markdown, but we will see how it goes. let’s jump right in! setup ===== there are several basic commands that are useful when setting up and working on a r project. for example, to obtain the location of the current working directory, simply type getwd we can also set the working directory. i don’t want to change the working directory here, so instead i will execute a dummy command. setwd) to see the list of variables stored in the environment, use , which is just r’s version of the linux command. ls to remove all stored variables, rm) basics ====== this section is titled “basics”, but we are going to skip over basic arithematic operations, just because they are boring. here, i document certain perks of the r language that may be useful to know about. r is slightly different from other programming languages in that slicing works differently, i.e. both the lower and upper bound are inclusive. x 5 one important point to note about vectors is that they cannot hold objects of different classes. for example, you will see that r casts all objects to become characters when different data types are passed as arguments. v let’s improve this plot with some visual additions. plot"", type=""l"", col=""skyblue"") that looks slightly better. plotting can also be performed with data frames. is a built in dataset in r that we will use here for demonstrative purposes. plot we can also create a pairplot, which shows the distributional relationship between each columns in the table. intuitively, i understand it as something like a visual analogue of a symmetric matrix, with each cell showing the distribution according to the row and column variables. pairs note that the function is versatile. we can specify which columns to plot, as well as set the labels of the plot to be created. for example, with ) equivalently, we could have used this command: plot apply functions =============== lapply let’s start with what i think is the easiet one: . in python terms, this would be something like . here is a very quick demo with a dummy example. movies we can get a bit more sophisticated by segmenting the data by some other axis, much like we did for . this can be achieved in r by the operator. concretely, boxplot just as a reminder, this is what we get with a function. notice that the results shown by the box plot is more inclusive in that it also provides information on the iqr aside from just the mean. tapply histogram creating histograms is not so much different form the other types of visualizations we have seen so far. to create a histogram, we can use the command. hist, main='median value of housing prices', xlab='median value', las=1) lines) note that we have already seen this graph previously, when we were discussing the basics of graphing in an earlier section. several modifications have been made to that graph, namely specifying the variables that go into the x and y axis, as well as some labeling and titling. we’ve also added a spline, which can be considered a form of regression line that explains the pattern in the data. conclusion ========== this tutorial got very long, but hopefully it gave you a review of what the r programming language is like and what you can do with it. as it is mainly a statistical computing language, it is geared towards many aspects of data science, and it is no coincidence that r is one of the most widely used language in this field, coming second after python. in the upcoming r tutorials, we will take a look at some other commands that might be useful for data analysis. stay tuned for more!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
2020-07-17-tinkering-docker,"docker was one of these things that i always wanted to learn, but never got into. part of the reason was that it seemed distant and even somewhat unnecessary to me. as someone who has only worked on relatively simple projects, i never felt the need to go beyond the notion of virtual environments. indeed, when i first read about docker in an attempt to learn more about what all the devops hype was about, i found myself wondering: is docker really that much different from a python virtual environment? well, some time has passed since then, and i got lucky enough to have landed an internship at a small startup. given that the team will be using some devops tools docker definitely being one of them i thought i'd get my hands dirty to get a sense of what docker is like and what it's primarily used for. instead of the youtube route, this time i decided to check out a book titled docker deep dive by nigel poulton. throughout this post, i will be referring to examples from his book. for those who want to get a beginner friendly introduction to docker, i highly recommend this book. at the point of writing, i've read up to chapter 8 of the book, ""containerizing an app,"" immediately before the next chapter on docker compose. this post is not intended as a comprehensive, well written introduction to docker; instead, it is in fact a playground environment i used to test out some docker commands as i was following along the book. with that out of the way, let's jump right in. before getting into any details about docker, it's perhaps necessary for me to clarify the setup in which this post was written. in testing out docker commands, i went back and forth between this jupyter notebook and the terminal. i mainly tried to use jupyter in order to record the commands i typed and their outputs in this post, but certain commands that require secondary input in interactive mode, such as was tested in the terminal. the sign in front of every docker command is necessary to run unix commands in jupyter. an exception is , which is a magic command in jupyter that allows the use of ; does not work, because the way jupyter interacts with the system is by attaching a shell subprocess. these details aside, the key takeaway is that the exclamation or percent symbols can be disregarded. in this section, we will learn about some basic docker commands to get started. here is the most basic one that allows us to check the version and configuration of docker: client: docker engine community version: 19.03.8 api version: 1.40 go version: go1.12.17 git commit: afacb8b built: wed mar 11 01:21:11 2020 os/arch: darwin/amd64 experimental: false server: docker engine community engine: version: 19.03.8 api version: 1.40 go version: go1.12.17 git commit: afacb8b built: wed mar 11 01:29:16 2020 os/arch: linux/amd64 experimental: false containerd: version: v1.2.13 gitcommit: 7ad184331fa3e55e52b890ea95e65ba581ae3429 runc: version: 1.0.0 rc10 gitcommit: dc9208a3303feef5b3839f4323d9beb36df0a9dd docker init: version: 0.18.0 gitcommit: fec3683 notice that the docker engine correctly identifies as as , whereas that of the server is noted as . in essence, this is saying that the server is running on a linux kernel. running a linux kernel on a macos host through docker is made possible via hypervisor and the linuxkit. at this point, all there is to know about the details is that docker originally used virtualbox to run a linux vm, but now uses a more lightweight setup thanks to the aforementioned tools. in unix, is a command that can be used to get a convenient list of files available in the current directory. similarly, can be used to look up what docker components are running or existent. for instance, to check which containers are running, we can type container id image command created status ports names if we want to check images instead of containers, we can simply replace the with . repository tag image id created size test latest 3ad97d9a5a5a 13 minutes ago 82.7mb alpine latest a24bb4013296 6 weeks ago 5.57mb golang 1.11 alpine e116d2efa2ab 10 months ago 312mb we can also use some filtering along with the command to target or specify our search. for instance, to search for only those images whose tags are , we can run repository tag image id created size test latest 3ad97d9a5a5a 2 hours ago 82.7mb ubuntu latest adafef2e596e 6 days ago 73.9mb alpine latest a24bb4013296 6 weeks ago 5.57mb to pull an image, we can use , where the ellipses are the name of the repository and the tag. for example, let's try pulling the latest ubuntu image from docker hub. latest: pulling from library/ubuntu 1b352adcf2: pulling fs layer 1b8a342707: pulling fs layer 1bb8e766f4: pulling fs layer 1bdigest: sha256:55cd38b70425947db71112eb5dddfa3aa3e3ce307754a3df2269069d2278ce474a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k1a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k3a2k2a2k1a2k status: downloaded newer image for ubuntu:latest docker.io/library/ubuntu:latest if we now check what images we have, we see the ubuntu image that was just pulled. repository tag image id created size test latest 3ad97d9a5a5a 13 minutes ago 82.7mb ubuntu latest adafef2e596e 6 days ago 73.9mb alpine latest a24bb4013296 6 weeks ago 5.57mb golang 1.11 alpine e116d2efa2ab 10 months ago 312mb we can also pull from other sources as well. in docker hub, there is this notion of namespaces. what this simply means is that some docker accounts, most likely huge companies or other established developers, have a first class namespace status. this means that the name of their repository is absolute. a good example is is a valid name of an image. for third party or individual developers like us, however, the namespace becomes slightly different. for example, to pull from poulton's repository on docker hub, we need to reference his image as :. for me, it would be . note that the name of the docker repository is effectively the name of the image. another useful thing to know about pulling is that docker intelligently knows when to pull new layers and when to use preexisting ones that are already on our system. for example, if i try pulling an image from docker hub, here is the output message i get on the terminal: latest: pulling from nigelpoulton/tu demo 1b3a933944: pulling fs layer 1b563217f5: pulling fs layer 1b7ec39263: pulling fs layer 1b26f0f7cc: pulling fs layer 1b2aee5115: pulling fs layer 1be9939cc3: pulling fs layer 1b38d27074: pulling fs layer 1b8469a194: pulling fs layer 1bdigest: sha256:c9f8e1882275d9ccd82e9e067c965d1406e8e1307333020a07915d6cbb9a74cf7a2k9a2k7a2k9a2k7a2k9a2k7a2k9a2k9a2k9a2k9a2k9a2k9a2k9a2k9a2k9a2k7a2k9a2k7a2k9a2k9a2k9a2k9a2k8a2k7a2k7a2k7a2k6a2k7a2k7a2k7a2k7a2k7a2k7a2k5a2k5a2k5a2k5a2k7a2k5a2k5a2k4a2k5a2k7a2k5a2k5a2k5a2k5a2k5a2k7a2k7a2k7a2k7a2k2a2k7a2k2a2k2a2k2a2k2a2k2a2k7a2k2a2k2a2k2a2k2a2k2a2k2a2k7a2k2a2k2a2k7a2k2a2k2a2k1a2k2a2k2a2k7a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k7a2k2a2k2a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k7a2k6a2k5a2k5a2k5a2k4a2k3a2k2a2k2a2k2a2k1a2k v1: pulling from nigelpoulton/tu demo 1b3a933944: already exists 1b563217f5: already exists 1b7ec39263: already exists 1b26f0f7cc: already exists 1b2aee5115: already exists 1be9939cc3: already exists 1b38d27074: already exists 1b8469a194: already exists 1bdigest: sha256:674cb034447ab34d442b8df03e0db6506a99390a1e282d126fb44af8598e4d2a v2: pulling from nigelpoulton/tu demo digest: sha256:c9f8e1882275d9ccd82e9e067c965d1406e8e1307333020a07915d6cbb9a74cf status: downloaded newer image for nigelpoulton/tu demo docker.io/nigelpoulton/tu demo notice that layers that already exist are skipped. for example, consider a situation where the docker image uses as a basis. then, since we already have in our system, docker simply assigns a pointer to reference that image instead of downloading duplicate contents again. a useful concept to have in mind when dealing with docker is the notion of images and containers. simply put, a docker image is a snapshot of this semi virtual machine. one can think of it as some sort of frozen specimen from which we can only read, not write. then how do use this image? this is where containers come in. containers are based off of images and allow users to interact with the virtual environment. for example, we can run the ubuntu image by spinning a container off of it through the following command: in docker commands, means interactive mode, meaning that the current terminal will turn into a command line interface within the docker container. due to constraints in jupyter, this process cannot be illustrated here, but you'll figure out what this means once you simply run the command. since we have a container running, if we use the command again but this time on containers we get the running container. notice that under the tab, we see the original image from which this container was created: . container id image command created status ports names 6258444a446a ubuntu:latest ""/bin/bash"" 35 seconds ago up 34 seconds compassionate_hofstadter we can stop containers that are running simply by explicitly stopping it. we can use either names or container ids to target the container we want to stop. compassionate_hofstadter but stopping a container doesn't mean that the container is gone. in fact, if we type , we see that is still on our system! container id image command created status ports names 6258444a446a ubuntu:latest ""/bin/bash"" about a minute ago exited 45 seconds ago compassionate_hofstadter a lot of times, we probably want to keep this docker container since we will probably be developing some application in this docker container. however, if we want to erase the container completely, we can use the command. compassionate_hofstadter and now we see that it is finally gone. container id image command created status ports names but of course, erasing a container doesn't mean that the image from which it was created is also deleted from the system. indeed, if we run on docker images, we still see . if we want to, we can always spin another container from this image. repository tag image id created size test latest 3ad97d9a5a5a 18 minutes ago 82.7mb ubuntu latest adafef2e596e 6 days ago 73.9mb alpine latest a24bb4013296 6 weeks ago 5.57mb golang 1.11 alpine e116d2efa2ab 10 months ago 312mb next, let's talk about dockerfiles. a dockerfile is a file that tells docker what sort of image we want to build. so far, we've only be dealing with default images available from docker hub, such as the latest version of ubuntu. but what if we want to build some customized image of our own for an application? after all, these images only contain absolutely necessary components. for instance, i've tried typing in the ubuntu image, but the command does not exist! so how do we build custom images? well, we basically stack images on top of each other. in this context, we call these images as layers. but the boundary between an image and a layer can get somewhat confusing, since an image composed of multiple images can be squashed into one layer, which would then produce one single layered image. but the overall idea is that we can stack components on top of each other to build a customized image. here is an example dockerfile from poulton's repository. from alpine label maintainer=""nigelpoulton@hotmail.com"" run apk add update nodejs nodejs npm copy . /src workdir /src run npm install expose 8080 entrypoint ""node"", ""./app.js"" in summary, we might visualize this docker image as follows: while this file is certainly not written in vernacular prose, we can sort of see what it's doing. first, we start some base image, which is in this case. then, we install some modules that will be necessary. we then copy the contents of the file to , a virtual directory in the docker container. then, we run some commands and expose the endpoint of the application. exposing the endpoint simply means that there is a port or url through which we can access the web application living in docker. as stated earlier, a dockerfile is a method of building custom images. how do we actually build an image off of it? all we need is a simple command. sending build context to docker daemon 100.9kb step 1/8 : from alpine a24bb4013296 step 2/8 : label maintainer=""nigelpoulton@hotmail.com"" using cache 2ead764f71cf step 3/8 : run apk add update nodejs nodejs npm using cache 6a652e727789 step 4/8 : copy . /src using cache 33eed66ed95e step 5/8 : workdir /src using cache e07f22f7a87b step 6/8 : run npm install using cache 57fcc62715f2 step 7/8 : expose 8080 using cache 889b9b226806 step 8/8 : entrypoint ""node"", ""./app.js"" using cache 3ad97d9a5a5a successfully built 3ad97d9a5a5a successfully tagged test:latest the in the command above simply tells docker that the dockerfile is available in the current directory. if it is in a subfolder, we will have to specify its location. now let's run the app! through on port 8080, we can now access the web application running on the docker container image. c6645ae79b55b87650c8468d1f605e34d3c22a948a2c99bf717f25753598f63a if we check which docker containers are up and running, we see the node application on the list right away. it also shows us the ports that are open. container id image command created status ports names c6645ae79b55 test:latest ""node ./app.js"" 19 seconds ago up 18 seconds 0.0.0.0:8080 8080/tcp web1 let's gracefully stop the container. c6645ae79b55 note that we can chain the two command together to gracefully stop and remove the container in one chained command. 8b867dd4a284 8b867dd4a284 earlier, we saw that the command could be used to delete docker images or containers. while this is true, there are certain things that we need to be careful of when deleting an image or container. for example, if we try to delete , we run into the following message: error response from daemon: conflict: unable to remove repository reference ""alpine:latest"" container 6295af1857c5 is using its referenced image a24bb4013296 this simply means that the image is referenced by another container, namely . from this, we can deduce that the dockerfile for probably starts off with , or at least uses as one of its layers at one point of the building process. like this, we need to make sure that one image is not a basis for another; only the children can be deleted, not its parent. sometimes, you might see images when you run commands for docker images. these might be dangling image layers, which can be checked for through the following command: repository tag image id created size to remove dangling layers, we can prune docker. warning! this will remove all dangling images. are you sure you want to continue? y/n ^c in some cases, however, pruning does not delete images. this means that these images are not dangling; most commonly, i've realized that these seemingly dangling images are simply the intermediate layers of some custom created image. a final note on a convenient command with which we can remove all current containers. although this is a one liner, it is really just a nested command in which we first look for containers that are open, get their identifications, and forcibly remove them from the system with the flag. note that enforcing does not constitute graceful shutdown and deletion, but it is a convenient command nonetheless. da65774cecf9 as mentioned earlier, docker hub is sort of the github for docker images. here, people can push and pull images that they themselves have created, or those that have been created by others. one convenient thing about docker hub is that we can use the command line interface to perform some quick searches. in this example, we search for poulton's images on docker hub, then pipe that result onto so that we don't end up getting too much search results. name description stars official automated nigelpoulton/pluralsight docker ci simple web app used in my pluralsight video … 23 ok nigelpoulton/tu demo voting web server used for various pluralsig… 12 nigelpoulton/ctr demo web server for simple docker demos 3 nigelpoulton/k8sbook simple web app used for demos in the kuberne… 2 nigelpoulton/vote fork of dockersamples voting app for docker… 1 nigelpoulton/dockerbook repo for examples used in docker deep dive b… 0 nigelpoulton/msb hello 0 nigelpoulton/web fe1 web front end 0 nigelpoulton/workshop101 kubernetes 101 workshop. 0 we can also apply filters on our search, just like we saw earlier how we can use the command along with . for instance, let's try to search for an official docker image whose name is . spoiler alert: turns out that there is only one, since has first class namespace status. name description stars official automated alpine a minimal docker image based on alpine linux… 6613 ok when we deal with custom created images, it's probably a good idea to run a quick inspection on the image, just to be sure that everything looks good and nothing is suspicious. the command can be used in this context, and running it gives us this long json style output that tells us a lot about how the image was created and what layers there are within it. }, ""dockerversion"": ""18.09.7"", ""author"": """", ""config"": , ""architecture"": ""amd64"", ""os"": ""linux"", ""size"": 73858282, ""virtualsize"": 73858282, ""graphdriver"": , ""name"": ""overlay2"" }, ""rootfs"": , ""metadata"": } okay, this is perhaps too much data, but there there are parts that are interesting that require our attention. for example, notice that under , the image shows us how many layers there are. granted, the layers are sha256 encrypted, so we can't really see what these individual layers are right away. nonetheless, we can still get an idea of who heavy the image is and how many layers it is composed of. potentially even more important that getting the number of layers from an inspection command is knowing what command the docker is instructed to run. for a better example, let's another image. }, ""tty"": false, ""openstdin"": false, ""stdinonce"": false, ""env"": ""path=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" , ""cmd"": ""/bin/sh"", "" c"", ""cmd \""/bin/sh\"" \"" c\"" \""cd /src && node ./app.js\"""" , ""image"": ""sha256:3eee35387b69036be84160c16d756c975ce6445f5460b19ada2c343d796a0a17"", ""volumes"": null, ""workingdir"": """", ""entrypoint"": null, ""onbuild"": null, ""labels"": }, ""dockerversion"": ""19.03.4"", ""author"": """", ""config"": }, ""tty"": false, ""openstdin"": false, ""stdinonce"": false, ""env"": ""path=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" , ""cmd"": ""/bin/sh"", "" c"", ""cd /src && node ./app.js"" , ""image"": ""sha256:3eee35387b69036be84160c16d756c975ce6445f5460b19ada2c343d796a0a17"", ""volumes"": null, ""workingdir"": """", ""entrypoint"": null, ""onbuild"": null, ""labels"": }, ""architecture"": ""amd64"", ""os"": ""linux"", ""size"": 604213387, ""virtualsize"": 604213387, ""graphdriver"": , ""name"": ""overlay2"" }, ""rootfs"": , ""metadata"": } if you look closely at the output, at one point you will see the section, which looks like this: this section tells us exactly what command the docker container is supposed to run. in this particular instance, we know that the command translates to the part that is in quotation marks is the actual command. from the looks of it, when the container is spun up, it will into the directly and run a node application. nice! so far, the only thing we know about running a container is that is an interactive mode and that running can simply be achieved with . there are some other details that might be helpful to know, in particular relating to automatic restarts. for example, we can pass in some flags such as , , and to specify what action the docker container should take when something breaks down, causing a halt. also note that we specified the name of the container in the example commands above as . we can also micro configure the container by specifically mapping ports from one to another. for example, if we run then we would be able to access the container on port 80. in other words, we would be browsing into , which would effectively be equivalent to browsing into port within the container. these are useful techniques that might come in handy when building a web application. so far, we've looked at pulling docker images from docker hub. we can also push our own images as well. as a simple example, let's take a look at how we might retag an image and perform a simple push. repository tag image id created size web latest 34b07893e6cf 10 seconds ago 82.8mb ubuntu latest adafef2e596e 6 days ago 73.9mb alpine latest a24bb4013296 6 weeks ago 5.57mb nigelpoulton/pluralsight docker ci latest dd7a37fe7c1e 5 months ago 604mb golang 1.11 alpine e116d2efa2ab 10 months ago 312mb the command basically uses a preexisting image and re tags it as specified. in this case, we've retagged into . if we look at the images that are on our system, we see the newly tagged image as well. repository tag image id created size jaketae/web latest 34b07893e6cf 30 minutes ago 82.8mb web latest 34b07893e6cf 30 minutes ago 82.8mb ubuntu latest adafef2e596e 6 days ago 73.9mb alpine latest a24bb4013296 6 weeks ago 5.57mb nigelpoulton/pluralsight docker ci latest dd7a37fe7c1e 5 months ago 604mb golang 1.11 alpine e116d2efa2ab 10 months ago 312mb now pushing is extremely easy: all we need to do is to use the command , where the ellipses contain the repository and tag of the image that we want to push. note that retagging was necessary for us to be able to use our own namespace equivalently, the docker id on docker hub. the push refers to repository docker.io/jaketae/web 1b8b6e0356: preparing 1b9a0747a8: preparing 1ba1bd40b4: preparing 2ba1bd40b4: pushed 54.46mb/51mb5mbine 2k1a2k4a2k2a2k4a2k2a2k4a2k2a2k4a2k2a2k2a2k4a2k3a2k4a2k4a2k4a2k2a2k4a2k4a2k4a2k2a2k4a2k2a2k4a2k2a2k4a2k4a2k2a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k4a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k4a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2k2a2klatest: digest: sha256:ffac23f83cc6f8e6a888db08dc95eca411b13548db499be994f24c26826ac532 size: 1161 in this post, we took a very quick blitz into the world of docker, images, and containers. the more i self study, the more i realize that i'm more of a person who learns through a hands on approach. i think this is especially the case when learning a new technology which allows one to tinker with and interact with the tools being used. i felt this when learning things like spark, and i feel it again in this post. on a special note, i will be working as a backend software development intern for a yale som based startup called rerent. i'm so excited about this opportunity, and i can't wait to apply my knowledge of docker in real production environments as we develop and deploy apps into the cloud. at the same time, however, this also means that i will probably be unable to write as many posts as i used to prior to work. i hope to find a good balance between working and self studying. i might also write posts about things i learn through the internship, such as django, using aws, and many more. thanks for reading this post. see you in the next one!",0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2020-02-05-neural-net,"welcome back to another episode of ""from scratch"" series on this blog, where we explore various machine learning algorithms by hand coding them from scratch. so far , we have looked at various machine learning models, such as knn, logistic regression, and naive bayes. now is time for an exciting addition to this mix: neural networks. around last year december, i bought my first book on deep learning, titled deep learning from scratch, by saito goki. it was a korean translation of a book originally published in japanese by o'reilly japan. many bloggers recommended the book as the go to introductory textbook on deep learning, some even going as far as to say that it is a must have. after reading a few pages in, i could see why: as the title claimed, the author used only to essentially recreate deep learning models, ranging from simple vanilla neural networks to convolutional neural networks. as someone who had just started to learn python, following the book was a lot harder than expected, but it was a worthwhile read indeed. inspired by that book, and in part in an attempt to test the knowledge i gained from having read that bok, i decided to implement my own rendition of a simple neural network supported by minibatch gradient descent. let's jump right into it. the default setup of my jupyter notebook, as always: before we start building our model, we should first prepare some data. instead of using hand made dummy data as i had done in some previous posts, i decided to use the library to generate random data points. this approach makes a lot more sense given that neural networks require a lot more input data than do machine learning models. in this particular instance, we will use the function to accomplish this task. let's take a look at what our data looks like. array as expected, the dataset contains the and coordinates of the points generated by the function. if you haven't heard about this function before, you might be wondering what all the moons deal is about. well, if we plot the data points, it will become a lot more obvious. as you can see, the generated points belong to either one of two classes, and together, each class of points seem to form some sort of moon like shape. our goal will be to build a neural network that is capable of determining whether a given point belongs to class 0 or class 1. in other words, this is a classic example of a binary classification problem. it is standard practice in any classification problem to convert class labels into one hot encodeded vectors. the reason why this preprocessing is necessary is that the class number is merely a label that does not carry any meaning. assume a simple classification problem with 3 labels: 0, 1, and 2. in that context, a class label of 2 is not at all related to adding two data points belonging to class 1, or any arithmatic operation of that kind. to prevent our model from making such arbitrary, unhelpful connections, we convert class labels to one hot encoded vectors. we could use external libraries such as to invoke the function, but instead let's just build a function ourselves since this is a relatively simple task. let's test the function on the training data. we don't need the entire data to see that it works, so let's slice the array to see its first five elements. array when we apply to the data, we see that the returned result is a two dimensional array containing one hot encoded vectors, as intended. array that's all the data and the preprocessing we will need for now. activation functions are important aspects of neural networks. in fact, it is what allows neural networks to model nonlinearities in data. as we will see in the next section, a neural network is essentially composed of layers and weights that can be expressed as matrix multiplications. no matter how complex a matrix may be, matrix multiplication is a linear operation, which means that is impossible to model nonlinearities. this is where activation functions kick in: by applying nonlinear transformation to layer outputs, we can make neural networks capable of modeling nonlinearities. this is why deep learning is such a powerful tool: it can be trained to detect nonlinear, complex patterns in data that a human might otherwise be unable to identify. our vanilla neural network will make use of two activation functions: softmax and relu. if you have read my previous post on the keras functional api, you might recall that we used softmax and relu for certain dense layers. back then, we considered them to be a blackbox without necessarily taking a look at what they do. let's explore the details and get our hands dirty today. mathematically speaking, the softmax function is a function that takes a vector as input and outputs a vector of equal length. concretely, where although the formula may appear complex, the softmax function is a lot simpler than it seems. first, note that all entries of the returned vector add up to 1. from here, it is possible to see that the softmax function is useful for ascribing the probability that a sample belongs to one of classes: the th element of would indicate the probability of the sample belonging to the th class. put another way, the index of the largest entry in is the class label number that is most probable. implementing the softmax function is extremely easy thanks to the vectorized computation made possible through . presented below is one possible implementation of the softmax function in python. this particular implementation, however, poses two problems. first, it is susceptible to arithematic overflow. because computing the softmax function require exponentiation, it is likely for the computer to end up with very large numerical quantities, making calculations unstable. one way to solve this problem is by subtracting values from the exponent. as the calculation shows, adding or subtracting the same value from the exponent of both the numerator and the denominator creates no difference for the output of the softmax function. therefore, we can prevent numbers from getting too large by subtracting some value from the exponent, thus yielding accurate results from stable computation. we can further improve the softmax function for the purposes of this tutorial by supporting batch computation. by batch, i simply mean multiple inputs in the form of arrays. the function shown above is only able to account for a single vector, presumably given as a list or a one dimensional numpy array. the implementation below uses a loop to calculate the softmax output of each instance in a matrix input, then returns the result. note that it also prevents arithematic overflow by subtracting the value of the input array. let's test the improved softmax function with a two dimensional array containing two instances. array as expected, the softmax function returns the softmax output applied to each individual instance in the list. note that the elements of each output instance add up to one, as expected. another crucial activation function is relu), or the rectified linear unit. relu is a piece wise function, and hence introduces nonlinearity, which is one of the purposes of having an activation function in a neural network. the formula for relu is extremely simple. if the input value i s greater or equal to zero, the relu function outputs the value without modification. however, if is smaller than zero, the returned value is also zero. there are other ways of expressing the relu function. one version that is commonly used and thus deserves our attention is written below. although this appears different from , both formulas express the same operation at their core. we can get a better sense of what the function with the help of python. assuming that the input is a vector, we can use vectorization to change only the elements in the input vector that are negative to zero, as shown below. let's see what the relu function looks like by plotting it on the plane. the visualization makes clear the point that relu is a piece wise function that flattens out negative values while leaving positive values unchanged. now that we have all the ingredients ready, it's time to build the neural network. earlier, i said that a neural network can be reduced to matrix multiplication. this is obviously an oversimplification, but there is a degree of truth to that statement. recall that a single neuron of a neural network can be expressed as a dot product of two vectors, as shown below. following conventional notation, represents weights; , input data; , bias. visually, we can imagine the neuron being lit up when the value is large. this is similar to how the human brain works, except that biological neurons are binary in that they either fires on or off; artifical neurons in a network typically take a range of values. if we expand the vector operation in , it becomes quickly obvious that we can represent an entire layer of neurons as a product of two matrices. our simple neural network model can thus be expressed as follows: the equations above represent our simple neural network model composed of two affine layers. the output of the first affine layer, , is modified by a relu unit. then, the output is passed onto the second affine layer, , the output of which is passed onto a softmax unit. the output of the softmax function is the final output of our model. note that relu and softmax are denoted as max and sigma, respectively. the code below is a function that intializes our network. because our data only has two classes, with each data point containing two entries corresponding to the and coordinates of that point, we set both and arguments to 2 by default. the number of neurons in the affine layers, denoted as , is arbitrarily set to 64. the returns a dictionary that contains all the weights of the model. note that we need to pay close attention to the dimensionality of our data to ensure that matrix multiplication is possible. we don't have to worry about the dimensionality of the bias since supports broadcasting by default. presented below is a visualization of our neural network, created using nn svg. instead of cluttering the diagram by attempting to visualize all 64 neurons, i decided to simplify the picture by assuming that we have 16 neurons in each of the affine layers. but with the power of imagination, i'm sure it's not so much difficult to see how the picture would change with 64 neurons. hopefully the visualization gave you a better understanding of what our model looks like. now that we have a function that creates our model, we are ready to run the model! at this point, our neural network model is only a dictionary that contains matrices of specified sizes, each containing randomly genereated numbers. you might be wondering how a dictionary can be considered a model after all, a dictionary is merely a data structure, and so is incapable of performing any operations. to make our model to work, therefore, we need a function that performs matrix multiplications and applies activation functions based on the dictionary. the function is precisely such a function that uses the weights stored in our model to return both the intermediary and final outputs, denoted as and respectively. note that we apply activation functions, such as and when appropriate. this process of deriving an output from an input using a neural network is known as forward propagation. forward propagation is great and all, but without appropriately trained weights, our model is obviously going to spit out meaningless predictions. the way to go about this is to use the gradient descent algorithm with back propagation. we will discuss more about back propagation in the next subsection, as it is a meaty topic that deserves space of its own. we deal primarily with the former in this section. where represents the parameters, or weights, represents the learning rate, and represents the loss function. this is the vanilla gradient descent algorithm, which is also referred to as batch gradient descent. minibatch gradient descent is similar to gradient descent. the only point of difference is that it calculates the gradient for each minibatch instead of doing so for the entire dataset as does batch gradient descent. the advantage of using a minibatch is that it is computationally lighter and less expensive. minibatch gradient descent can be considered a happy point of compromise between stochastic and batch gradient descent, which lie on the polar opposite ends of the spectrum. let's first take a look at the function, which divides the and into and given a . internally, the function calls the gradient descent algorithm to update the weights and finally returns the which contains updated parameters based on the training data. as mentioned above, each and are minibatches that will be feeded into our gradient descent function. note that the function is simply an implementation of equation . at the core of the function is the function, which is our implementation of back propagation. this provides a nice point of transition to the next section. back propagation is a smart way of calculating gradients. there are obviously many ways one might go about gradient calculation. we can simply imagine there being a loss function that is a function of all the thousands of weights and biases making up our neural network, and calculate partial derivatives for each parameter. however, this naive aproach is problematic because it is so computationally expensive. moreover, if you think about it for a second, you might realize that doing so would result in duplicate computations due to the chain rule. take the simple example below. if we were to calculate the gradient of the loss function with respect to and , all we need to compute is the gradient of , since that of will naturally be obtained along the way. in other words, computing the gradient simply requires that we start from the very end of the neural network and propagate the gradient values backwards to compute the partial derivatives according to the chain rule. this is what is at the heart of back propagation: in one huge swoop, we can obtain the gradient for all weights and parameters at once instead of having to calculate them individually. for a more detailed explanation of this mechanism, i strongly recommend that you take a look at this excellent blog post written by christopher olah. how do we go about back propagation in the case of our model? first, it is necessary to define a loss function. the most commonly used loss function in the context of classification problems is cross entropy, which we explored in this post previously on this blog. for a brief recap, presented below is the formula for calculating cross entropy given a true distribution and a predicted distribution : our goal is to train our neural network so that is output distribution is as close to as possible. in the case of binary classification, we might alter equation to the following form: the reformulation as shown in equation is the formula for what is known as binary cross entropy. this is the equation that we will be using in the context of our problem, since the dataset we have only contains two class labels of 0 and 1. now that we have an idea of what the loss function looks like, it's time to calculate the gradient. since we are going to be back propagating the gradient, it makes sense to start from the very back of the neural network. recall that our neural network is structured as follows: the last layer is a softmax unit that receives input to produce output . our goal, then, is to compute the gradient where and each represent the values taken by the th and th neuron in layers and , respectively. one point of caution is that it is important to consider whether and are equal, as this produces differences in the calculation of the gradient. first consider the case when : when : we see that the gradient is different in the two cases! this is certainly going to important for us when calculating the gradient of , the cross entropy loss function, with respect to . specifically, we have to consider the two cases separately by dividing up the summation expression into two parts, as shown below: that was a long ride, but in the end, we end up with a very nice expression! this tells us that the gradient of the cross entropy loss function with respect to the second affine layer is simply the size of the error term. in other words, if we expand the result in to apply to the entire matrix of layers, we get this provides a great place for us to start. we can commence from here to find the gradient of the loss function with respect to other layers more further down the neural network. for example, we can calculate the gradient with respect to the weights of the second affine layer as follows: we won't get into much mathematical details here, but a useful intuition we can use to derive equation is to pay close attention to the dimensionality of data. note that the dimension of the gradient as a matrix should equal to that of the layer itself. in other words, , so on and so forth. this is because the purpose of gradient computation is to update the matrix of parameters: to perform an element by element update with the gradient, it must necessarily be true that the dimensionality of the gradient equals that of the original matrix. using this observation, it is possible to navigate through the confusion of transposes and left, right matrix multiplication that one might otherwise encounter if they were to approach it without any intuition or heuristics. to expedite this post, i'll present the result of the gradient calculations for all parameters below. note that the indicator function, denoted as , is a simple gate function that calculates the gradient of the relu unit: it isn't difficult to see that the indicator function is simply a derivative of the relu function as shown in equation . now, it is time to translate our findings into python. because our neural network model is represented as a dictionary, i decided to adopt the same data structure for the gradient. indeed, that is how we designed the function above. the function below is an implementation of back propagation that encapsulates equations through . there is a subtlety that i did not discuss previously, which has to do with the bias terms. it may appear as if the gradient of the bias term does not match that of the bias term itself. indeed, that is a valid observation according to equation . the way we go about this is that we add up the elements of the matrix according to columns. this is exactly what we do with the command invoked when computing and , which represent the gradient of the bias terms. with all the complex math behind, here is the code implementation of back propagation. finally, our model is ready to be trained! here is a simple function which we can use to train and test our model. because each iteration can yield a different accuracy, we repeat the experiment multiple times or specifically, times to obtain the mean accuracy of our model. we also get a standard deviation of the mean accuracy estimate to see whether or not the performance of the model is reliable and consistent. let's test our model with the and data, with batch size set to 10. mean accuracy: 0.94541, standard deviation: 0.019558 the mean accuracy of our model is around 95 percent, which isn't bad for a simple neural network with just two layers. the standard deviation is also reasonably low, indicating that the performance of our model is consistent with little variations. i was almost about to stop here, but then decided that i wanted to express the neural network model as a python class. after all, that is how actual machine learning and deep learning libraries are implemented. i also decided that it can't hurt for me to practice object oriented thinking. so presented in the next section is a nicer, cleaner implementation of a neural network model based off of the functions we designed above. a simple neural network model in just 56 lines of code, ready to be initialized, trained, deployed, and tested! you will see that much of the code is literally just copy and pasted from the original functions we designed above. but just to make sure that everything works fine, let's try creating a neural network object and use the function to see how well our model performs. i chose 99 as the number of neurons in the affine layers for no reason. 0.9496 in this instance, the accuracy of this model is 95 percent, similar to what we had above. at this point, one question that popped up in my mind was the relationship between the number of neurons and the performance of the neural network model. intuitively, the more neurons there are, the higher the memory capacity of that model, and thus better the performance. of course, the larger the number of neurons, the larger the risk of overfitting our model, which can also negatively impact the performance of the neural network. this is conventional wisdom in the land of deep learning. let's create a function to plot the performance of a neural network and the number of its neurons. below is a function that achieves this task. the function receives , , and as arguments. the first two arguments specify the range for the number of neurons that we are interested in. for example, if we set them to 3 and 40, respectively, that means we want to see the accuracy of models with number of neurons ranging from 3 to 40 in a single layer. the argument specifies the number of experiments we want to conduct. this way, we can calculate the mean accuracy, just as we did previously. let's call the function to create a plot. the result shows that the performance of the neural network generally increases as the number of neurons increase. we don't see signs of overfitting, but we know it happens: recall that our neural network model with 99 and 64 hidden neurons hit an accuracy of about 95 percent, whereas the model with only 30 to 40 neurons seem to be outperforming this metric by an accuracy hovering around 98 percent. after having realized this, i considered re running the function with a different range, but eventuially decided to stop the experiment because running the function took a lot more time than i had expected, even on google colab. creating and training the model takes a long time, especially if we are repeating this process times. for now, the simple observation that the performance seems to increase with more neurons, then fall at one point once overfitting starts to happen, will suffice to satisfy our curiosity. in this post, we built a neural network only using and math. this was a lot more difficult than building other machine learning models from scratch particularly because of the heavy mathematics involved. however, it was definitely worth the challenge becasue completing and writing up this tutorial made me think a lot more about the clockwork of a neural network model. it is easy to think of neural networks as a black box, especially given the sheer ease of creating it. with just , one can build a simple neural network like this one in no time. indeed, the main reason why i love the keras functional api so much is that it is so easy to code and deploy a neural network model. however, when we write such models by depending on preexisting libraries, we sometimes grow oblivious to the intricacies the take place under the hood. it is my hope that reading and following along this post gave you a renewed sense of respect for the writers of such libraries, as well as the beauty of neural network models themselves. i hope you enjoyed reading this post. catch you up in the next one!",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0
2020-10-10-pytorch-tensor,"this is a very quick post in which i familiarize myself with basic tensor operations in pytorch while also documenting and clarifying details that initially confused me. as you may realize, some of these points of confusion are rather minute details, while others concern important core operations that are commonly used. this document may grow as i start to use pytorch more extensively for training or model implementation. let's get started. there appear to be two ways of specifying the size of a tensor. using as an example, let's consider the difference between tensor and tensor it confused me how the two yielded identical results. indeed, we can even verify that the two tensors are identical via true i thought different behaviors would be expected if i passed in more dimensions, plus some additional arguments like , but this was not true. tensor tensor the conclusion of this analysis is that the two ways of specifying the size of a tensor are exactly identical. however, one note of caution is that numpy is more opinionated than pytorch and exclusively favors the tuple approach over the unpacked one. array typeerror traceback in 1 np.ones ~/opt/anaconda3/envs/pytorch/lib/python3.7/site packages/numpy/core/numeric.py in ones 190 191 """""" 192 a = empty 193 multiarray.copyto 194 return a typeerror: cannot interpret '3' as a data type the conclusion of this analysis is that either approach is fine; it is perhaps a good idea to stick to one convention and stay consistent with that coding style throughout. resizing or reshaping a tensor is an incredibly important tensor operation that is used all the time. the interesting thing is that there seems to be many ways of achieving the same behavior. as someone who prefers a more opinionated guideline, this was rather confusing at first. however, here is what i have gathered while sifting through stack overflow and pytorch discussion forums. let's first start with a dummy random tensor. tensor the operation returns a new tensor whose dimensions match those that have been passed into the function as arguments. for example, the snippet below shows how we can reshape into a tensor. tensor one very important detail, however, is that this operation is not in place. in other words, if we check the size of again, you will realize that it is still a tensor, as was originally initialized. torch.size to change itself, we could do or even better, we can use , which is an in place operation by design. tensor notice that, unlike when we called , changes the tensor itself, in place. torch.size in older versions of pytorch, existed as a non in place operator. however, in newer versions of pytorch, this is no longer the case, and pytorch will complain with an informative deprecation error message. note that is not an in place operator, meaning its behavior will largely be identical to that of . /users/jaketae/opt/anaconda3/envs/pytorch/lib/python3.7/site packages/torch/tensor.py:358: userwarning: non inplace resize is deprecated warnings.warn tensor pytorch keeps an internal convention when it comes to differentiating between in place and copy operations. namely, functions that end with a are in place operators. for example, one can add a number to a tensor in place via , as opposed to the normal , which does not happen in place. tensor observe that the addition is not reflected in , indicating that no operations happened in place. tensor , however, achieves the result without copying and creating a new tensor into memory. tensor is another common function that is used to resize tensors. it has been part of the pytorch api for quite a long time before was introduced. without getting into too much technical detail, we can roughly understand view as being similar to in that it is not an in place operation. however, there are some notable differences. for example, this stack overflow post introduces an interesting example: runtimeerror traceback in 1 z = torch.zeros 2 y = z.t 3 y.view runtimeerror: view size is not compatible with input tensor's size and stride . use .reshape instead. on the other hand, does not run into this error. tensor the difference between the two functions is that, whereas can only be used on contiguous tensors. this so thread gives a nice explanation of what it means for tensors to be contiguous; the bottom line is that, some operations, such , do not create a completely new tensor, but returns a tensor that shares the data with the original tensor while having different index locations for each element. these tensors do not exist contiguously in memory. this is why calling after a transpose operation raises an error. , on the other hand, does not have this contiguity requirement. this felt somewhat overly technical, and i doubt i will personally ever use over , but i thought it is an interesting detail to take note of nonetheless. another point of confusion for me was the fact that there appeared to be two different ways of initializing tensors: and . not only do the two functions look similar, they also practically do the same thing. tensor tensor upon more observation, however, i realized that there were some differences, the most notable of which was the . seemed to be unable to infer the data type from the input given. torch.float32 on the other hand, was sable to infer the data type from the given input, which was a list of integers. torch.int64 sure enough, is generally non configurable, especially when it comes to data types. typeerror traceback in 1 torch.tensor typeerror: new received an invalid combination of arguments got , but expected one of: didn't match because some of the keywords were incorrect: dtype can accept as a valid argument. tensor the conclusion of this analysis is clear: use instead of . indeed, this so post also confirms the fact that should generally be used, as is more of a super class from which other classes inherit. as it is an abstract super class, using it directly does not seem to make much sense. in pytorch, there are two ways of checking the dimension of a tensor: and . note that the former is a function call, whereas the later is a property. despite this difference, they essentially achieve the same functionality. tensor torch.size torch.size to access one of the elements, we need appropriate indexing. in the case of , it suffices to consider the size as a list, meaning that square bracket syntax can be used. 2 in the case of , indices can directly be passed into as an argument to index individual elements in the size tensor. 2 these past few days, i've spent a fair amount of time using pytorch for basic modeling. one of the main takeaways from that experience is that an intuition on dimensionality and tensor operations in general is a huge plus. this gets especially important for things like batching. one very basic thing i learned admittedly perhaps too belatedly is the difference between and as dimensions. here is a concrete example. tensor this creates a one dimensional tensor, which is effectively a list. we can check the dimensions of this tensor by calling , which is very similar to how numpy works. 1 on the other hand, specifying the size as results in a two dimensional tensor. tensor the simple, barely passing answer to the question of why is two dimension would be that it has double layered brackets. more exactly speaking, having an additional layer means that it is capable of storing another tensor within it; hence, is living in a dimension that is one above that of . 2 as mentioned earlier, batch dimension is something that becomes very important later on. some pytorch layers, most notably rnns, even have an argument , which accepts a boolean value. if , pytorch expects the first dimension of the input to be the batch dimension. if , which is the case by default, pytorch assumes that the first dimension would be the sequence length dimension. a common operation that is used when dealing with inputs is , or its inverse, . before explaining what these operations perform, let's just take a look at an example. let's start with , the random tensor of size initialized above. tensor if we apply to , we essentially add a new dimension to the 0 th position of 's shape. tensor as you can see, now there is an additional batch dimension, thus resulting in a tensor whose shape is as opposed to the original . however, of course this operation is not performed in place, meaning that will still remain unchanged. there are in place versions of both and though, and that is simply adding a to the end of the function. for example, tensor equivalently, calling will remove the th dimension of the tensor. by default, is 0. tensor squeezing and unsqueezing can get handy when dealing with single images, or just single inputs in general. concatenation and stacking are very commonly used in deep learning. yet they are also operations that i often had trouble imagining in my head, largely because concatenation can happen along many axes or dimensions. in this section, let's solidify our understanding of what concatenation really achieves with some dummy examples. tensor tensor with a basic example, we can quickly verify that each tensor is a three dimensional tensor whose individual elements are two dimensional tensors of shape . tensor now, let's perform the first concatenation along the 0 th dimension, or the batch dimension. tensor we can verify that the concatenation occurred along the 0 th dimension by checking the shape of the resulting tensor. torch.size since we concatenated two tensors each of shape , we would expect the resulting tensor to have the shape of , which is indeed what we got. more generally speaking, we can think that concatenation effectively brought the two elements of each tensor together to form a larger tensor of four elements. i found concatenation along the first and second dimensions to be more difficult to imagine right away. the trick is to mentally draw a connection between the dimension of concatenation and the location of the opening and closing brackets that we should focus on. in the case of the example above, the opening and closing brackets were the outer most ones. in the example below in which we concatenate along the first dimension, the brackets are those that form the boundary of the inner two dimensional 3 by 4 tensor. let's take a look. tensor notice that the rows of were essentially appended to those of , thus resulting in a tensor whose shape is . torch.size for the sake of completeness, let's also take a look at the very last case, where we concatenate along the last dimension. here, the brackets of focus are the innermost ones that form the individual one dimensional rows of each tensor. therefore, we end up with a ""long"" tensor whose one dimensional rows have a total of 8 elements as opposed to the original 4. tensor in this post, we took a look at some useful tensor manipulation operations and techniques. although i do have some experience using keras and tensorflow, i never felt confident in my ability to deal with tensors, as that felt more low level. pytorch, on the other hand, provides a nice combination of high level and low level features. tensor operation is definitely more on the low level side, but i like this part of pytorch because it forces me to think more about things like input and the model architecture. i will be posting a series of pytorch notebooks in the coming days. i hope you've enjoyed this post, and stay tuned for more!",0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0
2020-03-04-open-source,"programming is difficult but fun. or maybe it's the other way around. either way, any developer would know that external libraries are something that makes programming so much easier. as a pythonista with an interest in deep learning, i cannot image a world without essential libraries like , , , or . it simply be impossible, at least for me, to achieve everything from scratch without using these libraries. it is only a few days ago that it dawned to me that all of the modules i frequently use are open source, publicly available and accessible on github. this not only meant that i could take a look at what happens under the hood when i use import statements or call internal functions, but also that i could actively contribute to maintaining and improving these libraries myself. this realization came to me roughly at a time when i was looking into a program called google summer of code, which provides a systematic platform for students enrolled in tertiary institutions to contribute to a specific open source project alongside a designated mentor. unfortunately, i was not eligible for this program due to my being on an authorized leave of absence; however, i decided that it would be great to contribute to an open source library and decided to embark on this journey nonetheless. and of course, with that decision came a number of different hurdles... and a sweet taste of fruition at last. it soon became immediately obvious that i had to learn how to use git. i had a faint idea of what git and github were, since i had to use github to maintain this very blog. however, i never used the command line for any commits; instead, i used a handy little application called github desktop that enabled novice users like me to interact with git and use its features without going through the complexities of the command line interface. the obvious benefit of relying on this applicaiton is that things are so simple: just work on the project, save your files, fire up the application, and it would automatically pull up all the documents that were modified, ready to be pushed. the downside of this approach, however, was that i had virtually zero working knowledge of git and version control. unsurprisingly in retrospect, this turned out to be a huge deficiency when i decided to attempt contributing to open source. for the next two days, i went on a googling spree, learning about basic git commands and jargons such as origin, remote, master, branch, fork, and clone, to name a few. and also because i believe that learning is best done by doing, i forked and cloned actual open source repositories to try out git commands myself. the main problem was that i was rushing myself. at one point, i tried a bunch of commands without knowing what i was really doing, and ultimately ended up creating a pull request to the upstream repository containing over a few hundred file modifications. of course, it wasn't me who had made these modification: they were made by other developers working simulataneously on their own branches, which had been merged to upstream master. in retrospect, this happened because i had pushed a commit after pulling from the upstream instead of merging or rebasing my fork. when the pull request was made, i found myself panicking, struggling to understand what i had done but more embarassed to see a stupid pull request made public on github. of course, i promptly closed the request, but i was both surprised and abashed at the same time. this embarassing episode taught me a number of things. first, to really start contributing to open source, i would have to learn git. learn it properly. second, open source contribution was definitely doable: had my pr been a good pr, i would not have closed it, and it would have been reviewed by members and authors of the repository, and hopefully even merged to master. with these in mind, i continued studying git and finally made my first pull request that week. after a few more days, i finally felt more comfortable with my knowledge of github. this became my default workflow: now, if i wanted to delete a branch for some reason, i could simply do the following: these commands would effectively delete the branch both from the remote and local repository. these commands are obviously very simple and in no way provides a good introduction or crash course on git commands they are left here mostly for my own personal reference. nonetheless, they are very simple commands that might as well help just about any student trying to get started with git and open source contributions. there are many ways one can contribute to open source. in this section, i detail few of these ways by categorizing my own contributions i have made thusfar into three discrete groups. call me audacious, but my target open source projects were big, popular modules such as , , and . intuitively, one would think that these repositories are most difficult to contribute to since they would presumably be the most well maintained given the sheer number of users. this proposition is true to an extent, but it fails to consider the flip side of the coin: because these projects are massive, there is always constant room for improvement. small repositories with just a few folders and files might be easier to maintain, and hence contain lesser lines that require fixing or refactoring. big, popular projects are also highly well maintained, but because of the sheer amount of features being rolled out and the number of pull requests being made, chances are there is always going to be room for contribution. at first, looking at the long lines of complicated code was rather intimidating because i had little understanding of the internal clockwork of the module. therefore, i found it easier to work on editing the docstrings, which are multi line comments that typically explain what a particular function does, its arguments, and return type. these comment appear on the official documentation that explains the api, which is why it is important to provide a clear, well written description of the function. i decided to start slow by taking a look at the docstrings for each function and developing a gradual intuition of how one module interacts with another, how the interplay of various hidden functions can achieve a particular functionality, etc. correcting mistakes or adding elaborations to docstrings still requires one to understand each function, but it also requires one to be a good presentor of information and a keen reviewer capable of detecting deficiencies in explanation. a lot of my pull requests fall under this category because, as a novice open source contributor, i still don't have a solid grasp of how tensorflow works, for example. at best, i'm only familar with the api, because this is the module that i use the most frequently. therefore, my strategy was to focus on docstrings for other sub modules while reviewing the code for . another huge portion of my work revolves around translating tutorials and guides. i found this work to be incredibly rewarding for two reasons: first, i found myself learning tremdously a lot from trying to understand the underlying concepts and code. the first tutorial i translated was on the topic of conditional variational autoenoders, and i ran into references to familiar concepts such as multivariate gaussians, kl divergence, and more. to translate, i had to make sure that i had a solid grasp of the material, so i spent some time brushing up on rusty concepts and learning gaps in knowledge that had to be filled. secondly, i love translating because i feel like translating is a way of disseminating knowledge. it feels incredibly rewarding to think that some other budding developer will read my translation of how to implement deepdream or perform a fast gradient sign method based attack on a neural network to do more amazing things with the knowledge in hand. in retrospect, this is another perk of trying to contribute to a huge open source project like tensorflow. small projects typically do not have multi language support, and it is only when projects grow big enough and attract a substational number of users that the authors and members decide to scale it to cater to a wider global audience. i'm happy to be given the opportunity to take part in this communal endeavor, and i only wish to do more. the last category, of coure, has to do with editing the actual source code. this is the most technical part as it requires knowledge of not only general coding, but also the ins and outs of the library. i have made only a few contributions that fall under this category, but it is when these types of pull requests are approved and merged that i feel the most excitement. one of my first contributions involved the library. it was a very minor edit to the code that made use of instead of the default python dictionary. however, making that pull request and eventually pushing it through until merge taught me a lot about optimization pull requests. when my pr was made public, one of the reviewers requested me to run a test to prove just how optimal my code would be. i was glad that my pr was reviewed, but at the same time daunted by this task. after a bit of research, i came up with my first primitive approach: this returned the following result: this proved that the performance boost was indeed there. to this point, the reviewers agreed, yet the guided me to use the feature instead to prove my point with a plausible input. this was my second attempt at the tasks given their feedback: running a returned the following result: this was better, as it directly proved that my pr optimized performance in the context of the function in question, instead of dealing with the more general question of versus . after this pr was merged into master, i got a better sense of what optimizing code entailed and what the expectations were from the point of view of the project maintainers. it was also great to learn how to tangibly measure code performance via , which is something that i did not know about previousely. optimization is not the only way to contribute to source code: my attempts have included adding robust check for arguments, adding a tensorflow model such as to the official list of models, and a few more. making contributions to source code definitely feels more difficult than editing docstrings, for instance, but i'm hoping that with more practice and experience, spotting errors and rooms for improvement will come more naturally. making open source contributions is not easy, but it is also not impossible. in fact, i would say that anyone who is determined to parse through code and comments can make a good pull request, however small the edit may be. i'm definitely in the process of learning about open source contributions, especially when it comes to reviewing code and adding features, but i hope to contribute what i can as a student and individual developer. after all, it feels good to know that there is at least a drop of my sweat involved in the statement . if anything, it is my hope that this post will also inspire you to contribute to open source; after all, collective intelligence is what makes open source projects so cool.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
2020-11-23-coin-toss,"while mindlessly browsing through math stack exchange, i stumbled across an interesting classic: what is the expected number of coin tosses needed to get 5 consecutive heads? this very short post is going to be an admittedly rough presentation of my attempt at solving this puzzle. the first thing that came to my mind was using a bit of recursion esque thought. generalizing the problem to be a question on consecutive heads, let's assume that we know the number of expected number of tosses needed to obtain consecutive heads. let's denote this as . then, our goal is to figure out . it can be seen that we can think about the two possibilities after successfully having landed on consecutive heads: either the next toss also lands on heads, or it lands on tails. if it lands on heads, then we have accomplished consecutive heads, and this is accounted for by the first term. the trickier part is the second term, which engages with the case in which the next coin toss lands on tails. in that case, the game is essentially ""reverted back to ground zero"" in the sense that the coin tosses we have made so far do not matter any more: even if we made it up until consecutive heads, it doesn't matter if we don't make it until consecutive ones. therefore, we get a recursive term including an . with some manipulation, simplifies into which means this is quite a neat solution, but it is somewhat incomplete since we have no way of obtaining a hard value for given some . we thus need to hash out this recurrence relation. we can easily derive a fully hashed out expression for by starting with small numbers, such a when is 0, 1, 2, and 3. the pattern may not be immediately obvious, but if we try to expand out the terms in our head starting from , it can be seen that we end up doing something like which can be rearranged as i won't pretend like i solved this correctly in one go. while i had the right idea, i actually started off with a wrong equation for at first. specifically, in the second term where the recursion takes place, i was missing 1, fallaciously thinking that the failed first attempt would be counted as part of the new game. this is incorrect because the first failed attempt has already occurred; and at the point in which it has already occurred, that failed try would not count into the expected toss count for the consecutive heads. just for the fun of it, i decided to write a simple python simulator to verify this analytical solution with some monte carlo approximation. and indeed, the results are promising. 61.549 62 in this particular instance with at 5 and set to 1000, we get a very good result. this made me wonder whether the accuracy of this measurement has anything to do with the magnitude of . it seems like there is no substantial difference. however, we see to see some divergence between the two as if we increase the and decrease . this result makes intuitive sense since, after all, the accuracy of this monte carlo experiment depends on ; the higher this number, the more accurate the quantity will be. also, if we increase , things get more unpredictable; it is easy to ""fail"" and restart all over again, or potentially get lucky. in the graph shown above, we got lucky, which is why the values derived from the simulation lags below that of the analytic estimate. this was a fairly simple yet interesting brain teaser that got me thinking more about math. it was very rewarding to see my little monte carlo simulator work its way through the problem. hopefully i can do a bit more of something like this in the future. i hope you've enjoyed reading this post. catch you up in the next one!",0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2020-07-28-gamma-zeta,"maintaining momentum in writing and self learning has admittedly been difficult these past few weeks since i’ve started my internship. normally, i would write one post approximately every four days, but this routine is no longer the norm. to my defense, i’ve been learning a ton about django and backend operations like querying and routing, and i might write a post about these in the future. but for today, i decided to revisit a topic we’ve previously explored on this blog, partially in the hopes of using nostalgia as positive energy in restarting my internal momentum. i must also note that i meant to write this post for a very long time after watching this video by blackpenredpen whose videos have been a source of mathematical learning and inspiration for me. let’s talk about the gamma and zeta functions. before we begin the derivation, perhaps it's a good idea to review what the two greek letter functions are. the gamma function is written as and we all know that the gamma function can be seen as an interpolation of the factorial function, since for non negative integers, the following relationship stands: note that there is also a variant of the gamma function, known as the pi function, which has somewhat of a nicer form: to the mathematically uninformed self, the pi function seems a lot more tractable and intuitive. nonetheless, the prevailing function is euler's gamma function instead of gauss's pi function. the reasons for gamma's dominance over pi is discussed extensively in this math overflow thread. at any rate, it's both interesting and yet also unsurprising to see that these two functions are brainchildren of gauss and euler, two names that arguably appear the most in the world of math. the riemann zeta function is perhaps one of the most famous functions in the world of analysis. it is also sometimes referred to as the euler riemann zeta function, but at this point, prefixing something with ""euler"" loses significance since just about everything in mathematics seems to have some euler prefix in front of it. the riemann zeta function takes the following form: but this definition, as simple and intuitive as it is, seems to erroneously suggest that the riemann zeta function is only defined over non negative integers. this is certainly not the case. in fact, the reason why the riemann zeta function is so widely studied in mathematics is that its domain ranges over the complex number plane. while we won't be discussing the complexity of the riemann zeta function in this regard , it is nonetheless important to consider about how we might calculate, say, . this is where the gamma function comes in. as hinted earlier, an alternative definition of the riemann zeta function can be constructed using the gamma function that takes the riemann zeta beyond the obvious realm of integers and into the real domain. we first start with a simple change of variables. specifically, we can substitute for . this means that , using which we can establish the following: with some algebraic implications, we end up with dividing both sides by , we get all the magic happens when we cast a summation on the entire expression: notice that now we have the riemann zeta function on the left hand side. all we have to do is to clean up what is on the right. as it stands, the integral is not particularly tractable; however, we can swap the integral and the summation expression to make progress. i still haven't figured out the details of when this swapping is possible, which has to do with absolute divergence, but i will be blogging about it in the future once i have a solid grasp of it, as promised before. the expression in the parentheses is just a simple sum of geometric series, which we know how to calculate. therefore, we obtain to make this integral look more nicer into a form known as the bose integral, let's multiply both the numerator and the denominator by . after some cosmetic simplifications, we end up with putting everything together, now we have derived a nice expression that places both the riemann zeta and the gamma functions together: or, alternatively, a definition of the riemann zeta in terms of the gamma: today's post was a short yet very interesting piece on the relationship between the gamma and the riemann zeta. one thing i think could have been executed better is the depth of the article for instance, what is the bose integral and when is it used? i've read a few comments on the original youtube video by blackpenredpen, where people were saying that the bose integral is used in statistical mechanics and the study of black matter, but discussing that would require so much domain knowledge to cover. regardless, i think the theoretical aspect of this derivation is interesting nonetheless. one thing i must do is writing a post on divergence and when the interchange of summation and integrals can be performed. i was originally planning to write a much longer article dividing deep into the gamma and the beta function as well as their distributions. however, i realized that what i need at this point in time is producing output and reorienting myself back to self studying blogger mode, perhaps taking a brief hiatus from the grinding intern spending endless hours in sublime text with django . in the end, we all need a healthy balance between many things in life, and self studying and working are definitely up on that list for me. hopefully i can find the middle ground that suits me best. i hope you've enjoyed reading this post. catch you up in the next one!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
2020-05-10-r-tutorial-3,"a few days ago, i saw a friend who posted an instagram story looking for partners to study r with. i jumped at the opportunity without hesitation—based on my experience these past six months, i knew all too well that studying alone is a lonely, difficult process. the hardest part about it is keeping oneself accountable and continuing a long streak without losing momentum. so i said that i’d love to join him and his crew. what we do as a group is nothing grandiose: we simply keep a log of what we’re studying and answer questions that others might have when they come up in our group chat. while the studying is mostly done on one’s own, the fact that we keep a semi public record of where we are in terms of our study should hopefully motivate all of us to keep making progress until the end. as for me, my goal is to finish the book r for data science, which i had meant to read but never went past chapter 1, mostly because i got carried away by other things. enough of the prologue, here’s a summary of what i’ve learned so far by from the book. introduction ============ is a powerful visualization package in r, much like in python. i’m not proficient enough in to make a direct comparison, but i’ve heard very positive things about eda with r, so i’m excited to learn and have an additional tool under my belt. let’s first load the library to get started. we will be dealing with the data frame, which is built into . since we’ve already loaded via , we can take a look at the data frame simply by typing its name. we will also be using the data set, also from . let’s take a look. basic syntax ============ let’s cut to the chase and take a look at a very simple example of a . things might look a bit confusing at first, but here is a brief rundown of the syntax: the obvious part is the declaration of data that we do inside the function. here, we simplify specify what data frame we are going to be using. then, we add s to the canvas. this is somewhat akin to calling and in python, where is like , , , or other variations, the is in a sense a set structure in r. as the name implies, maps various visualization attributes to the data. these attributes include basic things like and , as well as other aspects like , , or . for example, we might do something like in this case, the problem is that r only supports 6 different ticks or shapes by default, but we have 7 classes, making it impossible to render every data point. nonetheless, it demonstrates how we can toggle additional options within and . scoping we can also make use of scoping to reduce redundanceis. for example, consider the following graph declaration. this is cool, but notice that we are writing repated code for the mappings. instead, we can use global scoping under the function and streamline the code as follows: this is no rocket science: all that happened is that we moved the mapping arguments upward to , so that we no longer have to specify the mapping for each as we had done previously. this not only helps save time, but is also easier to maintain and read. we can also create subplots that separate out each plot for an axis or dimension of data. this can sound a bit abstract at first, and indeed i did have some trouble understanding what faceting meant when i first read the relevant portion, but it’s surprisingly simple. the executive summary is that facets can be considered as a row of plots extracted from a pair plot. enough talking, let’s take a look. as you can see, instead of having all data points in one graph, facetting allows us to divide up the data according to some axis, such as in this case. this might help us discover hidden trends that are not as obvious if the data were to be viewed in aggregate. we can also facet according to multiple axes instead of just one. the syntax is not so different from the previous example. the biggest difference is that instead of using , we use . here, we see the distribution of according to two axes, and . intuiting these facet graphs can get a bit more difficult as we start faceting around multiple axes, but simply think of it this way: instead of considering the data as a whole, we segment the data into certain groups according to their respective axeses or categories. fanciness is definitely not what defines a good visualization, but some degree of vibrance certainly helps portray information, if used correctly. let’s experiment with some colors. by specifying , we see that, as expected, the fill of each bar in the bar plot have been painted according to . this is good, but it doesn’t exactly add new information. we can perhaps get a bit more creative and add an additional dimension of information by specifying to be something other than , which is already handled by . for instance, now things look a bit more interesting. here, we not only see information on , but we also see the composition or distribution of level for each . this has certainly added a layer of information. we can also specify a positional arguments to modify the looks of the graph a bit further according to our tastes and needs. for example, makes the graphs such that it will fill the canvas. this information is informative in that it tells us that the higher the clarity of a dimaon, the more likely it is to be in a certain grade of cut. namely, the yellow clarity diamonds seems to belong to the most. there are other interesting options as well. for example, places overlapping objects next to each other. some other interesting options for scatter plots include . for the purposes of this notebook, however, we won’t go over every option there is: it suffices to demonstrate the role and functionality of the argument in r. the default coordinate system for is, as is the case with many other visualization packages, a cartesian coordinate. however, we can often apply transformations to alter the coordinate system. for example, consider the following bar plot: we applied some miscellaneous touches to the configuration of the graph, but the gist of it is what we have already seen: a bar plot with coloring. how can we make this graph more interesting? one way is to apply various transformations to the coordinates of the graph. for instance, let’s try flipping the axes: here, we used the function to literally flip the coordinates of the graph. this transformation can become particularly useful when the text labels of the data we are dealing with get very long. we can also transform the bar chart into a pie chart by moving to a polar coordinate from the cartesian. i personally find this visualization incredibly appealing. just a comment in passing. visualization syntax in this section, we’ve looked at various ways of creating visualizations and graphs. using this accumulated knowledge, we can now update the basic syntax of we’ve discussed in the previous section. recall our basic template: we can now add more bells and whistles to this formula: this contains a lot of information that we have dealt with so far, with the exception of the portion, which was dealt in the book but not in this notebook. i decided to leave that portion out because it appears to be a more intricate system that i might be interested as an intermediate user of . as of now, the default statistical transformations configured for each should suffice for most use cases. conclusion ========== is a powerful visualization library with many useful functions. although r’s vanilla plotting functions such as or , which we explored in this previous post are useful in their own right, offers more customizability and a wealth of functions that make it much more attractive for production. i hope to continue this series as i get through r for data science with my study buddies. i’ve realized that studying new programming languages, such as c and r, during quarantine period is a good way to stay motivated and productive during what could potentially be dull, grey hours. see you in the next post!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
2019-12-06-linear-regression,"if there is one thing i recall most succinctly from my high school chemistry class, it is how to use excel to draw basic plots. in the eyes of a naive freshman, visualizations seemed to add an air of professionalism. so i would always include a graph of some sort in my lab report, even when i knew they were superfluous. the final icing on the cake? a fancy regression with some r squared. in today’s post, i want to revisit what used to be my favorite tinkering toy in excel: regression. more specifically, we’ll take a look at linear regression, which deals with straight lines and planes instead of curved surfaces. although it sounds simple, the linear regression model is still widely used because it not only provides a clearer picture of obtained data, but can also be used to make predictions based on previous observations. linear regression is also incredibly simple to implement using existing libraries in programming languages such as python, as we will later see in today’s post. that was a long prologue— let’s jump right in. in this section, we will attempt to frame regression in linear algebra terms and use basic matrix operations to derive an equation for the line of best fit. in this section, we will use linear algebra to understand regression. an important theme in linear algebra is orthogonality. how do we determine if two vectors or more generally, two subspaces are orthogonal to each other? how do we make two non orthogonal vectors orthogonal? in our case, we love orthogonality because they are key to deriving the equation for the line of best fit through projection. to see what this means, let’s quickly assume a toy example to work with: assume we have three points, and , as shown below. as we can see, the three points do not form a single line. therefore, it’s time for some regression. let’s assume that this line is defined by . the system of equations which we will attempt to solve looks as follows: or if you prefer the vector matrix representation as i do, this system, remind ourselves, does not have a solution because we have geometrically observed that no straight line can pass through all three points. what we can do, however, is find a projection of the vector onto matrix so that we can identify a solution that is closest to , which we shall denote as . as you can see, this is where all the linear algebra kicks in. let’s start by thinking about , the projection of onto . after some thinking, we can convince ourselves that is the component of that lives within the column space of , and that is the error component of that lives outside the column space of . from this, it follows that is orthogonal to , since any non orthogonal component would have been factored into . concretely, since the transpose is an alternate representation of the dot product. we can further specify this equation by using the fact that can be expressed as a linear combination of the columns of . in other words, where is the solution to the system of equations represented by . let’s further unpackage using matrix multiplication. therefore, we finally have a formula for : let’s remind ourselves of what is and where we were trying to get at with projection in the context of regression. we started off by plotting three data points, which we observed did not form a straight line. therefore, we set out to identify the line of best fit by expressing the system of equations in matrix form, , where . but because this system does not have a solution, we ended up modifying the problem to , since this is as close as we can get to solving an otherwise unsolvable system. so that’s where we are with equation : a formula for , which contains the parameters that define our line of best fit. linear regression is now complete. it’s time to put our equation to the test by applying it to our toy data set. let’s apply in the context of our toy example with three data points to perform a quick sanity check. calculating the inverse of is going to be a slight challenge, but this process is going to be a simple plug and play for the most part. first, let’s remind ourselves of what and are: let’s begin our calculation: calculating the inverse, now, we can put this all together. the final result tells us that the line of best fit, given our data, is let’s plot this line alongside our toy data to see how the equation fits into the picture. it’s not difficult to see that linear regression was performed pretty well as expected. however, ascertaining the accuracy of a mathematical model with just a quick glance of an eye should be avoided. this point then begs the question: how can we be sure that our calculated line is indeed the best line that minimizes error? to that question, matrix calculus holds the key. we all remember calculus from school. we're not going to talk much about calculus in this post, but it is definitely worth mentioning that one of the main applications of calculus lies in optimization: how can we minimize or maximize some function, optionally with some constraint? this particular instance of application is particularly pertinent and important in our case, because, if we think about it, the linear regression problem can also be solved with calculus. the intuition behind this approach is simple: if we can derive a formula that expresses the error between actual values of and those predicted by regression, denoted as above, we can use calculus to derive that expression and ultimately locate the global minimum. and that's exactly what we're going to do. but before we jump into it, let's briefly go over some basics of matrix calculus, which is the variant of calculus we will be using throughout. much like we can derive a function by a variable, say or , loosely speaking, we can derive a function by a matrix. more strictly speaking, this so called derivative of a matrix is more formally known as the gradient. the reason why we introduced the gradient as a derivative by a matrix is that, in many ways, the gradient in matrix calculus resembles a lot of what we saw with derivatives in single variable calculus. for the most part, this intuition is constructive and helpful, and the few caveats where this intuition breaks down are beyond the purposes of this post. for now, let's stick to that intuition as we venture into the topic of gradient. as we always like to do, let's throw out the equation first to see what we're getting into before anything else. we can represent the gradient of function with respect to matrix is a matrix of partial derivatives, defined as while this formula might seem complicated, in reality, it is just a convenient way of packaging partial derivatives of the function into a compact matrix. let's try to understand what this operation entails through a simple dummy example. as you can see, instead of a m by n matrix, we have a column vector as an ingredient for a function. but don't worry: the formula in works for vectors as well, since vectors can be considered as matrices with only a single column. with that in mind, let's define our function as follows: great! we see that the is a scalar function that returns some value constructed using the entries of . equation tells us that the gradient of , then, is simply a matrix of partial derivatives whose dimension equals that of . concretely, in other words, notice that this is the single variable calculus equivalent of saying that . this analogue can be extended to other statements in matrix calculus. for instance, where is a symmetric matrix. we can easily verify this statement by performing the calculation ourselves. for simplicity's sake, let's say that is a two by two matrix, although it could theoretically be any by matrix where is some positive integer. note that we are dealing with square matrices since we casted a condition on that it be symmetrical. let's first define and as follows: then, we can now compute the gradient of this function according to : we have not provided an inductive proof as to how the same would apply to by matrices, but it should now be fairly clear that , which is the single variable calculus analogue of saying that . in short, with these propositions in mind, we are now ready to jump back into the linear regression problem. at this point, it is perhaps necessary to remind ourselves of why we went down the matrix calculus route in the first place. the intuition behind this approach was that we can construct an expression for the total error given by the regression line, then derive that expression to find the values of the parameters that minimize the error function. simply put, we will attempt to frame linear regression as a simple optimization problem. let's recall the problem setup from the linear algebra section above. the problem, as we framed it in linear algebra terms, went as follows: given some unsolvable system of equations , find the closest approximations of and , each denoted as and respectively, such that the system is now solvable. we will start from this identical setup with the same notation, but approach it slightly differently by using matrix calculus. the first agenda on the table is constructing an error function. the most common metric for error analysis is mean squared error, or mse for short. mse computes the magnitude of error as the squared distance between the actual value of data and that predicted by the regression line. we square the error simply to prevent positive and negative errors from canceling each other out. in the context of our regression problem, where denotes the error function. we can further break this expression down by taking note of the fact that the norm of a vector can be expressed as a product of the vector and its transpose, and that as established in the previous section of this post. putting these together, using distribution, we can simplify the above expression as follows: it's time to take the gradient of the error function, the matrix calculus analogue of taking the derivative. now is precisely the time when the propositions and we explored earlier will come in handy. in fact, observe that first term in corresponds to case ; the second term, case . the last term can be ignored because it is a scalar term composed of , which means that it will not impact the calculation of the gradient, much like how constants are eliminated during derivation in single variable calculus. now, all we have to do is to set the expression above to zero, just like we would do in single variable calculus with some optimization problem. there might be those of you wondering how we can be certain that setting this expression to zero would yield the minimum instead of the maximum. answering this question requires a bit more math beyond what we have covered here, but to provide a short preview, it turns out that our error function, defined as is a positive definite matrix, which guarantees that the critical point we find by calculating the gradient gives us a minimum instead of a maximum. this statement might sometimes be phrased differently along the lines of convexity, but this topic is better tabled for a separate future post. the key point here is that setting the gradient to zero would tell us when the error is minimized. this is equivalent to therefore, now we are done! just like in the previous section, gives us the parameters for our line of best fit, which is the solution to the linear regression problem. in fact, the keen reader might have already noted that is letter by letter identical to formula we derived in the previous section using plain old linear algebra! one the one hand, it just seems surprising and fascinating to see how we end up in the same place despite having taken two disparate approaches to the linear regression problem. but on the other hand, this is what we should have expected all along: no matter what method we use, the underlying thought process behind both modes of approach remain the same. whether it be through projection or through derivation, we sought to find some parameters, closest to the values we are approximating as much as possible, that would turn an otherwise degenerate system into one that is solvable. linear regression is a simple model, but i hope this post have done it justice by demonstrating the wealth of mathematical insight that can be gleaned from its derivation. projection: https://en.wikipedia.org/wiki/projection_ matrix calculus: https://en.wikipedia.org/wiki/matrix_calculus gradient: https://en.wikipedia.org/wiki/gradient partial derivatives: https://en.wikipedia.org/wiki/partial_derivative mean squared error: https://en.wikipedia.org/wiki/mean_squared_error positive definite matrix: https://en.wikipedia.org/wiki/definiteness_of_a_matrix",0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0
2020-01-26-typora,"disclaimer: i was not sponsored by the developers of typora to write this post, although that would have been great. so far, my default multi purpose text editor has been jupyter notebook. because many of my posts involve mathematical expressions written in commands as well as code snippets, jupyter notebook was a choice that made sense. however, lately i have realized that for posts that do not require code executions or visualizations, there are far superior options out there, one of which is the editor i am using right now to write this post: typora. i have always been a fervent supporter of minimalism. my odd penchant for minimalism bleeds into many areas of my life, big and small, significant and frivolous. for instance, i always keep my computer's desktop clean and empty. i have seen many people who are completely oblivious to the looks of their desktop a matter of personal preference that i fully respect and understand but for some inexplicable reason, i cannot stand looking at a desktop with files scattered about here and there. at the minimum, i use the sort function to make sure that the dekstop's aesthetics is passable by my standards. , although i highly doubt it given that my actual physical desktop is in a state of chaos most of the time. and by most, i mean always.) typora is a wondeful text editor that suits my minimalistic taste. it's ui is clean and simple, making the editor extremely intuitive and easy to use. upon installation, the user is sent to a refreshingly blank slate, with only a single cursor blinking at the user as if welcoming them to write and get creative. the minimalilstic looks of the editor makes it distraction free, allowing the user to concentrate on writing and writing only, which is precisely what a text editor is designed for. this is not to say that typora is lacking in functionality: it comes with full support for and snippet support, with real time rendering of course, my favorite part. i know that there are other popular editors and note taking applications out there, such as bear and notion, which i might try out in the future. however, not all such applications are free . also, many of them come with a wealth of additional features that i will perhaps never use. to me, the simplicity and powerfulness of typora seems to strike just the right balance. for now, my writing scheme will be splilt between jupyter notebooks for posts involving code execution, and typora for casual and math exclusive articles. i might also consider learning the syntax of r markdown, in which case r studio might become a third option, but that's only a possibility at this point, and at any rate a story for a later time. thanks for reading. see you in the next post!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
2019-12-12-gaussian-distribution,"if there is one thing that the field of statistics wouldn’t be complete without, it’s probably normal distributions, otherwise referred to as “the bell curve.” the normal distribution was discovered and studied extensively by carl friedrich gauss, which is why it is sometimes referred to as the gaussian distribution. we have seen gaussian distributions before in this blog, specifically on this post on likelihood and probability. however, normal distribution was introduced merely as an example back then. today, we will put the gaussian distribution on stage under undivided spotlight. of course, it is impossible to cover everything about this topic, but it is my goal to use the mathematics we know to derive and understand this distribution in greater detail. also, it’s just helpful to brush up on some multivariable calculus in a while. let’s start with the simplest case, the univariate gaussian distribution. the “univariate” part is just a fancier way of saying that we will dealing be dealing with one dimensional random variables, i.e. the distribution is going to be plotted on a two dimensional plane. we make this seemingly trivial distinction to distinguish it from the multivariate gaussian, which can be plotted on three dimensional space or beyond. we’ll take a look at the multivariate normal distribution in a later section. for now, let’s derive the univariate case. one of the defining properties of data that are said to be normally distributed when the rate at which the frequencies decrement is proportional to its distance from the mean and the frequencies themselves. concretely, this statement might be translated as we can separate the variables to achieve the following expression: integrating both sides yields let’s get rid of the logarithm by exponentiating both sides. that’s an ugly exponent. but we can make things look better by observing that the constant term can be brought down as a coefficient, since where we make the substitution . now, the task is to figure out what the constants and are. there is one constraint equation that we have not used yet: the integral of a probability distribution function must converge to 1. in other words, now we run into a problem. obviously we cannot calculate this integral as it is. instead, we need to make a clever substitution. here’s a suggestion: how about we get rid of the complicated exponential through the substitution then, it follows that therefore, the integral in now collapses into now that looks marginally better. but we have a very dirty constant coefficient at the front. our natural instinct when we see such a square root expression is to square it. what’s nice about squaring in this case is that the value of the expression is going to stay unchanged at 1. because the two integrals are independent, i.e. calculating one does not impact the other, we can use two different variables for each integral. for notational convenience, let’s use and . we can combine the two integrals to form an iterated integral of the following form: the term rings a bell, and that bell sounds like circles and therefore polar coordinates. let’s implement a quick change of variables to move to polar coordinates. now we have something that we can finally integrate. using the chain rule in reverse, we get we can consider there to be 1 in the integrand and continue our calculation. the result: from , we can express in terms of : after applying the substitution, now our probability density function looks as follows: to figure out what is, let’s try to find the variance of , since we already know that the variance should be equal to . in other words, from the definition of variance, we know that using , we get we can use integration by parts to evaluate this integral. this integral seems complicated, but if we take a closer look, we can see that there is a lot of room for simplification. first, because the rate of decay of an exponential function is faster than the rate of increase of a first order polynomial, the first term converges to zero. therefore, we have but since therefore, great! now we know what the constant is: plugging this expression back into , we finally have the equation for the probability distribution function of the univariate gaussian. and now we’re done! let’s perform a quick sanity check on by identifying its critical points. based on prior knowledge, we would expect to find the local maximum at , as this is where the bell curve peaks. if we were to dig a bit deeper into prior knowledge, we would expect the point of inflection to be one standard deviations away from the mean, left and right. let’s verify if these are actually true. from good old calculus, we know that we can obtain the local extrema by setting the first derivative to zero. we can ignore the constants as they are non zero. then, we end up with because the exponent is always positive, the only way for the expression to evaluate to zero is if this tells us that the local maximum of the univariate gaussian occurs at the mean of the distribution, as we expect. the inflection point can be obtained by setting the second order derivative of the probability distribution function equal to zero. luckily, we’re already halfway done with calculating the second order derivative since we’ve already computed the first order derivative above. as we have done above, let’s ignore the constants since they don’t affect the calculation. because the first exponential term cannot equal zero, we can simplify the equation to therefore, from this, we can see that the inflection point of the univariate gaussian is exactly one standard deviation away from the mean. this is one of the many interesting properties of the normal distribution that we can see from the formula for the probability distribution. so far, we’ve looked at the univariate gaussian, which involved only one random variable . however, what if the random variable in question is a vector that contains multiple random variables? it is not difficult to see that answering this question requires us to think in terms of matrices, which is the go to method of packaging multiple numbers into neat boxes, known as matrices. instead of deriving the probability distribution for the multivariate gaussian from scratch as we did for the univariate case, we’ll build on top of the equation for the univariate gaussian to provide an intuitive explanation for the multivariate case. in a previous post on linear regression, we took a look at matrix calculus to cover basic concepts such as the gradient. we established some rough intuition by associating various matrix calculus operations and their single variable calculus analogues. let’s try to use this intuition as a pivot point to extend the univariate gaussian model to the multivariate gaussian. for readability sake, here is the univariate model we have derived earlier. examining , the first observation we might make is that is no longer a coherent expression in the multivariable context. the fix to this is extremely simple: recall that in vector world. therefore, we can reexpress as this is the result of simply changing the squared term. continuing, the next subject of our interest would be , as the variance is only strictly defined for one variable, as expressed by its definition below: where is a random variable, which takes a scalar value. this necessarily begs the question: what is the multivariable equivalent of variance? to answer this question, we need to understand covariance and the covariance matrix. to jump right into the answer, the multivariable analogue of variance is covariance, which is defined as notice that equals variance, which is why we stated earlier that covariance is the multivariate equivalent of variance for univariate quantities. the intuition we can develop from looking at the equation is that covariance measures how far our random variables are from the mean in the and directions. more concretely, covariance is expresses the degree of association between two variables. simply put, if there is a positive relationship between two variables, i.e. an increase in one variable results in a corresponding increase in the other, the variance will be positive; conversely, if an increase in one variable results in a decrease in the other, covariance will be negative. a covariance of zero signifies that there is no linear relationship between the two variables. at a glance, the concept of covariance bears strong resemblance to the notion of correlation, which also explains the relationship between two variables. indeed, covariance and correlation are related: in fact, correlation is a function of covariance. the biggest difference between correlation and covariance is that correlation is bounded between 1 and 1, whereas covariance is unbounded. the bottom line is that both correlation and covariance measure the strength of linearity between two variables, with correlation being a normalized version of covariance. at this point in time, one might point out that covariance is not really a multivariate concept since it is defined for only two variables, not three or more. indeed, the expression is mathematically incoherent. however, covariance can be a multivariate metric since we can express the covariance of any pairs of random variables by constructing what is called the covariance matrix. simply put, the covariance matrix is a matrix whose elements are the pairwise covariance of two random variables in a random vector. before we get into the explanation, let's take a look at the equation for the covariance matrix: where and . this is the matrix analogue of the expression which is an alternate definition of variance. it is natural to wonder why we replaced the squared expression with instead of as we did earlier with the term in the exponent. the simplest answer that covariance is expressed as a matrix, not a scalar value. by dimensionality, produces a single scalar value, whereas creates a matrix of rank one. we can also see why is coherent by unpacking the expected values expression as shown below: using the linearity of expectation, we can rewrite the equation as therefore, we end up with which almost exactly parallels the definition of variance, which we might recall is where . the key takeaway is that the covariance matrix constructed from the random vector is the multivariable analogue of variance, which is a function of the random variable . to gain a better idea of what the covariance matrix actually looks like, however, it is necessary to review its structure element by element. here is the brief sketch of the by covariance matrix. this might seem complicated, but using the definition of covariance in , we can simplify the expression as: note that the covariance matrix is a symmetric matrix since . more specifically, the covariance matrix is a positive semi definite matrix. this flows from the definition of positive semi definiteness. let be some arbitrary non zero vector. then, you might be wondering how ends up as . although this relationship may not be immediately apparent, that the two expressions are identical can be seen by setting the random vector as and performing basic matrix vector multiplication operations. for the sake of brevity, this is left as an exercise for the reader. we now have all the pieces we need to complete the puzzle. recall that we were trying to derive the probability density function of the multivariate gaussian by building on top of the formula for the univariate gaussian distribution. we finished at then moved onto a discussion of variance and covariance. now that we understand that the covariance matrix is the analogue of variance, we can substitute with , the covariate matrix. instead of leaving at the denominator, let's use the fact that to rearrange the expression. this is another example of when the matrix scalar parallel intuition can come in handy: the scalar multiplicative identity is 1, whereas the equivalent in matrix world is the identity matrix . therefore, the reciprocal of a matrix can be interpreted as its inverse. from this observation, we can conclude that we are almost done, but not quite. recall the the constant coefficient of the probability distribution originates from the fact that we have to make some adjustments to the constant coefficient since, in the context of the multivariate gaussian, the integral translates into while it may not be apparent immediately, it is not hard to accept that the correcting coefficient in this case has to be as there are layers of iterated integrals to evaluate for each through . instead of the matrix , we use its determinant since we need the coefficient to be a constant, not a matrix term. we don't go into much detail about the derivation of the constant term; the bottom line is that we want the integral of the probability distribution function over the relevant domain to converge to 1. if we put the pieces of the puzzle back together, we finally have the probability distribution of the multivariate gaussian distribution: to develop a better intuition for the multivariate gaussian, let's take a look at a case of a simple 2 dimensional gaussian random vector with a diagonal covariance matrix. this example was borrowed from this source. using the formula for the multivariate gaussian we derived in , we can construct the probability distribution function given , , and . note that computing , the inverse of the covariance matrix, can be accomplished simply by taking the reciprocal of its diagonal entries since was assumed to be a diagonal matrix. continuing, in other words, the probability distribution of seeing a random vector given and is equal to the product of the two univariate gaussians. this result is what we would expect given that . for instance, if and are independent, i.e. observing a value of does not inform us of anything about and vice versa, it would make sense that the possibility of observing a random vector with entries and is merely the product of the independent probabilities of each observing and . this example illustrates the intuitive link between the multivariate and univariate gaussian distributions. in this post, we took a look at the normal distribution from the perspective of probability distributions. by working from the definition of what constitutes a normal data set, we were able to completely build the probability density function from scratch. the derivation of the multivariate gaussian was complicated by the fact that we were dealing with matrices and vectors instead of single scalar values, but the matrix scalar parallel intuition helped us a lot on the way. note that the derivation of the multivariate gaussian distribution introduced in this post is not a rigorous mathematical proof, but rather intended as a gentle introduction to the multivariate gaussian distribution. i hope you enjoyed reading this post on normal distributions. catch you up in the next one. this post: https://jaketae.github.io/study/likelihood/ previous post: https://jaketae.github.io/study/svd/ variance: https://en.wikipedia.org/wiki/variance covariance: https://en.wikipedia.org/wiki/covariance covariance matrix: https://en.wikipedia.org/wiki/covariance_matrix correlation: https://en.wikipedia.org/wiki/correlation_and_dependence positive semi definite matrix: https://en.wikipedia.org/wiki/definiteness_of_a_matrix this source: http://cs229.stanford.edu/section/gaussians.pdf",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
2020-06-09-sklearn-sprint,"last week, i had the pleasure of joining the first ever virtual scikit learn sprint, kindly organized by reshama shaikh of data umbrella and nyc pyladies. although it was just a four hour sprint held entirely online, it was a fantastic learning experience that helped me better understand collaborative workflow and the joy of contributing to open source. a salient feature of the scikit learn sprint is pair programming, where participants are grouped into teams consisting of two people. each pair works as a team to address open issues, report new issues, or submit pull requests. when i first learned about pair programming through the sprint, i thought it was a great idea since these pairs could be arranged in such a way that participants with relatively little experience could be paired up with more experienced developers to tackle complicated issues that they otherwise would not have been able to. on the day of the sprint, i almost didn't have a partner because they didn't show up. fortunately, however, thanks to the support from the organizing team ), i was quickly matched up with another participant. together, my newfound partner and i worked on a total of 1.5 issues for a duration of about two hours. i say 1.5 because we successfully pushed out a pull request and another draft pr in progress that was eventually closed due to the impossibility of retrieving word columns for the reuters news dataset. another minor pr i submitted a few days prior to the sprint also got merged. although the issues addressed were minor in significance, it was still a highly rewarding experience nonetheless. contributing to open source has an oddly addicting nature to it once a docs improvement pr is merged, you find yourself scrolling through github, trying to find issues relating to api improvements or feature requests that seem interesting, yet also doable by your standards. core developers offered prompt feedback on opened prs of course, not everything in the sprint panned out nicely for our team. for one, the sprint started at 1 in the morning in korean standard time, so i wasn't sure if i would be able to stay up until 5am. i ended up finishing a bit short a little past 4. my partner also decided to leave early on their end, so thankfully things worked out. besides the inevitable timezone issue, my partner and i also struggled to set up our workflow on github. my initial thought was that one of us would have to add the other as a collaborator on a forked repository. a few moments later, however, i was misguided by another idea, that perhaps we could fetch from a pr; for instance, my partner could open a pr, and i would fetch from that pr, make modification on my local, then push it up back to remote. it turned out that this was the wrong approach; pushing from my local wouldn't update the pr since tracking doesn't quite work that way. in the end, we resorted back to our initial plan of collaborating on a branch in a forked repository. hopefully, things worked out well from then on. despite it being entirely virtual, the scikit learn sprint was organized extremely well. discord was the main channel of communication, as well as zoom for an introductory orientation prior to the actual sprint. beginner friendly videos on how to contribute to scikit learn were posted on the data umbrella website to help newcomers get started. on top of all that, organizers and core developers also provided continuous assistance to participants with various aspects of the sprint our team, in particular, received helpful guidance with git as we were setting up our workflow as described earlier. i also saw others receive help with creating local builds or setting up virtual environments. they also provided prompt feedback on opened prs, kindly pointing out which test had failed for what reason, and how one might navigate the problem. although i had contributed to open source before, this was my first time working together with a partner. in retrospect, this collaborative nature is what made the sprint all the more enjoyable and productive. the numbers speak for themselves: in total, sprint participants submitted more than 50 prs, half of which have already been merged, and the rest still in progress: the sprint also introduced me to other helpful coding practices, such as unit testing and linting i had been using black for auto formatting, but exposure to flake8 was certainly helpful. i'll also shamelessly stick in here the fact that it was simply great interacting with scikit learn's core developers like andreas mueller, whom i had only seen on video lectures and ml related articles. and of course, do i even have to mention the fact that scikit learn is objectively the coolest ml package there is in python? thanks again to data umbrella and nyc pyladies for organizing this sprint!",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
2019-12-03-monte-carlo,"i have been putting off with blog postsings lately, largely because i was preoccupied with learning new languages i decided to pick up out of whim. although i'm still learning the basics of these languages, namely html, css, and javascript, i'm enjoying the process. i still have no idea where this spontaneous journey will take me, but hopefully i can make use of it in one way or another. the topic for today's post is monte carlo methods, something that i have been very interested in for a long time, admittedly because of its eye catching name. contrary to my original expectation, monte carlo is not named after an eponymous mathematician, but a gambling hot spot in monaco. the logic behind this nomenclature is that the simulation of random outcomes, such as in the context of an unpredictable gambling game, is what monte carlo methods are best suited for. to present a more formal definition, monte carlo methods refer to a broad category of algorithms that use repeated random sampling to make estimations of unknown parameters. basically, mc methods work by cleverly sampling from a distribution to estimate a variable of interest. this versatility is why mc method is such a powerful tool in the statistician's arsenal. in today's post, we will attempt to solve various bite sized tasks using mc methods. these tasks will be of varying difficulty, but taken together, they will collectively demonstrate the useful applications of mc methods. let's get started with the first up on the list: estimating . we all know from basic geometry that the value of approximates to . there are obviously various ways to derive this value. archimedes famously used hexagons to estimate that the value of lies between . with later advances in math, mathematicians began to approach this problem from the angle of infinite series or products, the result of which were the leibniz formula, wallis product, euler product, and the likes. and of course, modern computing now allows us to borrow the prowess of machinery to calculate this quantity with extreme accuracy. while the maths behind these derivations are fascinating, our approach will not take these routes; instead, we will use a crude monte carlo method. first, we draw a two by two square, inside of which we inscribe a circle of radius 1. for convenience purposes, let's center this circle and square both at the origin. next, we generate a series of random coordinates within the region of the square. then, we count the percentage of dots that fall within the area of the cricle. using a simple formula of proportions, we can calculate the area of the circle, through which we can then estimate the value of . before we get into the specifics of this algorithm, let's see hwo this plays out in code. now that we have the function ready, let's try calling it with some input parameters. below, we perform our little crude monte carlo simulation with a hundred randomly generated data points. invoking the function returns the value of the estimation. 2.8 the returned result is not abysmal, but we clearly can do a lot better. the reason behind this spotty estimation can be checked by drawing the plot of the monte carlo simulation, as shown below. as we can see, ten samples simply aren't enough to really cover the entire area of the plane or the circle. although we do get some points in the circle, it's really hard to tell if the proportion of points in and out of the circle is going to be representative of the actual proportion of area between the circle and the square. let's push our mc algorithm to do a bit more by sampling more data points, this time with 100000 randomly generated points. 3.13516 as expected, with more data points, we get a better estimation of pi. although the randomness created by the call in our function means that this estimation will fluctuate with each execution, the value is reliably close to the actual value of , differing only by about 0.01 or less. if we draw the plot for our experiment, it is clear that our data points accurately capture the proportionality between the area of the square and the circle. we can systematically verify that more samples tend to yield better results by graphing the magnitude of the error of our estimation plotted against the number of random samples generated. the plot shows that, with larger sample sizes, the error quickly converges to around 0. although the rate of convergence dramatically decreases after the first few iterations, the pattern of convergence is apparrent. so how does this work? the mechanism is extremely simple: if we were to randomly generate an infinite number of dots, the proportion of the number of dots that fall within the circle versus those that do not fall within it would converge to some constant, i.e. . why is this the case? intuitively, the larger the area, the larger the number of points that fall into that area. given this proportional relationship, the number of randomly generated points in an area after a simulation would mirror the actual area of the circle and the rectangle, hence the proportional expression above. by following this line of reasoning, we can then resort to monte carlo to generate these random points, after which we can make a reasonable estimation of . but approximation is not the only domain in which monte carlo methods become useful they can also be used to calculate complicated integrals. we all know from calculus class that integration can be difficult. everyone has encountered integrals of varying monstrosity at one point in their lives, scrambling to solve it with integration by parts or some obscure, creative substitution, only to realize that everything wounds up in the middle of nowhere. well, good news for all of us monte carlo methods can be used to estimate the value of mind pulverizing, complicated definite integrals. let's say we want to estimate the value of an integral of a function over some domain . now assume that there is some probability density function defined over . then, we can alter this integral as shown below. notice that this integral can now be understood as an expected value for some continuous random variable. in other words, collapses into the following expression. what does this tell us? this means that we can simply calculate an integral by randomly sampling the values of such that follows some probability distribution . the probability distribution part simply ensures that values of that are more probable are sampled more often than others. intuitively, we are effectively taking a weighted mean of the values of , which is the loose definition of expected values. now, to simplify things a bit, we are going to take a look at an example that does not involve much probability distributions. conside the following integal of sine, a classic in calculus 101: the reason why we chose this integral is that we know how to calculate it by hand. therefore, we can match the accuracy of our crude monte carlo against an actual, known value. let's fist quickly compute this integral. now time for monte carlo. notice that there is no probability distribution explicitly defined over the domain of integration in our example. in other words, simply follows a continuous uniform distribution, meaning that all values of within are equally likely. all we have to do, therefore, is to compute the expected value of the integrand by randomly generating a series of numbers within the specified domain, plug those values into the function , and take their average. this is a very elementary function that simply generates a specified number of samples given within the domain . these numbers are then plugged into the function , after which an unweighted mean of these values are computed to approximate an integral. let's test the accuracy of this crude monte carlo method by using our example of computed earlier. 0.3801234154353964 the result is a very poor approximation that is way off, most likely because we only used ten randomly generated numbers. much like earlier, however, we would expect monte carlo to perform better with larger samples. 0.4564203853429202 the example we have analyzed so far was a very simple one, so simple that we would probably have been better off calculating the integral by hand than writing code. that's why it's time to put our crude monte carlo to the test with integrals more difficult to compute. consider the following expression: one might try calculating this integral through substitution or integration by parts, but let's choose not to for the sake of our mental health. instead, we can model the integrand in python and ask monte carlo to do the work for us. concretely, this process might look as follows. we can now plug this function into our crude monte carlo and hope that the algorithm will provide us with an accurate estimation of this expression. 0.4676957788776292 so we have a number! but how do we know if this is an accurate estimation? unlike in previous problems, where we already knew the true value of our estimate and measured the error of our simulation by comparing it with the known value, the true value of the integral expression is unknown in this problem because we have not evaluated the integral by hand. one way we can go about this dilemma is to calculate variance. intuitively, if our estimate is indeed accurate, running the same monte carlo simulation would yield a value very similar to that of the previous. conversely, if our estimate is inaccurate, the variance would be large, suggesting that our estimate has not converged to a value yet. indeed, this is exactly what we attempted to visualize with the error plot above in our estimation example. however, in most cases where monte carlo methods are used, we have no idea about the true value of the quantity we wish to estimate, like the complicated integral problem in this case, which is why we cannot simply calculate error by substracting our estimate from the true value. as we might recall, variance measures, quite simply, the degree of variability in our data. the well known formula for variation goes as follows. using this formula, let's plot variance against the number of samples to see what effect increasing the sample size has on variance. the function accepts a list as an argument and returns the variance seen in the given data set. now that we have this function ready, let's use it to plot variance against the number of samples used in crude monte carlo integration notice that variance quickly converges to near zero as the number of samples gets larger! this means that, even if we do not know the true value of the integral expression, we can now be confident that the output of the crude monte carlo will have converged to an approximation of the true value with sampling size as big as 1000, or even something like 400. this gives us more confidence in saying that the integral expression in is approximates to 0.247. the crude monte carlo algorithm we employed here used simple random sampling to generate a series of random numbers to be used for our estimation. crude monte carlo is powerful, but in a way it is inefficient because we have to sample large amounts to ensure that the resulting sample is representative, which is a condition that must be satisfied to produce a reliable estimate. there are a plethora of mathematical techniques that build on top of crude monte carlo to ensure that sampling is done correctly and more efficiently, such as importance sampling, but for the purposes of this post, we will stop here and move onto the last task: simulating random walk. the last task we will deal with in this post is simulating what is known as the drunkard's walk, a version of which is introduced here. the drunkard's walk is a type of random walk with a specified termination condition. as the name suggests, the drunkard's walk involves a little story of an intoxicated man trying to reach some destination, whether that be a cliff or, in our case, a restroom. because he is drunk, he cannot walk to the restroom in a straight path as a normal person would do; instead, he stumbles this way and that, therefore producing a random walk. our goal here is to simulate this motion many times to estimate the probability that the man would successfully end up in the restroom to go about his business. this example was borrowed from this post by zacharia miller. before we start typing up some code, let's first lay down the ground rules of this simulation. first, we assume that the pub is modeled as a ten by ten grid, the bottom left point defined as and the top right . the drunkard will start his walk at his table, represented by the coordinate . for each walk, function will generate a random number to determine the direrction of his movement. the magnitude of each walk is 1 by default. beforer a walk is performed, we will invoke another function to check if his movements are legal, i.e. whether he stepped out of the boundary of the pub. if his moves are legal, we continue with the movement; if not, we stop and assume that the trial has yielded a failure. the goal of this random walk is to end up in the top right portion of the pub, a square defined by coordinates , and . now that we have established the basics of this game, let's start coding away. our little random walk simulator is now ready to go! let's perform a quick sanity check to see if the code works as expected. we see that the returned tuple contains the flag boolean value as well as a list containing the coordinates of the first ten steps the drunkard took in this experiment, which is exactly what we expected. now it is time to put this monte carlo application to the test by simulating the walk many times and counting the instances of successes verses failures. the function calls the function as many times specified by . the function compiles all the results to return two pieces of information: the percentage of success, represented in decimals, and the average number of steps it took for the drunkard to reach the restroom. notice that there is a 100 step cap, meaning if the drunkard was not able to find the restroom after a hundred steps, the trial was assumed a failure. we can verify the functionality of our design by calling the function. cool! the monte carlo algorithm thus tells us that the probability of success is about 10 percent, which is a lot smaller thant i had personally anticipated. think about how complicated it would have been to calculate this probability by hand. by simulating this game multiple times and counting the instances of successes, we can derive an estimation of the success rate of our particular random walk model. let's see what happens when we simulate the drunkard's walk thirty times. in the particular instance that i have below, we see that the drunkard successfully reached the rest room four out of thirty attempts, which roughly equals the success probability of ten percent we saw earlier. by now, hopefully you have been convinced that monte carlo is a wonderful method of solving problems. although the examples we looked at were mostly simple, these algorithms can easily be applied to solve much harder ones. simply put, monte carlo uses a brute force approach to simulate a particular instance of a model multiple times. through such repeated sampling, we are able to gain a better understanding of the parameters underlying the issue at hand, no matter how complex. this is a pattern that we saw with all three tasks we dealt with in today's post. this sums up our post on monte carlo methods. in a future post, we will take a look at markov chain monte carlo, particularly metropolis hastings, which uses the best of both worlds to analyze complicated probability distributions. i'm already excited for that post, because mcmc methods will bring together so many concepts that we have dealt with on this blog so far ranging from bayesian inference, probability distributions, markov chains, and so many more. catch you up on the next one! leibniz formula: https://en.wikipedia.org/wiki/leibniz_formula_for_π wallis product: https://jaketae.github.io/study/basel zeta/ euler product: https://en.wikipedia.org/wiki/euler_product importance sampling: https://en.wikipedia.org/wiki/importance_sampling here: https://medium.com/i math/the drunkards walk explained 48a0205d304 this post: http://zwmiller.com/projects/monte_carlo_part1.html markov chain monte carlo: https://en.wikipedia.org/wiki/markov_chain_monte_carlo metropolis hastings: https://en.wikipedia.org/wiki/metropolis–hastings_algorithm bayesian inference: https://jaketae.github.io/study/bayes/ markov chains: https://jaketae.github.io/study/pagerank and markov/",0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2020-01-17-naive-bayes,"welcome to part three of the ""from scratch"" series where we implement machine learning models from the ground up. the model we will implement today, called the naive bayes classifier, is an interesting model that nicely builds on top of the bayesian mindset we developed in the previous post on markov chain monte carlo. much like the logistic regression model, naive bayes can be used to solve classification tasks, as opposed to regression in which case the goal is to predict a continuous variable. the main difference between logistic regression and naive bayes is that naive bayes is built on a probabilistic model instead of an optimization model such as graident descent. hence, implementing naive bayes is somewhat easier from a programming point of view. enough of the prologue, let's cut to the chase. to understand naive bayes, we need not look further than bayes's theorem, which is probably the single most referenced theorem on this blog so far. i won't explain this much since we have already seen it so many times, but presented below is the familar formula for reference and readability's sake. very standard, perhaps with the exception of some minor notation. here, refers to a single instance, represented as a vector with entries; , the corresponding label or class for that instance. note that is not a feature matrix, but a single instance. more concretely, of course, is just a scalar value. this characterization is very apt in the context of machine learning. the underlying idea is that, given some data with feature columns, we can derive a probability distribution for the label for that intance. the naive assumption that the naive bayes classifier makes now you can guess where that name comes from is that each of the variables in the instance vector are independent of one another. in other words, knowing a value for one of the features does not provide us with any information about the values for the other feature columns. combining this assumption of independence with bayes' theorem, we can now restate as follows: pretty straight forward. we know that the demominator, which often goes by the name ""evidence"" in bayesian inference, is merely a normalizing factor to ensure that the posterior distribution integrates to 1. so we can discard this piece of information and distill down even farther: equation tells us that it is possible to calculate the probability of instance belonging to class systematically. why is this important? the simple answer is that we can use to train the naive bayes classifier. say we know that for a particular instance , the label is . then, we have to find the distribution for each feature such that we can maximize . does this ring any bells? yes it is maximum a posteriori estimation! in other words, our goal would be to maximize the posterior distribution for each training instance so that we can eventually build a model that would output the most likely label that the testing instance belongs to. in other words, our training scheme can be summarized as: but this is all to abstract. let's get into the details by implementing the naive bayes classifer from scratch. before we proceed, however, i must tell you that there are many variations of the naive bayes classifer. the variant that we will implement today is called the gaussian naive bayes classifer, because we assume that the distribution of the feature variables, denoted as , is normal. for a corresponding explanation of this model on , refer to this documentation. let's jump right into it. as per convention, we start by importing necessary modules for this tutorial. for reproducability, we specify a . the magic commands are for the configuration of this jupyter notebook. let's begin by building some toy data. to make things simple, we will recycle the toy data set we used in the previous post on logistic regression and k nearest neighbors. the advantage of using this data set is that we can easily visualize our data since all instances live in . in other words, we only have two axes: and . for convenience, we preprocess our toy data set and labels into arrays. the first step is to separate the data set by class values, since our goal is to find the distributions for each class that best describe the given data through map. to achieve this objective, we can create a function that returns a dictionary, where the key represents the class and the values contain the entries of the data set. one way to implement this process is represented below in the function. let's see if the function works properly by passing as into its argument. great! as expected, is a dictionary whose keys represent the class and values contain entries corresponding to that class. now that we have successfully separated out the data by class, its' time to write a function that will find the mean and standard deviation of each class data. this process is legitimate only because we assumed the data to be normally distributed hence the name ""gaussian naive bayes."" let's quickly see this in code. the function receives a data set as input and returns a nested list that contains the mean and standard deviation of each column of the data set. for example, if we pass the toy data set into , the returned list will contain two lists: the first list element corresponding to the mean and standard deviation of , and the second list element, . 4.8327021465, 2.6051838419314794, 2.4053928934, 1.1629554833322375 we can combine both and functions to create a new wrapper function that returns the mean and standard deviation of each column for each class. this is a crucial step that will allow us to perform a map approximation for the distribution of variables for each class. testing out the function on created earlier yields the desired result. notice that the returned dictionary contains information for each class, where the key corresponds to the label and the value contains the parameters calculated from . a good way to understand this data is through visualization. let's try to visualize what the distribution of and looks like for data labeled class . we can use the library to create a joint plot of the two random variables. we see that both variables are normally distributed. therefore, we can imagine data points for class to be distributed across a three dimensional gaussian distribution whose center lies at the point where the the plot has the darkest color, i.e. . i find this way of understanding data to be highly intuitive in this context. now, it's time to bake bayesian philosophy into code. recall that to perform bayesian analysis, we first need to specify a prior. although we could use an uninformed prior, in this case, we have data to work with. the way that makes the most sense would be to count the number of data points corresponding to each class to create a categorical distribution and use that as our prior, as shown below. if we use the created from , we should get a very simple prior whereby since there is an equal number of data points belonging to the two classes in the toy data set. indeed, this seems to be true. next, it's time to model the likelihood function. i won't get into the specifics of this function, but all it does is that it calculates the likelihood by using the parameters returned by the function to indicate the likelihood that a particular belongs to a certain class. as per convention of this tutorial, the returned dictionary has keys corresponding to each class and values indicating the likelihood that the belongs to that class. we can see the function in action by passing a dummy test instance. we are almost done! all that we have to do is to create a funcition that returns the predicted label of a testing instance given some labeled training data. implemenitng this process is straightforward since we have all the bayesian ingredients we need, namely the prior and the likelihood. the last step is to connect the dots with bayes' theorem by calculating the product of the prior and likelihood for each class, then return the class label with the largest posterior, as illustrated below. let's see if the works as expected by seeing if passing as argument , for which we know that its label is 1, actually returns 1. 1 the function is only able to process a single testing instance. let's complete our model construction by writing the function that takes labeled data and a testing set as its argument to return a array containing the predicted class labels for each instance in the testing set. done! let's import some data from the library. the wine set data is a classic multi class classfication data set. the data set contains three target classes, labeled as integers from 0 to 2, and thirteen feature columns, listed below: alcohol malic acid ash alcalinity of ash magnesium total phenols flavanoids nonflavanoid phenols proanthocyanins color intensity hue od280/od315 of diluted wines proline as i am not a wine afficionado, i have no idea what some of these columns represent, but that is irrelevant to the purpose of this tutorial. let's jump right in by loading the data. it always a good idea to get a sense of what the data looks like by verifying its dimension. note that we used the function we wrote in previous posts to shuffle and slice the data into training and validation sets. for convenience, the code for this function is presented below. now it's finally time to check our model by making predictions. this can simply be done by passing the training and testing data set into the function that represented our gaussian naive bayes model. array let's check if the predicted class labels match the answer key, i.e. the array. array eyeballing the results, it seems like we did reasonably well! in fact, the line below tells us that our model mislabeled only one test instance! array we can quantify the performance our model through the metric of accuracy. the function does this for us. note that instead of using the for loop approach used in previous posts, this function is more vectorized, making computation less expensive. the shorter code is also an added benefit. 0.9714285714285714 the accuracy of our from scratch model is 97 percent, which is not bad for a start. let's see if the model in outperforms our hand coded model. 0.9714285714285714 the accuray score yielded by is exactly identical to that achieved by our model! looks like the model in scikit learn does exactly what our model does, at least juding from the metric of accuracy. this is surprising, but since we basically followed the bayesian line of reasoning to buid our model, which is what naive bayes really is all about, perhaps this is not as astonishing as it seems. in this post, we built the gaussian naive bayes model from scratch. in the process, we reviewed key concepts such as bayesian inference and maximum a posteriori estimation, both of which are key statistical concepts used in many subdomains of machine learning. hopefully through this tutorial, you gained a better understanding of how gaussian mathematics and bayesian thinking can be used in the context of classification. the true power of naive bayes is not limited to the task of classificaiton, however. in fact, it is used in many fields, most notably natural language processing. perhaps we might look into the possible applications of naive bayes in the context of nlp in a future post. but for now, this level of modeling will do. thanks for reading. see you in the next post! post: https://jaketae.github.io/study/map mle/ logistic regression: https://jaketae.github.io/study/logistic regression/ k nearest neighbors: https://jaketae.github.io/study/knn/ previous post: https://jaketae.github.io/study/mcmc/",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0
2019-12-25-KNN,"these days, machine learning and deep neural networks are exploding in importance. these fields are so popular that, unless you're a cave man, you have probably heard it at least once. the exaggeration not withstanding, there is perhaps no necessity to justify the topic for today's blog post: exploring a machine learning algorithm by building it from scratch. apparently, ""from scratch"" is now a trendy pedagogical methodology employed in many websites and resources that claim to educate their readers about machine learning. to this, i agree: by constructing algorithms from the ground up, one can glean meaningful insights on how machine learning actually works, as opposed to viewing ml as some dark magic that suddenly makes computers intelligible beings. enough of the prologue, let's jump right in. as the name implies, the k nearest neighbors algorithm works by findinng the nearest neighbors of some give data. then, it looks at the labels of neighboring points to produce a classification prediction. here, is a parameter that we can tweak to build the knn model. for instance, let's say we have a binary classification problem. if we set to 10, the knn modell will look for 10 nearest points to the data presented. if among the 10 neighbors observed, 8 of them have the label 0 and 2 of them are labeled 1, the knn algorithm will conclude that the label of the provided data is most likely also going to be 0. as we can see, the knn algorithm is extremely simple, but if we have enough data to feed it, it can produce some highly accurate predictions. there are still missing pieces to this puzzle, such as how to find the nearest neighbors, but we will explore the specifics of the algorithm on the go as we build the model from scratch. for now, just remember the big picture. let's get into the nuts and bolts of the knn model. below are the dependencies we will need for this demonstration. one problem we need to start thinking about is how to measure distance between two data points. after all, the implementation of knn requires that we define some metric to measure the proximity between different points, rank them in order, and sort the list to find nearest neighbors. one way to go about this is to use euclidean distance, which is defined as follows: it is not difficult to build an implementation of in python. we can easily achieve this using . let's test the functionality of the function using some dummy dataset. this data set was borrowed from jason brownlee. 0.0 1.3290173915275787 1.9494646655653247 1.5591439385540549 0.5356280721938492 4.850940186986411 2.592833759950511 4.214227042632867 6.522409988228337 4.985585382449795 great! as expected, the distance between a point and itself is 0, and other calculated distances also seem reasonable. the next step is to write a function that returns the nearest neighbors of a point given a data set and parameter . there are many ways to implement this, but an example is shown below. first, we enumerate thorugh the data set to calculate all the distances between the given test instance and the data points in the data set. next, we sort the list. the returned result is a list that contains the indices of the nearest neighbors the algorithm found in the data set. note that we use the function we wrote above. let's see if the code works by testing it on our toy data. the task is to find 5 data points that are closest to the first row of . 0, 4, 1, 3, 2, 6, 7 as expected, the returned list contains the indices of the data points in that are closest to . we can confirm this by looking at the results of the distance calculation we obtained when testing the function. note that the indices are in order; that is, indice 0 corresponds to the closet neighbor it is in fact that data point itself and index 2 refers to the farthest neighbor among the selections. say we have successfully obtained the list of nearest neighbors. now what? well, it's time to look up the labels of these neighboring data points to see which class is the most prevalent. the knn model will then conclude that the most prevalent class label is the one that which the data point belongs to. because this function has to perform more tasks than the functions we wrote earlier, the example code is slightly longer, but here it goes: basically, the function counts the number of labels of each class and stores the results in a dictionary. then, it normalizes the values of the dictionary by dividing its values by the total number of data points seen. although this process is not necessary, it helps us interpret the results in terms of percentages. for example, the example below tells us that approximately 71 percent of the neighbors of are labeled 0; 28 percent are labeled 1. now we have all the building blocks we need. we can stop here, but let's nicely wrap all the functions we have build into a single function that we can use to train and test data. in retrospect, we could have built a class instead, but this implementation also works fine, so let's stick to it for now. let's see what the tells us about . here, we pass on onto the argument because we want to prevent the algorithm from making a prediction based on a data set that contains the data itself; that would defeat the purpose of making a prediction. let's see how the model performs. array the knn model rightly predicts that is labeled 0. great! but we have only been testing our model on a rather dumb data set. let's see whether the model works with larger, closer to real life data sets. the iris data set from the uci machine learning repository is perhaps one of the best known data sets in the field of machine learning. created by r. a. fisher, the data set contains 3 classes of 50 instances each, totaling to 150 independent observations of iris plants, specifically iris setosa, iris versicolour, and iris virginica. the feature columns include sepal length, sepal width, petal length, and petal width. let's begin by first loading the data set from the library. one preliminary step we might want to take is shuffling the data set and dividing it into a training set and a testing set. as the name implies, a testing set is a set of data we use to test the accuracy of our classification algorithm. a training set, on the other hand, is a data set the knn model is going to use to make predictions, i.e. it is the data set from which the algorithm will try to find close neighbors. there is a special function already in the library that does all the shuffling and the splitting for us, but in light of the ""from scratch"" spirit of this post, let's try to write up the function ourselves. great! we can now use this function to split the iris data set we have imported by using the following command. let's verify that the splitting has successfully been performed by checking the dimensions of the testing set. as we expect, the testing set is a 30 by 4 matrix. in other words, it contains 4 feature columns the width and length of sepals and petals, as mentioned earlier and 30 observation of iris plants. we can now use the knn model we have built to make predictions about these thirty samples. the choice of parameter as 10 was arbitrary. array that was very simple. the returned numpy array contains the class labels for each of the thirty observations in the matrix. in other words, the first test data was predicted to belong to class 1; second data, class 0, third data, class 1, and so on. let's compare this predicted result with the actual labels. array for the most part, it seems like our predicted result is quite similar to the actual labels. but there are some samples that our knn algorithm missed, such as the 27th data point: although its actual label is 2, our model predicted it to be 1. we can mathematically calculate the accuracy of our model by using the following function. the job of this function is quite simple: it goes through the two lists, element by element, and checks if the two values are identical. see the implementation below. the accuracy of our prediction turns out to be about 97 percent. 0.9666666666666667 but can we do better? recall that we arbitrarily chose to be 10 when we initialized the knn algorithm. would accuracy increase if we set the parameter to another number? let's try to answer this question by generating a list of accuracy scores for each value of ranging from 1 to 100. we can achieve this by building a function as shown below. passing 100 to the argument results in a list of accuracy scores. array we can go through this list and try to see for which value of accuracy is maximized. but this is a rather tedious job, and things would get quickly out of control if we were to deal with much larger data sets where the value of can be set to much larger numbers. instead, let's create a visualization to see how accuracy changes with respect to . the plot shows that accuracy is maximized for many values of , not just 1. also, we can learn that accuracy does not go beyond the 97 percent we saw earlier, which is a bit of sad news. an interesting insight we can glean, however, is that accuracy seems to drop past some certain thresholds, most notably around 80. one reasonable explanation might be that the model is looking at too many neighbors that it cannot produce a reliable estimate. at any rate, this visualization shows that hyperparameter tuning is an important job of a machine learning engineer even if the model is great, if the wrong value is used, the model will only demonstrate lackluster performance. this was perhaps the first post where we dealt with a machine learning algorithm. ml is sometimes treated as a black box, where some magic beneath the hood produces desirable results. however, i find exploring these mechanisms a lot more interesting than simply using pre existing modules and libraries, as important as they may be. hopefully, this post gave you some idea of how the knn model works. i plan to post more on machine learning algorithms in the future. however, at the same time, there will be other posts involving the use of popular preexisting libraries to help demonstrate how machine learning models are used in practice; after all, most practitioners don't build models themselves every time they embark on a project. the bottom line of this pslan is that we find a sweet spot between theory and practice, and eventually become versed at both. catch you up in the next one. happy new year! euclidean distance: https://en.wikipedia.org/wiki/euclidean_distance jason brownlee: https://machinelearningmastery.com/tutorial to implement k nearest neighbors in python from scratch/ iris data set: https://archive.ics.uci.edu/ml/datasets/iris k nearest neighbors algorithm: https://en.wikipedia.org/wiki/k nearest_neighbors_algorithm",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0
2020-09-28-matplotlib-seaborn-pandas,"this post is based on this article on medium, titled ""matplotlib + seaborn + pandas: an ideal amalgamation for statistical data visualization."" this article was recommended on a public facebook group that i happened to be a part of, and i'm glad i came across it. although i have used matplotlib, seaborn, and pandas before, my use case was very limited since i only used them in toy projects or tutorial blog posts. while this is a tutorial blog post of its own, today's focus is specifically going to be around learning how to use these libraries effectively and understanding some commonly overlooked details on those mo. specifically, the goals of this post can be boiled down as follows: matplotlib basics and terminology review integrating seaborn with pandas to create multidimensional plots with minimal code plot customization with seaborn and matplotlib and if you're crazy about details like i am and is wondering why i follow inconsistent capitalization with module names, it's because matplotlib capitalizes the name of the module on their website, whereas seaborn and pandas don't. with that detail out of the way, let's get started! matplotlib is one of the most widely used data visualization libraries in python. it is also the backend on which seaborn is built. in other words, seaborn uses matplotlib behind the scenes. hence, to understand data visualization with python, we must have some level of decent understanding of how matplotlib works. a typical matplotlib figure can be decomposed into the following segments. this image was taken from the medium article. previously, one point of confusion for me was the difference between figure, axes, and axis. let's get into more detail of what these are. figure: refers to the entire image that is created by matplotlib. a figure can, of course, contain multiple subplots within it. axes: nearly synonymous with subplots. a single figure can often contain multiple axes. axis: the actual axis of the graph, as in the and axis in a two dimensional plot. the distinction between figure and axes are important, as these concepts are also directly relevant with how seaborn functions can be called to create subplots in conjunction with matplotlib. for the sake of demonstration, let's import some modules and write up some code. normally, i like to configure matplotlib to have some style settings, such as , but we will pass on that for today as we will be using the seaborn library itself. this will help us easily differentiate between graphs that were created through matplotlib as seaborn. here is a very simple example of graph creation using . note that we can obtain both the figure and the axes by calling , with determining the number of subplots or axes that will be made available to us. in this case, we have one subplot in a figure. an interesting behavior of matplotlib worth noting with the function call is that the type of the object depends on the number of subplots to be created. below is the case in which there is only one subplot: and here is the case of many subplots: array as you can see, in the case of many subplots, the returned object is a numpy array whose elements are s. hence we can do the following: in this case, the obect is a numpy array whose shape is . therefore, we can access individual objects by two dimensional numpy indexing, i.e. . note that when , the shape will take the form of instead of . now that we have some idea of how to manipulate axes and subplots in matplotlib, let's get the fuller picture by creating graphs using matplotlib, seaborn, and pandas, as promised earlier. to do this, we will first need some dummy dataset. we could create them as we had done above, but there are limitations to this approach since ideally we would need pandas dataframes. instead, let's use seaborn to load some classic datasets. to create plots with matplotlib, we need and values. these values should be iterables, such as lists, tuples, or numpy arrays. one point to note is that each series in a pandas dataframe is also an iterable type. we can obtain the series by calling on a column, as shown below. an alternative method is to simply specify the the column names and then pass in the entire dataframe as a argument. the syntax for seaborn is not so much different. in fact, it is almost identical, with the call being the exception. seaborn applies some stylistic changes by default, which i personally prefer over matplotlib's defaults. you might be thinking that the choice between seaborn and matplotlib is a mutually exclusive one, i.e. we cannot use both libraries in one figure. well, turns out that this is not true, which is good news for us, since we can pick and choose which library we want for each subplot depending on the type of visualization we want to create. let's start with a vanilla example in which we use matplotlib to create a figure containing two subplots. note that we used call in order to transform the dataframe into a numpy array. while this works, i personally think it's better to use seaborn in this case. so let's leave the subplot on the left as is, and replace the boxplot with a seaborn boxplot. that looks a lot better. for one, the syntax is less convoluted in the sense that it does not require or any other type of conversion process; seaborn knows how to deal with pandas dataframes. also, because it can natively process dataframes, it was able to retrieve the column names and display them correctly below, even without any customizations applied. one interesting thing to note is that, we can also use the built in visualization functionality that pandas has in order to create the boxplot. the code for this example is shown below. i think it's pretty cool that we can use select and combine different libraries to create plots. this opens a lot of room for customization and visualization improvements, which are always valuable when trying to derive hidden insight. in this post, we took a look at the basics of data visualization with matplotlib and seaborn. although i call myself a pythonista, i've always felt that data visualization and basic data wrangling with libraries like matplotlib or pandas are my weak points. i hope to continue studying the basics to continuously improve in these domains. on a side note, you might be wondering at this point what the merits of using seaborn is, other than the fact that it produces prettier looking plots than matplotlib. one of the defining strengths of seaborn is figure level plots, which allows one to create complicated plots across many axes with just a few lines of code. such examples include , , and . in a future post, i hope to explore these plots in more detail. i hope you've enjoyed reading this post. stay tuned for more!",0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2019-11-26-basel-zeta,"the more i continue my journey down the rabbit hole of mathematics, the more often i stumble across one name: leonhard euler. nearly every concept that i learn, in one way or another, seems to be built on top of some strand of his work, not to mention the unending list of constants, formulas, and series that bears his name. it is simply mind blowing to imagine that a single person could be so creative, inventive, and productive to the extent that the field of mathematics would not be where it is today had it not been for his birth on april 15, 1707. why such intensive fanboying, you might ask. well, let’s remind ourselves of the fact that the interpolation of the factorial through the gamma function was spearheaded by euler himself. but this is just the start of the beginning. consider, for example, the basel problem, an infamous problem that mathematicians have been trying to solve for nearly a century with no avail, until the 28 year old euler came to the rescue. the basel problem can be stated as follows: at a glance, this seems like a fairly simple problem. indeed, we know that this series converges to a real value. we also know that integration would give us a rough approximation. however, how can evaluate this series with exactitude? euler’s solution, simple and elegant, demonstrates his genius and acute heuristics. euler begins his exposition by analyzing the taylor expansion of the sine function, which goes as follows: dividing both sides by , we obtain the following: now it’s time to follow euler’s amazing intuition. if we take a close look at the equation , we can convince ourselves that its solutions will adhere to the form , where is a non zero integer between . this is expected given the periodic behavior of the sine function and its intercepts with the axis. given these zeros, we can then reconstruct the original function as an infinite product using the fundamental theorem of algebra, or more specifically, weierstrass factorization. let’s try to factor out the coefficient of the term through some elementary induction. first, we observe that calculating the product of the first two terms produces the following expression: then, we can express the target coefficient, denoted by , as follows: where denotes the coefficient of obtained by expanding the rest of the terms following in the infinite product above. if we repeat this process once more, a clear pattern emerges: iterating over this process will eventually allow us to express our target coefficient as a sum of inverse squares multiplied by some constant, in this case : but then we already know the value of from the modification of the taylor polynomial for sine we saw earlier, which is ! therefore, the basel problem reduces to the following: therefore, and there we have the solution to the basel problem. in hindsight, solving the basel problem is not rocket science; it is a mere application of the taylor polynomial, coupled with some modifications and operations to mold the problem into a specific form. however, it takes extreme clairvoyance to see the link between the basel problem and the taylor polynomial of the sine function. at this point, it doesn’t even surprise us to know that euler applied this line of thinking to calculate the value of the sum of higher order inverses. an interesting corollary of euler’s solution to the basel problem is the wallis product, which is a representation of the quantity as an infinite product, as shown below: it seems mathematically unintuitive to say that an irrational number such as can be expressed as a product of fractions, which is a way of representing rational numbers. however, we can verify the soundness of the wallis product by substituting for in : taking the reciprocal of this entire expression reveals the wallis product. the basel problem is a specific case of the riemann zeta function, whose general form can be written as follows. a small digression: when , the zeta function converges to a value known as apéry’s constant, eponymously named after the french mathematician who proved its irrationality in the late 20th century. beyond the field of analytics and pure math, the zeta function is widely applied in fields such as physics and statistics. perhaps we will explore these topics in the future. so back to the zeta function. in the later segment of his life, euler found a way to express the zeta function as, you guessed it, an infinite product. this time, however, euler did not rely on taylor polynomials. instead, he employed a more general approach to the problem. it is here that we witness euler’s clever manipulation of equations again. we commence from the zeta function, whose terms are enumerated below. much like how we multiply the ratio to a geometric sequence to calculate its sum, we adopt a similar approach by multiplying the second term, to the entire expression. this operations yields by subtracting this modified zeta function from the original, we derive the following expression below. now, we only have what might be considered as the odd terms of the original zeta function. we then essentially repeat the operation we have performed so far, by multiplying the expression by and subtracting the result from the odd term zeta function. it is not difficult to see that iterating through this process will eventually yield euler’s product identity for the zeta function. the key to understanding this identity is that only prime numbers will appear as a component of the product identity. we can see this by reminding ourselves of the clockwork behind the sieve of eratosthenes, which is basically how the elimination and factorization works in the derivation of euler’s identity. taking this into account, we can deduce that euler’s identity will take the following form: this expression is euler’s infinite product representation of the zeta function. these days, i cannot help but fall in love with euler’s works. his proofs and discoveries are simple and elegant yet also fundamental and deeply profound, revealing hidden relationships between numbers and theories that were unthought of during his time. i tend to avoid questions like “who was the best in history” because they most often lead to unproductive discussions that obscure individual achievements amidst meaningless comparisons, but i dare profess here my belief that only a handful of mathematicians can rival euler in terms of his genius and prolific nature. that is enough euler for today. i’m pretty sure that this is not going to be the last post on euler given the sheer amount of work he produced during his lifetime. my exploration of the field of mathematics is somewhat like a random walk, moving from one point to another with no apparent pattern or purpose other than my interest and google’s search suggestions, but my encounter with euler will recur continuously throughout this journey for sure. but for now, i’m going to take a brief break from euler and return back to the topic of good old statistical analysis, specifically bayesian methods and monte carlo methods. catch you up in the next one! basel problem: https://en.wikipedia.org/wiki/basel_problem euler’s solution: http://www.17centurymaths.com/contents/euler/e041tr.pdf weierstrass factorization: https://en.wikipedia.org/wiki/weierstrass_factorization_theorem fundamental theorem of algebra: https://en.wikipedia.org/wiki/fundamental_theorem_of_algebra apéry’s constant: https://en.wikipedia.org/wiki/apéry%27s_constant sieve of eratosthenes: https://en.wikipedia.org/wiki/sieve_of_eratosthenes wallis product: https://en.wikipedia.org/wiki/wallis_product bayesian methods: https://jaketae.github.io/study/bayes/",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0
2020-01-22-exponential-family,"normal, binomial, exponential, gamma, beta, poisson... these are just some of the many probability distributions that show up on just about any statistics textbook. until now, i knew that there existed some connections between these distributions, such as the fact that a binomial distribution simulates multiple bernoulli trials, or that the continuous random variable equivalent of the geometric distribution is the exponential. however, reading about the concept of the exponential family of distributions has lent me new insight, and i wish to share that renewed understanding on probability distributions through this post. in this section, we will take a look at what the exponential family of distributions is all about. we will begin by laying out a few mathematical definitions, then proceed to see examples of probability distributions that belong to the exponential family. to cut to the chase, the exponential family simply denotes a group of probability distributions that satisfy a certain condition, namely that they can be factorized and parametrized into a specific form, as show below: here, is a log noramlizing constant that ensures that the probability distribution integrates to 1. there are other alternative forms that express the same factorization. one such variant that i prefer and find more intuitive uses a simple fractional approach for normalization instead of adding complications to the exponential term. for notational convenience, i will follow the fractional normalization approach shown below throughout this post. before we proceed any further, it is probably a good idea to clarify the setup of the equations above. first, denotes a dimensional random variable of interest; , a dimensional parameter that defines the probability distribution. is known as the sufficient statistic function. below is a brief summary concerning the mappings of these different functions. you will notice that i used and instead of and as shown in equation . this is because assumes vectorization of these functions as follows. we could have expressed without vectorization, but doing so would be rather verbose. so we instead adhere to the vectorized convention in throughout this post. as i hinted earlier, the exponential family covers a wide range of probability distributions, most pdfs and pmfs. in fact, most probability distributions that force themselves onto the page of statistics textbooks belong to this powerful family. below is a non comprehensive list of distributions that belong to the exponential family. probability density functions exponential gaussian beta gamma chi squared probability mass functions bernoulli binomial poisson geometric multinomial of course, there are examples of common distributions that do not fall under this category, such as the uniform distribution or the student distribution. this point notwithstanding, the sheer coverage of the exponential family makes it worthy of exploration and analysis. also, notion of an exponential family itself is significant in that it allows us to frame problems in meaningful ways, such as through the notion of conjugate priors: if you haven't noticed, the distributions outlined above all have conjugate priors that also belong to the exponential family. in this sense, the exponential family is particularly of paramount importance in the field of bayesian inference, as we have seen many times in previous posts. let's concretize our understanding of the exponential family by applying factorization to actual probability distributions. the easiest example, as you might have guessed, is the exponential distribution. recall that the formula for the exponential distribution is where the indicator function, denoted as , takes the following form: the indicator function is a simple modification applied to ensure that the function is well defined across the entire real number domain. normally, we omit the indicator function since it is self apparent, but for the sake of robustness in our analysis, i have added it here. how can we coerce equation to look more like , the archetypal form that defines the exponential family? well, now it's just a matter of drag and match: by paying close attention to the variables, parameters, and the output of each function, we can reconstruct to take the form of . the easeist starting point is to observe the exponent to identify and , after which the rest of the surrounding functions can be inferred. the end result is presented below: after substituting each function with their prescribed value in , it isn't difficult to see that the exponential distribution can indeed by factorized according to the form outlined in . although this is by no means a rigorous proof, we see not only the evident fact that the exponential distribution indeed belongs to the exponential family, but also that the factorization formula in isn't just a complete soup of equations and variables. we can do the same for the bernoulli distribution, which also falls under the exponential family. the formula for the bernoulli distribution goes as follows: again, i have added a very simple indicator function to ensure that the the probability mass function is well defined across the entire real number line. again, the indicator function is a simple boolean gate function that checks whether is an element within a set of zero and one: factorizing the bernoulli is slightly more difficult than doing the same for the exponential distribution, largely because it is not apparent from how factorization can be achieved. for example, we do not see any exponential term embedded in as we did in the case of the exponential distributions. therefore, a simple one to one correspondence cannot be identified. the trick to get around this problem is to introduce a log transformation, then reapplying an exponential. in other words, by applying this manipulation, we can artificially create an exponential term to more easily coerce into the factorization mold. specifically, observe that the power of the exponent can be expressed as a dot product between two vectors, each parameterized by and , respectively. this was the hard part: now, all that is left is to configure the rest of the functions to complete the factorization. one possible answer is presented below: by now, it should be sufficienty clear that the definition of the exponential family is robust enough to encompass at least the two probability distributions: the exponential and the bernoulli. although we do not go over other examples in this article, the exponential family is a well defined set of probability distributions that, at thei core, are defined by a common structure. and as we will see in the next section, this underlying similarity makes certain calculations surprisingly convenient. in a previous post, we explorerd the notion of maximum likelihood estimation, and contrasted it with maximum a posteriori estimation. the fundamental question that maximum likelihood estimation seems to answer is: given some data, what parameter of a distribution best explains that observation? this is an interesting question that merits exploration in and of itself, but the discussion becomes a lot more interesting and pertinent in the context of the exponential family. before diving into mle, let's define what is known as the canonical form of the exponential family. despite its grandiose nomenclature, the canonical form simply refers to a specific flavor of factorization scheme where in which case simplifies to we will assume some arbitrary distribution in the exponential family following this canonical form to perform maxmimum likelihood estimation. much like in the previous post on maximum likelihood estimation, we begin with some data set of independent and identically distributed observations. this is going to be the setup of the mle problem. given this dataset, the objective of maximum likelihood estimation is to identify some parameter that maximizes the likelihood, i.e. the probability of observing these data points under a probability distribution defined by . in other words, how do we identify this parameter? well, the go to equipment in a mathematician's arsenal for an optimization problem like this one is calculus. recall that our goal is to maximize the likelihood function, which can be calculated as follows: the first equality stands due to the assumption that all data are independent and identically distributed. maximizing is a complicated task, especially because we are dealing with a large product. products aren't bad, but we typically prefer sums because they are easier to work with. a simple hack that we almost always use when dealing with maximum likelihood, therefore, is to apply a log transformation to calculate the log likelihood, since the logarithm is a monotonically increasing function. in other words, what does the log likelihood look like? well, all we have to do is to apply a log function to , which yields the following result. maximizing the log liklihood can be achieved by setting the gradient to zero, as the gods of calculus would tell us. as you might recall from a previous post on some very basic matrix calculus, the gradient is simply a way of packaging derivatives in a multivariate context, typically involving vectors. if any of this sounds unfamilar, i highly recommend that you check out the linked post. we can compute the partial derivative of the log likelihood function with respect to as shown below. observe that the last term in is eliminated because it is a constant with respect to . this is a good starting point, but we still have no idea how to derive the log of . to go about this problem, we have to derive an expression for . recall from the definition of the exponential family that is a normalizing constant that exists to ensure that the probability function integrates to one. in other words, this necessarily implies that now that we have an expression for to work with, let's try to compute the derivative term we left unsolved in . the first and second equalities stand due to the chain rule, and the third equality is a simple algebraic manipulation that recreates the probability function within the integral, allowing us to ultimately express the partial derivative as an expected value of for the random variable . this is a surprising result, and a convenient one indeed, because we can now use this observation to conclude that the gradient of the log likelihood function is simply the expected value of the sufficient statistic. therefore, starting again from , we can continue our calculation of the gradient and set the quantity equal to zero to calculate the mle estimate of the parameter. it then follows that how do we interpret the final result in equation ? it looks nice, simple, and concise, but what does it mean to say that the expected value of the sufficient statistic is the average of the sufficient statistic for each observed individual data points? to remove abstractness, let's employ a simple example, the exponential distribution, and attempt to derive a clearer understanding of the final picture. recall that the probability density function of the exponential distribution takes the following form according to the factorizations outlined below: computing the derivative of the log of the normalizing term as we did in , because we know that the resulting quantity is the expected value of the sufficient statistic, we know that and indeed, this is true: the expected value of the random variable characterized by an exponential distribution is simply the inverse of the parameter defining that distribution. note that the parameter for the exponential distribution is most often denoted as , in which case the expected value of the distribution would simply be written as . this is all great, but there is still an unanswered question lingering in the air: what is the mle estimate of the parameter ? this moment is precisely when equation comes in handy. recall that therefore, finally, we have arrived at our destination: we finally know how to calculate the parameter under which the likelihood of observing given data is maximized. the beauty of this approach is that it applies to all probability distributions that belong to the exponential family because our analysis does not depend on which distribution is in question; we started from the canonical form of the exponential family to derive a set of generic equations. this is the convenience of dealing with the exponential family: because they are all defined by the same underlying structure, the mle equations hold general applicability. in this post, we explored the exponential family of distributions, which i flippantly ascribed the title ""the medici of probability distributions."" this is obviously my poor attempt at an intellectual joke, to which many of you might cringe, but i personally think it somewhat captures the idea that many probability distributions that we see on the textbook are, in fact, surprisingly more related than we might think. at least to me, it wasn't obvious from the beginning that the exponential and the bernoulli distributions shared the same structure, not to mention the wealth of other distributions that belong to the exponential family. also, the convenient factorization is what allowed us to perform an mle estimation, which is an important concept in statistics with wide ranging applications. this post in no way claims to give a full, detailed view of the exponential family, but hopefully it gave you some understanding of what it is and why it is useful. in the next post, we will take a look at maximum a posteriori estimation and how it relates to the concept of convex combinations. stay tuned for more.",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
2019-11-17-eulers-identity,"at a glance, euler’s identity is a confusing, mind boggling mishmash of numbers that somehow miraculously package themselves into a neat, simple form: i remember staring at this identity in high school, trying to wrap my head around the seemingly discordant numbers floating around the equation. today, i want to share some ideas i have learned since and demonstrate the magic that euler’s identity can play for us. the classic proof for euler’s identity flows from the famous taylor series, a method of expressing any given function in terms of an infinite series of polynomials. i like to understand taylor series as an approximation of a function through means of differentiation. recall that a first order derivative gives the slope of the tangent line at any given point of a function. the second order derivative provides information regarding the convexity of the function. through induction, we can convince ourselves that higher order derivatives will convey information about the curvature of the function throughout coordinate system, which is precisely the underlying mechanism behind taylor’s series. in a more concise notation, we have notice that is the starting point of our approximation. therefore, the taylor series will provide the most accurate estimation of the original function around that point, and the farther we get away from , the worse the approximation will be. for the purpose of our analysis, let’s examine the taylor polynomials for the following three functions: , and . recall that the derivative of is , which is precisely what the taylor series suggests. it is also interesting to see that the taylor series for is an odd function, while that for is even, which is coherent with the features of their respective original functions. last but not least, notice that the derivative of taylor polynomial of gives itself, as it should. now that we have the taylor polynomials, proving euler’s identity becomes a straightforward process of plug and play. let’s plug into the taylor polynomial for : notice that we can separate the terrms with and without : in short, ! with this generalized equation in hand, we can plug in into to see euler’s identity: the classic proof, although fairly straightforward, is not my favorite mode of proving euler’s identity because it does not reveal any properties about the exponentiation of an imaginary number, or an irrational number for that matter. instead, i found geometric interpretations of euler’s formula to be more intuitive and thought provoking. below is a version of a proof for euler’s identity. let’s start by considering the complex plane. there are two ways of expressing complex numbers on the argand diagram: points and vectors. one advantage of the vector approach over point representation is that we can borrow some simple concepts from physics to visualize through the former: namely, a trajectory of a point moving along the complex plane with respect to some time parameter . notice that introducing this new parameter does not alter the fundamental shape or path of the vector ; it merely specifies the speed at which the particle is traversing the complex plane. you might recall from high school physics that the velocity vector is a derivative of the position vector with respect to time. in other words, where is a vector that denotes the position of an object at time . now, let’s assume that is such a position vector. then, it follows from the principles of physics that its derivative will be a velocity vector. therefore, we have what is so special about this velocity vector? for one, we can see that it is a scalar multiple of the original position vector, . upon closer examination, we might also convince ourselves that this vector is in fact orthogonal to the position vector. this is because multiplying a point or vector by in the complex plane effectively flips the object’s and components, which is precisely what a 90 degree rotation entails. what does it mean to have a trajectory whose instantaneous velocity is perpendicular to that of the position vector? hint: think of planetary orbits. yes, that’s right: this relationship is characteristic of circular motions, a type of movement in which an object rotates around a center of axis. the position vector of a circular motion points outward from the center of rotation, and the velocity vector is tangential to the circular trajectory. the implication of this observation is that the trajectory expressed by the vector is essentially that of a circle, with respect to time . more specifically, we see that at , , or , which means that the circle necessarily passes through the point on the complex plane expressed as an argand graph. from this analysis, we can learn that the trajectory is not just any circle, but a unit circle centered around the origin. but there’s even more! recall that the velocity vector of the trajectory is a 90 degree rotation of the position vector, i.e. , . earlier, we concluded that the trajectory expressed by the vector is a unit circle, which necessarily means that for all values of . then, syllogism tells us that is also one, i.e. the particle on the trajectory moves at unit speed along the unit circle! now we finally have a full visualization of the position vector. the blue arrow represents the position vector at ; green, the velocity vector also at . figure 1: representation of euler's identity at t = 0 why is speed important? unit speed implies that the particle moves by distance units after time units. let’s say that time units have passed. where would the particle be on the trajectory now? after some thinking, we can convince ourselves that it would lie on the point , since the unit circle has a total circumference of . and so we have proved that , euler’s identity. but we can also go a step further to derive the generalized version of euler’s identity. recall that a unit circle can be expressed by the following equation in the cartesian coordinate system: on the complex plane mapped in polar coordinates, this expression takes on an alternate form: notice that this contains the same exact information that euler’s identity provides for us. it expresses: a unit circle trajectory centered around the origin oriented counter clockwise with a constant speed of 1 from this geometric interpretation, we can thus conclude that we now know the exact value that represents in the complex number system! urban legend goes that mathematician benjamin peirce famously said the following about euler’s identity: gentlemen, that is surely true, it is absolutely paradoxical; we cannot understand it, and we don’t know what it means. but we have proved it, and therefore we know it must be the truth. but contrary to his point of view, euler’s identity is a lot more than just an interesting, coincidental jumble of imaginary and irrational numbers that somehow churn out a nice, simple integer. in fact, it can be used to better understand fundamental operations such as logarithms and powers. consider, for example, the value of the following expression: imaginary powers are difficult to comprehend by heart, and i no make no claims that i do. however, this mind pulverizing expression starts to take more definite meaning once we consider the generalized form of euler’s identity, . let . then we have take both sides to the power of i: interestingly enough, we see that takes on a definitive, real value. we can somewhat intuit this through euler’s identity, which is basically telling us that there exists some inextricable relationship between real and imaginary numbers. understood from this point of view, we see that the power operation can be defined in the entire space that is complex numbers. we can also take logarithms of negative numbers. this can simply be shown by starting from euler’s identity and taking the natural log on both sides. in fact, because is a periodic function around the unit circle, any odd multiple of will give us the same result. while it is true that logarithmic functions are undefined for negative numbers, this proposition is only true in the context of real numbers. once we move onto the complex plane, what may appear as unintuitive and mind boggling operations suddenly make mathematical sense. this is precisely the magic of euler’s identity: the marriage of different numbers throughout the number system, blending them together in such a way that seems so simple, yet so incomprehensibly complex and profound. circular motions: https://en.wikipedia.org/wiki/circular_motion the following: http://mathshistory.st andrews.ac.uk/quotations/peirce_benjamin.html euler’s identity: https://en.wikipedia.org/wiki/euler%27s_identity taylor series: http://mathworld.wolfram.com/taylorseries.html argand diagram: http://mathworld.wolfram.com/arganddiagram.html a version of a proof: http://www.science4all.org/article/eulers identity/",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
2020-06-23-zeta-prime,"the other day, i came across an interesting article by chris henson on the relationship between the riemann zeta function and prime numbers. after encountering a similar post on the math stack exchange, i thought i'd write an article on the same topic as well, perhaps as sort of a prologue to a previous article posted on this blog, basel, zeta, and some more euler. the code introduced in this blog are adaptations of those written by chris henson, so all credits go to the original author. with that said, let's dive right into it. the riemann zeta function is perhaps one of the most deeply studied functions in all of modern mathematics. it is sometimes also referred to as the euler riemann zeta function, but i will not adhere to this convention not only because it is needlessly long and cumbersome, but also because euler already has so many constants and functions and theorems bearing his name. anyhow, the riemann zeta function looks as follows: while there are so many layers to explore with this function, one relatively simple and interesting route is to factorize this function. we performed this factorization in the previous post while introducing euler's infinite product representation of the zeta function. let's go through this step again, as it is pivotal for our segue into the topic of prime numbers and probability. the idea is that, much like we multiply the ratio to a geometric sequence to calculate its sum, we can multiply terms to the zeta function to factor out their multiples. for instance, let's consider the case when we multiply to the zeta function. since the zeta function itself is an infinite series, we can subtract this result from the original zeta function. this effectively filters or sieves out all terms whose denominator is a multiple of 2, effectively leaving only the odd terms. concretely, if we repeat this process for all prime numbers, we will eventually be left with the following: where denotes the set of all prime numbers. from this, we can come up with the following alternative representation of the riemann zeta function: which can also be expressed as other than the fact that the factorization of the riemann zeta function is satisfying in and of itself, the result in also provides us with an interesting probabilistic interpretation on coprimeness. the intuition is pretty simple: given some random natural number , the probability that a prime will divide is simply . for example, if we come up with some random number, the probability that 2 will divide that number is 0.5; the probability that the number will be a multiple of 3 is one thirds. a corollary of this simple analysis is that we can now express the probability that a given random number will be a prime number as follows, using : in other words, the reciprocal of the zeta function tells us the probability that a randomly chosen number will be a prime! the more interesting part of the story is that, we can now extend this single number example to the case of multiple numbers. the only difference is that, instead of considering whether a single number is prime or not, we will consider the notion of coprimeness, or relative primeness. imagine we randomly sample numbers, ranging from all the way up to . what is the probability that these numbers are all coprime to each other, i.e. the greatest common divisor for these numbers is 1? with some rumination, it isn't difficult to convince ourselves that this probability can be expressed as let's think for a second why this is the case. the probability that some prime number divides all through is going to be , as dividing each number can be considered an independent event. therefore, the probability that some prime number does not divide all numbers i.e. it may divide none or some, but definitely not all can be expressed as the complement of , or equivalently, . if we apply this analysis to all prime numbers, we end up with . now, let's simulate the process of random sampling to empirically verify the probabilistic interpretation of the riemann zeta function. before we get into the specifics, below are the dependencies we will need. the first function we will build is one that randomly samples natural numbers from 1 to , and checks if all number pairs within this sample is coprime. for this, we use , which reduces the result of applying to two pairs of numbers in the randomly sampled number list. is a quite common operation in functional programming, and we saw an example of this operation in the context of spark in a previous post as well. let's see this function in action. i've added a flag for convenience of testing and demonstration. let's toggle this option on for now and see what we get. 6 7 1 true the gcd of 6, 7, and 1 are 1, so the returned result is as we expect. we also notice that three numbers were returned since . next, we define a testing function that will simulate multiple runs of the test for us. because this is a monte carlo simulation, to have confidence in our estimate, we need to iterate the sampling process multiple times until we have sufficient amount of data. the parameter determines the number of simulations we will perform. and much like in the previous function, the parameter determines the upper bound of our sampling range. therefore, the total number of simulations we run will effectively be times. last but not least, indicates how many numbers we want to sample each time this parameter is semantically identical to the parameter we saw in the function above. also, for easier plotting, we will return the domain range object alongside the result of our simulation. the function is just a two liner we were able to reduce the number of lines thanks to list comprehension. now, let's see if this code works as expected. here, our experiment consisted of sampling two numbers, starting from range , all the way up to range , with 1000 simulations for each range. let's plot the results to get a better idea of what was going on. first, notice that when , the probability that two sampled numbers are coprime is 1. this is unsurprising, since sampling from range simply means that both the sampled numbers were 2 and of course, 2 and 2 are coprimes. however, as the range expands all the way up to 200, we see that the probability of two numbers being coprime drops precipitously, roughly converging to the value of as we expect. indeed, the dots seem to cluster around the gold line, which represents the value of the zeta function evaluated at 2. but does this result generalize to cases where we sample more than just two numbers? in other words, if we sample numbers, would the probability that the numbers be coprime approach ? let's find out. given a range of values, the function returns a plot for each , which is effectively in our mathematical notation. we see that, for the three values of that were tested 2, 3, and 4 the zeta function seems to approximate the probability of relative primeness pretty well. based on our earlier mathematical analysis, we would expect this convergence to get even better as we expand out the range, with an upper bound that is greater than the current 200. while jumping around in wikipedia, i came across the dirichlet eta function, which is a slight variant of the riemann zeta function. this function looks as follows: as you can see, this is essentially the alternating version of the riemann zeta function. given this design, we can derive what may appear to be apparent to some yet nonetheless interesting relationship between the eta and zeta. deriving this relationship requires a very similar operation to the sieving or factorizing we performed earlier to derive the probabilistic interpretation of the zeta function. for a bit of intuition, observe that the eta function can be split up into what may be referred to as even and odd terms. in other words, the idea is that the even terms are just a multiple of the zeta function, namely then, the odd terms can also be seen as the zeta function minus this multiple: we now have successfully expressed both the even and odd terms of the eta function in terms of the zeta function. if we put the two together, we will then be able to express the entirety of the eta function fully in terms of the zeta function. to cut to the chase, we get and there we have it, the relationship between the dirichlet eta function and the riemann zeta function! there are many more interesting things about the eta function, such as its convergence property, but that is a topic for another post. in this post, we developed an intuition for the implications of the riemann zeta function from the perspective of relative primeness and probability. the zeta function is one of those things in mathematics that appear so simple on the surface, yet is so wonderfully complex and convoluted in the inside. although we haven't discussed these other intricacies of the riemann zeta function in particular, its relationship to the riemann hypothesis, which states that the zeta function has roots at negative even integers and complex numbers whose real part is but the approach we took here with prime numbers are probabilities is fascinating in its own right, providing ample food for thought. many thanks to chris henson again for the code and the post. it's always a lot of fun to mesh mathematics with programming, and i think this is why i enjoyed writing this post. on a related note, i've recently gotten into project euler, so i might post about some problems now and then as well. i hope you've enjoyed reading this post. see you in the next one.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0
