title,body,tensorflow,linear_algebra,statistics,machine_learning,probability_distribution,deep_learning,from_scratch,analysis
Dissecting the Gaussian Distribution,"We can also see why (9) is coherent by unpacking the expected values expression as shown below: Using the linearity of expectation, we can rewrite the equation as Therefore, we end up with which almost exactly parallels the definition of variance, which we might recall is where . The key takeaway is that the covariance matrix constructed from the random vector  is the multivariable analogue of variance, which is a function of the random variable . To gain a better idea of what the covariance matrix actually looks like, however, it is necessary to review its structure element-by-element. Here is the brief sketch of the -by- covariance matrix. This might seem complicated, but using the definition of covariance in (8), we can simplify the expression as: Note that the covariance matrix is a symmetric matrix since . More specifically, the covariance matrix is a positive semi-definite matrix. This flows from the definition of positive semi-definiteness. Let  be some arbitrary non-zero vector. Then, You might be wondering how (9) ends up as (10). Although this relationship may not be immediately apparent, that the two expressions are identical can be seen by setting the random vector as and performing basic matrix vector multiplication operations. For the sake of brevity, this is left as an exercise for the reader. We now have all the pieces we need to complete the puzzle. Recall that we were trying to derive the probability density function of the multivariate Gaussian by building on top of the formula for the univariate Gaussian distribution.",0,0,1,0,1,0,0,0
Dissecting the Gaussian Distribution,"Continuing, the next subject of our interest would be , as the variance is only strictly defined for one variable, as expressed by its definition below: where  is a random variable, which takes a scalar value. This necessarily begs the question: what is the multivariable equivalent of variance? To answer this question, we need to understand covariance and the covariance matrix. To jump right into the answer, the multivariable analogue of variance is covariance, which is defined as Notice that  equals variance, which is why we stated earlier that covariance is the multivariate equivalent of variance for univariate quantities. The intuition we can develop from looking at the equation is that covariance measures how far our random variables are from the mean in the  and  directions. More concretely, covariance is expresses the degree of association between two variables. Simply put, if there is a positive relationship between two variables, i.e. an increase in one variable results in a corresponding increase in the other, the variance will be positive; conversely, if an increase in one variable results in a decrease in the other, covariance will be negative. A covariance of zero signifies that there is no linear relationship between the two variables. At a glance, the concept of covariance bears strong resemblance to the notion of correlation, which also explains the relationship between two variables. Indeed, covariance and correlation are related: in fact, correlation is a function of covariance.",0,0,1,0,1,0,0,0
"PyTorch, From Data to Modeling","Defining the training loop may seem difficult at first, especially if you’re coming from a Keras background, but actually a lot of it is boiler plate, and things are not as difficult as they may seem. First, we define a list to hold the loss values per iteration. We will be using this list for visualization later. The exciting part comes next. For each epoch, we load the images in the . Note that the loader returns a tuple of images and labels, which we can unpack directly within the  loop itself. We then move the two objects to , which would be necessary if we were running this one a Cuda-enabled computer. Then, we calculate the loss by calling , the loss function, and append the loss to the  list. Note that we have to call  since  itself is a one-by-one PyTorch tensor. Then comes the important part where we perform backprop. The idea is that we would The three steps correspond to each of the lines in the code above, starting from . As you might be able to guess from the name of the function, we zero the gradients to make sure that we aren’t accumulating gradient values from one iteration to the next. Calling  corresponds to calculating the new gradient values, and  performs the backprop. The last block of code is simply a convenient print function I’ve written to see the progress of training at certain intervals.",0,0,0,0,0,1,0,0
A Step Up with  Variational Autoencoders,"In the context of probability, Jensen’s inequality can be summarized as follows. Given a convex function , We won’t get into rigorous proofs here, but it’s not difficult to see why this inequality stands with some basic geometric intuition. Due to its bow-like shape, the expected value of a convex function evaluated across a given interval will always be greater or equal to the function evaluated at the expected value of the random variable.  How is Jensen’s inequality related to the non-negativity of KL divergence? Let’s return back to the definition of KL divergence. For simplicity and to reduce notational burden, we briefly depart from conditional probabilities  and return back to generic distributions  and . Notice that the definition of KL divergence itself is an expected value expression. Also, note that  is a convex function— itself is concave, but the negative sign flips the concavity the other way. With these observations in mind, we can apply Jensen’s inequality to derive the following: Therefore, we have shown that KL divergence is always greater or equal to zero, which was our end goal. There is another version of a proof that I found a lot more intuitive and easier to follow than the previous approach. This derivation was borrowed from this post. We start from the simple observation that a logarithmic function is always smaller than  a linear one. In other words, This is no rocket science, and one can easily verify (11) by simply plotting the two functions on a Cartesian plane.",1,0,0,0,0,1,0,0
A sneak peek at Bayesian Inference,"In other words, to say that a coin is fair is to say that, theoretically, performing an infinite number of coin flips would result in 50 percent heads and 50 percent tails. However, the Bayesian approach we explored today presents a drastically different picture. In Bayesian statistics, probability is an embodiment of our subjective beliefs about a parameter, such as the fairness of a coin. By performing trials, infinite or not, we gain more information about the parameter of our interest, which affects the posterior probability. Both interpretations of probability are valid, and they help complement each other to help us gain a broader understanding of what the notion of probability entails. I hope this post gave you a better understanding as to why distributions are important—specifically in the context of conjugate priors. In a future post, we will continue our exploration of the Beta distribution introduced today, and connect the dots between Beta, Gamma, and many more distributions in the context of Bayesian statistics. See you in the next one.",0,0,1,0,1,0,0,0
The Magic of Euler’s Identity,"The implication of this observation is that the trajectory expressed by the vector  is essentially that of a circle, with respect to time . More specifically, we see that at , , or , which means that the circle necessarily passes through the point  on the complex plane expressed as an Argand graph. From this analysis, we can learn that the trajectory is not just any circle, but a unit circle centered around the origin. But there’s even more! Recall that the velocity vector of the trajectory is a 90-degree rotation of the position vector, i.e. , . Earlier, we concluded that the trajectory expressed by the vector  is a unit circle, which necessarily means that  for all values of . Then, syllogism tells us that  is also one, i.e. the particle on the trajectory moves at unit speed along the unit circle! Now we finally have a full visualization of the position vector. The blue arrow represents the position vector at ; green, the velocity vector also at . Why is speed important? Unit speed implies that the particle moves by  distance units after  time units. Let’s say that  time units have passed. Where would the particle be on the trajectory now? After some thinking, we can convince ourselves that it would lie on the point , since the unit circle has a total circumference of . And so we have proved that , Euler’s identity. But we can also go a step further to derive the generalized version of Euler’s identity.",0,0,0,0,0,0,0,1
A Brief Introduction to Recurrent Neural Networks,"For control our experiment, we will train all four models over the same , , and . There isn’t much exciting here to look at it terms of code; it’s just a matter of patience, waiting for the models to hopefully converge to a global minimum. For future reference, all training history is dumped in the  object where  corresponds to the model number. After a long time of waiting, the training is finally complete! If you are following this tutorial on your local workstation, please note that the time required for training may vary depending on your hardware configurations or the specification of our instance if you are using a cloud-based platform like AWS. None of our models reached the threshold of ninety percent accuracy, but they all managed to converge to some reasonable number, hovering around the high seventies to low eighties. Let’s test the performance of our models by using the  and  data, both of which none of our models have seen before. Based on the results, it looks like the LSTM model performed best, beating other models by a small margin. At this point, we cannot conclude as to whether or not this marginal boost in performance is significant. Judging this would not only depend on the context, but also most likely require us to have a larger test dataset that captures the statistics of the population data. This point notwithstanding, it is certainly beneficial to know that LSTM networks are good at detecting sequential patterns in data.",1,0,0,0,0,1,0,0
Convex Combinations and MAP,"Recall the formula for the univariate Gaussian that describes our data: Then, from (1), we know that the likelihood function is simply going to be a product of the univariate Gaussian distribution. More specifically, the log likelihood is going to be the sum of the logs of the Gaussian probability distribution function. There is the log likelihood function! All we need now is the log prior. Recall that the prior is a normal distribution centered around mean  with standard deviation of 1. In PDF terms, this translates to The log prior can simply be derived by casting the logarithmic function to the probability distribution function. Now we are ready to enter the maximization step of the sequence. To calculate the maximum of the posterior distribution, we need to derive the posterior and set the gradient equal to zero. For a more robust analysis, it would be required to show that the second derivative is smaller than 0, which is indeed true in this case. However, for the sake of simplicity of demonstration, we skip that process and move directly to calculating the gradient. Let’s rearrange the final equality in (7). From (8), we can finally derive an expression for . This value of the parameter is one that which maximizes the posterior distribution.",0,0,1,0,0,0,0,0
A Step Up with  Variational Autoencoders,"Namely, Because  is an approximation of , we naturally assume the same model for the approximate distribution: Now we can derive an expression for the negative KL divergence sitting in the ELBO expression: This may seem like a lot, but it’s really just plugging in the distributions into the definition of KL divergence as an expectation and using some convenient properties of logarithms to perform simple algebraic simplifications. To proceed further, observe that the first term is a constant that can escape out of the expectation: From the definition of variance and expectation, we know that Therefore, we can simplify (17) as follows: Let’s zoom in on the expected value term in (19). Our goal is to use (18) again so that we can flesh out another one half from that term. This can be achieved through some clever algebraic manipulation: But since the the expected value of  is constant and that of  is zero, We can now plug this simplified expression back into the calculation of KL divergence, in (19): Since we will standardize our input such that  and , we can plug these quantities into (22) and show that We are almost done with deriving the expression for ELBO.",1,0,0,0,0,1,0,0
Scikit-learn Pipelines with Titanic,"All the steps prior to the model would involve wrangling the data; the last step would have the model use the data to make a prediction. Therefore, calling  should apply only to the last model after  is called on all the preprocessing steps. If the pipeline itself is just a bundle of preprocessors, on the other hand, we should only be able to call . Scikit-learn’s models are great, but in a sense they are too great. This is because there are a lot of hypterparameters to tune. Fortunately for us, we can somewhat resort to a quasi-brute force approach to deal with this: train models on a number of different combinations of hyperparameters and find the one that performs best! Well, this is what  does.  is not quite as bad in that it doesn’t create and test all possible models that distinct combinations of hyperparameters can yield: instead, it relies on a randomized algorithm to perform a search of the hyperparameter space. This is why  is a lot quicker than , with marginal sacrifices in model performance. Let’s see how we might be able to perform hyperparameter search given a pipeline like the one we have built above. The parameter space we are searching for here is by no means exhaustive, but it covers a fair amount of ground. Of course, we can go crazy with randomized search, basically shoving Scikit-learn with every possible configuration and even running a grid search instead. However, that would take an extreme amount of time and computing resources.",0,0,0,1,0,0,0,0
Bayesian Linear Regression,"where  is a design matrix given by and  is a column vector given by Before calculating the posterior, let’s recall what the big picture of Bayesian inference looks like. where  denotes the parameter of interest for inference. In plain terms, the proposition above can be written as In other words, the posterior distribution can be obtained by calculating the product of the prior distribution and the likelihood function. In many real-world cases, this process can be intractable, but because we are dealing with two Gaussian distributions, the property of conjugacy ensures that this problem is not only tractable, but also that the resulting posterior would also be Gaussian. Although this may not be immediately apparent, observe that the exponent is a quadratic that follows the form after making appropriate substitutions Therefore, we know that the posterior for  is indeed Gaussian, parameterized as follows: Let’s try to obtain the MAP estimate of of , i.e. simplify  Notice the similarity with the MLE estimate, which is the solution to normal equation, which I otherwise referred to as vanilla linear regression: This is no coincidence: in a previous post on MAP and MLE, we observed that the MAP and MLE become identical when we have a uniform prior. In other words, the only cause behind the divergence between MAP and MLE is the existence of a prior distribution. We can thus consider the additional term in (10) absent in (11) as a vestige of the prior we defined for .",0,1,0,0,0,0,0,0
Demystifying Entropy (And More),"This is why we use bytes to represent the amount of disk storage in computers, for instance. It is also worth mentioning that the alternative name for bits is Shannons, named eponymously after the mathematician who pioneered the field of information theory, as mentioned above. Now that we have some idea of what information is and how we can quantify it using binary numbers in bits, it’s time to get into the math. Information can be calculated through the formula where  is the information need to express the random event , and  is the probability that event  occurs, i.e. . There are different versions of this formula, such as the one that uses Euler’s constant as the log base instead of 2. Whereas the unit of information as measured through (1) is in bits, that calculated through (2) as shown below is in the unit of nats. For the purposes of this post, we will be using equation (1) instead of two. This is primarily because we will be using the binary number analogy to build an intuition for information computation. Let’s quickly create a visualization that shows the relationship between probability and information in bits. As equation (1) describes this relationship quite concisely, let’s try plotting it on a graph.  So that’s how we calculate randomness in a random event—the amount of information that is needed to represent randomness as probability.",0,0,1,0,0,0,0,0
Building Neural Network From Scratch,"Internally, the  function calls the  gradient descent algorithm to update the weights and finally returns the  which contains updated parameters based on the training data. As mentioned above, each  and  are minibatches that will be feeded into our  gradient descent function. Note that the  function is simply an implementation of equation (7). At the core of the  function is the  function, which is our implementation of back propagation. This provides a nice point of transition to the next section. Back propagation is a smart way of calculating gradients. There are obviously many ways one might go about gradient calculation. We can simply imagine there being a loss function that is a function of all the thousands of weights and biases making up our neural network, and calculate partial derivatives for each parameter. However, this naive aproach is problematic because it is so computationally expensive. Moreover, if you think about it for a second, you might realize that doing so would result in duplicate computations due to the chain rule. Take the simple example below. If we were to calculate the gradient of the loss function with respect to  and , all we need to compute is the gradient of , since that of  will naturally be obtained along the way. In other words, computing the gradient simply requires that we start from the very end of the neural network and propagate the gradient values backwards to compute the partial derivatives according to the chain rule.",0,1,0,0,0,1,1,0
Markov Chain and Chutes and Ladders,"Although we won’t go into the formal proofs here, having a full span of independent eigenvectors implies full rank, which is why we must check if the stochastic matrix is singular before jumping into eigendecomposition. Unfortunately, the stochastic matrix is singular because , the number of columns or rows. This implies that our matrix is degenerate, and that the best alternative to eigendecomposition is the singular value decomposition. But for the sake of simplicity, let’s resort to the brute force calculation method instead and jump straight into some statistical analysis. We first write a simple function that simulates the Chutes and Ladders game given a starting position vector . Because a game starts at the th cell by default, the function includes a default argument on  as shown below: Calling this function will give us , which is a 101-by-1 vector whose th entry represents the probability of the player being on the th cell after a single turn. Now, we can plot the probability distribution of the random variable , which represents the number of turns necessary for a player to end the game. This analysis can be performed by looking at the values of  since the last entry of this vector encodes the probability of the player being at the th cell, i.e. successfully completing the game after  rounds.",0,1,0,0,0,0,0,0
Gaussian Process Regression,"The distance function of the squared exponential kernel looks as follows: We can apply distortions to the RBF function by adding things like coefficients, but for simplicity sake we omit them here. The key takeaway is that the RBF kernel function functions as a distance metric between two points. As an extreme example, let’s consider the case when , the diagonal entries of the covariance matrix, which is effectively the variance along those components. Then, Conversely, when  and  are extremely different points, We can thus deduce that the distance function returns a value between 0 and 1 that indicates the similarity or closeness between two points. This is exactly the sort of behavior we want for the covariance matrix. In short, the multivariate Gaussian that we will be using for GP regression can simply be summarized as where covariance  denotes the kernel matrix. In the RBF kernel function above, we were assuming a function without any noise, namely that . However, once we add in Gaussian noise as we saw above with  where , then we need to make adjustments to the kerne to account for added variations. With some thinking, we can persuade ourselves that the only modification that is needed pertains to the diagonal entries of the covariance matrix. This is because  only affects variance that exists within the univariate Gaussian for each point on the -axis without affecting the non-diagonal entries, which otherwise pertain to covariance between two points.",0,0,0,1,0,0,1,0
Wonders of Monte Carlo,"Let’s say we want to estimate the value of an integral of a function  over some domain . Now assume that there is some probability density function  defined over . Then, we can alter this integral as  shown below. Notice that this integral can now be understood as an expected value for some continuous random variable. In other words,  collapses into the following expression. What does this tell us? This means that we can simply calculate an integral by randomly sampling the values of  such that  follows some probability distribution . The probability distribution part simply ensures that values of  that are more probable are sampled more often than others. Intuitively, we are effectively taking a weighted mean of the values of , which is the loose definition of expected values. Now, to simplify things a bit, we are going to take a look at an example that does not involve much probability distributions. Conside the following integal of sine, a classic in calculus 101: The reason why we chose this integral is that we know how to calculate it by hand. Therefore, we can match the accuracy of our crude Monte Carlo against an actual, known value. Let’s fist quickly compute this integral. Now time for Monte Carlo. Notice that there is no probability distribution explicitly defined over the domain of integration in our example. In other words,  simply follows a continuous uniform distribution, meaning that all values of  within  are equally likely.",0,0,1,0,0,0,0,0
My First GAN,"Now that we have an idea of what the function accomplishes, let’s use it to start training. The  seems to fluctuate a bit, which is not necessarily a good sign but also quite a common phenomenon in GAN training. GANs are notoriously difficult to train, since it requires balancing the performance of the generator and the discriminator in such a way that one does not overpower the other. This is referred to as a min-max game in game theory terms, and finding an equilibrium in such structures are known to be difficult. Let’s take a look at the results now that the iterations are over.  The created images are admittedly fuzzy, pixelated, and some even somewhat alien-looking. This point notwithstanding, I find it incredibly fascinating to see that at least some generated images actually resemble ships in the sea. Of particular interest to me are the red ships that appear in  and . Given the simplicity of the structure of our network, I would say that this is a successful result. Let’s take a look at the learning curve of the GAN.  As you might expect, the loss is very spiky and erratic. This is why it is hard to determine when to stop training a GAN. Of course, there are obvious signs of failure: when the loss of one component starts to get exponentially larger or smaller than its competitor, for instance. However, this did not happen here, so I let the training continue until the specified number of interactions were over.",1,0,0,0,0,1,0,0
Revisiting Basel with Fourier,"Then, we get With a very small bit of algebra, we end up with And there we have it, the value of ! It’s interesting to see how all this came out of the fourier series of . In this section, we will be taking a look at some interesting representations of the Basel problem, mysteriously packaged in integrals. At a glance, it’s somewhat unintuitive to think that an infinite summation problem can be stated as an integral in exact terms; however, the translation from summation to integrals are not out of the blue. Using things like Taylor series, it is in fact possible to show that the Basel problem can be stated as an integral. For instance, consider this integral One thing I am starting to realize these past few days is that some of these integrals are extremely difficult despite being deceptively simple in their looks. This is a good example. To get started, we might consider making a quick change of variables, namely . This will effectively get rid of the rather messy-looking denominator sitting in the fraction. To make further progress, at this point let’s consider the Taylor series expansion of . We can derive this by considering the following integral: since this integral evaluates to . One way to look at (10) would be to consider it as a sum of some geometric series whose first term begins with 1 and has a constant ratio of . In other words, Here is where a bit of complication comes in.",0,0,0,0,0,0,0,1
How lucky was I on my shift?,"We could, for instance, tweak the Poisson distribution function and perform integration. The following is a code block that produces a visualization of what this integral would look like on a graph. Here is the figure produced by executing the code block above. You might notice from the code block that the integrand is not quite the Poisson distribution—instead of a factorial, we have an unfamiliar face, the  function. Why was this modification necessary? Recall that integrations can only be performed over smooth and continuous functions, hence the classic example of the absolute value as a non-integrable function. Factorials, unfortunately, also fall into this category of non-integrable functions, because the factorial operation is only defined for integers, not all real numbers. To remedy this deficiency of the factorial, we resort to the gamma function, which is essentially a continuous version of the factorial. Mathematically speaking, the gamma function satisfies the recursive definition of the factorial: Using the gamma distribution function, we can then calculate the area of the shaded region on the figure above. Although I do not present the full calculation process here, the value is approximately equal to that we obtained above, . So to answer the title of this post: about 2 in every 100 days, I will have a chill shift where I get lesser than five calls in eight hours. But all of this aside, I should make it abundantly clear in this concluding section that I like my job, and that I love answering calls on the phone.",0,0,1,0,1,0,0,0
Demystifying Entropy (And More),"This yields Recall that the definition of entropy goes as Plugging in this definition to (11) yields the simplified definition of cross entropy: If KL divergence represents the average amount of additional information needed to represent an event with  instead of , cross entropy tells us the average amount of total information needed to represent a stochastic event with  instead of . This is why cross entropy is a sum of the entropy of the distribution  plus the KL divergence between  and . Instead of dwelling in the theoretical realm regurgitating different definitions and interpretations of cross entropy and KL divergence, let’s take a look at a realistic example to gain a better grasp of these concepts. Say we have constructed a neural network to solve a task, such as MNIST hand-written digit classification. Let’s say we have fed our neural network an image corresponding to the number 2. In that case, the true distribution that we are trying to model, represented in vector form, will be  as shown below. The  statement is there to make sure that the probabilities sum up to 1. Let’s assume that our neural network made the following prediction about image. These two distributions, although similar, are different. But the question is, how different? Creating a visualization might give us some idea about the difference between the two distributions.  The two distributions are quite similar, meaning that our neural network did a good job of classifying given data.",0,0,1,0,0,0,0,0
Convex Combinations and MAP,"And as we obtain larger quantities of data, the relative importance of the prior distribution starts to diminish. Imagine that we have an infinite number of data points. Then,  will soley be determined by the likelihood function, as the weight ascribed to the prior will decrease to zero. In other words, Conversely, we can imagine how having no data points at all would cause the weight values to shift in favor of the prior such that no importance is ascribed to the MLE estimate of the parameter. In this short article, we reviewed the concept of maximum a posteriori and developed a useful intuition about its result from the perspective of convex combinations. Maximum a posteriori, alongside its close relative maximum likelihood estimation,  is an interesting topic that deserives our attention. Hopefully through this post, you gained a better understanding of what the result of an MAP estimation actually means from a Bayesian point of view: a weighted average between the prior mean and the MLE estimate, where the weight is determined by the number of data points at our disposal. Also, I just thought that the name “convex conbinations” is pretty cool. The fancier the name, the more desire to learn—it is a natural human instinct indeed. I hope you enjoyed reading this article. In the next post, we will continue our discussion by exploring the concept of conjugate priors, which is something that we have always assumed as true and glossed over, yet is arguably what remains at the very core of Bayesian analysis.",0,0,1,0,0,0,0,0
Scikit-learn Pipelines with Titanic,"Before proceeding with any data analysis, it’s always a good idea to pay attention to missing values—how many of them there are, where they occur, et cetera. Let’s take a look. The  is useful, but is doesn’t really show us how many values are missing for each column. To probe into this issue in more detail, we need to use  instead. I recently realized that there is also a very cool data visualization library called  for observing missing data.  This visualization gives us a more intuitive sense of where the values are missing. In this case, the missing values seem to be distributed somewhat evenly or randomly. However, we can also imagine cases were missing values might have something to do with an inherent attribute in the dataset (e.g. only male participants of a survey might reply “N/A” to some health questionaire involving inquiries on pregnancy). In such cases, using this library to visualize where missing values occur is a good idea, as this is an additional dimension of information that calling  wouldn’t be able to reveal. Now that we have a rough sense of where missing values occur, we need to decide from one of few choices: Indeed, in this case, we will go ahead and drop the  attribute. This choice becomes more obvious when we compute the percentage of null values. This shows us that 77 percent of the rows have missing  attribute values. Given this information, it’s probably a bad idea to try and impute these values.",0,0,0,1,0,0,0,0
"PyTorch, From Data to Modeling","However, I actually prefer this low-levelness because it requires me to really think through what is happening in each iteration, namely what the dimension of each batch is, what the model expects to receive as input in the forward computation, and what loss function is appropriate given the output and label. Let’s see what all of this means. First, we begin by actually initializing an instance of the model, a loss function named , and an , which is Adam in this case. A quick note of caution: if you dig into the PyTorch documentation or look at other example classifiers, you will realize, like me, there are two loss functions you can typically use:  and , or negative log likelihood loss. The difference between the former and latter is that, while the former applies a softmax function to the output before calculating the actual loss, the latter does not. In our case, since we simply output the raw logits instead of applying a softmax calculation, we need to use the former. Let’s return where we were. Before we jump into training and defining the training loop, it’s always a good idea to see if the output of the model is what you’d expect. In this case, we can simply define some random dummy input and see if the output is correct. Now that we’ve verified the input and output dimensions, we can move onto defining the training loop.",0,0,0,0,0,1,0,0
"Beta, Bayes, and Multi-armed Bandits","Let’s take a look at the posterior distribution for each bandit. We can easily plot them using , as shown below. Notice that I have also included the uniform prior for reference purposes.  The result is as we would expect: the bandit with the highest success parameter of 0.7 seems to have been pulled on the most, which explains why its variance is the smallest out of the bunch. Moreover, the mean of that particular posterior is also close to 0.7, its true value. Notice that the rest of the posteriors also somewhat have this trend, although more uncertainty is reflected into the shape of the distributions via the spread. It is interesting to see how we can go from a uniform prior, or , to almost normal-shaped distributions as we see above. To corroborate our intuition that the best bandit was indeed the most pulled, let’s quickly see the proportion of the pulls through a pie chart.  As you can see, there seems to be a positive correlation between the success parameter and the number of pulls. This is certainly good, since we want the gambler to pull on the best bandit the majority of times while avoiding the worse ones as much as possible. This certainly seems to be the case, given that the bandit with the worse parameter—0.1—was pulled the least. Last but not least, let’s revisit the cumulative regret graph introduced earlier.",0,0,1,0,1,0,0,0
Markov Chain and Chutes and Ladders,"In recursive form, this statement can be expressed as follows: The math-inclined thinkers in this room might consider the possibility of conducting an eigendecomposition on the stochastic matrix to simply the calculation of matrix powers. There is merit to considering this proposition, although later on we will see that this approach is inapplicable to the current case. Eigendecomposition refers to a specific method of factorizing a matrix in terms of its eigenvalues and eigenvectors. Let’s begin the derivation: let  be the matrix of interest,  a matrix whose columns are eigenvectors of , and , a matrix whose diagonal entries are the corresponding eigenvalues of . Let’s consider the result of multiplying  and . If we view multiplication as a repetition of matrix-times-vector operations, we yield the following result. But recall that  are eigenvectors of , which necessarily implies that Therefore, the result of  can be rearranged and unpacked in terms of : 
 In short, Therefore, we have , which is the formula for eigendecomposition of a matrix. One of the beauties of eigendecomposition is that it allows us to compute matrix powers very easily. Concretely, Because  and  nicely cross out, all we have to compute boils down to ! This is certainly good news for us, since our end goal is to compute powers of the stochastic matrix to simulate the Markov chain. However, an important assumption behind eigendecomposition is that it can only be performed on nonsingular matrices.",0,1,0,0,0,0,0,0
The Magic of Euler’s Identity,"We can somewhat intuit this through Euler’s identity, which is basically telling us that there exists some inextricable relationship between real and imaginary numbers. Understood from this point of view, we see that the power operation can be defined in the entire space that is complex numbers. We can also take logarithms of negative numbers. This can simply be shown by starting from Euler’s identity and taking the natural log on both sides. In fact, because  is a periodic function around the unit circle, any odd multiple of  will give us the same result. While it is true that logarithmic functions are undefined for negative numbers, this proposition is only true in the context of real numbers. Once we move onto the complex plane, what may appear as unintuitive and mind-boggling operations suddenly make mathematical sense. This is precisely the magic of Euler’s identity: the marriage of different numbers throughout the number system, blending them together in such a way that seems so simple, yet so incomprehensibly complex and profound.",0,0,0,0,0,0,0,1
Convex Combinations and MAP,"In a previous post, we briefly explored the notion of maximum a posteriori and how it relates to maximum likelihood estimation. Specifically, we derived a generic formula for MAP and explored how it compares to that for MLE. Today’s post is going to be an interesting sequel to that story: by performing MAP on the univariate Gaussian, we will show how MAP can be interpreted as a convex combination, thus motivating a more intuitive understanding of what MAP actually entails under the hood. Let’s jump right into it. The univariate Gaussian is a good example to work with because it is simple and intuitive yet also complex enough for meaningful analysis. After all, it is one of the most widely used probability distributions and also one that models many natural phenomena. With that justification firmly in mind, let’s take a look at the setup of the MAP of the  mean for the univariate Gaussian. As always, we begin with some dataset of  independent observations. In this case, because we are dealing with the univariate Gaussian, each observations will simply be a scalar instead of a vector. In other words, Let’s assume that the random variable is normally distributed according to some parameter . We will assume that the standard deviation of the random variable is given as . We could have considered standard deviation to be a parameter, but since the goal of this demonstration is to conduct MAP estimation on the mean of the univariate Gaussian, we assume that the standard deviation is known.",0,0,1,0,0,0,0,0
Wonders of Monte Carlo,"There are a plethora of mathematical techniques that build on top of crude Monte Carlo to ensure that sampling is done correctly and more efficiently, such as importance sampling, but for the purposes of this post, we will stop here and move onto the last task: simulating random walk. The last task we will deal with in this post is simulating what is known as the drunkard’s walk, a version of which is introduced here. The drunkard’s walk is a type of random walk with a specified termination condition. As the name suggests, the drunkard’s walk involves a little story of an intoxicated man trying to reach (or avoid) some destination, whether that be a cliff or, in our case, a restroom. Because he is drunk, he cannot walk to the restroom in a straight path as a normal person would do; instead, he stumbles this way and that, therefore producing a random walk. Our goal here is to simulate this motion many times to estimate the probability that the man would successfully end up in the restroom to go about his business. This example was borrowed from this post by Zacharia Miller. Before we start typing up some code, let’s first lay down the ground rules of this simulation. First, we assume that the pub is modeled as a ten-by-ten grid, the bottom-left point defined as  and the top-right . The drunkard will start his walk at his table, represented by the coordinate .",0,0,1,0,0,0,0,0
Markov Chain and Chutes and Ladders,"Because the multiplying any arbitrary vector by the identity matrix returns the vector itself, all vectors in the dimensional space can be considered an eigenvector to the matrix , with  = 1. A formal way to calculate eigenvectors and eigenvalues can be derived from the equation above. Since  is assumed as a nonzero vector, we can deduce that the matrix  is a singular matrix with a nontrivial null space. In fact, the vectors in this null space are precisely the eigenvectors that we are looking for. Here, it is useful to recall that the a way to determine the singularity of a matrix is by calculating its determinant. Using these set of observations, we can modify the equation above to the following form: By calculating the determinant of , we can derive the characteristic polynomial, from which we can obtain the set of eigenvectors for  representing some linear transformation . Now that we have reviewed some underlying concepts, perhaps it is time to apply our knowledge to a concrete example. Before we move on, I recommend that you check out this post I have written on the Markov process, just so that you are comfortable with the material to be presented in this section. In this post, we turn our attention to the game of Chutes and Ladders, which is an example of a Markov process which demonstrates the property of “memorylessness.",0,1,0,0,0,0,0,0
A PyTorch Primer,"Let’s remind ourselves of Kera’s basic sequential model API: Now let’s compare this method with PyTorch’s way of declaring sequential models: This model declaration is in fact exactly identical to the simple model we have declared above. You can easily see how similar this code snippet is to the Keras example. The only difference is that the activation function is declared independently of the layer itself in PyTorch, whereas Keras combines them into one via  argument. Of course, you don’t have to specify this argument, and we can import the ReLU function from TensorFlow to make it explicit like the PyTorch example. The point, however, is that the sequential model API for both libraries are pretty similar. Another way to build models is by subclassing . The  submodule in PyTorch is the one that deals with neural networks; hence the . This subclassing might look as follows: This model is no different from the  we defined earlier. The only notable difference is that we didn’t define a separate  type function. For the most part, the overall idea boils down to Now let’s take a look at what the training code looks like. Although things might look a bit different, there’s not much going on in this process, other than the fact that some of the functions and logic we wrote before are now abstracted away by PyTorch. For example, we see , which is effectively the mean squared error loss, similar to how we defined  above.",0,0,0,0,0,1,0,0
The Exponential Family,"As you might recall from a previous post on some very basic matrix calculus, the gradient is simply a way of packaging derivatives in a multivariate context, typically involving vectors. If any of this sounds unfamilar, I highly recommend that you check out the linked post. We can compute the partial derivative of the log likelihood function with respect to  as shown below. Observe that the last term in (18) is eliminated because it is a constant with respect to . This is a good starting point, but we still have no idea how to derive the log of . To go about this problem, we have to derive an expression for . Recall from the definition of the exponential family that  is a normalizing constant that exists to ensure that the probability function integrates to one. In other words, This necessarily implies that Now that we have an expression for  to work with, let’s try to compute the derivative term we left unsolved in (19). The first and second equalities stand due to the chain rule, and the third equality is a simple algebraic manipulation that recreates the probability function within the integral, allowing us to ultimately express the partial derivative as an expected value of  for the random variable . This is a surprising result, and a convenient one indeed, because we can now use this observation to conclude that the gradient of the log likelihood function is simply the expected value of the sufficient statistic.",0,0,1,0,1,0,0,0
First Neural Network with Keras,"Note that the hidden layer uses the  function as its activation function. The dropout layers ensure that our model does not overfit to the data. The last output layer has 10 neurons, each corresponding to digits from 0 to 9. The activation fuction of this last layer is the softmax function, which allows us to interpret the final results as a categorical distribution. Let’s double check that the layers have been formed correctly as per our intended design. Everything looks good, which means we are now ready to compile and train our model. Before we do that, however, it is always a good idea to use the  module to ensure that gradient descent stops when no substantial weight adjustments are being made to our model. In other words, when the model successfully finds the local minimum (or preferably the global minimum), the  will kick in and stop gradient descent from proceeding with further epochs. We are now ready to go! Let’s compile the model by making some configurations, namely the , , and . Simply put, an  specifies which flavor of the gradient descent algorithm we want to choose. The simplest version is known as , or the stochastic gradient descent.  can be considered an improved version of the stochastic gradient descent in that its learning rate changes depending on the slope of the loss function, defined here as cross entropy. If you recall, cross entropy is basically a measurement of the pseudo-distance between two distributions, i.e. how different two distributions are.",1,0,0,1,0,1,0,0
"Beta, Bayes, and Multi-armed Bandits","Recently, I fortuitously came across an interesting blog post on the multi-armed bandit problem, or MAB for short. I say fortuitous because the contents of this blog nicely coincided with a post I had meant to write for a very long time: revisiting the Beta distribution, conjugate priors, and all that good stuff. I decided that the MAB would be a refreshing way to discuss this topic. “Bayesian” is a buzz word that statisticians and ML people love anyway, me shamelessly included. In this post, we will start off with a brief introduction into what the MAB problem is, why it is relevant, and how we can use some basic Bayesian analysis with Beta and Bernoulli distributions to derive a nice sampling algorithm, known as Thompson sampling. Let’s dive right into it. The multi-armed bandit problem is a classical gambling setup in which a gambler has the choice of pulling the lever of any one of  slot machines, or bandits. The probability of winning for each slot machine is fixed, but of course the gambler has no idea what these probabilities are. To ascertain out the values of these parameters, the gambler must learn through some trial and error. Given this setup, what is the optimal way of going about the problem? One obvious way is to start randomly pulling on some slot machines until they get a rough idea of what the success probabilities are.",0,0,1,0,1,0,0,0
A PyTorch Primer,"To build our simple model, let’s first write out some variables to use, starting with the configuration of our model and its dimensions. We will also need some input and output tensors to be fed into the model for trainining and optimization. Next, here are the weight matrices we will use. For now, we assume a simple two layered dense feed forward network. Last but not least, let’s define a simple squared error loss function to use during the training step. With this entire setup, we can now hash out what the entire training iteration is going to look like. Wrapped in a loop, we perform one forward pass, then perform backpropagation to adjust the weights. Great! We see that the loss drops as more epochs elapse. While there is no problem with this approach, things can get a lot more unwieldy once we start building out more complicated models. In these cases, we will want to use the auto differentiation functionality we reviewed earlier. Let’s see this in action. Also, let’s make this more PyTorch-y by making use of classes. We will revisit why class-based implementations are important in the next section. Notice we didn’t have to explicitly specify the backpropagation formula with matrix derivatives: by simply calling  properties for each of the weights matrices, we were able to perform gradient descent.",0,0,0,0,0,1,0,0
An Introduction to Markov Chain Monte Carlo,"The objective of Bayesian statistical analysis is to update our beliefs about some probability, known as the posterior, given a preestablished belief, called the prior, and a series of data observations, which might be decomposed into likelihood and evidence. Concretely, This statement is equivalent to where  denotes the likelihood function. In plain language, Bayesian statistics operates on the assumption that all probabilities are reflections of subjective beliefs about the distribution of some random variable. A prior expectation or belief we might have about this distribution is referred to as the prior. Then, we can update our prior belief based on sample observations, resulting in a posterior distribution. Roughly speaking, the posterior can be considered as the “average” between the prior and the observed data. This process, which we went over in detail in this post, is at the heart of Bayesian inference, a powerful tool through which data and distributions can be understood. Theoretically, everything should work fine: given some prior and some sample observation data, we should be able to derive a posterior distribution for the random variable of interest. No big deal. Or so it seems. If we take a look again at equation (3), we will realize that there is an evidence term that we have to calculate sitting in the denominator. The formula for evidence can be expressed as follows: Computing this quantity is not as easy as it may appear.",0,0,0,0,0,0,1,0
Riemann Zeta and Prime Numbers,"The other day, I came across an interesting article by Chris Henson on the relationship between the Riemann Zeta function and prime numbers. After encountering a similar post on the math Stack Exchange, I thought I’d write an article on the same topic as well, perhaps as sort of a prologue to a previous article posted on this blog, Basel, Zeta, and some more Euler. The code introduced in this blog are adaptations of those written by Chris Henson, so all credits go to the original author. With that said, let’s dive right into it. The Riemann Zeta function is perhaps one of the most deeply studied functions in all of modern mathematics. It is sometimes also referred to as the Euler-Riemann Zeta function, but I will not adhere to this convention not only because it is needlessly long and cumbersome, but also because Euler already has so many constants and functions and theorems bearing his name. Anyhow, the Riemann Zeta function looks as follows: While there are so many layers to explore with this function, one relatively simple and interesting route is to factorize this function. We performed this factorization in the previous post while introducing Euler’s infinite product representation of the Zeta function. Let’s go through this step again, as it is pivotal for our segue into the topic of prime numbers and probability.",0,0,0,0,0,0,0,1
A Step Up with  Variational Autoencoders,"I say almost, because we still have not dealt with the trailing term in (13): At this point, it is extremely useful to recall the definition of cross entropy, which is generically defined as follows: Therefore, we see that the trailing term in (13) is just a cross entropy between two distributions! This was a circumlocutions journey, but that is enough math we will need for this tutorial. It’s time to get back to coding. All that math was for this simple code snippet shown below: As you can see, this short code snippet shows, in essence, how we can define a compile a model with a custom loss function. In this case,  refers to the reconstruction loss, which is the cross entropy term we saw earlier. , as you might expect, simply refers to KL divergence. Notice how there is a  multiplying factor in the  expression, just like we did when we derived it in the section above. With some keen observations and comparisons, you will easily see that the code is merely a transcription of (13), with some minor differences given dimensionality. One important fact to note is that the gradient descent algorithm, by default, seeks to minimize the loss function. However, we discussed above how the objective of VAE is to maximize ELBO. Therefore, we modify ELBO into a loss function that is to be minimized by defining the loss function as the negative of ELBO. In other words, the cost function  is defined as ; hence the difference in sign.",1,0,0,0,0,1,0,0
Gaussian Process Regression,"Simply put, this means that we don’t have to consider things like the typical  in the context of linear regression. Normally, we would start off with something like This is sometimes also written in terms of a weight vector  or a function . Here, we also have some Gaussian noise, denoted by : However, since GPs are non-parametric, we do not have to specify anything about the model. How do we remove this consideration? The short answer is that we marginalize out the model from the integral. Let  denote the model,  the data,  the predictions. Normally, an integral like the one above would be intractable without a solution in closed form. Hence, we would have to rely on random sampling methods such as MCMC. However, in this case, we do have a closed form solution, and we know what it looks like: a Gaussian! This means that we uniquely identify the final posterior distribution through GP regression—all we need is the mean and covariance. Let’s start with the easy one first: the mean. The mean is a trivial parameter because we can always normalize the mean to zero by subtracting the mean from the data. Therefore, for simplicity purposes, we assume a zero mean throughout this post. The interesting part lies in the covariance. Recall that the covariance matrix is defined as follows: Roughly speaking, covariance tells us how correlated two entries of the random vector are.",0,0,0,1,0,0,1,0
BLEU from scratch,"Now we’re almost done. The last example to consider is the following translation: This is obviously a bad translation. However, due to the way modified precision is currently being calculated, this sentence will likely earn a high score. To prevent this from happening, we need to apply what is known as brevity penalty. As the name implies, this penalizes short candidate translations, thus ensuring that only sufficiently long machine translations are ascribed a high score. Although this might seem confusing, the underlying mechanism is quite simple. The goal is to find the length of the reference sentence whose length is closest to that of the candidate translation in question. If the length of that reference sentence is larger than the candidate sentence, we apply some penalty; if the candidate sentence is longer, than we do not apply any penalization. The specific formula for penalization looks as follows: The brevity penalty term is multiplied to the n-gram modified precision. Therefore, a value of 1 means that no penalization is applied. Let’s perform a quick sanity check to see whether the brevity penalty function works as expected. Finally, it’s time to put all the pieces together. The formula for BLEU can be written as follows: First, some notation clarifications.  specifies the size of the bag of word, or the n-gram.  denotes the weight we will ascribe to the modified precision——produced under that -gram configuration. In other words, we calculate the weighted average of log precision, exponentiate that sum, and apply some brevity penalty.",0,0,0,0,0,1,1,0
Maximum A Posteriori Estimation,"In a previous post on likelihood, we explored the concept of maximum likelihood estimation, a technique used to optimize parameters of a distribution. In today’s post, we will take a look at another technique, known as maximum a posteriori estimation, or MAP for short. MLE and MAP are distinct methods, but they are more similar than different. We will explore the similar mathematical underpinnings behind the methods to gain a better understanding of how distributions can be tweaked to best fit some given data. Let’s begin! Before we jump right into comparing MAP and MLE, let’s refresh our memory on how maximum likelihood estimation worked. Recall that likelihood is defined as In other words, the likelihood of some model parameter  given data observations  is equal to the probability of seeing  given . Thus, likelihood and probability are inevitably related concepts that describe the same landscape, only from different angles. The objective of maximum likelihood estimation, then, is to determine the values for a distribution’s parameters such that the likelihood of observing some given data is maximized under that distribution. In the example in the previous post on likelihoods, we showed that MLE for a normal distribution is equivalent to setting  as the sample mean; , sample variance. But this convenient case was specific only to the Gaussian distribution.",0,0,1,0,0,0,0,0
"Newton-Raphson, Secant, and More","It’s time to put the methods we developed in the preceding sections to use for solving non-linear equations. Specifically, we’ll begin by taking look at a classic algorithm, the Newton-Raphson method. The Newton-Raphson method is one of the many ways of solving non-linear equations. The intuition behind the Newton-Raphson method is pretty straightforward: we can use tangent lines to approximate the x-intercept, which is effectively the root of the equation . Specifically, we begin on some point on the graph, then obtain the tangent line on that point. Then, we obtain the -intercept of that tangent line, and repeat the process we’ve just completed by starting on a point on the graph whose -value is equal to that -intercept. The following image from Wikipedia illustrates this process quite well. (A digression: It’s interesting to see how “function” and “tangent” are written in German—in case you are wondering, I don’t know a word of German.)  Mathematically, the Newton-Raphson method can be expressed recursively as follows: Deriving this formula is quite simple. Say we start at a point on the graph, . The tangent line from that point will have a slope of . Therefore, the equation of the tangent line can be expressed as Then, the -intercept can simpy be obtained by finding an  value that which makes . Let  denote that point. Then, we arrive at the following update rule. Since we will be using  as the value for the next iteration, , and now we have the update rule as delineated in (4).",0,0,0,0,0,0,0,1
The Exponential Family,"In this sense, the exponential family is particularly of paramount importance in the field of Bayesian inference, as we have seen many times in previous posts. Let’s concretize our understanding of the exponential family by applying factorization to actual probability distributions. The easiest example, as you might have guessed, is the exponential distribution. Recall that the formula for the exponential distribution is where the indicator function, denoted as , takes the following form: The indicator function is a simple  modification applied to ensure that the function is well-defined across the entire real number domain. Normally, we omit the indicator function since it is self-apparent, but for the sake of robustness in our analysis, I have added it here. How can we coerce equation (6) to look more like (3), the archetypal form that defines the exponential family? Well, now it’s just a matter of drag and match: by paying close attention to the variables, parameters, and the output of each function, we can reconstruct (3) to take the form of (6). The easeist starting point is to observe the exponent to identify  and , after which the rest of the surrounding functions can be inferred. The end result is presented below: After substituting each function with their prescribed value in (8), it isn’t difficult to see that the exponential distribution can indeed by factorized according to the form outlined in (3).",0,0,1,0,1,0,0,0
Logistic Regression Model from Scratch,"There is one more tiny little step we have to make to concretize this equation, and that is to consider the average of the total gradient, since (13) as it stands applies to only one data observation. Granted, this derivation is not meant to be a rigorous demonstration of mathematical proof, because we glossed over some details concerning matrix transpose, dot products, and dimensionality. Still, it provides a solid basis for the construction of the gradient descent algorithm in code, as shown below. To avoid compensating code readability, I made a stylistic choice of using  and  to denote the vector of coefficients instead of using  for notational consistency. Other than adding some switch optional parameters such as  or , the code simply follows the gradient descent algorithm outlined above. Note that equation (6) is expressed via ; equation (14) is expressed by the line . Let’s quickly check that the  function works as expected using the dummy data we created earlier. Great! We see that the average cross entropy decreases with more iterations. The returned  array contains the coefficients of the logistic regression model, which we can use to now make predictions. We can stop here, but just like we did in the post on k-nearest neighbors, let’s wrap all the functions we have created so far into a single function that represents the logistic regression model. Our model is ready. Time for testing with some real-world data. Let’s import some data from the web.",0,0,0,1,0,0,1,0
A Simple Autocomplete Model,"the charm surpose again, in 
swret feathryst, form of kinne of the world bejud–age–implaasoun ever? but that the is any 
appearance has clenge: the? a plexable gen preducl=s than condugebleines and aligh to advirenta-nasure; 
findiminal it as, not take. the ideved towards upavanizing, would be 
thenion, in all pespres: it is of 
a concidenary, which, well founly con-utbacte udwerlly upon mansing–frauble of “arrey been can the pritarnated from their 
christian often–think prestation of mocives.” legt, lenge:–this deps 
telows, plenhance of decessaticrances). hyrk an interlusally” tone–under good haggy,” 
is have we leamness of conschous should it, of 
sicking ummenfeckinal zerturm erienweron of noble of 
himself-clonizing there is conctumendable prefersy 
exaitunia states,” whether 
they deve oves any of hispyssesss. int The results are fascinating. Granted, our model is still bad at immitating Nietzsche’s style of writing, but I think the performance is impressive given that this was a character-based text generation model. Think about it for a second: to write even a single word, say “present,” the model has to correctly predict “p”, “r”, “e”, “s”, “e”, “n”,  and “t,” all in tandem. Imagine doing this for extended cycles, long enough to generate text that is comfortably a paragraph long. It’s amazing how the text it generates even makes some sense at all. Then, as temperature rises, we see more randomness and “creativity” at work.",1,0,0,0,0,1,0,0
Natural Gradient and Fisher,"Conceptually, we can think of  as the previous point of the parameter and  as the newly updated parameter. In this context, the KL divergence would tell us the effect of one iteration of natural gradient descent. This time, instead of using integral, let’s try to simplify a bit by expressing quantities as expectations. We see the familiar log likelihood term. Given the fact that the Fisher matrix is the negative expected Hessian of the log likelihood, we should be itching to derive this expression twice to get a Hessian out of it. Let’s first obtain the gradient, then get its Jacobian to derive a Hessian. This derivation process was heavily referenced from Agustinus Kristiadi’s blog. Let’s do this one more time to get the Hessian. This conclusion tells us that the curvature of KL divergence is defined by Fisher’s matrix. In hindsight, this is not such a surprising result given that the KL divergence literally had a term for expected log likelihood. Applying the Leibniz rule twice to move the derivative into the integral, we quickly end up with Fisher’s matrix. At this point, you might be wondering about the implications of this conclusion. It’s great that KL divergence and the Fisher matrix are closely related via the Hessian, but what implication does it have for the gradient descent algorithm in distribution space? To answer this question, we first need to perform a quick multivariate second order Taylor expansion on KL divergence.",0,0,1,1,0,0,0,0
MLE and KL Divergence,"These days, I’ve been spending some time trying to read published research papers on neural networks to gain a more solid understanding of the math behind deep learning. This is a rewarding yet also a very challenging endeavor, mostly because I have not studied enough math to really understand all of what is going on. While reading the groundbreaking research paper Wasserstein GAN by Martin Arjovsky, I came across this phrase: … asymptotically, maximum likelihood estimation amounts to minimizing the Kullback-Leibler divergence… I was particularly interested in the last portion of this sentence, that MLE amounts to minimizing KL divergence. We discussed MLE multiple time on this blog, including this introductory post and a related post on MAP. Neither is KL divergence an entirely unfamiliar topic. However, I had not thought about these two concepts together in one setting. In this post, let’s try to hash out what the quote from the paper means. Let’s start with a very quick review of what MLE and KL divergence each are. After all, it’s been a while since I’ve written the linked posts, and for a fruitful, substantive discussion on this topic, it’s necessary to make sure that we have a solid grasp of what MLE and KL divergence are. MLE is a technique used to find the optimal parameter of a distribution that best describes a set of data. To cut to the chase, this statement can be expressed as follows: From here, we can start making assumptions, such as that observations in  are i.i.",0,0,1,0,0,0,0,0
On Expectations and Integrals,"Expectation is a core concept in statistics, and it is no surprise that any student interested in probability and statistics may have seen some expression like this: In the continuous case, the expression is most commonly presented in textbooks as follows: However, this variant might throw you off, which happened to me when I first came across it a few weeks ago: I mean, my calculus is rusty, but it kind of makes sense: the probably density function is, after all, a derivative of the cumulative density function, and so notationally there is some degree of coherency here. But still, this definition of the expected value threw me off quite a bit. What does it mean to integrate over a distribution function instead of a variable? After some research, however, the math gurus at Stack Exchange provided me with an answer. So here is a brief summary of my findings. The integral that we all know of is called the Riemann integral. The confusing integral is in fact a generalization of the Riemann integral, known as the Riemann-Stieltjes integral (don’t ask me how to pronounce the name of the Dutch mathematician). There is an even more general interpretation of integrals called the Lebesgue integral, but we won’t get into that here. First, let’s take a look at the definition. The definition of the integral is actually a lot simpler than what one might imagine. Here,  is a value that falls within the interval . In short, we divide the interval of integration  into  infinitesimal pieces.",0,0,1,0,0,0,0,0
Logistic Regression Model from Scratch,"If the output of an algorithm given some data point is larger than 0.5, it is likely that the given input is a spam; if it is smaller than the 0.5 threshold, chances are the email is not spam. Let’s take a look at the shape of the sigmoid function, which is a special case of the logistic function that we will use throughout this post. To plot the sigmoid function, we need to import some libraries. The sigmoid function is defined as follows: We can express this as a Python function, as demonstrated in the code snippet below. Let’s quickly plot the graph to see what the sigmoid function looks like.  As we can see, the sigmoid is a smooth, differentiable function that is bounded between 0 and 1. It is also symmetrical around the point , which is why we can use 0.5 as a threshold for determining the class of a given data point. The logistic regression model uses the sigmoid function to generate predictions, but how exactly does it work? Recall that, in the case of linear regression, our goal was to determine the coefficients of some linear function, specifically Logistic regression is not so different from linear regression.",0,0,0,1,0,0,1,0
A Brief Introduction to Recurrent Neural Networks,"When we read, we don’t process a given text at once in its totality; instead, we break them down into pieces, such as a word or a bag of words, and build our understanding based on information obtained from the previous sequence of text. In other words, processing information through reading is at best understood as a process of continuously receiving new information while retaining information obtained from the previous sequence. This is why recurrent neural network models are frequently employed in the context of natural language processing. But the applications of RNNs extends beyond the domain of NLP. For example, say we are given a dataset of temperature recording of a city district. Obviously, the structural integrity of that dataset is very important, i.e. we should not shuffle the datasets because making weather predictions requires us to understand temporal patterns. In predicting the weather 24 hours from today, data points pertaining to the last two days will be obviously much more important that those recorded a month ago. In such instances of time series analysis, recurrent neural networks perform better than other models we have looked at thus far. To better understand how RNNs work, let’s try to build a very simple recurrent neural network from scratch with . We will only implement forward propagation for the sake of simplicity,  but with some matrix calculus, one can imagine how deriving the formula for back propagation will be possible.",1,0,0,0,0,1,0,0
BLEU from scratch,"This is where BLEU comes to the rescue. The way BLEU works is simple. Given some candidate translation of a sentence and a group of reference sentences, we use a bag-of-word approach to see how many occurences of BOWs co-occur in both the translation and reference sentences. BOW is a simple yet highly effective way of ensuring that the machine translation contains key phrases or words that reference translations also contain. In other words, BLEU compares candidate translations with human-produced, annotated reference translations and compares how many hits there are in the candidate sentence. The more BOW hits there are, the better the translation. Of course, there are many more details that go beyond this. For instance, BLEU is able to account for situations in which meaningless words are repeated throughout the machine translation to simply increase BOW hits. It can also penalize translations that are too short. By combining this BOW precision-based approach with some penalization terms, BLEU provides a robust means of evaluating machine translations. With this high-level overview in mind, let’s start implementing BLEU from scratch. First, let’s begin by defining some simple preprocessing and helper functions that we will be using throughout this tutorial. The first on the list is , which converts a given sentence into lowercase and splits it into tokens, which are, in this case, English words. We could make this more robust using regular expressions to remove punctuations, but for the purposes of this demonstration, let’s make this simpler.",0,0,0,0,0,1,1,0
On Expectations and Integrals,"In fact, it is possible to calculate the expectation using the Riemann-Stieltjes integral quite easily, despite the discontinuity! The integral we wish to calculate is the following: Therefore, we should immediately start visualizing splitting up the domain of integration, the real number line, into infinitesimal pieces. Each box will be of height  and width . In the context of the contrived example, this definition makes the calculation extremely easy, since   equals zero in all locations but the jumps where the discontinuities occur. In other words, We can easily extend this idea to calculating things like variance or other higher moments. A more realistic example might be the Dirac delta function. Consider a constant random variable (I know it sounds oxymoronic, but the idea is that the random variable takes only one value and that value only). In this case, we can imagine the probability density function as a literal spike in the sense that the PDF will peak at  and be zero otherwise. The cumulative density function will thus exhibit a discontinuous jump from zero to 1 at . And by the same line of logic, it is easy to see that the expected value of this random variable is , as expected. Although this is a rather boring example in that the expectation of a constant is of course the constant itself, it nonetheless demonstrates the potential applications of Riemann-Stieltjes. I hope you enjoyed reading this post. Lately, I have been busy working on some interesting projects.",0,0,1,0,0,0,0,0
Markov Chain and Chutes and Ladders,"The indexing is key here: for each column, th rows were assigned the probability of . Let’s say that a player is in the th cell. Assuming no chutes or ladders, a single roll of a dice will place him at one of the cells from  to ; hence the indexing as presented above. However, this algorithm has to be modified for  bigger or equal to 95. For example if , there are only three probabilities: , , and , each of values , , and  respectively. The  statements are additional corrective mechanisms to account for this irregularity. So now we’re done with the stochastic matrix! … or not quite. Things get a bit more complicated once we throw the chutes and ladders into the mix. To achieve this, we first build a dictionary containing information on the jump from one cell to another. In this dictionary, the keys correspond to the original position; the values, the index of the cell after the jump, either through a chute or a ladder. For example,  represents the first ladder on the game board, which moves the player from the first cell to the thirty eighth cell. To integrate this new piece of information into our code, we need to build a permutation matrix that essentially “shuffles up” the entries of the stochastic matrix  in such a way that the probabilities can be assigned to the appropriate entries.",0,1,0,0,0,0,0,0
Fisher Score and Information,"Therefore, given some dataset, often times we use the empirical Fisher as a drop-in substitute for Fisher’s information. The empirical Fisher is defined quite simply as follows: In other words, it is simply an unweighted average of the covariance of the score function for each observed data point. Although this is a subtlety, it helps to clarify nonetheless. Something that may not be immediately apparent yet nonetheless true and very important about Fisher’s information is the fact that it is the negative expected value of the second derivative of the log likelihood. In our multivariate context where  is a vector, the second derivative is effectively the Hessian. In other words, You might be wondering how the information matrix can be defined in two says, the covariance and the Hessian. Indeed, this threw me off quite a bit as well, and I struggled to find and understand a good resource that explained why this was the case. Thankfully, Mark Reid’s blog and an MIT lecture contained some very helpful pointers that got me a long way. The derivation is not the easiest, but I’ll try to provide a concise version based on my admittedly limited understanding of this topic. Let’s start from some trivially obvious statements. First, from the definition of a PDF and the derivative operation, we know that Therefore, both the first and second derivative of this function are going to be zero. In multivariate speak, both the gradient and the Hessian are zero vectors and matrices, respectively.",0,0,1,0,0,0,0,0
Traveling Salesman Problem with Genetic Algorithms,"Let’s see if this everything works as expected by generating a dummy population. Now we need some function that will determine the fitness of a chromosome. In the context of TSP, fitness is defined in very simple terms: the shorter the total distance, the fitter and more superior the chromosome. Recall that all the distance information we need is nicely stored in . We can calculate the sum of all the distances between two adjacent cities in the chromosome sequence. Next, we evaluate the population. Simply put, evaluation amounts to calculating the fitness of each chromosome in the total population, determining who is best, storing the score information, and returning some probability vector whose each element represents the probability that the th element in the population bag is chosen as a parent. We apply some basic preprocessing to ensure that the worst performing chromosome has absolutely no chance of being selected. When we call , we get a probability vector as expected. From the result, it appears that the last element is the best chromosome; the second chromosome in the population bag is the worst. When we call , notice that we get the last element in the population, as previously anticipated. We can also access the score of the best chromosome. In this case, the distance is said to be 86.25. Note that the lower the score, the better, since these scores represent the total distance a salesman has to travel to visit all the cities.",0,0,0,0,0,0,1,0
k-Nearest Neighbors Algorithm from Scratch,"We can achieve this by building a function as shown below. Passing 100 to the  argument results in a list of accuracy scores. We can go through this list and try to see for which value of  accuracy is maximized. But this is a rather tedious job, and things would get quickly out of control if we were to deal with much larger data sets where the value of  can be set to much larger numbers. Instead, let’s create a visualization to see how accuracy changes with respect to .  The plot shows that accuracy is maximized for many values of , not just 1. Also, we can learn that accuracy does not go beyond the 97 percent we saw earlier, which is a bit of sad news. An interesting insight we can glean, however, is that accuracy seems to drop past some certain thresholds, most notably around 80. One reasonable explanation might be that the model is looking at too many neighbors that it cannot produce a reliable estimate. At any rate, this visualization shows that hyperparameter tuning is an important job of a machine learning engineer—even if the model is great, if the wrong  value is used, the model will only demonstrate lackluster performance. This was perhaps the first post where we dealt with a machine learning algorithm. ML is sometimes treated as a black box, where some magic beneath the hood produces desirable results.",0,0,0,1,0,0,1,0
Revisiting Basel with Fourier,"To recap, from a very high level, Fourier expansion is a way of expressing some function in terms of trigonometric functions. If Taylor expansion used polynomials as the building block, Fourier expansion uses sines and cosines. A generic formula for the Fourier transform can be expressed as follows: With some integration, it can be shown that where  refers to the domain of integration. For instance, if we are integrating from  to , . A classic interval that is most commonly used is , and this is no coincidence: notice that, when , the Taylor series shown in (2) simplifies into the following: And indeed this is the format and the interval we will be using when constructing a Fourier series to tackle the Basel problem. To continue, we can derive a very similar expression for , given the specified interval from . Now that we have reviewed what Fourier series is and how we can construct it, let’s jump into the Basel problem. Just like the Taylor series, we can use Fourier expansion to represent any function continuous function. For our purposes, let’s try to expand a simple polynomial function, , using Fourier. We can begin with . Let’s continue with finding the even coefficients corresponding to the cosines. With some integration by parts, we can all agree that where the  terms appear because we end up plugging  into , a periodic function. And we can do the same for sine.",0,0,0,0,0,0,0,1
Dissecting the Gaussian Distribution,"We finished at then moved onto a discussion of variance and covariance. Now that we understand that the covariance matrix is the analogue of variance, we can substitute  with , the covariate matrix. Instead of leaving  at the denominator, let’s use the fact that to rearrange the expression. This is another example of when the matrix-scalar parallel intuition can come in handy: the scalar multiplicative identity is 1, whereas the equivalent in matrix world is the identity matrix . Therefore, the reciprocal of a matrix can be interpreted as its inverse. From this observation, we can conclude that We are almost done, but not quite. Recall the the constant coefficient of the probability distribution originates from the fact that We have to make some adjustments to the constant coefficient since, in the context of the multivariate Gaussian, the integral translates into While it may not be apparent immediately, it is not hard to accept that the correcting coefficient in this case has to be as there are  layers of iterated integrals to evaluate for each  through . Instead of the matrix , we use its determinant  since we need the coefficient to be a constant, not a matrix term. We don’t go into much detail about the derivation of the constant term; the bottom line is that we want the integral of the probability distribution function over the relevant domain to converge to 1.",0,0,1,0,1,0,0,0
A Brief Introduction to Recurrent Neural Networks,"First, note that our model accepts input of size . The dimensionality of the output is a little more tricky because of the  option. What concatenate does is that it basically flattens all  number of outputs into a single list. In this case, because we set the option to , we get a flattened list containing , or 6400 elements. The main takeaway is that recurrent neural networks can be used to implement some sort of memory functionality, which is useful when dealing with datasets where there exists some sort of sequential structure. One way to implement memory is by using the output of the previous sequence to define a  variable, which is used to compute the next output as we have done above. Now let’s get down to business with the  API. Implementing a recurrent neural network is not so much different from building a simple feed forward or convolutional neural network: we simply import a RNN-specific layer and arrange these layers to construct a working model. Before we proceed any further, let’s first import all necessary dependencies for this tutorial. As with any tutorial, we need to start by loading and preprocessing data. Luckily, there isn’t going to be much preprocessing involved since we will be using a dataset available from the  library, the IMBD movie reviews dataset. The dataset contains 25,000 movie reviews from IMDB, labeled as either positive or negative. Each review is encoded as a sequence of integers according to a consistent encoding scheme.",1,0,0,0,0,1,0,0
Recommendation Algorithm with SVD,"This function tells us that our movie application should recommend to our user Movies 3 and 4, in that order. This result is not surprising given the fact that we have already observed the closeness between Movies 2 and 3—if a user likes Movie 2, we should definitely recommend Movie 3 to them. Our algorithm also tells us that the distance between Movie 2 and 4 is also pretty close, although not as close as the distance between Movies 2 and 3. What is happening behind the scene here? Our function simply calculates the distance between the vector representation of each movies as a dot product. If we were to print the local variable  array defined within the  function, for instance, we would see the following result. This tells us how close Movies 0, 1, 3, and 4 are with Movie 2. The larger the dot product, the closer the movie; hence, the more compelling that recommendation. The  function then sorts the  array and outputs the first  movies as a recommendation. Of course, we could think of an alternate implementation of this algorithm that makes use of the  matrix instead of , but that would be a slightly different recommendation system that uses past user’s movie ratings as information to predict whether or not the particular individual would like a given movie. As we can see, SVD can be used in countless ways in the domain of recommendation algorithms, which goes to show how powerful it is as a tool for data analysis.",0,1,0,0,0,0,1,0
Riemann Zeta and Prime Numbers,"The idea is that, much like we multiply the ratio to a geometric sequence to calculate its sum, we can multiply terms to the Zeta function to factor out their multiples. For instance, let’s consider the case when we multiply  to the Zeta function. Since the Zeta function itself is an infinite series, we can subtract this result from the original Zeta function. This effectively filters or sieves out all terms whose denominator is a multiple of 2, effectively leaving only the odd terms. Concretely, If we repeat this process for all prime numbers, we will eventually be left with the following: where  denotes the set of all prime numbers. From this, we can come up with the following alternative representation of the Riemann Zeta function: which can also be expressed as Other than the fact that the factorization of the Riemann Zeta function is satisfying in and of itself, the result in (5) also provides us with an interesting probabilistic interpretation on coprimeness. The intuition is pretty simple: given some random natural number , the probability that a prime  will divide  is simply . For example, if we come up with some random number, the probability that 2 will divide that number (that the number is even) is 0.5; the probability that the number will be a multiple of 3 is one-thirds.",0,0,0,0,0,0,0,1
Demystifying Entropy (And More),"The other day, my friend and I were talking about our mutual friend Jeremy. “He’s an oddball,” my friend Sean remarked, to which I agreed. Out of nowhere, Jeremy had just told us that he would not be coming back to Korea for the next three years. “He is just about the most random person I know.” And both of us, being aspiring statistics majors, began wondering: is there a systematic way of measuring randomness? It is from here that we went down the rabbit hole of Google and Wikipedia search. I ended up landing on entropy land, which is going to be the topic for today’s post. It’s a random post on the topic of randomness. To begin our discussion of randomness, let’s take a look at how scientists measure the randomness involved in natural phenomena, such as particle movement. If you are a chemist or physicist, you might be familiar with the concept of entropy, one of the core elements of thermodynamics and a topic that recurs throughout many subfields of natural sciences.",0,0,1,0,0,0,0,0
Wonders of Monte Carlo,"Notice that there is a 100-step cap, meaning if the drunkard was not able to find the restroom after a hundred steps, the trial was assumed a failure. We can verify the functionality of our design by calling the function. Cool! The Monte Carlo algorithm thus tells us that the probability of success is about 10 percent, which is a lot smaller thant I had personally anticipated. Think about how complicated it would have been to calculate this probability by hand. By simulating this game multiple times and counting the instances of successes, we can derive an estimation of the success rate of our particular random walk model. Let’s see what happens when we simulate the drunkard’s walk thirty times. In the particular instance that I have below, we see that the drunkard successfully reached the rest room four out of thirty attempts, which roughly equals the success probability of ten percent we saw earlier.  By now, hopefully you have been convinced that Monte Carlo is a wonderful method of solving problems. Although the examples we looked at were mostly simple, these algorithms can easily be applied to solve much harder ones. Simply put, Monte Carlo uses a brute force approach to simulate a particular instance of a model multiple times. Through such repeated sampling, we are able to gain a better understanding of the parameters underlying the issue at hand, no matter how complex. This is a pattern that we saw with all three tasks we dealt with in today’s post.",0,0,1,0,0,0,0,0
Naive Bayes Model From Scratch,"As per convention of this tutorial, the returned dictionary has keys corresponding to each class and values indicating the likelihood that the  belongs to that class. We can see the  function in action by passing a dummy test instance. We are almost done! All that we have to do is to create a funcition that returns the predicted label of a testing instance given some labeled training data. Implemenitng this process is straightforward since we have all the Bayesian ingredients we need, namely the prior and the likelihood. The last step is to connect the dots with Bayes’ theorem by calculating the product of the prior and likelihood for each class, then return the class label with the largest posterior, as illustrated below. Let’s see if the  works as expected by seeing if passing as argument , for which we know that its label is 1, actually returns 1. The function is only able to process a single testing instance. Let’s complete our model construction by writing the  function that takes labeled data and a testing set as its argument to return a  array containing the predicted class labels for each instance in the testing set. Done! Let’s import some data from the  library. The wine set data is a classic multi-class classfication data set.",0,0,0,1,0,0,1,0
First Neural Network with Keras,"But because cross entropy is often not easy to intuitively wrap our minds around, let’s pass the  metric to the  function, as shown below. It’s time to train the neural network with the training data,  and , over a specified number of epochs. As promised, we will use the  to stop graident descent from making unnecessary computations down the road. We also specify that  and  are components of the validation set. Keras shows us how much our neural network improves over each epoch. This is convenient, but can we do better? The answer is a sure yes. Let’s quickly plot a graph to see how model accuracy improves over time, while cross entropy loss decreases with more epochs.  As the last step, we might want to save our trained model. This can be achieved with a single line of code. We can load pre-saved models as well. That’s it for today! Obviously there are a lot more we can do with , such as building deeper neural networks or non-sequential models such as CNN or GAN, but these are topics we might look at a later date when I grow more proficient with the Keras API and deep learning in general. For now, consider this to be a gentle introduction to neural networks with Keras. Thanks for reading! Catch you up in the next one.",1,0,0,1,0,1,0,0
Complex Fibonacci,"Okay, maybe I’m being too melodramatic about a graph, but there is no denying that this pattern is geometrically interesting and pleasing to the eye. Everything looks so intentional and deliberate. The comments on the aesthetics of the snail shell aside, one point that deserves our attention is what appears to be a straight line. Well, turns out that this is, in fact, not a straight line. The only reason why it appears straight is that the snail pattern overshadows the little vibrations on this portion of the graph. Indeed, zooming in, we see that there is an interesting damping motion going on. This is what the fibonacci sequence would have looked like had we plotted only the positive domain of the real number line.  In this post, we took a look at the fibonacci sequence and its interpolation across the real number line. We could go even crazier, as did Matt Parker in his own video, by attempting to interpolate the sequence on the complex number plane, at which point we would now have a mapping from two dimensions to two dimensions, effectively forcing us to think in terms of four dimensions. There is no fast, handy way of drawing or visualizing four dimensions, as we are creatures that are naturally accustomed to three dimensions. There are interesting observations to be made with the full-fledged complex interpolation of the sequence, but I thought this is already interesting as it is nonetheless.",0,0,0,0,0,0,0,1
Logistic Regression Model from Scratch,"Now what’s next? Since we have a loss function, we need to build an algorithm that will allow us to minimize this cost function. One of the most common methods used to achieve cost minimization is gradient descent. As you might be able to tell, this algorithm has a lot to do with gradients, which can loosely be understood as a fancy way of saying derivatives. Below is an illustration of the gradient descent algorithm in action, sourced from this blog.  Basically, what gradient descent does is that it takes the derivative of the loss function with respect to the weight vector every epoch, or iteration, and takes a small step in the opposite direction of that derivative. If you think of this in the context of two dimensions as shown in the illustration, the gradient descent algorithm ends up moving down the parabola, taking little steps each time, until it eventually reaches the global minimum. In mathematical notation, we might express this process as follows: If we were to perform this in vectorized format, where  represents a vector containing the weight coefficients of the logistic regression model: The  notation is used to denote gradients, an important operation in matrix calculus which we explored in when deriving the normal equation solution to linear regression on this blog. The  denotes a hyperparameter known as the learning rate, which essentially determines how big of a step the gradient descent model takes with each iteration.",0,0,0,1,0,0,1,0
Fourier Series,"Taylor series is used in countless areas of mathematics and sciences. It is a handy little tool in the mathematicians arsenal that allows us to decompose any function into a series of polynomials, which are fairly easy to work with. Today, we are going to take a brief look at another type of series expansion, known as Fourier series. Note that these concepts are my annotations of Professor Gilbert Strang’s amazing lecture, available on YouTube. The biggest difference between Taylor series and Fourier series is that, unlike Taylor series, whose basic fundamental unit is a polynomial term, the building block of a Fourier series is a trigonometric function, namely one of either sine or cosine. Concretely, a generic formula of a Fourier expansion looks as follows: Personally, I found this formula to be more difficult to intuit than the Taylor series. However, once you understand the underlying mechanics, it’s fascinating to see how periodic wave functions can be decomposed as such. First, let’s begin with an analysis of orthogonality. Commonly, we define to vectors  and  as being orthogonal if That is, if their dot product yields zero. This follows from the definition of a dot product, which has to do with cosines. With a stretch of imagination, we can extend this definition of orthogonality to the context of functions, not just vectors. For vectors, a dot product entails summing the element-wise products of each component. Functions don’t quite have a clearly defined, discrete component. Therefore, instead of simply adding, we integrate over a given domain.",0,0,1,0,0,0,0,1
PyTorch Tensor Basics,"Not only do the two functions look similar, they also practically do the same thing. Upon more observation, however, I realized that there were some differences, the most notable of which was the .  seemed to be unable to infer the data type from the input given. On the other hand,  was sable to infer the data type from the given input, which was a list of integers. Sure enough,  is generally non-configurable, especially when it comes to data types. can accept  as a valid argument. The conclusion of this analysis is clear: use  instead of . Indeed, this SO post also confirms the fact that  should generally be used, as  is more of a super class from which other classes inherit. As it is an abstract super class, using it directly does not seem to make much sense. In PyTorch, there are two ways of checking the dimension of a tensor:  and . Note that the former is a function call, whereas the later is a property. Despite this difference, they essentially achieve the same functionality. To access one of the  elements, we need appropriate indexing. In the case of , it suffices to consider the size as a list, meaning that square bracket syntax can be used. In the case of , indices can directly be passed into as an argument to index individual elements in the size tensor. These past few days, I’ve spent a fair amount of time using PyTorch for basic modeling.",0,0,0,0,0,1,0,0
"Beta, Bayes, and Multi-armed Bandits","Note that a lot of the code in this post was largely inspired by Peter’s blog. Before we get into any modeling, let’s first import the modules we’ll need and set things up. Instead of using the approach outlined in the blog I’ve linked to, I decided to use objects to model bandits. The rationale is that this approach seems to make a little bit more intuitive sense to me. Also, working with Django these days has made me feel more affinity with Python classes. At any rate, let’s go ahead. Now, we can initialize a bandit with some predetermined parameter, . Of course, our goal would be to determine the true value of this parameter through sampling and Bayesian magic. In this case, we have created a fair bandit with a  of 0.5. We can also simulate level pulls by repeatedly calling on . Note that this will accumulate the result of each Bernoulli trial in the  list object. Notice that we also have . This is an a list object that c Now that we have performed all the basic sanity checks we need, let’s quickly go ahead and create three bandit objects for demonstration purposes. Now we get into the details of how to perform the Bayesian update. More specifically, we’re interested in how we are going to use posterior probabilities to make decisions on which slot machine to pull on. This is where Thompson sampling comes in.",0,0,1,0,1,0,0,0
Logistic Regression Model from Scratch,"In fact, we can borrow the same notation we used for linear regression to frame logistic regression as follows: In other words, logistic regression can be understood as a process in which our goal is to find the weight coefficients in the equation above the best describe the given data set. Unlike in linear regression, where the predicted value is computed simply by passing the data as arguments into a linear function, logistic regression outputs numbers between 0 and 1, making binary classification possible. However, there is certainly an element of linearity involved, which is part of the reason why both linear and logistic regression models fall under a larger family of models called generalized linear models. Now that we know the basic maths behind logistic regression using the sigmoid function, it’s time to implement it via code. Welcome to the next part of the tutorial, where we start building the actual model from scratch. As always, it’s a good idea to have some dummy data ready for disposal so that we can develop some basic intuition about dimensionality of our data when handling inputs and outputs of our functions. Here is the data we used in the last post on k-nearest neighbors algorithm, slightly modified for the purposes of this post. Let’s start by translating equation (2) into executable code.",0,0,0,1,0,0,1,0
