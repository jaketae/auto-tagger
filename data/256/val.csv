title,body,probability_distribution,analysis,linear_algebra,pytorch,machine_learning,from_scratch,statistics,deep_learning
Scikit-learn Pipelines with Titanic,"Before proceeding with any data analysis, it‚Äôs always a good idea to pay attention to missing values‚Äîhow many of them there are, where they occur, et cetera. Let‚Äôs take a look. The  is useful, but is doesn‚Äôt really show us how many values are missing for each column. To probe into this issue in more detail, we need to use  instead. I recently realized that there is also a very cool data visualization library called  for observing missing data.  This visualization gives us a more intuitive sense of where the values are missing. In this case, the missing values seem to be distributed somewhat evenly or randomly. However, we can also imagine cases were missing values might have something to do with an inherent attribute in the dataset (e.g. only male participants of a survey might reply ‚ÄúN/A‚Äù to some health questionaire involving inquiries on pregnancy). In such cases, using this library to visualize where missing values occur is a good idea, as this is an additional dimension of information that calling  wouldn‚Äôt be able to reveal. Now that we have a rough sense of where missing values occur, we need to decide from one of few choices: Indeed, in this case, we will go ahead and drop the  attribute. This choice becomes more obvious when we compute the percentage of null values. This shows us that 77 percent of the rows have missing  attribute values. Given this information, it‚Äôs probably a bad idea to try and impute these values.",0,0,0,0,1,0,0,0
Building Neural Network From Scratch,"ReLU is a piece-wise function, and hence introduces nonlinearity, which is one of the purposes of having an activation function in a neural network. The formula for ReLU is extremely simple. If the input value  i s greater or equal to zero, the ReLU function outputs the value without modification. However, if  is smaller than zero, the returned value is also zero. There are other ways of expressing the ReLU function. One version that is commonly used and thus deserves our attention is written below. Although this appears different from (3), both formulas express the same operation at their core. We can get a better sense of what the function with the help of Python. Assuming that the input is a  vector, we can use vectorization to change only the elements in the input vector that are negative to zero, as shown below. Let‚Äôs see what the ReLU function looks like by plotting it on the plane.  The visualization makes clear the point that ReLU is a piece-wise function that flattens out negative values while leaving positive values unchanged. Now that we have all the ingredients ready, it‚Äôs time to build the neural network. Earlier, I said that a neural network can be reduced to matrix multiplication. This is obviously an oversimplification, but there is a degree of truth to that statement. Recall that a single neuron of a neural network can be expressed as a dot product of two vectors, as shown below. Following conventional notation,  represents weights; , input data; , bias.",0,0,1,0,0,1,0,1
PyTorch RNN from Scratch,"Now that we have downloaded the data we need, let‚Äôs take a look at the data in more detail. First, here are the dependencies we will need. We first specify a directory, then try to print out all the labels there are. We can then construct a dictionary that maps a language to a numerical label. We see that there are a total of 18 languages. I wrapped each label as a tensor so that we can use them directly during training. Let‚Äôs store the number of languages in some variable so that we can use it later in our model declaration, specifically when we specify the size of the final output layer. Now, let‚Äôs preprocess the names. We first want to use  to standardize all names and remove any acute symbols or the likes. For example, Once we have a decoded string, we then need to convert it to a tensor so that the model can process it. This can first be done by constructing a  mapping, as shown below. We see that there are a total of 59 tokens in our character vocabulary. This includes spaces and punctuations, such as ` .,:;-‚Äò(num_char, 59)(59,)`. We can now build a function that accomplishes this task, as shown below: If you read the code carefully, you‚Äôll realize that the output tensor is of size , which is different from the explanation above. Well, the reason for that extra dimension is that we are using a batch size of 1 in this case.",0,0,0,1,0,1,0,1
Stirling Approximation,"It‚Äôs about time that we go back to the old themes again. When I first started this blog, I briefly dabbled in real analysis via Euler, with a particular focus on factorials, interpolation, and the Beta function. I decided to go a bit retro and revisit these motifs in today‚Äôs post, by introducing Stirling‚Äôs approximation of the factorial. There are many variants of Stirling‚Äôs approximation, but here we introduce the general form as shown: Let‚Äôs begin the derivation by first recalling the Poisson distribution. The Poisson distribution is used to model the probability that a certain event occurs a specified number of times within a defined time interval given the rate at which these events occur. The formula looks as follows: One interesting fact about the Poisson distribution is that, when the parameter  is sufficiently large, the Poisson approximates the Gaussian distribution whose mean and variance are both . This happens when the random variable . We can easily simplify (2) since the power of the exponent is zero. Thus, we have By simply rearranging (3), we arrive at Stirling‚Äôs approximation of the factorial: This is cool, but we still haven‚Äôt really shown why a Poisson can be used to approximate a Gaussian‚Äîafter all, this premise was the bulk of this demonstration. To see the intuition behind this approximation, it is constructive to consider what happens when we add independent Poisson random variables. Say we have  and , both of which are independent Poisson random variables with mean  and .",1,1,0,0,0,0,1,0
InceptionNet in PyTorch,"In today‚Äôs post, we‚Äôll take a look at the Inception model, otherwise known as GoogLeNet. I‚Äôve actually written the code for this notebook in October üò± but was only able to upload it today due to other PyTorch projects I‚Äôve been working on these past few weeks (if you‚Äôre curious, you can check out my projects here and here). I decided to take a brief break and come back to this blog, so here goes another PyTorch model implementation blog post. Let‚Äôs jump right into it! First, we import PyTorch and other submodules we will need for this tutorial. Because Inception is a rather big model, we need to create sub blocks that will allow us to take a more modular approach to writing code. This way, we can easily reduce duplicate code and take a bottom-up approach to model design. The  module is a simple convolutional layer followed by batch normalization. We also apply a ReLU activation after the batchnorm. Next, we define the inception block. This is where all the fun stuff happens. The motivating idea behind InceptionNet is that we create multiple convolutional branches, each with different kernel (also referred to as filter) sizes. The standard, go-to kernel size is three-by-three, but we never know if a five-by-five might be better or worse. Instead of engaging in time-consuming hyperparameter tuning, we let the model decide what the optimal kernel size is.",0,0,0,1,0,0,0,1
An Introduction to Markov Chain Monte Carlo,"We covered the topic of Markov chains on two posts, one on PageRank and the other on the game of chutes and ladders. Nonetheless, some recap would be of help.  defines Markov chains as follows: A Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. In other words, a Markov chain is a method of generating a sequence of random variables where the current value of that random variable probabilistically dpends on its prior value. By recursion, this means that the next value of that random variable only depends on its current state. To put this into context, we used Markovian analysis to assess the probability that a user on the internet would move to one site to another in the context of analyzing Google‚Äôs PageRank algorithm. Markov chains also popped up when we dealt with chutes and ladders, since the next position of the player in game only depends on their current position on the game board. These examples all demonstrate the Markov property, also known as memorylessness. Later on, we will see how Markov chains come in handy when we decide to ‚Äújump‚Äù from one number to the next when sampling from the posterior distribution to derive an approximation of the parameter of interest. We also explored Monte Carlo in some detail here on this blog.",0,0,0,0,0,1,0,0
Markov Chain and Chutes and Ladders,"This block produces the following figure: I doubt that anyone would play Chutes and Ladders for this long, but after about 150 rolls of the dice, we can expect with a fair amount of certainty that the game will come to an end. The graph above presents information on cumulative fractions, but we can also look at the graph for marginal probabilities by examining its derivative: And the result: From the looks of it, the maximum of the graph seems to exist somewhere around . To be exact, . This result tells us that we will finish the game in 19 rolls of the dice more often than any other number of turns. We can also use this information to calculate the expected value of the game length. Recall that Or if the probability density function is continuous, In this case, we have a discrete random variable, so we adopt the first formula for our analysis. The formula can be achieved in Python as follows: This result tells us that the typical length of a Chutes and Ladders game is approximately 36 turns. But an issue with using expected value as a metric of analysis is that long games with infinitesimal probabilities are weighted equally to short games of substantial probability of occurrence. This mistreatment can be corrected for by other ways of understanding the distribution, such as median: This function tries to find the point in the cumulative distribution where the value is closest to , i.e. the median of the distribution.",0,0,1,0,0,0,0,0
Demystifying Entropy (And More),"where  denotes cross entropy; ,entropy, and the last term, KL divergence. Now, let‚Äôs try to understand what each of them means. KL divergence has many interpretations. One possible definition of KL divergence is that it measures the average number of extra information content required to represent a message with distribution  instead of . In Machine Learning: A Probabilistic Perspective, Kevin P. Murphy describes KL divergence as follows: ‚Ä¶ the KL divergence is the average number of extra bits needed to encode the data, due to the fact that we used distribution  to encode the data instead of the true distribution . Put differently, KL divergence is the amount of information that is lost when  is used to approximate . Therefore, if  and  are close, KL divergence would be low, whereas the converse would be true when the two distributions are different. We can also extend this notion a bit farther to apply it in the context of Bayesian inference. Recall that Bayesian inference is the process by which we start from some prior distribution and update our beliefs about the distribution with more data input to derive a posterior. In this context, KL divergence can be viewed as the amount of information gained as we move from the prior  to the posterior . Let‚Äôs derive the mathematical definition of KL divergence using likelihoods. The derivation process to be introduced is based on this source.",0,0,0,0,0,0,1,0
Scikit-learn Pipelines with Titanic,"In today‚Äôs post, we will explore ways to build machine learning pipelines with Scikit-learn. A pipeline might sound like a big word, but it‚Äôs just a way of chaining different operations together in a convenient object, almost like a wrapper. This abstracts out a lot of individual operations that may otherwise appear fragmented across the script. I also personally think that Scikit-learn‚Äôs ML pipeline is very well-designed. So here is a brief introduction to ML pipelines is Scikit-learn. For the purposes of this tutorial, we will be using the classic Titanic dataset, otherwise known as the course material for Kaggle 101. I‚Äôm still trying to get my feet into Kaggle, so it is my hope that this tutorial will also help those trying to break into data science competitions. First, let‚Äôs import the modules and datasets needed for this tutorial. Scikit-learn is the go-to library for machine learning in Python. It contains not only data loading utilities, but also imputers, encoders, pipelines, transformers, and search tools we will need to find the optimum model for the task. Let‚Äôs load the dataset using . Let‚Äôs observe the data by calling . By default, this shows us the first five rows and as many columns as it can fit within the notebook. Let‚Äôs take a look at the data in more depth to build a foundation for our analysis. This step typically involves the following steps: Let‚Äôs proceed in order.",0,0,0,0,1,0,0,0
Gaussian Mixture Models,"Now it becomes increasingly apparent why the EM algorithm is needed to find a converging solution for the MLE estimates. We can take a similar approach to calculate the MLE of the other two remaining paramters, namely  and . The derivation is more complicated since  is a matrix;  is subject to constraints that apply to any categorical distribution: all elements must be positive and must sum up to one. For my own personal reference and the curious-minded, here is a link to a resource that contains the full derivation. But the fundamental idea is that we would commence from the log likelihood function and derive our way to the solution. The solutions are presented below: So the full picture is now complete: given the inter-dependence of derived quantities, we seek to optimize them using the Expectation Maximization algorithm. Specifically, the EM method works as follows: In today‚Äôs post, we took a deep dive into Gaussian mixture models. I find GMMs to be conceptually very intuitive and interesting at the same time. I‚Äôm also personally satisfied and glad that I have learned yet another ML/mathematical concept that starts with the word Gaussian. Much like how I felt when learning about Gaussian process regression, now I have an even greater respect for the Gaussian distribution, although I should probably be calling it normal instead, just like everybody else. I‚Äôm also happy that I was able to write this blog post in just a single day.",0,0,0,0,1,0,1,0
Dissecting the Gaussian Distribution,"If we put the pieces of the puzzle back together, we finally have the probability distribution of the multivariate Gaussian distribution: To develop a better intuition for the multivariate Gaussian, let‚Äôs take a look at a case of a simple 2-dimensional Gaussian random vector with a diagonal covariance matrix. This example was borrowed from this source. Using the formula for the multivariate Gaussian we derived in (11), we can construct the probability distribution function given , , and . Note that computing , the inverse of the covariance matrix, can be accomplished simply by taking the reciprocal of its diagonal entries since  was assumed to be a diagonal matrix. Continuing, In other words, the probability distribution of seeing a random vector  given  and  is equal to the product of the two univariate Gaussians. This result is what we would expect given that . For instance, if  and  are independent, i.e. observing a value of  does not inform us of anything about  and vice versa, it would make sense that the possibility of observing a random vector  with entries  and  is merely the product of the independent probabilities of each observing  and . This example illustrates the intuitive link between the multivariate and univariate Gaussian distributions. In this post, we took a look at the normal distribution from the perspective of probability distributions. By working from the definition of what constitutes a normal data set, we were able to completely build the probability density function from scratch.",1,0,0,0,0,0,1,0
Gaussian Process Regression,"In other words, the new kernel matrix now becomes This can be seen as a minor correction to the kernel matrix to account for added Gaussian noise. Before we jump straight into code implementation, it‚Äôs necessary to discuss the Cholesky decomposition to get some technicality out of the way. The Cholesky decomposition is a specialization of the LDU decomposition, applied to symmetric matrices. The idea is that we can factor a symmetric matrix  as Let‚Äôs begin by considering the LDU decomposition of the  matrix. We know that for symmetric matrices, . This can easily be shown by comparing the LDU decomposition of  and  respectively: Therefore, we can rewrite the LDU decomposition of A as A nice property of diagonal matrices is that we can easily identify its square, namely, where  is a matrix whose diagonal entries are each square root of the corresponding originals in . The tranpose is not necessary since  is a diagonal matrix, but we do so for convenience purposes later on in the derivation. Note the trivial case of the identity matrix, whose square root is equal to itself since all diagonal elements take the value of 1 (and ). Given this piece of information, what we can now do is to rewrite the factorization of  as where . This is the Cholesky decomposition of symmetric matrices‚Äîto be more exact, positive semi-definite matrices. The reason why the Cholesky decomposition can only be performed on positive semi-definite matrices becomes apparent when we think about the definition of positive semi-definiteness.",0,0,0,0,1,1,0,0
"Linear Regression, in Two Ways","This statement might sometimes be phrased differently along the lines of convexity, but this topic is better tabled for a separate future post. The key point here is that setting the gradient to zero would tell us when the error is minimized. This is equivalent to Therefore, Now we are done! Just like in the previous section,  gives us the parameters for our line of best fit, which is the solution to the linear regression problem. In fact, the keen reader might have already noted that (7) is letter-by-letter identical to formula (2) we derived in the previous section using plain old linear algebra! One the one hand, it just seems surprising and fascinating to see how we end up in the same place despite having taken two disparate approaches to the linear regression problem. But on the other hand, this is what we should have expected all along: no matter what method we use, the underlying thought process behind both modes of approach remain the same. Whether it be through projection or through derivation, we sought to find some parameters, closest to the values we are approximating as much as possible, that would turn an otherwise degenerate system into one that is solvable. Linear regression is a simple model, but I hope this post have done it justice by demonstrating the wealth of mathematical insight that can be gleaned from its derivation.",0,0,1,0,0,0,0,0
The Magic of Euler‚Äôs Identity,"Recall that a unit circle can be expressed by the following equation in the Cartesian coordinate system: On the complex plane mapped in polar coordinates, this expression takes on an alternate form: Notice that this contains the same exact information that Euler‚Äôs identity provides for us. It expresses: From this geometric interpretation, we can thus conclude that We now know the exact value that  represents in the complex number system! Urban legend goes that mathematician Benjamin Peirce famously said the following about Euler‚Äôs identity: Gentlemen, that is surely true, it is absolutely paradoxical; we cannot understand it, and we don‚Äôt know what it means. But we have proved it, and therefore we know it must be the truth. But contrary to his point of view, Euler‚Äôs identity is a lot more than just an interesting, coincidental jumble of imaginary and irrational numbers that somehow churn out a nice, simple integer. In fact, it can be used to better understand fundamental operations such as logarithms and powers. Consider, for example, the value of the following expression: Imaginary powers are difficult to comprehend by heart, and I no make no claims that I do. However, this mind-pulverizing expression starts to take more definite meaning once we consider the generalized form of Euler‚Äôs identity, . Let . Then we have Take both sides to the power of i: Interestingly enough, we see that  takes on a definitive, real value.",0,1,0,0,0,0,0,0
An Introduction to Markov Chain Monte Carlo,"But before we move into code, there is something that should be corrected before we move on. In (6), we derived an experssion for the jump condition. The jump condition is not wrong per se, yet a slight modification has to be made to fully capture the gist of Metrapolis-Hastings. The precise jump condition for the sampler goes as follows: where This simply means that we accept the prorposed pararmeter if the quantity calculated in (7) is larger than a random number between 0 and 1 sampled from a uniform distribution. This is why MCMC models involve a form of random walk‚Äîwhile leaving room for somewhat unlikely parameters to be selected, the model samples relatively more from regions of high posterior probability. Now that we have some understanding of how Markov Chain Monte Carlo and the Metropolis-Hastings algorithm, let‚Äôs implement the MCMC sampler in Python. As per convention, listed below are the dependencies required for this demonstration. Let‚Äôs start by generating some toy data for our analysis. It‚Äôs always a good idea to plot the data to get a sense of its shape.  The data looks roughly normal. This is because we created the toy data using the  function, which generates random numbers from a normal distribution centered around 0. The task for this tutorial, given this data, is going to be estimating the mean of the posterior distribution, assuming we know its standard deviation to be 1.",0,0,0,0,0,1,0,0
Dissecting LSTMs,"The raw material that we pass into this filter is in fact the cell state, processed by a  activation function. This is also a familiar structure we saw earlier in the information update sequence of the network. Only this time, we use the cell state to generate output. This makes sense somewhat intuitively, since the cell state is essentially the memory of the network, and hence to generate output would require the use of this memory. Of course, this should not be construed so literally since what ultimately happens during backpropagation is entirely up to the network, and at that point we simply lay back and hope for the network to learn the best. This point notwithstanding, I find this admittedly coarse heuristic to be nonetheless useful in intuiting the clockwork behind LSTMs. At this point, we‚Äôre not quite done yet;  is not a vector of probabilities indicating which letter is the most likely in a one-hot encoded representation. Therefore, we will need to pass it through another affine layer, than apply a softmax activation. Hence,  is the final output of an LSTM layer. Here comes the tricky part: backprop. Thankfully, backprop is somewhat simple in the case of LSTMs due to the use of Hadamard products. The routine is not so much different from a vanilla neural network, so let‚Äôs try to hash out the equations. As we already know, backpropagation in neural networks is merely an extended application of the chain rule, with some minor caveats that matrix calculus entails.",0,0,0,0,0,0,0,1
Introduction to seq2seq models,"Also, the decoder has a fully connected layer that acts as a classifier. Hence, the size of the output dimension is equal to the vocabulary size of the target language. Now, it‚Äôs time to put the two models together in a sequence-to-sequence model. The overall flow of data looks as follows: I resorted to a convenient bullet point listing to summarize everything, but let‚Äôs break this down a bit. Within the forward pass of the seq2seq model, the encoder encodes input data, which, in this case, are German sentences. Then, the decoder accepts the hidden and cell states of the encoder, as well as the zeroth index of the target language batch. This zeroth index will simply be a bunch of starting tokens, as we saw earlier. Then, the decoder will generate a prediction using these starting tokens and encoder states. The interesting part comes thereafter. We set some teacher force ratio, which is a number between zero and one. There are two ways through which the decoder can generate the next prediction. Either it can use its own prediction from the previous time step, or, as ‚Äúteachers,‚Äù we can nudge the decoder in the correct direction by telling them what the correct prediction should have been from the previous time step. This teacher guidance is helpful, since at the beginning of training, the model might struggle to generate correct predictions using its own previous predictions. Below is the full implementation of the model.",0,0,0,1,0,0,0,1
Bayesian Linear Regression,"Whereas vanilla linear regression only gives us a single point estimate given an input vector, Bayesian linear regression gives an entire distribution. For the purposes of our demonstration, we will define the predictive posterior to take the following form as shown below, with precision  pre-given. Precision is simply the reciprocal of variance and is commonly used as an alternative way of parametrizing Gaussian distributions. In other words, we assume the model Our goal will be to derive a posterior for this distribution by performing Bayesian inference on , which corresponds to the slope of the linear regression equation, where  denotes noise and randomness in the data, thus affecting our final prediction. To begin Bayesian inference on parameter , we need to specify a prior. Our uninformed prior will look as follows. where  denotes precision, the inverse of variance. Note that we have a diagonal covariance matrix in place of variance, the distribution for  will be a multivariate Gaussian. The next ingredient we need for our recipe is the likelihood function. Recall that likelihood can intuitively be understood as an estimation of how likely it is to observe the given data points provided some parameter for the true distribution of these samples. The likelihood can easily be computed by referencing back to equation (1) above. Note that the dot product of  with itself yields the sum of the exponents, which is precisely the quantity we need when computing the likelihood.",0,0,1,0,0,0,0,0
Demystifying Entropy (And More),"This yields Recall that the definition of entropy goes as Plugging in this definition to (11) yields the simplified definition of cross entropy: If KL divergence represents the average amount of additional information needed to represent an event with  instead of , cross entropy tells us the average amount of total information needed to represent a stochastic event with  instead of . This is why cross entropy is a sum of the entropy of the distribution  plus the KL divergence between  and . Instead of dwelling in the theoretical realm regurgitating different definitions and interpretations of cross entropy and KL divergence, let‚Äôs take a look at a realistic example to gain a better grasp of these concepts. Say we have constructed a neural network to solve a task, such as MNIST hand-written digit classification. Let‚Äôs say we have fed our neural network an image corresponding to the number 2. In that case, the true distribution that we are trying to model, represented in vector form, will be  as shown below. The  statement is there to make sure that the probabilities sum up to 1. Let‚Äôs assume that our neural network made the following prediction about image. These two distributions, although similar, are different. But the question is, how different? Creating a visualization might give us some idea about the difference between the two distributions.  The two distributions are quite similar, meaning that our neural network did a good job of classifying given data.",0,0,0,0,0,0,1,0
A Brief Introduction to Recurrent Neural Networks,"The biggest difference between a simple recurrent neural network and an LSTM is that LSTMs have a unique parameter known as the carrier that encodes an additional layer of information about the state of the cell. I might write a separate post devoted to the intricacies of the LSTM, but if you‚Äôre an avid reader who‚Äôs itching to know more about it right away, I highly recommend this excellent post by Christiopher Olah. For now, let‚Äôs just say that LSTMs represent a huge improvement over conventional RNNs, and that we can implement them in  by simply calling the  layer as shown below: Because LSTM layers take a lot longer to train than others, and because the representational capacity of a single LSTM layer is higher than that of others, I decided to use only one LSTM layer instead of two. Let‚Äôs initialize this model to take a better look.  The last model we will create is a convnet, which we explored on this previous post on image classification. Convolutional neural networks are great at identifying spatial patterns in data, which is why they also perform reasonably well in natural language processing. Another huge advantage of convents over recurrent networks is that they took a lot lesser time and resources to train. This is why it is often a good idea to build a convent to establish a baseline performance metric. Let‚Äôs initialize the model with identical parameters and take a look at its internal structure.  Let‚Äôs train all four models using the training data.",0,0,0,0,0,0,0,1
A PyTorch Primer,"To build our simple model, let‚Äôs first write out some variables to use, starting with the configuration of our model and its dimensions. We will also need some input and output tensors to be fed into the model for trainining and optimization. Next, here are the weight matrices we will use. For now, we assume a simple two layered dense feed forward network. Last but not least, let‚Äôs define a simple squared error loss function to use during the training step. With this entire setup, we can now hash out what the entire training iteration is going to look like. Wrapped in a loop, we perform one forward pass, then perform backpropagation to adjust the weights. Great! We see that the loss drops as more epochs elapse. While there is no problem with this approach, things can get a lot more unwieldy once we start building out more complicated models. In these cases, we will want to use the auto differentiation functionality we reviewed earlier. Let‚Äôs see this in action. Also, let‚Äôs make this more PyTorch-y by making use of classes. We will revisit why class-based implementations are important in the next section. Notice we didn‚Äôt have to explicitly specify the backpropagation formula with matrix derivatives: by simply calling  properties for each of the weights matrices, we were able to perform gradient descent.",0,0,0,1,0,0,0,1
Traveling Salesman Problem with Genetic Algorithms,"Let‚Äôs see if this everything works as expected by generating a dummy population. Now we need some function that will determine the fitness of a chromosome. In the context of TSP, fitness is defined in very simple terms: the shorter the total distance, the fitter and more superior the chromosome. Recall that all the distance information we need is nicely stored in . We can calculate the sum of all the distances between two adjacent cities in the chromosome sequence. Next, we evaluate the population. Simply put, evaluation amounts to calculating the fitness of each chromosome in the total population, determining who is best, storing the score information, and returning some probability vector whose each element represents the probability that the th element in the population bag is chosen as a parent. We apply some basic preprocessing to ensure that the worst performing chromosome has absolutely no chance of being selected. When we call , we get a probability vector as expected. From the result, it appears that the last element is the best chromosome; the second chromosome in the population bag is the worst. When we call , notice that we get the last element in the population, as previously anticipated. We can also access the score of the best chromosome. In this case, the distance is said to be 86.25. Note that the lower the score, the better, since these scores represent the total distance a salesman has to travel to visit all the cities.",0,0,0,0,0,1,0,0
Scikit-learn Pipelines with Titanic,"Then, we one-hot encode the categorical variables since most machine learning models cannot accept non-numerical values as input. The last PCA step might seem extraneous. However, as discussed in this Stack Overflow thread, the judicious combination of one-hot plus PCA can seldom be beat by other encoding schemes. PCA finds the linear overlap, so will naturally tend to group similar features into the same feature. I don‚Äôt have enough experience to attest to the veracity of this claim, but mathematically or statistically speaking, this proposition seems valid. The idea is that one-hot encoding all categorical variables may very well lead to an unmanageable number of columns, thus causing one to flounder in the curse of dimensionality. A quick fix, then, is to apply PCA or some other dimensionality reduction technique onto the results of one-hot encoding. Back to the implementation, note that we can look inside the individual components of  by simply treating it as an iterable, much like a list or tuple. For example, Next, we need to do something similar for numerical variables. Only this time, we wouldn‚Äôt be one-hot encoding the data; instead, what we want to do is to apply some scaling, such as normalization or standardization. Recently in one of Andreas Mueller‚Äôs lectures on YouTube, I learned about the , which uses median and IQR instead of mean and standard deviation as does the . This makes the  a superior choice in the presence of outliers. Let‚Äôs try using it here.",0,0,0,0,1,0,0,0
PyTorch Tensor Basics,"More generally speaking, we can think that concatenation effectively brought the two elements of each tensor together to form a larger tensor of four elements. I found concatenation along the first and second dimensions to be more difficult to imagine right away. The trick is to mentally draw a connection between the dimension of concatenation and the location of the opening and closing brackets that we should focus on. In the case of the example above, the opening and closing brackets were the outer most ones. In the example below in which we concatenate along the first dimension, the brackets are those that form the boundary of the inner two-dimensional 3-by-4 tensor. Let‚Äôs take a look. Notice that the rows of  were essentially appended to those of , thus resulting in a tensor whose shape is . For the sake of completeness, let‚Äôs also take a look at the very last case, where we concatenate along the last dimension. Here, the brackets of focus are the innermost ones that form the individual one-dimensional rows of each tensor. Therefore, we end up with a ‚Äúlong‚Äù tensor whose one-dimensional rows have a total of 8 elements as opposed to the original 4. In this post, we took a look at some useful tensor manipulation operations and techniques. Although I do have some experience using Keras and TensorFlow, I never felt confident in my ability to deal with tensors, as that felt more low-level. PyTorch, on the other hand, provides a nice combination of high-level and low-level features.",0,0,0,1,0,0,0,1
VGG PyTorch Implementation,"In today‚Äôs post, we will be taking a quick look at the VGG model and how to implement one using PyTorch. This is going to be a short post since the VGG architecture itself isn‚Äôt too complicated: it‚Äôs just a heavily stacked CNN. Nonetheless, I thought it would be an interesting challenge. Full disclosure that I wrote the code after having gone through Aladdin Persson‚Äôs wonderful tutorial video. He also has a host of other PyTorch-related vidoes that I found really helpful and informative. Having said that, let‚Äôs jump right in. We first import the necessary  modules. Let‚Äôs first take a look at what the VGG architecture looks like. Shown below is a table from the VGG paper.  We see that there are a number of different configurations. These configurations typically go by the name of VGG 11, VGG 13, VGG 16, and VGG 19, where the suffix numbers come from the number of layers. Each value of the dictionary below encodes the architecture information for each model. The integer elements represents the out channel of each layer.  represents a max pool layer. You will quickly see that the dictionary is just a simple representation of the tabular information above. Now it‚Äôs time to build the class that, given some architecture encoding as shown above, can produce a PyTorch model. The basic idea behind this is that we can make use of iteration to loop through each element of the model architecture in list encoding and stack convolutional layers to form a sub-unit of the network.",0,0,0,1,0,0,0,1
"Beta, Bayes, and Multi-armed Bandits","In the simple, greedy frequentist approach, we would determine which bandit to pull on given our historical rate of success. If the first slot machine approximately yielded success 60 percent of the time, whereas the second one gave us 40, we would choose the first. Of course, this approach is limited by the fact that, perhaps we only pulled on the second machine 5 times and got only 2 success out of them, whereas we pulled on the first bandit a hundred times and got 60 successes. Maybe it turns out that the second bandit actually has a higher success rate, and that we were simply unlucky those five turns. Thompson sampling remedies this problem by suggesting a different approach: now that we have Bayesian posteriors, we can now directly sample from those posteriors to get an approximation of the parameter values. Since we are sampling from a distribution instead of relying on a point estimate as we do for the greedy approach, this allows for both exploration and exploitation to happen at reasonable frequencies. If a posterior distribution has large variance, this means that we will explore that particular bandit slightly more than others. If a posterior has a large mean‚Äîa high success parameter‚Äîthen we will exploit that machine a bit more to earn more profit, or, in this context, to minimize regret. Before we move on any farther, perhaps‚Äô it‚Äôs worth discussing what the term ‚Äúregret‚Äù means in this context.",1,0,0,0,0,0,1,0
PyTorch Tensor Basics,"Not only do the two functions look similar, they also practically do the same thing. Upon more observation, however, I realized that there were some differences, the most notable of which was the .  seemed to be unable to infer the data type from the input given. On the other hand,  was sable to infer the data type from the given input, which was a list of integers. Sure enough,  is generally non-configurable, especially when it comes to data types. can accept  as a valid argument. The conclusion of this analysis is clear: use  instead of . Indeed, this SO post also confirms the fact that  should generally be used, as  is more of a super class from which other classes inherit. As it is an abstract super class, using it directly does not seem to make much sense. In PyTorch, there are two ways of checking the dimension of a tensor:  and . Note that the former is a function call, whereas the later is a property. Despite this difference, they essentially achieve the same functionality. To access one of the  elements, we need appropriate indexing. In the case of , it suffices to consider the size as a list, meaning that square bracket syntax can be used. In the case of , indices can directly be passed into as an argument to index individual elements in the size tensor. These past few days, I‚Äôve spent a fair amount of time using PyTorch for basic modeling.",0,0,0,1,0,0,0,1
Gaussian Process Regression,"Then, This means that we can calculate the mean  as Similarly, for the covariance , we can introduce an intermediate variable  from which we obtain Notice that the final expressions for mean and covariance do not require any form of inversion, which was our end goal for efficient and accurate computation. Let‚Äôs transcribe everything back to code. Let  refer to  (and by the same token  refers to ). Then, Just to be safe, let‚Äôs check that  is of the desired shape, namely a vector with 50 entries. Continuing with our computation of the posterior covariance, As expected,  is a 50-by-50 matrix. We are now almost done. Since we have computed the mean  and covariance , all there is left is to generate samples from this distribution. For that, we resort to Cholesky decomposition again, recalling the idea discussed earlier in (19). Let‚Äôs sample a total of 50 samples. now contains 50 samples generated from the posterior. It‚Äôs important to keep in mind that these samples are each 50-dimensional vectors‚Äîin a sense, they can be considered as ‚Äúfunctions‚Äù, which is why the Gaussian process is often referred to as sampling functions from a multivariate Gaussian. Let‚Äôs plot the final result, alongside the actual function . In red, I‚Äôve also plotted the average of all the 50 samples to see how accurate the result holds up.  The model behaves exactly as we would expect: where there is data, we are confident; where there is no data, we are uncertain.",0,0,0,0,1,1,0,0
Likelihood and Probability,"And that concludes today‚Äôs article on (maximum) likelihood. This post was motivated from a rather simple thought that came to my mind while overhearing a conversation that happened at the PMO office. Despite the conceptual difference between probability and likelihood, people will continue to use employ these terms interchangeably in daily conversations. From a mathematician‚Äôs point of view, this might be unwelcome, but the vernacular rarely strictly aligns with academic lingua. In fact, it‚Äôs most often the reverse; when jargon or scholarly terms get diffused with everyday language, they often transform in meaning and usage. I presume words such as ‚Äúlikelihood‚Äù or ‚Äúlikely‚Äù fall into this criteria. All of this notwithstanding, I hope this post provided you with a better understanding of what likelihood is, and how it relates to other useful statistical concepts such as maximum likelihood estimation. The topic for our next post is going to be Monte Carlo simulations and methods. If ‚ÄúMonte Carlo‚Äù just sounds cool to you, as it did to me when I first came across it, tune in again next week. Catch you up in the next one.",0,0,0,0,0,0,1,0
Convolutional Neural Network with Keras,"Recently, a friend recommended me a book, Deep Learning with Python by Francois Chollet. As an eager learner just starting to fiddle with the Keras API, I decided it was a good starting point. I have just finished the first section of Part 2 on Convolutional Neural Networks and image processing. My impression so far is that the book is more focused on code than math. The apparent advantage of this approach is that it shows readers how to build neural networks very transparently. It‚Äôs also a good introduction to many neural network models, such as CNNs or LSTMs. On the flip side, it might leave some readers wondering why these models work, concretely and mathematically. This point notwithstanding, I‚Äôve been enjoying the book very much so far, and this post is a reflection of just that. Today, we will use TensorFlow‚Äôs  module to build a convolutional neural network for image detection. This code is based on what I have learned from the book, so much credit goes to Deep Learning with Python. I have also looked at Machine Learning Mastery blog for additional reference. Let‚Äôs begin! Below are the modules that we will need to import for this demonstration. Note that this Jupyter Notebook was written on Google Colaboratory. The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x. We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the  magic: more info.",0,0,0,0,0,0,0,1
A Simple Autocomplete Model,"We start to see more words that aren‚Äôt really words (the one I personally like is ‚Äúfarmeduition‚Äù‚Äîit sounds like it could be either some hard, obscure word that no one knows, or a failed jumble of ‚Äúfarm,‚Äù ‚Äúeducation,‚Äù and ‚Äúintuition‚Äù). At temperature 1.2, the model is basically going crazy with randomness, adding white spaces where there shouldn‚Äôt be and sounding more and more like a speaker of Old English or German, something that one might expect to see in English scripts written in pre-Shakesperean times. At any rate, it is simply fascinating to see how a neural network can be trained to immitate some style of writing. Hopefully this tutorial gave you some intuition of how autocomplete works, although I presume business-grade autocomplete functions on our phones are based on much more complicated algorithms. Thanks for reading this post. In the next post, we might look at another example of a generative model known as generative adversarial networks, or GAN for short. This is a burgeoning field in deep learning with a lot of prospect and attention, so I‚Äôm already excited to put out that post once it‚Äôs done. See you in the next post. Peace!.",0,0,0,0,0,0,0,1
Principal Component Analysis,"Principal component analysis is one of those techniques that I‚Äôve always heard about somewhere, but didn‚Äôt have a chance to really dive into. PCA would come up in papers on GANs, tutorials on unsupervised machine learning, and of course, math textbooks, whether it be on statistics or linear algebra. I decided that it‚Äôs about time that I devote a post to this topic, especially since I promised one after writing about  on this blog some time ago. So here it goes. What do we need principal component analysis for? Or more importantly, what is a principal component to begin with? Well, to cut to the chase, PCA is a way of implementing dimensionality reduction, often referred to as lossy compression. This simply means that we want to transform some data living in high dimensional space into lower dimensions. Imagine having a data with hundreds of thousands of feature columns. It would take a lot of computing power to apply a machine learning model to fit the data and generate predictions. This is when PCA comes in: with PCA, we can figure out which dimensions are the most important and apply a transformation to compress that data into lower dimensions, making it a lot more tractable and easier to work with. And in case you‚Äôre still wondering, principal components refer to those new extracted dimensions used to newly represent data! Let‚Äôs derive PCA with some good old linear algebra tricks. I used Ian Goodfellow‚Äôs Deep Learning and a lecture slide from Columbia references for this post.",0,0,1,0,0,0,1,0
Moments in Statistics,"In the end, both (12) and (13) are pointing at the same quantity, namely the third moment of the exponential distribution. Perhaps the complexity of calculating either quantity is similar, and the question might just boil down to a matter of preference. However, this example shows that the MGF is a robust method of calculating moments of a distribution, and even more, potentially less computationally expensive than using the brute force method to directly calculate expected values. This was a short post on moments and moment generating functions. Moments was one of these terms that I had come across on Wikipedia or math stackexchange posts, but never had a chance to figure out. Hopefully, this post gave you some intuition behind the notion of moments, as well as how moment generating functions can be used to compute useful properties that explain a distribution. In the next post, we will take a brief break from the world of distributions and discuss some topics in information theory that I personally found interesting. If you would like to dwell on the question like ‚Äúhow do we quantify randomness,‚Äù don‚Äôt hesitate to tune in again in a few days!.",1,0,0,0,0,0,1,0
Gaussian Process Regression,"Simply put, this means that we don‚Äôt have to consider things like the typical  in the context of linear regression. Normally, we would start off with something like This is sometimes also written in terms of a weight vector  or a function . Here, we also have some Gaussian noise, denoted by : However, since GPs are non-parametric, we do not have to specify anything about the model. How do we remove this consideration? The short answer is that we marginalize out the model from the integral. Let  denote the model,  the data,  the predictions. Normally, an integral like the one above would be intractable without a solution in closed form. Hence, we would have to rely on random sampling methods such as MCMC. However, in this case, we do have a closed form solution, and we know what it looks like: a Gaussian! This means that we uniquely identify the final posterior distribution through GP regression‚Äîall we need is the mean and covariance. Let‚Äôs start with the easy one first: the mean. The mean is a trivial parameter because we can always normalize the mean to zero by subtracting the mean from the data. Therefore, for simplicity purposes, we assume a zero mean throughout this post. The interesting part lies in the covariance. Recall that the covariance matrix is defined as follows: Roughly speaking, covariance tells us how correlated two entries of the random vector are.",0,0,0,0,1,1,0,0
Complex Fibonacci,"Okay, maybe I‚Äôm being too melodramatic about a graph, but there is no denying that this pattern is geometrically interesting and pleasing to the eye. Everything looks so intentional and deliberate. The comments on the aesthetics of the snail shell aside, one point that deserves our attention is what appears to be a straight line. Well, turns out that this is, in fact, not a straight line. The only reason why it appears straight is that the snail pattern overshadows the little vibrations on this portion of the graph. Indeed, zooming in, we see that there is an interesting damping motion going on. This is what the fibonacci sequence would have looked like had we plotted only the positive domain of the real number line.  In this post, we took a look at the fibonacci sequence and its interpolation across the real number line. We could go even crazier, as did Matt Parker in his own video, by attempting to interpolate the sequence on the complex number plane, at which point we would now have a mapping from two dimensions to two dimensions, effectively forcing us to think in terms of four dimensions. There is no fast, handy way of drawing or visualizing four dimensions, as we are creatures that are naturally accustomed to three dimensions. There are interesting observations to be made with the full-fledged complex interpolation of the sequence, but I thought this is already interesting as it is nonetheless.",0,1,0,0,0,0,0,0
Naive Bayes Model From Scratch,"In other words, our training scheme can be summarized as: But this is all to abstract. Let‚Äôs get into the details by implementing the naive Bayes classifer from scratch. Before we proceed, however, I must tell you that there are many variations of the naive Bayes classifer. The variant that we will implement today is called the Gaussian naive Bayes classifer, because we assume that the distribution of the feature variables, denoted as , is normal. For a corresponding explanation of this model on , refer to this documentation. Let‚Äôs jump right into it. As per convention, we start by importing necessary modules for this tutorial. For reproducability, we specify a . The  magic commands are for the configuration of this Jupyter Notebook. Let‚Äôs begin by building some toy data. To make things simple, we will recycle the toy data set we used in the previous post on logistic regression and k-nearest neighbors. The advantage of using this data set is that we can easily visualize our data since all instances live in . In other words, we only have two axes:  and . For convenience, we preprocess our toy data set and labels into  arrays. The first step is to separate the data set by class values, since our goal is to find the distributions for each class that best describe the given data through MAP. To achieve this objective, we can create a function that returns a dictionary, where the key represents the class and the values contain the entries of the data set.",0,0,0,0,1,1,0,0
GAN in PyTorch,"The generator, on the other hand, tries to generate images that are as realistic as possible, and so is referred to as the counterfeiter. Below is our generator network. Although we could have created DCGANs, or deep convolutional adversarial networks, let‚Äôs go simple here and just use fully connected layers. Notice that I‚Äôve used the  PyTorch API instead of using class-based models. In this particular instance, we won‚Äôt have a complicated forward method, so the sequential API will suffice. Next, we create the generator. It is also a sequential model, inside of which are stacks of linear layers with ReLU activations. Before we jump into training, let‚Äôs move these networks to the  object we‚Äôve created earlier. The interesting part starts here. Notice that we have different optimizers for the discriminator and the generator. This is expected, since we are going to be training two different networks in a different manner. Before we get into the details of training, here is a simple utility function in which we zero the gradients for both the generator and the discriminator. We want the discriminator to be able to distinguish real from fake images. Therefore, the discriminator will have a combined loss: the loss that comes from falsely identifying real images as fake, and the loss that comes from confusing fake, generated images as real ones. For the generator, the loss is actually dependent upon the classifier: the loss comes from the classifier correctly identifying generated images as fake. Let‚Äôs see what this means below.",0,0,0,1,0,0,0,1
The Math Behind GANs,"Generative Adversarial Networks refer to a family of generative models that seek to discover the underlying distribution behind a certain data generating process. This distribution is discovered through an adversarial competition between a generator and a discriminator. As we saw in an earlier introductory post on GANs, the two models are trained such that the discriminator strives to distinguish between generated and true examples, while the generator seeks to confuse the discriminator by producing data that are as realistic and compelling as possible. In this post, we‚Äôll take a deep dive into the math behind GANs. My primary source of reference is Generative Adversarial Nets by Ian Goodfellow, et al. It is in this paper that Goodfellow first outlined the concept of a GAN, which is why it only makes sense that we commence from the analysis of this paper. Let‚Äôs begin! GAN can be seen as an interplay between two different models: the generator and the discriminator. Therefore, each model will have its own loss function. In this section, let‚Äôs try to motivate an intuitive understanding of the loss function for each. To minimize confusion, let‚Äôs define some notation that we will be using throughout this post. The goal of the discriminator is to correctly label generated images as false and empirical data points as true.",0,0,0,0,0,0,1,1
Demystifying Entropy (And More),"‚Äù While working at Bell Labs, Shannon was experimenting with methods to most efficiently encode and transmit data without loss of information. It is in this context that Shannon proposed the notion of entropy, which he roughly defined as the smallest possible size of lossless encoding of a message that can be achieved for transmission. Of course, there is a corresponding mathematical definition for entropy. But before we jump straight into entropy, let‚Äôs try to  develop some preliminary intuition on the concept of information, which is the building block of entropy. What is information? Warren Weaver, who popularized Shannon‚Äôs works and together developed the field of information theory, pointed out that information is not related to what is said, but what could be said. This element of uncertainty involved in one‚Äôs degree of freedom is what makes the notion of information inseparable from probability and randomness. As Ian Goodfellow put it in Deep Learning, The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred. In other words, a low probability event expresses a lot of information, while a high probability event expresses low information as its occurrence provides little information of value to the informed. Put differently, rare events require more information to represent than common ones. Consider, for example, how we might represent the amount of information involved in a fair coin toss.",0,0,0,0,0,0,1,0
A Step Up with  Variational Autoencoders,"This is why VAEs are considered to be generative models: if we feed the VAE some two-dimensional vector living in the latent space, it will spit out a digit. Whether or not that digit appears convincing depends on the random vector the decoder was provided as input: if the vector is close to the learned mean, , then the result will be convincing; if not, we might see a confusing blob of black and white. Let‚Äôs see what exactly is going on in the fuzzy region of the image, because that is apparently where all the digits mingle together and seem indistinguishable from one another. Put differently, if we vary the random vector little by little across that region, we will be able to see how the digit slowly morphs into another number.  How cool is that? We were able to get a VAE to show us how one digit can shift across a certain domain of the latent space. This is one of the many cool things we can do with a generative model like a variational autoencoder. In this post, we took a deep dive into the math behind variational autoencoders. It was a long journey, but definitely worth it because it exposed us to many core concepts in deep learning and statistics. At the same time, I found it fascinating to see how a model could learn from a representation to generate numbers, as we saw in the very last figure.",0,0,0,0,0,0,0,1
(Attempt at) Knowledge Distillation,"From a dark knowledge point of view, this is not great, as it makes it more difficult for the student model to learn the dark knowledge from the teacher. Thus, the authors of the paper introduce a temperature parameter to modify the softmax function. Let‚Äôs denote this as softmax prime. When , it is identical to the softmax function. When  gets larger, it smoothens the distribution. This is because the derivative of the exponential function near smaller values are smaller compared to that at larger values. To put this into perspective, let‚Äôs write a simple function that demonstrates this observation visually. is a code translation of (2) above. Note that we could have made this more numerically stable by subtracting the maximum value from the logits. But for sake of simplicity, we implement the formula as-is. Now, we can see how different temperature values yield different softmax outputs. Here, we consider integer temperature values from 1 to 5, inclusive.  From the graph, one can easily see that the higher the temperature value, the smoother the distribution becomes. In other words, it makes it easier for the student model to learn dark knowledge. This softmax variation is key to knowledge distillation. Since we now train the student model on both the soft label logits of the teacher model and the hard labels from the dataset, it is necessary to revisit what the loss function is.",0,0,0,1,0,0,0,1
A Simple Autocomplete Model,"3: is a woman‚Äìwhat then? is there not ground
 for suspecting that the experience and present strange of the soul is also as the stand of the most profound that the present the art and possible to the present spore as a man and the morality and present self instinct, and the subject that the presence of the surcessize, and also it is an action which the philosophers and the spirit has the consider the action to the philosopher and possess and the spirit is not be who can something the predicess of the constinate the same and self-interpatence, the disconsises what is not to be more profound, as if it is a man as a distance of the same art and ther strict to the presing to the result the problem of the present the spirit what is the consequences and the development of the same art of philosophers and security and spirit and for the subjective in the disturce, as in the contrary and present stronger and present could not be an inclination and desires of the same and distinguished that is the discoverty in such a person itself influence and ethers as Generated text at temperature 0.",0,0,0,0,0,0,0,1
"Beta, Bayes, and Multi-armed Bandits","Recently, I fortuitously came across an interesting blog post on the multi-armed bandit problem, or MAB for short. I say fortuitous because the contents of this blog nicely coincided with a post I had meant to write for a very long time: revisiting the Beta distribution, conjugate priors, and all that good stuff. I decided that the MAB would be a refreshing way to discuss this topic. ‚ÄúBayesian‚Äù is a buzz word that statisticians and ML people love anyway, me shamelessly included. In this post, we will start off with a brief introduction into what the MAB problem is, why it is relevant, and how we can use some basic Bayesian analysis with Beta and Bernoulli distributions to derive a nice sampling algorithm, known as Thompson sampling. Let‚Äôs dive right into it. The multi-armed bandit problem is a classical gambling setup in which a gambler has the choice of pulling the lever of any one of  slot machines, or bandits. The probability of winning for each slot machine is fixed, but of course the gambler has no idea what these probabilities are. To ascertain out the values of these parameters, the gambler must learn through some trial and error. Given this setup, what is the optimal way of going about the problem? One obvious way is to start randomly pulling on some slot machines until they get a rough idea of what the success probabilities are.",1,0,0,0,0,0,1,0
So What are Autoencoders?,"Of course, some information is inevitably going to be lost‚Äîafter all, how can five numbers describe the entirety of an image? However, what‚Äôs important and fascinating about autoencoders is that, with appropriate training and configuration, they manage to find ways to best compress input data into latent vectors that can be decoded to regenerate a close approximation of the input data. For the purposes of this demonstration, let‚Äôs configure the latent dimension of the encoder to be 128 dimensions‚Äîin other words, each 28-by-28, single-channel image will be encoded into vectors living in 128 dimensional space. It‚Äôs time to build the autoencoder model. In summary, an autoencoder is composed of two components: an encoder and a decoder. The encoder transfers input data into the latent dimension, and the decoder performs the exact reverse: it takes vectors in the latent space and rearranges it to bring it back into its original dimension, which is, in this case, a 28-by-28, single-channel image. The followign code snippet implements this logic using the  functional API. Let‚Äôs declare the encoder and autoencoder model by invoking the  function with the specified image shape and the dimensionality of the latent space. Just to get a sense of what operations are taking place dimensionality-wise, here is a look at the output shapes of the autoencoder model. Notice that the input is of shape , and that the final output is also of the same shape , as expected. Here‚Äôs the image of the model for the fancy bells and whistles.",0,0,0,0,0,0,0,1
Recommendation Algorithm with SVD,"We would expect singular value decomposition to capture these observations in some way, albeit approximately. Now, let‚Äôs actually perform singular value decomposition on the ratings matrix. We could try to do this manually by hand, but let‚Äôs utilize the power of modern computing to save ourselves of the time and mental effort involved in calculating the eigenvalues and eigenvectors of a ten-by-ten matrix. Luckily for us, the  module contains some excellent functionality to help us with singular value decomposition. Using this library, singular value decomposition can very simply be achieved with just a few lines of code. The parameters of the  function are , the ratings matrix, and , the number of non-trivial entries of  to select for dimensionality reduction, as we have seen earlier. More technically speaking,  corresponds to the number of ‚Äúconcepts‚Äù or dimensions that we will extract from the matrix. Let‚Äôs see what this means by actually running this function. Great! This is what dimensionality reduction means in the loosest sense. Instead of having 5 entries each row, as we had with the original ratings matrix , we now have 3 entries per row. In other words, the information on users has been compressed into three dimensions. Unlike in , where each column corresponded to some movie, we don‚Äôt really know what the columns of  stand for. It might be some genre, actress, or any hidden patterns in the data set that we are not aware of. Regardless, what‚Äôs important here is that we can now understand data more easily in smaller dimensions.",0,0,1,0,0,1,0,0
"Basel, Zeta, and some more Euler","In the later segment of his life, Euler found a way to express the zeta function as, you guessed it, an infinite product. This time, however, Euler did not rely on Taylor polynomials. Instead, he employed a more general approach to the problem. It is here that we witness Euler‚Äôs clever manipulation of equations again. We commence from the zeta function, whose terms are enumerated below. Much like how we multiply the ratio to a geometric sequence to calculate its sum, we adopt a similar approach by multiplying the second term,  to the entire expression. This operations yields By subtracting this modified zeta function from the original, we derive the following expression below. Now, we only have what might be considered as the odd terms of the original zeta function. We then essentially repeat the operation we have performed so far, by multiplying the expression by  and subtracting the result from the odd-term zeta function. It is not difficult to see that iterating through this process will eventually yield Euler‚Äôs product identity for the zeta function. The key to understanding this identity is that only prime numbers will appear as a component of the product identity. We can see this by reminding ourselves of the clockwork behind the sieve of Eratosthenes, which is basically how the elimination and factorization works in the derivation of Euler‚Äôs identity. Taking this into account, we can deduce that Euler‚Äôs identity will take the following form: This expression is Euler‚Äôs infinite product representation of the zeta function.",0,1,0,0,0,0,0,0
Recommendation Algorithm with SVD,"In eigendecomposition, the factors were all square matrices whose dimension was identical to that of the matrix that we sought to decompose. In SVD, however, since the target matrix can be rectangular, the factors are always of the same shape. The second point to note is that  and  are orthogonal matrices; , a diagonal matrix. This decomposition structure is similar to that of eigendecomposition, and this is no coincidence: in fact, formula (1) can simply be shown by performing an eigendecomposition on  and . Let‚Äôs begin by calculating the first case, , assuming formiula (1). This process looks as follows: The last equality stands since the inverse of an orthogonal matrix is equal to its transpose. Substituting  for , equation (2) simplifies to And we finally have what we have seen with eigendecomposition: a matrix of independent vectors equal to the rank of the original matrix, a diagonal matrix, and an inverse. Indeed, what we have in (3) is an eigendecomposition of the matrix . Intuitively speaking, because matrix  is not necessarily square, we calculate  to make it square, then perform the familiar eigendecomposition. Note that we have orthogonal eigenvectors in this case because  is a symmetric matrix‚Äîmore specifically, positive semi-definite. We won‚Äôt get into this subtopic too much, but we will explore a very simple proof for this property, so don‚Äôt worry. For now, let‚Äôs continue with our exploration of the SVD formula by turning our attention from matrix ‚Äîa factor of eigendecomposition on ‚Äîto the matrix .",0,0,1,0,0,1,0,0
Dissecting LSTMs,"Given an intermediate variable we can express the gradient in the following fashion: Then, we can obtain  by un-concatenation: where  denotes the number of neurons in the LSTM layer. We can do the same for . This is a lot simpler: These gradients, of course, will be passed onto the next iteration of backpropagation, just like we had assumed that the values of  and  were given from the previous sequence of backpropagation. Because DL libraries make it extremely easy to declare and train LSTM networks, it‚Äôs often easy to gloss over what actually happens under the hood. However, there is certainly merit to dissecting and trying to understand the inner-working of DL models like LSTM cells, which offer a fascinating way of understanding the notion of memory. This is also important since RNNs are the basis of other more complicated models such as attention-based models or transformers, which is arguably the hottest topic these days in the field of NLP with the introduction of GPT-3 by OpenAI. I hope you have enjoyed reading this post. Catch you up in the next one!.",0,0,0,0,0,0,0,1
A sneak peek at Bayesian Inference,"Given that  and  are discrete events, we can break down  as a union of intersections between  and , where  represents subsets within event . In concrete form, we can rewrite this as Additionally, we can rewrite the conditional probability  in terms of  and  according to the definition of conditional probability we observed earlier. Applying these alterations to (4) to rewrite  produces equation (5): This is the equation of Bayes‚Äô theorem. In simple language, Bayes‚Äô theorem tells us that the conditional probability of some subset  given  is equal to its relevant fraction within a weighted summation of the conditional probabilities  given . Although this equation may seem complicated at a glance, we can develop an intuition for this formula by reminding ourselves of the definition of conditional probabilities, as well as the fact that independent events can be expressed as a union of intersections. At the end of the day, Bayes‚Äô theorem provides a powerful tool through which we can calculate a conditional probability in terms of its reverse, i.e. calculate  by utilizing . Why is this important at all? Let‚Äôs return back to our example of the potential patient. Recall that the conditional probability of our interest was while the pieces of information we were provided were This is where Bayes‚Äô theorem comes in handy. Notice that we have expressed  in terms of  and .",1,0,0,0,0,0,1,0
The Exponential Family,"Much like in the previous post on maximum likelihood estimation, we begin with some data set of  independent and identically distributed observations. This is going to be the setup of the MLE problem. Given this dataset, the objective of maximum likelihood estimation is to identify some parameter  that maximizes the likelihood, i.e. the probability of observing these data points under a probability distribution defined by . In other words, How do we identify this parameter? Well, the go-to equipment in a mathematician‚Äôs arsenal for an optimization problem like this one is calculus. Recall that our goal is to maximize the likelihood function, which can be calculated as follows: The first equality stands due to the assumption that all data are independent and identically distributed. Maximizing (16) is a complicated task, especially because we are dealing with a large product. Products aren‚Äôt bad, but we typically prefer sums because they are easier to work with. A simple hack that we almost always use when dealing with maximum likelihood, therefore, is to apply a log transformation to calculate the log likelihood, since the logarithm is a monotonically increasing function. In other words, What does the log likelihood look like? Well, all we have to do is to apply a log function to (16), which yields the following result. Maximizing the log liklihood can be achieved by setting the gradient to zero, as the gods of calculus would tell us.",1,0,0,0,0,0,1,0
"Beta, Bayes, and Multi-armed Bandits","Note that a lot of the code in this post was largely inspired by Peter‚Äôs blog. Before we get into any modeling, let‚Äôs first import the modules we‚Äôll need and set things up. Instead of using the approach outlined in the blog I‚Äôve linked to, I decided to use objects to model bandits. The rationale is that this approach seems to make a little bit more intuitive sense to me. Also, working with Django these days has made me feel more affinity with Python classes. At any rate, let‚Äôs go ahead. Now, we can initialize a bandit with some predetermined parameter, . Of course, our goal would be to determine the true value of this parameter through sampling and Bayesian magic. In this case, we have created a fair bandit with a  of 0.5. We can also simulate level pulls by repeatedly calling on . Note that this will accumulate the result of each Bernoulli trial in the  list object. Notice that we also have . This is an a list object that c Now that we have performed all the basic sanity checks we need, let‚Äôs quickly go ahead and create three bandit objects for demonstration purposes. Now we get into the details of how to perform the Bayesian update. More specifically, we‚Äôre interested in how we are going to use posterior probabilities to make decisions on which slot machine to pull on. This is where Thompson sampling comes in.",1,0,0,0,0,0,1,0
PyTorch RNN from Scratch,"It‚Äôs also not entirely fair game for the model since there are many names that might be described as multi-national: perhaps there is a Russian person with the name of Demirkan. I learned quite a bit about RNNs by implementing this RNN. It is admittedly simple, and it is somewhat different from the PyTorch layer-based approach in that it requires us to loop through each character manually, but the low-level nature of it forced me to think more about tensor dimensions and the purpose of having a division between the hidden state and output. It was also a healthy reminder of how RNNs can be difficult to train. In the coming posts, we will be looking at sequence-to-sequence models, or seq2seq for short. Ever since I heard about seq2seq, I was fascinated by tthe power of transforming one form of data to another. Although these models cannot be realistically trained on a CPU given the constraints of my local machine, I think implementing them themselves will be an exciting challenge. Catch you up in the next one!.",0,0,0,1,0,1,0,1
The Math Behind GANs,"This is because the gradient of the function  is steeper near  than that of the function , meaning that trying to maximize , or equivalently, minimizing  is going to lead to quicker, more substantial improvements to the performance of the generator than trying to minimize . Now that we have defined the loss functions for the generator and the discriminator, it‚Äôs time to leverage some math to solve the optimization problem, i.e. finding the parameters for the generator and the discriminator such that the loss functions are optimized. This corresponds to training the model in practical terms. When training a GAN, we typically train one model at a time. In other words, when training the discriminator, the generator is assumed as fixed. We saw this in action in the previous post on how to build a basic GAN. Let‚Äôs return back to the min-max game. The quantity of interest can be defined as a function of  and . Let‚Äôs call this the value function: In reality, we are more interested in the distribution modeled by the generator than . Therefore, let‚Äôs create a new variable, , and use this substitution to rewrite the value function: The goal of the discriminator is to maximize this value function.",0,0,0,0,0,0,1,1
Fisher Score and Information,"We square that quantity to prevent negative values from canceling out positive ones. Covariance is just an extension of this concept applied to a comparison of two random variables instead of one. Here, we consider how two variables move in tandem. And the variance-covariance matrix is simply a matrix that contains information on the covariance of multiple random variables in a neat, compact matrix form. A closed-form expression for the covariance matrix  given a random vector , which follows immediately from aforementioned definitions and some linear algebra, looks as follows: Enough of the prologue and review, now we‚Äôre ready to start talking about Fisher. The information matrix is defined as the covariance matrix of the score function as a random vector. Concretely, Note that the 0‚Äôs follow straight from the earlier observation that . Intuitively, Fisher‚Äôs information gives us an estimate of how certain we are about the estimate of the parameter . This can be seen by recognizing the apparent similarity between the definition of the covariance matrix we have defined above and the definition of Fisher‚Äôs information. In fact, the variance of the parameter  is explained by the inverse of Fisher‚Äôs information matrix, and this concept is known as the Cramer-Rao Lower Bound. For the purposes of this post, I won‚Äôt get deep into what CRLB is, but there are interesting connections we can make between Fisher‚Äôs information, CRLB, and the likelihood, which we will get into later. Because Fisher‚Äôs information requires computing the expectation given some probability distribution, it is often intractable.",0,0,0,0,0,0,1,0
A sneak peek at Bayesian Inference,"Let‚Äôs start by coming up with a model representation of the likelihood function, which we might recall is the probability of having a parameter value of  given some data . It is not difficult to see that the best distribution for the likelihood function given the setup of the problem is the binary distribution since each coin flip is a Bernoulli trial. Let  denote a random variable that represents the number of tails in  coin flips. For convenience purposes, we define 1 to be heads and 0 to be tails. Then, the conditional probability of obtaining  heads given a fairness parameter  can be expressed as We can perform a quick sanity check on this formula by observing that, when , the probability of observing  heads diminishes to 0, unless , in which case the probability becomes 1. This behavior is expected since  represents a perfectly biased coin that always shows tails. By symmetry, the same logic applies to a hypothetical coin that always shows heads, and represents a fairness parameter of 1. Now that we have derived a likelihood function, we move onto the next component necessary for Bayesian analysis: the prior. Determining a probability distribution for the prior is a bit more challenging than coming up with the likelihood function, but we do have certain clues as to what characteristics our prior should look possess. First, the domain of the prior probability distribution should be contained within . This is because the range of the fairness parameter  is also defined within this range.",1,0,0,0,0,0,1,0
Recommendation Algorithm with SVD,"I‚Äôve been using a music streaming service for the past few weeks, and it‚Äôs been a great experience so far. I usually listen to some smoothing new age piano or jazz while I‚Äôm working, while I prefer K-pop on my daily commutes and bass-heavy house music during my workouts. Having processed these information through repeated user input on my part, the streaming application now regularly generates playlists each reflective of the three different genres of music that I enjoy most. This got me wondering: what is the underlying algorithm beind content selection and recommendation? How do prominent streaming services such as Netflix and Spotify provide recommendations to their users that seem to reflect their personal preferences and tastes? From a business perspective, these questions carry extreme significance since the accuracy of a recommendation algorithm may directly impact sales revenue. In this post, we will dive into this question by developing an elementary recommendation engine. The mechanism we will use to achieve this objective is a technique in linear algebra known as singular value decomposition or SVD for short. SVD is an incredibly powerful way of processing data, and also ties in with other important techniques in applied statistics such as principal component analysis, which we might also take a look at in a future post. Enough with the preface, let‚Äôs dive right into developing our model. Before we start coding away, let‚Äôs first try to understand what singular value decomposition is.",0,0,1,0,0,1,0,0
"Newton-Raphson, Secant, and More","Below is an implementation of the Newton-Raphson method in Python. I‚Äôve added some parameters to the function for functionality and customization.  is simply some small value we use to decide when to stop the update; if the change in the value of the root is so small that it is not worth the extra compute, we should stop.  determines how many iterations we want to continue. If the algorithm is unable to find the root within  iterations, it likely means that the function provided does not have a root, or at the very least, the root is not discoverable via the algorithm. Lastly,  is a flag that determines whether we return the full update history or simply the last value in the iteration as a single value. One peculiarity that deserves attention is the  exception, which occurs in this case if the number of arguments passed into the function does not match. I added this   block to take into account the fact that the  method and other approximate derivative calculation methods such as  have differing numbers of parameters. Let‚Äôs see if this actually works by using the example we‚Äôve been reusing thus far, , or  and , both of which we have already defined and initialized above. The root seems to be around 2.7. And indeed, if we cube it, we end up with a value extremely close to 20. In other words, we have successfully found the root to . Instead of the direct derivative, , we can also use approximation methods.",0,1,0,0,0,0,0,0
Principal Component Analysis,"PCA is a very useful technique used in many areas of machine learning. One of the most common applications is to apply PCA to a high-dimensional dataset before applying a clustering algorithm. This makes it easier for the ML model to cluster data, since the data is now aligned in such a way that it shows the most variance. Upon some more research, I also found an interesting paper that shows that there is a solid mathematical relationship between K-means clustering and PCA. I haven‚Äôt read the paper from top to bottom, but instead glossed over a summary of the paper on this thread on stack overflow. It‚Äôs certainly a lot of information to take in, and I have no intent of covering this topic in this already rather lengthy post on PCA. So perhaps this discussion will be tabled for a later time, as interesting as it seems. I hope you enjoyed reading this post. Amidst the chaos of the COVID19 pandemic, let‚Äôs try to stay strong and find peace ruminating over some matrices and formulas. Trust me, it works better than you might think.",0,0,1,0,0,0,1,0
Fourier Series,"Taylor series is used in countless areas of mathematics and sciences. It is a handy little tool in the mathematicians arsenal that allows us to decompose any function into a series of polynomials, which are fairly easy to work with. Today, we are going to take a brief look at another type of series expansion, known as Fourier series. Note that these concepts are my annotations of Professor Gilbert Strang‚Äôs amazing lecture, available on YouTube. The biggest difference between Taylor series and Fourier series is that, unlike Taylor series, whose basic fundamental unit is a polynomial term, the building block of a Fourier series is a trigonometric function, namely one of either sine or cosine. Concretely, a generic formula of a Fourier expansion looks as follows: Personally, I found this formula to be more difficult to intuit than the Taylor series. However, once you understand the underlying mechanics, it‚Äôs fascinating to see how periodic wave functions can be decomposed as such. First, let‚Äôs begin with an analysis of orthogonality. Commonly, we define to vectors  and  as being orthogonal if That is, if their dot product yields zero. This follows from the definition of a dot product, which has to do with cosines. With a stretch of imagination, we can extend this definition of orthogonality to the context of functions, not just vectors. For vectors, a dot product entails summing the element-wise products of each component. Functions don‚Äôt quite have a clearly defined, discrete component. Therefore, instead of simply adding, we integrate over a given domain.",0,1,0,0,0,0,1,0
Word2vec from Scratch,"This process is exactly what embedding is: as we start training this model with the training data generated above, we would expect the row space of this weight matrix to encode meaningful semantic information from the training data. Continuing onwards, here is the second layer that receives as input the embeddings, then uses them to generate a set of outputs. We are almost done. All we now need in the last layer is a softmax layer. When the output is passed into this layer, it is converted into probability vectors whose elements sum up to one. This final output can be considered as context predictions, i.e. which words are likely to be in the window vicinity of the input word. In training‚Äîspecifically error calculation and backpropagation‚Äîwe would be comparing this prediction of probability vectors with its true one-hot encoded targets. The error function that we use with softmax is cross entropy, defined as I like to think of this as a dot product of the target vector and the log of the prediction, because that is essentially what the summation is doing. In this alternate formulation, the cross entropy formula can be rewritten as Because  a one-hot encoded vector in this case, all the elements in  whose entry is zero will have no effect on the final outcome. Indeed, we simply end up taking the negative log of the prediction. Notice that the closer the value of the prediction is to 1, the smaller the cross entropy, and vice versa.",0,0,0,0,0,1,0,1
A Brief Introduction to Recurrent Neural Networks,"To better understand this phenomena, we probably have to run more trials with more data over longer iterations than we have done in this tutorial. This point notwithstanding, it is interesting to see how a single layer LSTM network can outperform a stacked RNN network.  The last up on this list is the one-dimensional convolutional neural network. The convent produced very remarkable results in this experiment, especially given its extremely short training time. Recurrent neural networks typically take a lot of time to train‚Äîeven when they are not stacked‚Äîbecause each neuron is defined by a rather complicated operation involving many parameters, such as states, carriage, and so on. Convents, on the other hand, are relatively simpler, and thus take noticeably shorter to train and deploy. This tutorial demonstrates that convents can perform as well as simple recurrent networks to establish a baseline performance metric.  In this post, we briefly introduced and explored the concept of recurrent neural networks, how they work, and how to build them using the  functional API. Recurrent neural networks are one of the hottest topics in the contemporary deep learning academia because it presents numerous possibilities for applications. Hopefully this post gave you a better understanding of what all the hype is about, why RNNs are effective at what they do, and how they can be used in the context of basic natural language processing. In the next post, we will take a look at another interesting natural language processing task.",0,0,0,0,0,0,0,1
First Neural Network with Keras,"Note that the hidden layer uses the  function as its activation function. The dropout layers ensure that our model does not overfit to the data. The last output layer has 10 neurons, each corresponding to digits from 0 to 9. The activation fuction of this last layer is the softmax function, which allows us to interpret the final results as a categorical distribution. Let‚Äôs double check that the layers have been formed correctly as per our intended design. Everything looks good, which means we are now ready to compile and train our model. Before we do that, however, it is always a good idea to use the  module to ensure that gradient descent stops when no substantial weight adjustments are being made to our model. In other words, when the model successfully finds the local minimum (or preferably the global minimum), the  will kick in and stop gradient descent from proceeding with further epochs. We are now ready to go! Let‚Äôs compile the model by making some configurations, namely the , , and . Simply put, an  specifies which flavor of the gradient descent algorithm we want to choose. The simplest version is known as , or the stochastic gradient descent.  can be considered an improved version of the stochastic gradient descent in that its learning rate changes depending on the slope of the loss function, defined here as cross entropy. If you recall, cross entropy is basically a measurement of the pseudo-distance between two distributions, i.e. how different two distributions are.",0,0,0,0,1,0,0,1
Building Neural Network From Scratch,"Forward propagation is great and all, but without appropriately trained weights, our model is obviously going to spit out meaningless predictions. The way to go about this is to use the gradient descent algorithm with back propagation. We will discuss more about back propagation in the next subsection, as it is a meaty topic that deserves space of its own. We deal primarily with the former in this section. This is not the first time that we have come across gradient descent on this blog. In fact, we used gradient descent to optimizse our logistic regression model in this post. Recall that the gradient descent algorithm can be summarized as where  represents the parameters, or weights,  represents the learning rate, and  represents the loss function. This is the vanilla gradient descent algorithm, which is also referred to as batch gradient descent. Minibatch gradient descent is similar to gradient descent. The only point of difference is that it calculates the gradient for each minibatch instead of doing so for the entire dataset as does batch gradient descent. The advantage of using a minibatch is that it is computationally lighter and less expensive. Minibatch gradient descent can be considered a happy point of compromise between stochastic and batch gradient descent, which lie on the polar opposite ends of the spectrum. Let‚Äôs first take a look at the  function, which divides the  and  into  and  given a .",0,0,1,0,0,1,0,1
