title,body,tensorflow,linear_algebra,statistics,machine_learning,probability_distribution,deep_learning,from_scratch,analysis
Gamma and Zeta,"The article also notes, however, that this definition only applies in a limited number of cases. This is because we’ve assumed, in using the summation of the geometric series formula, the fact that . Today’s post was a short yet very interesting piece on the relationship between the Gamma and the Riemann zeta. One thing I think could have been executed better is the depth of the article—for instance, what is the Bose integral and when is it used? I’ve read a few comments on the original YouTube video by blackpenredpen, where people were saying that the Bose integral is used in statistical mechanics and the study of black matter, but discussing that would require so much domain knowledge to cover. Regardless, I think the theoretical aspect of this derivation is interesting nonetheless. One thing I must do is writing a post on divergence and when the interchange of summation and integrals can be performed. I was originally planning to write a much longer article dividing deep into the Gamma and the Beta function as well as their distributions. However, I realized that what I need at this point in time is producing output and reorienting myself back to self-studying blogger mode, perhaps taking a brief hiatus from the grinding intern spending endless hours in Sublime text with Django (of course, I’m doing that because I enjoy and love the dev work).",0,0,0,0,0,0,0,1
Maximum A Posteriori Estimation,"This can be seen by simply interpreting the posterior from a conditional probability point of view: the posterior denotes the probability of the value of the model parameter is  given data . Put differently, the value of  that maximizes the posterior is the optimal parameter value that best explains the sample observations. This is why at its heart, MAP is not so much different from MLE: although MLE is frequentist while MAP is Bayesian, the underlying objective of the two methods are fundamentally identical. And indeed, this similarity can also be seen through math. And we see that (5) is almost identical to (3), the formula for MLE! The only part where (5) differs is the inclusion of an additional term in the end, the log prior. What does this difference intuitively mean? Simply put, if we specify a prior distribution for the model parameter, the likelihood is no longer just determined by the likelihood of each data point, but also weighted by the specified prior. Consider the prior as an additional “constraint”, construed in a loose sense. The optimal parameter not only has to conform to the given data, but also not deviate too much from the established prior. To get a more intuitive hold of the role that a Bayesian prior plays in MAP, let’s assume the simplest, most uninformative prior we can consider: the uniform distribution. A uniform prior conveys zero beliefs about the distribution of the parameter, i.e. all values of  are equally probable.",0,0,1,0,0,0,0,0
Bayesian Linear Regression,"In today’s post, we will take a look at Bayesian linear regression. Both Bayes and linear regression should be familiar names, as we have dealt with these two topics on this blog before. The Bayesian linear regression method is a type of linear regression approach that borrows heavily from Bayesian principles. The biggest difference between what we might call the vanilla linear regression method and the Bayesian approach is that the latter provides a probability distribution instead of a point estimate. In other words, it allows us to reflect uncertainty in our estimate, which is an additional dimension of information that can be useful in many situations. By now, hopefully you are fully convinced that Bayesian linear regression is worthy of our intellectual exploration. Let’s take a deep dive into Bayesian linear regression, then see how it works out in code using the  library. In this section, we will derive the formula for Bayesian linear regression step-by-step. If you are feeling rusty on linear algebra or Bayesian analysis, I recommend that you go take a quick review of these concepts before proceeding. Note that I borrowed heavily from this video for reference. For any regression problem, we first need a data set. Let  denote this pre-provided data set, containing  entries where each entry contains an -dimensional vector and a corresponding scalar. Concretely, where The goal of Bayesian linear regression is to find the predictive posterior distribution for . This is where the difference between Bayesian linear regression and the normal equation method becomes most apparent.",0,1,0,0,0,0,0,0
Riemann Zeta and Prime Numbers,"Based on our earlier mathematical analysis, we would expect this convergence to get even better as we expand out the  range, with an upper bound that is greater than the current 200.  While jumping around in Wikipedia, I came across the Dirichlet Eta function, which is a slight variant of the Riemann Zeta function. This function looks as follows: As you can see, this is essentially the alternating version of the Riemann Zeta function. Given this design, we can derive what may appear to be apparent to some yet nonetheless interesting relationship between the Eta and Zeta. Deriving this relationship requires a very similar operation to the sieving or factorizing we performed earlier to derive the probabilistic interpretation of the Zeta function. For a bit of intuition, observe that the Eta function can be split up into what may be referred to as even and odd terms. In other words, The idea is that the even terms are just a multiple of the Zeta function, namely Then, the odd terms can also be seen as the Zeta function minus this multiple: We now have successfully expressed both the even and odd terms of the Eta function in terms of the Zeta function. If we put the two together, we will then be able to express the entirety of the Eta function fully in terms of the Zeta function.",0,0,0,0,0,0,0,1
Gaussian Process Regression,"In this post, we will explore the Gaussian Process in the context of regression. This is a topic I meant to study for a long time, yet was never able to due to the seemingly intimidating mathematics involved. However, after consulting some extremely well-curated resources on this topic, such as Kilian’s lecture notes and UBC lecture videos by Nando de Freitas, I think I’m finally starting to understand what GP is. I highly recommend that you check out these resources, as they are both very beginner friendly and build up each concept from the basics. With that out of the way, let’s get started. Let’s begin by considering the classic setup of a regression problem. The goal of regression is to predict some values given a set of observations, otherwise referred to as a training set. There are of course many variants of the regression problem. For instance, in a previous post we took a look at Bayesian linear regression, where instead of a single point estimate, we tried to derive a distribution of the predicted data at a given test point. Gaussian Processes (GPs) are similar to Bayesian linear regression in that the final result is a distribution which we can sample from. The biggest point of difference between GP and Bayesian regression, however, is that GP is a fundamentally non-parametric approach, whereas the latter is a parametric one.",0,0,0,1,0,0,1,0
The Gibbs Sampler,"The goal is to derive is the conditional distribution, . This derivation was heavily adapted from this source and this thread on Stack Exchange. It is certainly a somewhat lengthy derivation, but there is nothing too conceptually difficult involved—it’s just a lot of algebra and simplifications. We begin from the formula for the multivariate Gaussian: For convenience purposes, let Then, Let Note that this is not a one-to-one correspondence, i.e. . The blocks are only one-to-one insofar as being dimensionally equivalent. Then, using block matrix multiplication, Notice that the final result should be a single scalar given the dimensions of each matrix. Therefore, we can further simply the expression above using the fact that . Specifically, the second and third terms are transposes of each other. Although we simply resorted a convenient substitution in (6), we still need to derive an expression for the inverse of the covariance matrix. Note that the inverse of the covariance matrix can intuitively be understood as the precision matrix. We won’t derive the block matrix inversion formula here. The derivation is just a matter of simply plugging in and substituting one expression for another. For a detailed full derivation, checkout this link or this journal article. To cut to the chase, we end up with Plugging these results back into (8), and with some elided simplification steps, we end up with Note that we can more conveniently express the result in the following fashion: We’re now almost done.",0,1,1,0,0,0,0,0
An Introduction to Markov Chain Monte Carlo,"The trace plot below shows that, although the MCMC model does manage to sample many values, it likes to stay too much in its current state, thus making taking much longer for the sampler to properly estimate the posterior by sampling a wide range of values. The bottom line is that setting the right proposal distribution is important, and that trace plots are a good place to start to check if the proposal distribution is set up properly.  Now, it’s time to look at the answer key and see if our sampler has done well. Let’s plot  sampled by our Metropolis-Hastings sampler with the analytic posterior to see if they roughly match.  Fantastic! Although the estimated posterior is not exactly equal to the analytic posterior, the two are quite similar to each other. We could quantify how similar or different they are by using metrics such as , but for simplicity’s sake, let’s contain the post within the realm of Bayes as we have done so far. This goes to show just how useful and powerful Markov Chain Monte Carlo can be: even if a complicated likelihood function in high dimensional space, we would be able to use a similar sampling sequence to estimate the posterior. What’s even more fascinating about Markov Chain Monte Carlo is that, regardless of the value we start off with in the proposal distribution, we will eventually be able to approximate the posterior.",0,0,0,0,0,0,1,0
Understanding the  Leibniz Rule,"Before I begin, I must say that this video by Brian Storey at Olin College is the most intuitive explanation of the Leibniz rule I have seen so far. Granted, my greedy search over the internet space was by no means exhaustive, so I’ve probably missed some other hidden gems  here and there. Also, the video is intended as a visual explanation for beginners rather than a robust analytical proof of the Leibniz rule. This point notwithstanding, I highly recommend that you check out the video. This post is going to provide a short, condensed summary of the proof presented in the video, minus the fancy visualization that pen and paper can afford. The Leibniz rule, sometimes referred to as Feynman’s rule or differentiation-under-the-integral-sign-rule, is an interesting, highly useful way of computing complicated integrals. A simple version of the Leibniz rule might be stated as follows: As you can see, what this rule essentially tells us is that integrals and derivatives are interchangeable under mild conditions. We’ve used this rule many times in a previous post on Fisher’s information matrix when computing expected values that involved derivatives. Why is this the case? It turns out that the Leibniz rule can be proved by using the definition of derivatives and some Taylor expansion. Recall that the definition of a derivative can be written as This is something that we’d see straight out of a calculus textbook.",0,0,0,0,0,0,0,1
Logistic Regression Model from Scratch,"05, 0.1, and 0.5. Just like before, we cap the number of iterations to 200 epochs.  The graph shows that the larger the learning rate, the quicker the decrease in cross entropy loss. This result is coherent with what the previous visualizations on accuracy suggested: the higher the learning rate, the quicker the model learns from the training data. In this post, we built the logistic regression model from scratch by deriving an equation for gradient descent on cross entropy given a sigmoid function. In the process, we brought together many useful concepts we explored on this blog previously, such as matrix calculus, cross entropy, and more. It’s always exciting to see when seemingly unrelated concepts come together to form beautiful pictures in unexpected ways, and that is what motivates me to continue my journey down this road. The logistic regression model is simple yet incredibly powerful in the context of binary classification. As we saw earlier with the application of the model to the task of bank notes authentication, the logistic regression model can, when tweaked with the appropriate parameters, make surprisingly accurate predictions given sufficient amount of training data. Of course, the processing of training and tweaking is not always easy because we have to determine some hyperparameters, most notably the learning rate of the gradient descent algorithm, but the fact that logistic regression is a robust model is unchanged nonetheless. Hopefully this post gave you some idea of what happens behind the scene in a regression-based machine learning model.",0,0,0,1,0,0,1,0
A Step Up with  Variational Autoencoders,"Let’s start out by taking a look at the sampling function we will use to define one of the layers of the variational Autoencoder network. Simply put, the  above below takes as arguments  and  in the form of a bundled list. As you can guess from the name of the variables, these two  parameters refer to the mean and log variance of the random vector living in our predefined latent space. Note that we are assuming a diagonal Gaussian here: in other words, the covariance matrix of the multi-dimensional Gaussian is assumed to be diagonal, meaning that each elements of the vector are independent. If any of this sounds foreign to you, I recommend that you read this post on the Gaussian distribution. Let’s continue our discussion with the sampling function. The goal here is to sample a random vector in the latent space from the distribution specified by the two parameters, mean and log variance. The sampling process can be expressed as follows: where  denotes the mean, corresponding to ,  denotes a tensor of random numbers sampled from the standard normal distribution, and  denotes the standard deviation (we will see how this is related to  in just a moment). Essentially, the goal here is to use a resampling technique such that we can sample from a standard normal distribution centered around mean 0 and a standard deviation of 1, but consequentially sample from a distribution of  living in the latent space.",1,0,0,0,0,1,0,0
A PyTorch Primer,"For instance, we can easily create a matrice of ones as follows: The  is a parameter we pass into the function to tell PyTorch that this is something we want to keep track of later for something like backpropagation using gradient computation. In other words, it “tags” the object for PyTorch. Let’s make up some dummy operations to see how this tagging and gradient calculation works. Note that  performs element-wise multiplication, otherwise known as the dot product for vectors and the hadamard product for matrics and tensors. Let’s look at how autograd works. To initiate gradient computation, we need to first call  on the final result, in which case . Then, we can simply call  to tell PyTorch to calculate the gradient. Note that this works only because we “tagged”  with the  parameter. If we try to call  on any of the other intermediate variables, such as  or , PyTorch will complain. Let’s try to understand the result of this computation. Let  denote the final  tensor. Since we called , and since  has a total of four elements, we can write out our dummy calculations mathematically in the following fashion: Using partial differentiation to obtain the gradients, Since , Since  is just an arbitrary, non-specific index out of a total of four, we can easily see that the same applies for all other indices, and hence we will end up with a matrix whose all four entries take the value of 4.5, as PyTorch has rightly computed.",0,0,0,0,0,1,0,0
Dissecting LSTMs,"The equation for  is even simpler, since there is no matrix multiplication involved. Thus, the gradient flows backwards without any modification. Moving down a layer, we come across (6): Let’s begin by trying to find the gradient for . You might be wondering what the  term is doing in that equation. After all, isn’t that quantity precisely what we are trying to calculate? This is the one tricky yet also interesting part about RNN backpropagation. Recall that the whole point of a recurrent neural network is its use of variables from the previous forward pass. For example, we know that in the next forward pass,  will be concatenated with the input . In the backpropagation step corresponding to that forward pass, we would have computed ; thus, this gradient flows into the current backpropagation as well. Although this diagram applies to a standard RNN instead of an LSTM, the recurrent nature of backprop still stands. I present it here because I find this diagram to be very intuitive.  If you look at the right, the star represents the gradient from the last pass. If you look to the left, you will see that there is going to be a gradient for  that will eventually be passed over to the next backpropgation scheme. Since the forward pass is recurrent, so is the backward pass. Since we have , now it’s time to move further. Let’s derive the expression for the gradient for . Let’s do the same for the other term, .",0,0,0,0,0,1,0,0
Principal Component Analysis,"Principal component analysis is one of those techniques that I’ve always heard about somewhere, but didn’t have a chance to really dive into. PCA would come up in papers on GANs, tutorials on unsupervised machine learning, and of course, math textbooks, whether it be on statistics or linear algebra. I decided that it’s about time that I devote a post to this topic, especially since I promised one after writing about  on this blog some time ago. So here it goes. What do we need principal component analysis for? Or more importantly, what is a principal component to begin with? Well, to cut to the chase, PCA is a way of implementing dimensionality reduction, often referred to as lossy compression. This simply means that we want to transform some data living in high dimensional space into lower dimensions. Imagine having a data with hundreds of thousands of feature columns. It would take a lot of computing power to apply a machine learning model to fit the data and generate predictions. This is when PCA comes in: with PCA, we can figure out which dimensions are the most important and apply a transformation to compress that data into lower dimensions, making it a lot more tractable and easier to work with. And in case you’re still wondering, principal components refer to those new extracted dimensions used to newly represent data! Let’s derive PCA with some good old linear algebra tricks. I used Ian Goodfellow’s Deep Learning and a lecture slide from Columbia references for this post.",0,1,1,0,0,0,0,0
The Gibbs Sampler,"Theoretically, it doesn’t matter what these random numbers are—asymptotically speaking, we should still be able to approximate the final distribution, especially if given the fact that we take burn-in into account. On the first iteration, we will begin by sampling from the first probability distribution. Note that we simply used the initial random values for  through  to sample the first value from a conditional probability distribution. Now, we do the same to sample . Only this time, we can use the result from earlier, namely . We can see how this might help us yield a slightly more convincing result than simply using the random data. We still have to use random values for  through  since we haven’t sampled from their relevant conditional distributions just yet. However, as we go through all the random variables in order, it becomes obvious that we will no longer be using randomly initialized values at one point. Specifically, on the th iteration, we would expect something like this to happen:  can be any number between 1 and , since it is used to represent the th random variable. As we repeat more iterations of sampling, we will eventually end up with a plausible representation of -dimensional vectors, which is what we sought to sample from the intractable distribution! In this section, we will take a look at a very simple example, namely sampling from a bivariate Gaussian distribution.",0,1,1,0,0,0,0,0
The Math Behind GANs,"Through a partial derivative of  with respect to , we see that the optimal discriminator, denoted as , occurs when Rearranging (12), we get And this is the condition for the optimal discriminator! Note that the formula makes intuitive sense: if some sample  is highly genuine, we would expect  to be close to one and  to be converge to zero, in which case the optimal discriminator would assign 1 to that sample. On the other hand, for a generated sample , we expect the optimal discriminator to assign a label of zero, since  should be close to zero. To train the generator, we assume the discriminator to be fixed and proceed with the analysis of the value function. Let’s first plug in the result we found above, namely (12), into the value function to see what turns out. To proceed from here, we need a little bit of inspiration. Little clever tricks like these are always a joy to look at. If you are confused, don’t worry, you aren’t the only one. Basically, what is happening is that we are exploiting the properties of logarithms to pull out a  that previously did not exist. In pulling out this number, we inevitably apply changes to the terms in the expectation, specifically by dividing the denominator by two. Why was this necessary? The magic here is that we can now interpret the expectations as Kullback-Leibler divergence: And it is here that we reencounter the Jensen-Shannon divergence, which is defined as where .",0,0,1,0,0,1,0,0
Fisher Score and Information,"Using the Leibniz rule we saw earlier, we can interchange the derivative and come up with the following expressions. Granted, these expressions somewhat muffle the shape of the quantity we are dealing with, namely vectors and matrices, but it is concise and intuitive enough for our purposes. With these statements in mind, let’s now begin the derivation by first taking a look at the Hessian of the score function. From the chain rule, we know that This does not look good at all. However, let’s not fall into despair, since our goal is not to calculate the second derivative or the Hessian itself, but rather its negative expected value. In calculating the expected value, we will be using integrals, which is where the seemingly trivial statements we established earlier come in handy. By linearity of expectation, we can split this expectation up into two pieces. Let’s use integrals to express the first expectation. The good news is that now we see terms canceling out each other. Moreover, from the Leibniz rule and the interchanging of the integral and the derivative, we have shown that the integral in fact evaluates to zero. This ultimately leaves us with Therefore we have established that And we’re done! In this post, we took a look at Fisher’s score and the information matrix. There are a lot of concepts that we can build on from here, such as Cramer Rao’s Lower Bound or natural gradient descent, both of which are interesting concepts at the intersection of machine learning and statistics.",0,0,1,0,0,0,0,0
The Gibbs Sampler,"Although we have dealt with -dimensional examples in the algorithm analysis above, for the sake of demonstration, let’s work on a simple example that we can also easily visualize and intuit. For this reason, the bivariate Gaussian distribution is a sensible choice. For this post, I’ll be using , which is a data visualization library built on top of . I’ll simply be using  to display a bivariate Gaussian. For reproducibility’s sake, we will also set a random seed. The code for the Gibbs sampler is simple, partially because the distribution we are dealing with is a bivariate Gaussian, not some high-dimensional intractable distribution. This point notwithstanding, the  function shows the gist of how Gibbs sampling works. Here, we pass in parameters for the conditional distribution, and start sampling given an initial  value corresponding to . As stated earlier, this random value can be chosen arbitrarily. Of course, if we start from a value that is way off, it will take much longer for the algorithm to converge, i.e. we will have to discard a large portion of initially sampled values. This is known as burn-in. In this case, however, we will apply a quick hack and start from a plausible value to begin with, reducing the need for burn-in. We then take turns sampling from the conditional probability distributions using the sampled values, and append to a list to accumulate the result. The more difficult part here is deriving the equation for the conditional probabiilty distributions of the bivariate Gaussian.",0,1,1,0,0,0,0,0
Introduction to tf-idf,"This is because the way we built ordinal indexing in corpus is probably different from how scikit-learn implements it internally. This point notwithstanding, it’s clear that the values of each vectors are identical, disregarding the fact that the result produced by our algorithm has more decimal points due to floating point arithmetic. This was a short introductory post on tf-idf vectors. When I first heard about tf-idf vectors from a friend studying computational linguistics, I was intimidated. However, now that I have a project I want to complete, namely an auto-tagging and classification NLP model, I’ve mustered more courage and motivation to continue my study the basics of NLP. I hope you’ve enjoyed reading this post. Catch you up in the next one! (Yes, this is a trite ending comment I use in almost every post, so the idf scores for the words in these two sentences are going to be very low.).",0,0,0,0,0,0,1,0
Bayesian Linear Regression,"MAP versus MLE is a recurring theme that appears throughout the paradigmatic shift from frequentist to Bayesian, so it merits discussion. Now that we have a posterior distribution for  which we can work with, it’s time to derive the predictive distribution. We go about this by marginalizing  using the property of conditional probability, as illustrated below. This may seem like a lot, but most of it was simple calculation and distributing vectors over parentheses. It’s time to use the power of conjugacy again to extract a normal distribution out of the equation soup. Let’s complete the square of the exponent according to the Gaussian form after making the appropriate substitutions Again, observing this is not a straightforward process, especially if we had no idea what the final distribution is going to look like. However, given that the resulting predictive posterior will take a Gaussian form, we can backtrack using this knowledge to obtain the appropriate substitution parameters in (13). Continuing, where the last equality stands because we can pull out terms unrelated to  by considering them as constants. Why do we bother to pull out the exponent? This is because the integral of a probability density function evaluates to 1, leaving us only with the exponential term outside the integral. To proceed further from here, let’s take some time to zoom in on  for a second. Substituting , we get We can now plug this term back into (15) as shown below.",0,1,0,0,0,0,0,0
Bayesian Linear Regression,"Note that, being a novice in , I borrowed heavily from this resource available on the  official documentation. First, let’s begin by importing all necessary modules. Let’s randomly generate two hundred data points to serve as our toy data set for linear regression. Below is a simple visualization of the generated data points alongside the true line which we will seek to approximate through regression.  Now is the time to use the  library. In reality, all of the complicated math we combed through reduces to an extremely simple, single-line command shown below. Under the hood, the  using variations of random sampling to produce an approximate estimate for the predictive distribution. Now that the trace plot is ready, let’s see what the estimated vallues are like. We drop the first hundred sampled values may have been affected by a phenomena known as . Intuitively, the sampler needs some time to stabilize around the mean value, which is why the first few samples may contain more noise and provide information of lesser value compared to the rest.  We see two lines for each plot because the sampler ran over two chains by default. What do those sampled values mean for us in the context of linear regression? Well, let’s plot some sampled lines using the  function conveniently made available through the  library.  We see that the gray lines, sampled by , all seem to be a good estimate of the true regression line, colored in gold.",0,1,0,0,0,0,0,0
Word2vec from Scratch,"Now, we are finally ready to build and train our embedding network. At this point, you might be wondering how it is that training a neural network that predicts some nearby context word given an input token can be used to embed words into vectors. After all, the output of the network is going to be some probability vector that passed through a softmax layer, not an embedding vector. This is entirely correct, and this is a question that came to my mind as well. However, this is the part that gets the most interesting: the rows of the intermediate weight matrix is the embedding we are looking for! This becomes much more apparent once we consider the dimensions of the weight matrices that compose the model. For simplicity purposes, say we have a total of 5 words in the corpus, and that we want to embed these words as three-dimensional vectors. More specifically, here is the first weight layer of the model: A crucial observation to make is that, because the input is a sparse vector containing one-hot encoded vectors, the weight matrix effectively acts as a lookup table that moves one-hot encoded vectors to dense vectors in a different dimension—more precisely, the row space of the weight matrix. In this particular example, the weight matrix was a transformation of . This is exactly what we want to achieve with embedding: representing words as dense vectors, a step-up from simple one-hot encoding.",0,0,0,0,0,1,1,0
The Math Behind GANs,"Therefore, we might consider the following to be the loss function of the discriminator: Here, we are using a very generic, unspecific notation for  to refer to some function that tells us the distance or the difference between the two functional parameters. (If this reminded you of something like cross entropy or Kullback-Leibler divergence, you are definitely on the right track.) We can go ahead and do the same for the generator. The goal of the generator is to confuse the discriminator as much as possible such that it mislabels generated images as being true. The key here is to remember that a loss function is something that we wish to minimize. In the case of the generator, it should strive to minimize the difference between 1, the label for true data, and the discriminator’s evaluation of the generated fake data. A common loss function that is used in binary classification problems is binary cross entropy. As a quick review, let’s remind ourselves of what the formula for cross entropy looks like: In classification tasks, the random variable is discrete. Hence, the expectation can be expressed as a summation. We can simplify this expression even further in the case of binary cross entropy, since there are only two labels: zero and one. This is the  function that we have been loosely using in the sections above.",0,0,1,0,0,1,0,0
Likelihood and Probability,"After a little bit of rearranging, we end up with Finally, we have obtained the parameter values for the mean and variance of a normal distribution that maximizes the likelihood of our data. Notice that, in the context of normal distributions, the ML parameters are simply the mean and standard deviation of the given data point, which closely aligns with our intuition: the normal distribution that best explains given data would have the sample mean and variance as its parameters, which is exactly what our result suggests. Beyond the specific context of normal distributions, however, MLE is generally very useful when trying to reconstruct or approximate the population distribution using observed data. Let’s wrap this up by performing a quick verification of our formula for maximum likelihood estimation for normal distributions. First, we need to prepare some random numbers that will serve as our supposed observed data. We then calculate the optimum parameters  and  by using the formulas we have derived in (5) and (6). We then generate two subplots of the log likelihood function as expressed in (4), where we vary  while keeping  at  in one and flip this in the other. This can be achieved in the following manner. Executing this code block produces the figure below. From the graph, we can see that the maximum occurs at the mean and standard deviation of the distribution as we expect. Combining these two results, we would expect the maximum likelihood distribution to follow  where  =  and  =  in our code.",0,0,1,0,0,0,0,0
A Step Up with  Variational Autoencoders,"We need the first two parameters to later sample from the latent distribution; , of course, is needed to train the decoder. The decoder network looks similar to the the encoder, except that much of the architecture is in reverse order. Most notably, we use  to undo the convolution done by the encoder. This allows us to effectively scale up the input back to its original dimension, which is what we want to do with a generative model like a VAE. One subtly worth mentioning is the fact that we use a sigmoid activation in the end. This is because we want the pixel values of the output to be between 0 and 1, just as the original input was normalized before it was fed into the encoder network via division by 255. The summary of the decoder network is presented below: Now that we have both the encoder and the decode network fully defined, it’s time to wrap them together into one autoencoder model. This can simply achieved by defining the input as the input of the encoder—the normalized MNIST images—and defining the output as the output of the decoder when fed a latent vector. Concretely, this process might look as follows: Let’s look a the summary of the CVAE. Note that the encoder and the decoder look like individual layers in the grand scheme of the VAE architecture. We have almost everything we need, but there is one crucial step that is missing: compiling the model with an optimizer and a loss function.",1,0,0,0,0,1,0,0
k-Nearest Neighbors Algorithm from Scratch,"As we can see, the KNN algorithm is extremely simple, but if we have enough data to feed it, it can produce some highly accurate predictions. There are still missing pieces to this puzzle, such as how to find the nearest neighbors, but we will explore the specifics of the algorithm on the go as we build the model from scratch. For now, just remember the big picture. Let’s get into the nuts and bolts of the KNN model. Below are the dependencies we will need for this demonstration. One problem we need to start thinking about is how to measure distance between two data points. After all, the implementation of KNN requires that we define some metric to measure the proximity between different points, rank them in order, and sort the list to find  nearest neighbors. One way to go about this is to use Euclidean distance, which is defined as follows: It is not difficult to build an implementation of in Python. We can easily achieve this using . Let’s test the functionality of the  function using some dummy dataset. This data set was borrowed from Jason Brownlee. Great! As expected, the distance between a point and itself is 0, and other calculated distances also seem reasonable. The next step is to write a function that returns the  nearest neighbors of a point given a data set and parameter . There are many ways to implement this, but an example is shown below.",0,0,0,1,0,0,1,0
Moments in Statistics,"The word “moment” has many meanings. Most commonly, it connotes a slice of time. In the realm of physics, moment refers to the rotational tendency of some object, similar to how torque measures the change in an object’s angular momentum. As statisticians, however, what we are interested in is what moment means in math and statistics. In this post, we will attempt to shed new light on the topic of probability distributions through moment generating functions, or MGF for short. The mathematical definition of moments is actually quite simple. And of course, we can imagine how the list would continue: the th moment of a random variable would be . It is worth noting that the first moment corresponds to the mean of the distribution, . The second moment is related to variance, as . The third moment relates to the symmetry of the distribution, or the lack thereof, a quality which goes by the name of skewness. The fourth moment relates to kurtosis, which is a measure of how heavy the tail of a distribution is. Higher kurtosis corresponds to many outliers, while the converse would signify that the distribution contains little deviations. As you can see, the common theme is that the moment contains information about the defining features of a distribution, which is why it is such a convenient way to present information about a distribution. As the name suggests, MGF is a function that generates the moments of a distribution.",0,0,1,0,1,0,0,0
"Beta, Bayes, and Multi-armed Bandits","I personally enjoyed writing this post, not only because I hadn’t written in a long time, but also because it helped me revisit some statistics, which is something that I desperately needed to do—I’m spending way too much time dealing with Django and Selenium these days. Time and again, I realize that there is an element of intuitiveness to Bayesian statistics that, though not obvious at first, starts to make more sense as I explore more into that realm. Of course, frequentist statistics offers a quick and easy way of interpreting certain metrics, but the notion of expectation is something that I had thought was complicated and unnecessary at first, but have gradually come to understand, embrace, and even support. Perhaps writing this post has reaffirmed my Bayesian identity as a budding statistician. I hope you’ve enjoyed reading this post. Catch you up in the next one!.",0,0,1,0,1,0,0,0
So What are Autoencoders?,"Now that the autoencoder model is fully ready, it’s time to see what it can do! Although autoencoders present countless exciting possibilities for application, we will look at a relatively simple use of an autoencoder in this post: denoising. There might be times when the photos we take or image data we use are tarnished by noise—undesired dots or lines that undermine image quality. An autoencoder can be trained to remove these noises fairly easily as we will see in thi post. First, let’s import the MNIST data set for this tutorial. Nothing much exciting is happening below, except for the fact that we are rearranging and preprocessing the dataset so as to maximize training efficiency. Next, we will add  noise to the data. Note that the MNIST dataset does not contain noise by default: we will have to artificially and intentionally tarnish the dataset to produce a noisy training set for the autoencoder model. The  function precisely performs this function. Using the  function, we can create a noisy sample. Note that  was set to 0.5, although I’d imagine other values within reasonable range would work equally well as well. Training the model is very simple: the training data is , the noisy dataset, and the predicted label is . Through this configuration, we essentially expect the autoencoder to be able to see noisy images, after which encoding and decoding is performed via a transformation to a latent dimension to ultimately reproduce a pristine image devoid of any noise.",1,0,0,0,0,1,0,0
A sneak peek at Bayesian Inference,"In other words, the prior would appear to be a uniform distribution, which is really a specific instance of a Beta distribution with . Presented below is a code snippet that simulates 500 coin flips, throughout which we perform five calculations to update our posterior. Executing this code block produces the following figure. This plot shows us the change in our posterior distribution that occurs due to Bayesian update with the processing of each data chunk. Specifically, we perform this Bayesian update after  trials. When no coin flips are performed, as shown in the first subplot, the prior follows a uniform distribution as detailed above. As more coin tosses are performed, however, we start to develop an understanding of the fairness of the coin. When we only have a few data points, the more probability there is that we obtain skewed data, which is why the mean estimate of our posterior seems skewed as well. However, with a larger number of trials, the law of large numbers guarantees that we will eventually be able to identify the value of our parameter , which is indeed the case. The key takeaway from this code block is the line . This is all the Bayesian method there is in this updating procedure.",0,0,1,0,1,0,0,0
Word2vec from Scratch,"The first step, as is the approach taken in many NLP tasks, is to tokenize the text, i.e. splitting the text up into smaller units like words, getting rid of punctuations, and so on. Here is a function that does this trick using regular expressions. Let’s create tokens using the Wikipedia excerpt shown above. The returned object will be a list containing all the tokens in . Another useful operation is to create a map between tokens and indices, and vice versa. In a sense, we are creating a lookup table that allows us to easily convert from words to indices, and indices to words. This will be particularly useful later on when we perform operations such as one-hot encoding. Let’s check if the word-to-index and index-to-word maps have successfully been created. As we can see, the lookup table is a dictionary object containing the relationship between words and ids. Note that each entry in this lookup table is a token created using the  function we defined earlier. Now that we have tokenized the text and created lookup tables, we can now proceed to generating the actual training data, which are going to take the form of matrices. Since tokens are still in the form of strings, we need to encode them numerically using one-hot vectorization. We also need to generate a bundle of input and target values, as this is a supervised learning technique. This then begs the question of what the input and target values are going to look like.",0,0,0,0,0,1,1,0
Stirling Approximation,"Then,  will be a new Poisson random variable with mean . If we extend this idea to apply to  independent random variables instead of just two, we can conclude that  collection of independent random variables from  to  sampled from a population of mean  will have mean . And by the nature of the Poisson distribution, the same goes for variance (We will elaborate on this part more below). The Central Limit Theorem then tells us that the distribution of the sum of these random variables will approximate a normal distribution. This concludes a rough proof of the Stirling approximation. For those of you who are feeling rusty on the Poisson distribution as I was, here is a simple explanation on the Poisson—specifically, its mean and variance. By the virtue of the definition of the parameter, it should be fairly clear why :  is a rate parameter that indicates how many events occur within a window of unit time. The expected calculation can easily be shown using Taylor expansion: Next, we prove that the variance of a Poisson random variable defined by parameter  is equal to . Let  be a Poisson random variable. Then, Then, using the definition of variance, we know that From this, we are once again reminded of the defining property of the Poisson, which is that both the mean and variance of a Poisson random variable is defined by the parameter . Let’s tie this back to our original discussion of the Central Limit Theorem.",0,0,1,0,1,0,0,1
Principal Component Analysis,"How do we go about searching for multiple eigenvectors? This can be done, once again, with Lagrangians, with the added caveat that we will have more trailing terms in the end. Let’s elaborate on this point further. Here, we assume that we have already obtained the first component, , and our goal is to find the next component, . With induction, we can easily see how this analysis would apply to finding . Simply put, the goal is to maximize  under the constraint that  is orthogonal to  while also satisfying the constraint that it is a unit vector. (In reality, the orthogonality constraint is automatically satisfied since the covariance matrix is symmetric, but we demonstrate this nonetheless.) Therefore, Using Lagrangians, In the last equality, we make a trivial substitution to simplify and get rid of the constant. We also use the fact that the covariance matrix is symmetric. If we left multiply (18) by , But since , the first two terms go to zero. Also, the last term reduces to  since . This necessarily means that . If we plug this result back into (18), we end up with the definition of the eigenvector again, but this time for . Essentially, we iterate this process to find a specified number of principal components, which amounts to finding   number of eigenvectors of the sample covariance matrix. A while back, we discussed both eigendecomposition as well as singular value decomposition, both of which are useful ways of decomposing matrices into discrete factors.",0,1,1,0,0,0,0,0
So What are Autoencoders?,"For experimental puposes, I tried using the  callback on Google Colab.  is a platform that gives developers full view of what happens during and after the training process. It makes observing metrics like loss and accuracy a breeze. I highly recommend that you check out this tutorial on how to use and configure this functionality on your notebook. Now that the training is over, what can we do with this autoencoder? Well, let’s see if the autoencoder is now capable of removing noise from tainted image files. But before we jump right into that, let’s first build a simple function that displays images for our convenience. Using the  function, we can now display 25 test images that we will feed into the autoencoder.  Let’s add noise to the data.  Finally, the time has come! The autoencoder will try to “denoise” the contaminated images. Let’s see if it does a good job.  Lo and behold, the autoencoder produces pristine images, almost reverting them back to their original state! I find autoencoders interesting for two reasons. First, they can be used to compress images into lower dimensions. Our original image was of size 28-by-28, summing up to a total of 784 pixels. Somehow, the autoencoder finds ways to decompress this image into vectors living in the predefined 128 dimensions. This is interesting in and of itself, since it presents ways that we might be able to compress large files with minimal loss of information.",1,0,0,0,0,1,0,0
"Beta, Bayes, and Multi-armed Bandits","Simply put, regret refers to the amount that we have comparatively lost by making a sub-optimal choice from the get go. Here is a visual diagram I came across on Analytics Vidhya.  The maximum reward would obviously be achieved if we pull on the slot machine with the highest success parameter from trial 1. However, this does not happen since the gambler dives into the game without this prior knowledge. Hence, they have to learn what the optimal choice is through exploration and exploitation. It is of course in this learning process that the greedy algorithm or Bayesian analysis with Thompson sampling comes into play. The amount that we have theoretically lost—or, in other words, the extent to which we are far away from the maximum amount we could have earned—is denoted as regret. Thus, to maximize reward is to minimize regret, and vice versa. Now let’s simulate a hundred pulls on the lever using Bayesian analysis using the Beta-Binomial model and Thompson sampling. Nothing much fancy here, all we’re doing is Thompson sampling from the Beta distribution via , then obtaining the index of the bandit with the highest parameter, then pulling the machine that corresponds to that index. We will also keep cumulative track of our results to reproduce the regret diagram shown above. And we’re done with the hundred round of simulations! Hopefully our simulator gambler has made some good choices by following Bayesian update principles, with the Beta-Binomial model and Thompson sampling under their belt.",0,0,1,0,1,0,0,0
Demystifying Entropy (And More),"where  denotes cross entropy; ,entropy, and the last term, KL divergence. Now, let’s try to understand what each of them means. KL divergence has many interpretations. One possible definition of KL divergence is that it measures the average number of extra information content required to represent a message with distribution  instead of . In Machine Learning: A Probabilistic Perspective, Kevin P. Murphy describes KL divergence as follows: … the KL divergence is the average number of extra bits needed to encode the data, due to the fact that we used distribution  to encode the data instead of the true distribution . Put differently, KL divergence is the amount of information that is lost when  is used to approximate . Therefore, if  and  are close, KL divergence would be low, whereas the converse would be true when the two distributions are different. We can also extend this notion a bit farther to apply it in the context of Bayesian inference. Recall that Bayesian inference is the process by which we start from some prior distribution and update our beliefs about the distribution with more data input to derive a posterior. In this context, KL divergence can be viewed as the amount of information gained as we move from the prior  to the posterior . Let’s derive the mathematical definition of KL divergence using likelihoods. The derivation process to be introduced is based on this source.",0,0,1,0,0,0,0,0
"Linear Regression, in Two Ways","We can further specify this equation by using the fact that  can be expressed as a linear combination of the columns of . In other words, where  is the solution to the system of equations represented by . Let’s further unpackage (1) using matrix multiplication. Therefore, We finally have a formula for : Let’s remind ourselves of what  is and where we were trying to get at with projection in the context of regression. We started off by plotting three data points, which we observed did not form a straight line. Therefore, we set out to identify the line of best fit by expressing the system of equations in matrix form, , where . But because this system does not have a solution, we ended up modifying the problem to , since this is as close as we can get to solving an otherwise unsolvable system. So that’s where we are with equation (2): a formula for , which contains the parameters that define our line of best fit. Linear regression is now complete. It’s time to put our equation to the test by applying it to our toy data set. Let’s apply (2) in the context of our toy example with three data points to perform a quick sanity check. Calculating the inverse of  is going to be a slight challenge, but this process is going to be a simple plug-and-play for the most part.",0,1,0,0,0,0,0,0
Building Neural Network From Scratch,"Because computing the softmax function require exponentiation, it is likely for the computer to end up with very large numerical quantities, making calculations unstable. One way to solve this problem is by subtracting values from the exponent. As the calculation shows, adding or subtracting the same value from the exponent of both the numerator and the denominator creates no difference for the output of the softmax function. Therefore, we can prevent numbers from getting too large by subtracting some value from the exponent, thus yielding accurate results from stable computation. We can further improve the softmax function for the purposes of this tutorial by supporting batch computation. By batch, I simply mean multiple inputs in the form of arrays. The function shown above is only able to account for a single vector, presumably given as a list or a one-dimensional numpy array. The implementation below uses a loop to calculate the softmax output of each instance in a matrix input, then returns the result. Note that it also prevents arithematic overflow by subtracting the  value of the input array. Let’s test the improved softmax function with a two-dimensional array containing two instances. As expected, the softmax function returns the softmax output applied to each individual instance in the list. Note that the elements of each output instance add up to one, as expected. Another crucial activation function is ReLU, or the rectified linear unit.",0,1,0,0,0,1,1,0
"PyTorch, From Data to Modeling","is a super class from which we can inherit to build anything from full-fledged models to custom blocks or layers to be used in some other larger model. In the initialization function, we also define a number of layers that will be used in forward propagation. You might be wondering why these have to initialized in the initialization function, as opposed to the forward function itself. While I don’t have a complete, technically cogent explanation to that question, intuitively, we can understand a model’s layers as being components of the model itself. After all, the weights of these layers are adjusted with each iteration or epoch. In that sense, we want the layers to be attached to the model instance itself; hence the OOP design of PyTorch’s model class. In this particular instance, we define a number of convolutional layers, a pooling layer, and two fully connected layers used for classification output. The declaration of the layers themselves are not too different from other frameworks, such as TensorFlow. Also, I’ve written out all the named arguments so that it is immediately clear what each argument is configuring. Once we’ve declared all the necessary components in the initialization function, the next steps to actually churn out forward propagation results given some input. In PyTorch, this is done by defining the  function. As you can see above, we basically call on the layers we’ve declared in the initialization function via  and pass in any parameters we want.",0,0,0,0,0,1,0,0
"Linear Regression, in Two Ways","First, let’s remind ourselves of what  and  are: Let’s begin our calculation: Calculating the inverse, Now, we can put this all together. The final result tells us that the line of best fit, given our data, is Let’s plot this line alongside our toy data to see how the equation fits into the picture.  It’s not difficult to see that linear regression was performed pretty well as expected. However, ascertaining the accuracy of a mathematical model with just a quick glance of an eye should be avoided. This point then begs the question: how can we be sure that our calculated line is indeed the best line that minimizes error? To that question, matrix calculus holds the key. We all remember calculus from school. We’re not going to talk much about calculus in this post, but it is definitely worth mentioning that one of the main applications of calculus lies in optimization: how can we minimize or maximize some function, optionally with some constraint? This particular instance of application is particularly pertinent and important in our case, because, if we think about it, the linear regression problem can also be solved with calculus. The intuition behind this approach is simple: if we can derive a formula that expresses the error between actual values of  and those predicted by regression, denoted as  above, we can use calculus to derive that expression and ultimately locate the global minimum. And that’s exactly what we’re going to do.",0,1,0,0,0,0,0,0
Principal Component Analysis,"PCA is a very useful technique used in many areas of machine learning. One of the most common applications is to apply PCA to a high-dimensional dataset before applying a clustering algorithm. This makes it easier for the ML model to cluster data, since the data is now aligned in such a way that it shows the most variance. Upon some more research, I also found an interesting paper that shows that there is a solid mathematical relationship between K-means clustering and PCA. I haven’t read the paper from top to bottom, but instead glossed over a summary of the paper on this thread on stack overflow. It’s certainly a lot of information to take in, and I have no intent of covering this topic in this already rather lengthy post on PCA. So perhaps this discussion will be tabled for a later time, as interesting as it seems. I hope you enjoyed reading this post. Amidst the chaos of the COVID19 pandemic, let’s try to stay strong and find peace ruminating over some matrices and formulas. Trust me, it works better than you might think.",0,1,1,0,0,0,0,0
Word2vec from Scratch,"This process is exactly what embedding is: as we start training this model with the training data generated above, we would expect the row space of this weight matrix to encode meaningful semantic information from the training data. Continuing onwards, here is the second layer that receives as input the embeddings, then uses them to generate a set of outputs. We are almost done. All we now need in the last layer is a softmax layer. When the output is passed into this layer, it is converted into probability vectors whose elements sum up to one. This final output can be considered as context predictions, i.e. which words are likely to be in the window vicinity of the input word. In training—specifically error calculation and backpropagation—we would be comparing this prediction of probability vectors with its true one-hot encoded targets. The error function that we use with softmax is cross entropy, defined as I like to think of this as a dot product of the target vector and the log of the prediction, because that is essentially what the summation is doing. In this alternate formulation, the cross entropy formula can be rewritten as Because  a one-hot encoded vector in this case, all the elements in  whose entry is zero will have no effect on the final outcome. Indeed, we simply end up taking the negative log of the prediction. Notice that the closer the value of the prediction is to 1, the smaller the cross entropy, and vice versa.",0,0,0,0,0,1,1,0
A Simple Autocomplete Model,"6: is a woman–what then? is there not ground 
for suspecting to and the world will had to a such that the basis of the incussions of the spirit as the does not because actian free spirits of intellect of the commstical purtious expression of men are so much he is not unnor experiences of self-conturity, and 
as anifegently religious in the man would not consciously, his action is not be actian at in accombs life for the such all procees of great and the heart of this conduct the spirity of the man can provate for in any 
once in any of the suriticular conduct that which own needs, when they are therefore, as 
such action and some difficulty that the strength, it, himself which has to its fine term of pricismans the exacte in its self-recuphing and every strength and man to wist the action something man as the worst, that the was of a longent that the whole not be all the very subjectical proves the stronger extent he is necessary to metaphysical figure of the faith in the bolity in the pure belief–as “the such a successes of the values–that is he ​ Generated text at temperature 0.",1,0,0,0,0,1,0,0
Scikit-learn Pipelines with Titanic,"Checking data types is necessary both for imputation and general data preprocessing. Specifically, we need to pay attention as to whether a given column encodes categorical or numerical variables. For example, we can’t use the mean to impute categorical variables; instead, something like the mode would make much more sense. The best way to determine whether a variable is categorical or not is simply to use domain knowledge and actually observe the data. Of course, one might use hacky methods like the one below: Although you might think that this is a working hack, this approach is in fact highly dangerous, even in this toy example. For example, consider , which is supposedly a numerical variable of type . However, earlier with , we saw that  is in fact a ordinal variable taking discrete values, one of 1.0, 2.0, and 3.0. So hacky methods must not be used in isolation; at the very least, they need to be complemented with some form of human input. Let’s try to use a simple pipeline to deal with missing values in some categorical variables. This is going to be our first sneak peak at how pipelines are declared and used. Here, we have declared a three-step pipeline: an imputer, one-hot encoder, and principal component analysis. How this works is fairly simple: the imputer looks for missing values and fills them according to the strategy specified. There are many strategies to choose from, such as most constant or most frequent.",0,0,0,1,0,0,0,0
Recommendation Algorithm with SVD,"The takeaway is that dimensionality reduction is a meaningful way to extract important information from our data. Now that we have performed SVD on the ratings matrix, let’s move onto the last step: crafting a model for our recommendation algorithm. My personal pet theory is that using any word in conjunction with “algorithm” makes the concept sound more complex than it actually is. This is exactly what we are doing here, because in reality, our so-called algoithm for movie recommendations is going to be very simple. The intuition behind the recommendation system is distance calculation. Simply put, if users have similar movie preferences, the points representing the two users will appear to be close when plotted on a graph. Let’s see what this means by plotting  using . This can be achieved with the following code. We can pass  as an argument for the  function to see a three-dimensional plot of users’ movie preferences, as shown below.  Note that the points corresponding to User 6 and User 8 exactly overlap, which is why the points look darker despite being positioned near the corner of the plot. This is also why we can only count seven points in total despite having plotted eight data points. In short, this visualization shows how we might be able to use distance calculation to give movie recommendations to a new user. Assume, for instance, that we get a new suscriber to our movie application.",0,1,0,0,0,0,1,0
The Magic of Euler’s Identity,"It is also interesting to see that the Taylor series for  is an odd function, while that for  is even, which is coherent with the features of their respective original functions. Last but not least, notice that the derivative of Taylor polynomial of  gives itself, as it should. Now that we have the Taylor polynomials, proving Euler’s identity becomes a straightforward process of plug and play. Let’s plug  into the Taylor polynomial for : Notice that we can separate the terrms with and without : In short, ! With this generalized equation in hand, we can plug in  into  to see Euler’s identity: The classic proof, although fairly straightforward, is not my favorite mode of proving Euler’s identity because it does not reveal any properties about the exponentiation of an imaginary number, or an irrational number for that matter. Instead, I found geometric interpretations of Euler’s formula to be more intuitive and thought-provoking. Below is a version of a proof for Euler’s identity. Let’s start by considering the complex plane. There are two ways of expressing complex numbers on the Argand diagram: points and vectors. One advantage of the vector approach over point representation is that we can borrow some simple concepts from physics to visualize  through the former: namely, a trajectory of a point moving along the complex plane with respect to some time parameter .",0,0,0,0,0,0,0,1
A sneak peek at Bayesian Inference,"We will revisit this problem in a separate post. Another reason why the Beta distribution is an excellent choice for our prior representation is that it is a conjugate prior to the binomial distribution. Simply put, this means that using the Beta distribution as our prior, combined with a binomial likelihood function, will produce a posterior  that also follows a Beta distribution. This fact is crucial for Bayesian analysis. Recall that the beauty of Bayesian inference originates from repeated applicability: a posterior we obtain after a single round of calculation can be used as a prior to perform the next iteration of inference. In order to ensure the ease of this procedure, intuitively it is necessary for the prior and the posterior to take the same form of distribution. Conjugate priors streamline the Bayesian process of updating our priors with posteriors by ensuring that this condition is satisfied. In simple language, mathematicians have found that certain priors go well with certain likelihoods. For instance, a normal prior goes along with a normal likelihood; Gamma prior, Poisson likelihood; Gamma prior, normal likelihood, and so on. Our current combination, Beta prior and binomial likelihood, is also up on this list. To develop some intuition, here is a graphical representation of the Beta function for different values of  and . This code block produces the following diagram. Graphically speaking, the larger the value of  and , the more bell-shaped it becomes. Also notice that a larger  corresponds to a rightward shift, i.e.",0,0,1,0,1,0,0,0
PyTorch RNN from Scratch,"Now that we have downloaded the data we need, let’s take a look at the data in more detail. First, here are the dependencies we will need. We first specify a directory, then try to print out all the labels there are. We can then construct a dictionary that maps a language to a numerical label. We see that there are a total of 18 languages. I wrapped each label as a tensor so that we can use them directly during training. Let’s store the number of languages in some variable so that we can use it later in our model declaration, specifically when we specify the size of the final output layer. Now, let’s preprocess the names. We first want to use  to standardize all names and remove any acute symbols or the likes. For example, Once we have a decoded string, we then need to convert it to a tensor so that the model can process it. This can first be done by constructing a  mapping, as shown below. We see that there are a total of 59 tokens in our character vocabulary. This includes spaces and punctuations, such as ` .,:;-‘(num_char, 59)(59,)`. We can now build a function that accomplishes this task, as shown below: If you read the code carefully, you’ll realize that the output tensor is of size , which is different from the explanation above. Well, the reason for that extra dimension is that we are using a batch size of 1 in this case.",0,0,0,0,0,1,1,0
Understanding the  Leibniz Rule,"As simple as it seems, we can in fact analyze Leibniz’s rule by applying this definition, as shown below: Thus we have shown that, if the limits of integration are constants, we can switch the order of integration and differentiation. But because our quench for knowledge is insatiable, let’s consider the more general case as well: when the limits are not bounded by constant, but rather functions. Specifically, the case we will consider looks as follows. In this case, we see that  and  are each functions of variable . With some thinking, it is not difficult to convince ourselves that this will indeed introduce some complications that require modifications to our original analysis. Now, not only are we slightly moving the graph of  in the  axis, we are also shifting the limits of integration such that there is a horizontal shift of the area box in the  axis. But fear not, let’s apply the same approach to answer this question. This may appear to be a lot of computation, but all we’ve done is just separating out the integrals while paying attention to the domains of integration. Let’s continue by doing the same for the remaining terms. The first two terms in the limit go away since  goes to zero. While the same applies to the fractional terms, one difference is that they are also divided by , which is why they remain. We have simplified quite a bit, but we still have two terms in the limit expression that we’d like to remove.",0,0,0,0,0,0,0,1
An Introduction to Markov Chain Monte Carlo,"But before we move into code, there is something that should be corrected before we move on. In (6), we derived an experssion for the jump condition. The jump condition is not wrong per se, yet a slight modification has to be made to fully capture the gist of Metrapolis-Hastings. The precise jump condition for the sampler goes as follows: where This simply means that we accept the prorposed pararmeter if the quantity calculated in (7) is larger than a random number between 0 and 1 sampled from a uniform distribution. This is why MCMC models involve a form of random walk—while leaving room for somewhat unlikely parameters to be selected, the model samples relatively more from regions of high posterior probability. Now that we have some understanding of how Markov Chain Monte Carlo and the Metropolis-Hastings algorithm, let’s implement the MCMC sampler in Python. As per convention, listed below are the dependencies required for this demonstration. Let’s start by generating some toy data for our analysis. It’s always a good idea to plot the data to get a sense of its shape.  The data looks roughly normal. This is because we created the toy data using the  function, which generates random numbers from a normal distribution centered around 0. The task for this tutorial, given this data, is going to be estimating the mean of the posterior distribution, assuming we know its standard deviation to be 1.",0,0,0,0,0,0,1,0
Logistic Regression Model from Scratch,"It’s time for some training and prediction generation. Because we did all the work in the previous section, training and predicting can be achieved with just a single line of command. To see how quickly average cross entropy is decreasing, I turned on the  as true. This way, we can see how quickly the loss is declining over every 50 epochs. It’s now time to see how well our model has done. Let’s compare , the list that contains the model’s predictions, with , which is essentially the answer key. This is great news. The result shows us that we have correctly predicted 272 values while making wrong predictions in only 2 cases. Let’s systematize this quantity by creating a  function that returns how accurate our model is given  and . Let’s use this function to test how well our model performed. 99 percent is not a bad estimate at all. One interesting question to consider is how much boost in accuracy we see with each epoch, i.e. what is the bang-per-buck of each iteration cycle? This is an important question to consider because gradient descent is computationally expensive; if we can train our model in just 10 epochs instead of 1000, why not choose the former? To answer this question, let’s plot accuracy against epoch. For fun, I added the learning parameter  as an argument to the  function as well. Let’s create a plot to see how accuracy changes over 200 epochs, given a learning rate of 0.1.",0,0,0,1,0,0,1,0
Traveling Salesman Problem with Genetic Algorithms,"Considering the fact that there are a total of  possibilities, the fact that the algorithm was able to narrow it down to a plausible route that beats the baseline is still very interesting. Genetic algorithms belong to a larger group of algorithms known as randomized algorithms. Prior to learning about genetic algorithms, the word “randomized algorithms” seemed more like a mysterious black box. After all, how can an algorithm find an answer to a problem using pseudo-random number generators, for instance? This post was a great opportunity to think more about this naive question through a concrete example. Moreover, it was also interesting to think about the traveling salesman problem, which is a problem that appears so simple and easy, belying the true level of difficulty under the surface. There are many other ways to approach TSP, and genetic algorithms are just one of the many approaches we can take. It is also not the most effective way, as iterating over generations and generations can often take a lot of time. The contrived semi-circle example, for instance, took somewhere around five to ten minutes to fully run on my 13-inch MacBook Pro. Nonetheless, I think it is an interesting way well worth the time and effort spent on implementation. I hope you’ve enjoyed reading this post. Catch you up in the next one!.",0,0,0,0,0,0,1,0
PyTorch Tensor Basics,"More generally speaking, we can think that concatenation effectively brought the two elements of each tensor together to form a larger tensor of four elements. I found concatenation along the first and second dimensions to be more difficult to imagine right away. The trick is to mentally draw a connection between the dimension of concatenation and the location of the opening and closing brackets that we should focus on. In the case of the example above, the opening and closing brackets were the outer most ones. In the example below in which we concatenate along the first dimension, the brackets are those that form the boundary of the inner two-dimensional 3-by-4 tensor. Let’s take a look. Notice that the rows of  were essentially appended to those of , thus resulting in a tensor whose shape is . For the sake of completeness, let’s also take a look at the very last case, where we concatenate along the last dimension. Here, the brackets of focus are the innermost ones that form the individual one-dimensional rows of each tensor. Therefore, we end up with a “long” tensor whose one-dimensional rows have a total of 8 elements as opposed to the original 4. In this post, we took a look at some useful tensor manipulation operations and techniques. Although I do have some experience using Keras and TensorFlow, I never felt confident in my ability to deal with tensors, as that felt more low-level. PyTorch, on the other hand, provides a nice combination of high-level and low-level features.",0,0,0,0,0,1,0,0
Moments in Statistics,"More specifically, we can calculate the th moment of a distribution simply by taking the th derivative of a moment generating function, then plugging in 0 for parameter . We will see what  is in a moment when we look at the default formula for MGF. This sounds good and all, but why do we want an MGF in the first place, one might ask. Well, given that moments convey defining properties of a distribution, a moment generating function is basically an all-in-one package that contains every bit of information about the distribution in question. Enough of the abstract, let’s get more specific by taking a look at the mathematical formula for an MGF. If  is a continuous random variable, we would take an integral instead. Now, you might be wondering how taking the th derivative of  gives us the th moment of the distribution for the random variable . To convince ourselves of this statement, we need to start by looking at the Taylor polynomial for the exponential. It’s not difficult to see the coherency of this expression by taking its derivative—the derivative of the polynomial is equal to itself, as we would expect for . From here, we can sufficiently deduce that The coherency of (3) can simply be seen by making the substitution . To continue, now that we have an expression for , we can now calculate , which we might recall is the definition of a moment generating function. where the second equality stands due to linearity of expectation.",0,0,1,0,1,0,0,0
k-Nearest Neighbors Algorithm from Scratch,"Because this function has to perform more tasks than the functions we wrote earlier, the example code is slightly longer, but here it goes: Basically, the  function counts the number of labels of each class and stores the results in a dictionary. Then, it normalizes the values of the dictionary by dividing its values by the total number of data points seen. Although this process is not necessary, it helps us interpret the results in terms of percentages. For example, the example below tells us that approximately 71 percent of the neighbors of  are labeled 0; 28 percent are labeled 1. Now we have all the building blocks we need. We can stop here, but let’s nicely wrap all the functions we have build into a single function that we can use to train and test data. In retrospect, we could have built a class instead, but this implementation also works fine, so let’s stick to it for now. Let’s see what the  tells us about . Here, we pass on  onto the  argument because we want to prevent the algorithm from making a prediction based on a data set that contains the data itself; that would defeat the purpose of making a prediction. Let’s see how the model performs. The KNN model rightly predicts that  is labeled 0. Great! But we have only been testing our model on a rather dumb data set. Let’s see whether the model works with larger, closer-to-real-life data sets.",0,0,0,1,0,0,1,0
Revisiting Basel with Fourier,"Or, even better, with the key insight that  is an even function, we might intelligently deduce that there will be no sine terms at all, since sine functions are by nature odd. In other words, all . This can of course be shown through derivation as we have done above for the cosine coefficients. Therefore, putting everything together, we end up with If we consider the case when , we have Do you smell the basel problem in the air? The summation on the right hand side is a great sign that we are almost done in our derivation. Moving the fractional term to the left hand side, we get: Diding both sides by 4, And there you have it, the answer to the Basel problem, solved using Fourier series! We can also derive a convergence value of the Dirichelt Eta function from this Fourier series as well. Recall that the Eta function looks as follows: Now how can we get a Dirichelt Eta function out of the fourier series of ? Well, let’s get back to (8) and think our way through. One noteworthy observation is that we already have  in the summation, which looks awfully similar to the Dirichlet Eta function. Since we want to get rid of the cosine term, we can simply set —this will make all cosine terms evaluate to 1, effectively eliminating them from the expression.",0,0,0,0,0,0,0,1
An Introduction to Markov Chain Monte Carlo,"This is due to the Markov chain part of MCMC: one of the most interesting properties of Markov chains is that, no matter where we start, we end up in the same . Together, these properties makes MCMC models like Metropolis-Hastings incredible useful for solving intractable problems. is a library made specifically for Bayesian analysis. Of course, it includes functions that implement Markov Chain Monte Carlo models. Although building the Metropolis-Hastings algorithm from scratch was a worthy challenge, we can’t build models from scratch every time we want to conduct from Bayesian analysis involving an intractable posterior, which is why packages like  always come in handy. With just a few lines of code, we can perform the exact same operation we performed above. In fact, let’s compare our Metropolis-Hastings sampler with the built-in function in the  library.  Pretty similar, wouldn’t you say? Markov Chain Monte Carlo is a powerful method with which we can estimate intractable posterior distributions. It is undoubtedly one of the most important tools that a Bayesian statistician should have under their belt. And even if you are frequentist, I still think MCMC models are worth looking at because it’s cool to see just how easily we can estimate a distribution with little to no knowledge about the mathematics involved in calculating the posterior. It’s also fascinating to see how the marriage of two seemingly unrelated concepts that arose out of different contexts–Monte Carlo methods and Markov chains—can produce such a powerful algorithm.",0,0,0,0,0,0,1,0
A sneak peek at Bayesian Inference,"For example, a simple random sampling of the population might reveal that 15 percent of the sample population is affected with the flu, but this would most likely produce a normal distribution with mean centered around 0.15 instead of a point probability. From a Bayesian standpoint, we would then replace the point probability in our example above with an equation for the distribution, from which we can proceed with the Bayesian analysis of updating our prior with the posterior through repeated testing and computation. Bayes’ theorem, specifically in the context of statistical inference, can be expressed as where  stands for observed or measured data,  stands for parameters, and  stands for some probability distribution. In the language of Bayesian inference,  is the posterior distribution for the parameter ,  is the likelihood function that expresses the likelihood of having parameter  given some observed data ,  is the prior distribution for the parameter , and  is evidence, the marginal probability of seeing the data, which is determined by summing or integrating across all possible values of the parameter, weighted by how strongly we believe in those particular values of . Concretely, Notice that this is not so different from the expansion of the denominator we saw with Bayes’ theorem, specifically equation (5). The only difference here is that the integral takes continuous probability density functions into account, as opposed to discrete point probabilities we dealt with earlier.",0,0,1,0,1,0,0,0
Principal Component Analysis,"The setup of a classic PCA problem might be summarized as follows. Suppose we have a dataset of  points, each living in -dimensional space. In other words, 
where Our goal is to find a way to compress the data into lower dimensional space  where . We might imagine this as a transformation, i.e. the objective is to find a transformation So that applying  will yield a new vector  living in lower dimensional space. We can also imagine there being a reverse transformation or a decoding function  that achieves Because PCA is in essence a linear transformation, it is most natural to express and understand it as a matrix. Let’s define this transformation as , and the matrix corresponding to the decoding . In other words, PCA makes a number of assumptions to simplify this problem. The most important assumption is that each column of  is orthogonal to each other. As we will see later in an alternate derivation with statistics, this has to do with the notion of covariance. Another restriction is that the columns of  must have a Euclidean norm of one. This constraint is necessary for us to find a unique matrix  that achieves compression—otherwise, we could have any multiples, leading to an infinite number of such matrices. We make one more convenient assumption about the given data points, . That is,  is assumed to have a mean of zero, i.e. . If this is not the case, we can easily perform standardization by subtracting the mean from the data.",0,1,1,0,0,0,0,0
Recommendation Algorithm with SVD,"Imagine performing the same analysis on a much larger matrix, from which we extract  number of non-trivial entries of . On scale, singular value decomposition becomes more powerful, as it allows large amounts of data to be processed in managable bites. This is more than enough theory on SVD. Now is finally the time to jump into building our recommendation model with singular value decomposition. In this section, we will generate some random data, namely the ratings matrix. The row of the ratings matrix can be interpreted as users; the columns, movies. In other words,  denotes the ratings the th user gave for the th movie. The example we will use was borrowed from this post by Zacharia Miller. Let’s quickly build this ratings matrix using  and  as shown below. Let’s first see what this matrix looks like. We can do this simply by calling the  function and saving it to some variable. For notational consistency, let’s name this variable . Great! Now we have a matrix of binary numbers, where  denotes the fact that the user liked the movie and  the fact that they disliked it. We can make some cursory qualitative observations of this toy data. Note, for instance, that users who like Movie 2 also tend to like Movie 3. Also, User 6 and User 8 have identical prefernece for movies—perhaps they both like a particular genre, or tend to like the movie starred by some actor or actress.",0,1,0,0,0,0,1,0
