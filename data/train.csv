title,body,deep_learning,linear_algebra,machine_learning,from_scratch,statistics,pytorch,analysis,probability_distribution
Stirling Approximation,"It’s about time that we go back to the old themes again. When I first started this blog, I briefly dabbled in real analysis via Euler, with a particular focus on factorials, interpolation, and the Beta function. I decided to go a bit retro and revisit these motifs in today’s post, by introducing Stirling’s approximation of the factorial. There are many variants of Stirling’s approximation, but here we introduce the general form as shown: Let’s begin the derivation by first recalling the Poisson distribution. The Poisson distribution is used to model the probability that a certain event occurs a specified number of times within a defined time interval given the rate at which these events occur. The formula looks as follows: One interesting fact about the Poisson distribution is that, when the parameter math variable is sufficiently large, the Poisson approximates the Gaussian distribution whose mean and variance are both math variable. This happens when the random variable math variable. We can easily simplify (2) since the power of the exponent is zero. Thus, we have By simply rearranging (3), we arrive at Stirling’s approximation of the factorial: This is cool, but we still haven’t really shown why a Poisson can be used to approximate a Gaussian—after all, this premise was the bulk of this demonstration. To see the intuition behind this approximation, it is constructive to consider what happens when we add independent Poisson random variables. Say we have math variable and math variable, both of which are independent Poisson random variables with mean math variable and math variable. Then, math variable will be a new Poisson random variable with mean math variable. If we extend this idea to apply to math variable independent random variables instead of just two, we can conclude that math variable collection of independent random variables from math variable to math variable sampled from a population of mean math variable will have mean math variable. And by the nature of the Poisson distribution, the same goes for variance (We will elaborate on this part more below). The Central Limit Theorem then tells us that the distribution of the sum of these random variables will approximate a normal distribution. This concludes a rough proof of the Stirling approximation. For those of you who are feeling rusty on the Poisson distribution as I was, here is a simple explanation on the Poisson—specifically, its mean and variance. By the virtue of the definition of the parameter, it should be fairly clear why math variable: math variable is a rate parameter that indicates how many events occur within a window of unit time. The expected calculation can easily be shown using Taylor expansion: Next, we prove that the variance of a Poisson random variable defined by parameter math variable is equal to math variable. Let math variable be a Poisson random variable. Then, Then, using the definition of variance, we know that From this, we are once again reminded of the defining property of the Poisson, which is that both the mean and variance of a Poisson random variable is defined by the parameter math variable. Let’s tie this back to our original discussion of the Central Limit Theorem. CLT states that, even if a distribution of a random variable is not normal, the distribution of the sums of these random variables will approximate a normal distribution. Using this handy property on the math variable independent and identically distributed Poisson random variables of mean and variance math variable, we can see how the sum of these random variables approximates a Gaussian distribution math variable. Hence the Stirling approximation! math expression",0,0,0,0,1,0,1,1
Naive Bayes Model From Scratch,"Welcome to part three of the “from scratch” series where we implement machine learning models from the ground up. The model we will implement today, called the naive Bayes classifier, is an interesting model that nicely builds on top of the Bayesian mindset we developed in the previous post on Markov Chain Monte Carlo. Much like the logistic regression model, naive Bayes can be used to solve classification tasks, as opposed to regression in which case the goal is to predict a continuous variable. The main difference between logistic regression and naive Bayes is that naive Bayes is built on a probabilistic model instead of an optimization model such as graident descent. Hence, implementing naive Bayes is somewhat easier from a programming point of view. Enough of the prologue, let’s cut to the chase. To understand naive Bayes, we need not look further than Bayes’s theorem, which is probably the single most referenced theorem on this blog so far. I won’t explain this much since we have already seen it so many times, but presented below is the familar formula for reference and readability’s sake. Very standard, perhaps with the exception of some minor notation. Here, math variable refers to a single instance, represented as a vector with math variable entries; math variable, the corresponding label or class for that instance. Note that math variable is not a feature matrix, but a single instance. More concretely, Of course, math variable is just a scalar value. This characterization is very apt in the context of machine learning. The underlying idea is that, given some data with math variable feature columns, we can derive a probability distribution for the label for that intance. The naive assumption that the naive Bayes classifier makes—now you can guess where that name comes from—is that each of the math variable variables in the instance vector are independent of one another. In other words, knowing a value for one of the features does not provide us with any information about the values for the other math variable feature columns. Combining this assumption of independence with Bayes’ theorem, we can now restate (1) as follows: Pretty straight forward. We know that the demominator, which often goes by the name “evidence” in Bayesian inference, is merely a normalizing factor to ensure that the posterior distribution integrates to 1. So we can discard this piece of information and distill (2) down even farther: Equation (3) tells us that it is possible to calculate the probability of instance math variable belonging to class math variable systematically. Why is this important? The simple answer is that we can use (3) to train the naive Bayes classifier. Say we know that for a particular instance math variable, the label is math variable. Then, we have to find the distribution for each feature such that we can maximize math variable. Does this ring any bells? Yes—it is maximum a posteriori estimation! In other words, our goal would be to maximize the posterior distribution for each training instance so that we can eventually build a model that would output the most likely label that the testing instance belongs to. In other words, our training scheme can be summarized as: But this is all to abstract. Let’s get into the details by implementing the naive Bayes classifer from scratch. Before we proceed, however, I must tell you that there are many variations of the naive Bayes classifer. The variant that we will implement today is called the Gaussian naive Bayes classifer, because we assume that the distribution of the feature variables, denoted as math variable, is normal. For a corresponding explanation of this model on , refer to this documentation. Let’s jump right into it. As per convention, we start by importing necessary modules for this tutorial. For reproducability, we specify a . The magic commands are for the configuration of this Jupyter Notebook. Let’s begin by building some toy data. To make things simple, we will recycle the toy data set we used in the previous post on logistic regression and k-nearest neighbors. The advantage of using this data set is that we can easily visualize our data since all instances live in math variable. In other words, we only have two axes: math variable and math variable. For convenience, we preprocess our toy data set and labels into arrays. The first step is to separate the data set by class values, since our goal is to find the distributions for each class that best describe the given data through MAP. To achieve this objective, we can create a function that returns a dictionary, where the key represents the class and the values contain the entries of the data set. One way to implement this process is represented below in the function. Let’s see if the function works properly by passing as into its argument. Great! As expected, is a dictionary whose keys represent the class and values contain entries corresponding to that class. Now that we have successfully separated out the data by class, its’ time to write a function that will find the mean and standard deviation of each class data. This process is legitimate only because we assumed the data to be normally distributed—hence the name “Gaussian naive Bayes.” Let’s quickly see this in code. The function receives a data set as input and returns a nested list that contains the mean and standard deviation of each column of the data set. For example, if we pass the toy data set into , the returned list will contain two lists: the first list element corresponding to the mean and standard deviation of math variable, and the second list element, math variable. We can combine both and functions to create a new wrapper function that returns the mean and standard deviation of each column for each class. This is a crucial step that will allow us to perform a MAP approximation for the distribution of variables for each class. Testing out the function on created earlier yields the desired result. Notice that the returned dictionary contains information for each class, where the key corresponds to the label and the value contains the parameters calculated from . A good way to understand this data is through visualization. Let’s try to visualize what the distribution of math variable and math variable looks like for data labeled class . We can use the library to create a joint plot of the two random variables. We see that both variables are normally distributed. Therefore, we can imagine data points for class to be distributed across a three-dimensional Gaussian distribution whose center lies at the point where the the plot has the darkest color, i.e. math variable. I find this way of understanding data to be highly intuitive in this context. Now, it’s time to bake Bayesian philosophy into code. Recall that to perform Bayesian analysis, we first need to specify a prior. Although we could use an uninformed prior, in this case, we have data to work with. The way that makes the most sense would be to count the number of data points corresponding to each class to create a categorical distribution and use that as our prior, as shown below. If we use the created from , we should get a very simple prior whereby math variable since there is an equal number of data points belonging to the two classes in the toy data set. Indeed, this seems to be true. Next, it’s time to model the likelihood function. I won’t get into the specifics of this function, but all it does is that it calculates the likelihood by using the parameters returned by the function to indicate the likelihood that a particular belongs to a certain class. As per convention of this tutorial, the returned dictionary has keys corresponding to each class and values indicating the likelihood that the belongs to that class. We can see the function in action by passing a dummy test instance. We are almost done! All that we have to do is to create a funcition that returns the predicted label of a testing instance given some labeled training data. Implemenitng this process is straightforward since we have all the Bayesian ingredients we need, namely the prior and the likelihood. The last step is to connect the dots with Bayes’ theorem by calculating the product of the prior and likelihood for each class, then return the class label with the largest posterior, as illustrated below. Let’s see if the works as expected by seeing if passing as argument , for which we know that its label is 1, actually returns 1. The function is only able to process a single testing instance. Let’s complete our model construction by writing the function that takes labeled data and a testing set as its argument to return a array containing the predicted class labels for each instance in the testing set. Done! Let’s import some data from the library. The wine set data is a classic multi-class classfication data set. The data set contains three target classes, labeled as integers from 0 to 2, and thirteen feature columns, listed below: As I am not a wine afficionado, I have no idea what some of these columns represent, but that is irrelevant to the purpose of this tutorial. Let’s jump right in by loading the data. It always a good idea to get a sense of what the data looks like by verifying its dimension. Note that we used the function we wrote in previous posts to shuffle and slice the data into training and validation sets. For convenience, the code for this function is presented below. Now it’s finally time to check our model by making predictions. This can simply be done by passing the training and testing data set into the function that represented our Gaussian naive Bayes model. Let’s check if the predicted class labels match the answer key, i.e. the array. Eyeballing the results, it seems like we did reasonably well! In fact, the line below tells us that our model mislabeled only one test instance! We can quantify the performance our model through the metric of accuracy. The function does this for us. Note that instead of using the for-loop approach used in previous posts, this function is more vectorized, making computation less expensive. The shorter code is also an added benefit. The accuracy of our from-scratch model is 97 percent, which is not bad for a start. Let’s see if the model in outperforms our hand-coded model. The accuray score yielded by is exactly identical to that achieved by our model! Looks like the model in scikit-learn does exactly what our model does, at least juding from the metric of accuracy. This is surprising, but since we basically followed the Bayesian line of reasoning to buid our model, which is what naive Bayes really is all about, perhaps this is not as astonishing as it seems. In this post, we built the Gaussian naive Bayes model from scratch. In the process, we reviewed key concepts such as Bayesian inference and maximum a posteriori estimation, both of which are key statistical concepts used in many subdomains of machine learning. Hopefully through this tutorial, you gained a better understanding of how Gaussian mathematics and Bayesian thinking can be used in the context of classification. The true power of naive Bayes is not limited to the task of classificaiton, however. In fact, it is used in many fields, most notably natural language processing. Perhaps we might look into the possible applications of naive Bayes in the context of NLP in a future post. But for now, this level of modeling will do. Thanks for reading. See you in the next post!",0,0,1,1,0,0,0,0
Likelihood and Probability,"“I think that’s very unlikely.” “No, you’re probably right.” These are just some of the many remarks we use in every day conversations to express our beliefs. Linguistically, words such as “probably” or “likely” serve to qualify the strength of our professed belief, that is, we express a degree of uncertainty involved with a given statement. In today’s post, I suggest that we scrutinize the concept of likelihood—what it is, how we calculate it, and most importantly, how different it is from probability. Although the vast majority of us tend to conflate likelihood and probability in daily conversations, mathematically speaking, these two are distinct concepts, though closely related. After concretizing this difference, we then move onto a discussion of maximum likelihood, which is a useful tool frequently employed in Bayesian statistics. Without further ado, let’s jump right in. As we have seen in an earlier post on Bayesian analysis, likelihood tells us—and pardon the circular definition here—how likely a certain parameter is given some data. In other words, the likelihood function answers the question: provided some list of observed or sampled data math expression, what is the likelihood that our parameter of interest takes on a certain value math expression? One measurement we can use to answer this question is simply the probability density of the observed value of the random variable at that distribution. In mathematical notation, this idea might be transcribed as: At a glance, likelihood seems to equal probability—after all, that is what the equation (1) seems to suggest. But first, let’s clarify the fact that math expression is probability density, not probability. Moreover, the interpretation of probability density in the context of likelihood is different from that which arises when we discuss probability; likelihood attempts to explain the fit of observed data by altering the distribution parameter. Probability, in contrast, primarily deals with the question of how probable the observed data is given some parameter math expression. Likelihood and probability, therefore, seem to ask similar questions, but in fact they approach the same phenomenon from opposite angles, one with a focus on the parameter and the other on data. Let’s develop more intuition by analyzing the difference between likelihood and probability from a graphical standpoint. To get started, recall the that This is the good old definition of probability as defined for a continuous random varriable math expression, given some probability density function math expression with parameter math expression. Graphically speaking, we can consider probability as the area or volume under the probability density function, which may be a curve, plane, or a hyperplane depending on the dimensionality of our context. Unlike probability, likelihood is best understood as a point estimate on the PDF. Imagine having two disparate distributions with distinct parameters. Likelihood is an estimate we can use to see which of these two distributions better explain the data we have in our hands. Intuitively, the closer the mean of the distribution is to the observed data point, the more likely the parameters for the distribution would be. We can see this in action with a simple line of code. This code block creates two distributions of different parameters, math expression and math expression. Then, we assume that a sample of value 1 is observed. Then, we can compare the likelihood of the two parameters given this data by comparing the probability density of the data for each of the two distributions. In this case, math expression seems more likely, i.e. it better explains the data math expression since math expression, which is larger than math expression. To sum up, likelihood is something that we can say about a distribution, specifically the parameter of the distribution. On the other hand, probabilities are quantities that we ascribe to individual data. Although these two concepts are easy to conflate, and indeed there exists an important relationship between them explained by Bayes’ theorem, yet they should not be conflated in the world of mathematics. At the end of the day, both of them provide interesting ways to analyze the organic relationship between data and distributions. Maximum likelihood estimation, or MLE in short, is an important technique used in many subfields of statistics, most notably Bayesian statistics. As the name suggests, the goal of maximum likelihood estimation is to find the parameters of a distribution that maximizes the probability of observing some given data math expression. In other words, we want to find the optimal way to fit a distribution to the data. As our intuition suggests, MLE quickly reduces into an optimization problem, the solution of which can be obtained through various means, such as Newton’s method or gradient descent. For the purposes of this post, we look at the simplest way that involves just a bit of calculus. The best way to demonstrate how MLE works is through examples. In this post, we look at simple examples of maximum likelihood estimation in the context of normal distributions. We have never formally discussed normal distributions on this blog yet, but it is such a widely used, commonly referenced distribution that I decided to jump into MLE with this example. But don’t worry—we will derive the normal distribution in a future post, so if any of this seems overwhelming, you can always come back to this post for reference. The probability density function for the normal distribution, with parameters math expression and math expression, can be written as follows: Assume we have a list of observations that correspond to the random variable of interest, math expression. For each math expression in the sample data, we can calculate the likelihood of a distribution with parameters math expression by calculating the probability densities at each point of the PDF where math expression. We can then make the following statement about these probabilities: In other words, to maximize the likelihood simply means to find the value of a parameter that which maximizes the product of probabilities of observing each data point. The assumption of independence allows us to use multiplication to calculate the likelihood in this manner. Applied in the context of normal distributions with math expression observations, the likelihood function can therefore be calculated as follows: But finding the maximum of this function can quickly turn into a nightmare. Recall that we are dealing with distributions here, whose PDFs are not always the simplest and the most elegant-looking. If we multiply math expression terms of the normal PDF, for instance, we would end up with a giant exponential term. To prevent this fiasco, we can introduce a simple transformation: logarithms. Log is a monotonically increasing function, which is why maximizing some function math expression is equivalent to maximizing the log of that function, math expression. Moreover, the log transformation expedites calculation since logarithms restructure multiplication as sums. With that in mind, we can construct a log equation for MLE from (3) as shown below. Because we are dealing with Euler’s number, math expression, the natural log is our preferred base. Using the property in (3), we can simplify the equation above: To find the maximum of this function, we can use a bit of calculus. Specifically, our goal is to find a parameter that which makes the first derivative of the log likelihood function to equal 0. To find the optimal mean parameter math expression, we derive the log likelihood function with respect to math expression while considering all other variables as constants. From this, it follows that Rearranging this equation, we are able to obtain the final expression for the optimal parameter math expression that which maximizes the likelihood function: As part 2 of the trilogy, we can also do the same for the other parameter of interest in the normal distribution, namely the standard deviation denoted by math expression. We can simplify this equation by multiplying both sides by math expression. After a little bit of rearranging, we end up with Finally, we have obtained the parameter values for the mean and variance of a normal distribution that maximizes the likelihood of our data. Notice that, in the context of normal distributions, the ML parameters are simply the mean and standard deviation of the given data point, which closely aligns with our intuition: the normal distribution that best explains given data would have the sample mean and variance as its parameters, which is exactly what our result suggests. Beyond the specific context of normal distributions, however, MLE is generally very useful when trying to reconstruct or approximate the population distribution using observed data. Let’s wrap this up by performing a quick verification of our formula for maximum likelihood estimation for normal distributions. First, we need to prepare some random numbers that will serve as our supposed observed data. We then calculate the optimum parameters math expression and math expression by using the formulas we have derived in (5) and (6). We then generate two subplots of the log likelihood function as expressed in (4), where we vary math expression while keeping math expression at in one and flip this in the other. This can be achieved in the following manner. Executing this code block produces the figure below. From the graph, we can see that the maximum occurs at the mean and standard deviation of the distribution as we expect. Combining these two results, we would expect the maximum likelihood distribution to follow math expression where math expression = and math expression = in our code. And that concludes today’s article on (maximum) likelihood. This post was motivated from a rather simple thought that came to my mind while overhearing a conversation that happened at the PMO office. Despite the conceptual difference between probability and likelihood, people will continue to use employ these terms interchangeably in daily conversations. From a mathematician’s point of view, this might be unwelcome, but the vernacular rarely strictly aligns with academic lingua. In fact, it’s most often the reverse; when jargon or scholarly terms get diffused with everyday language, they often transform in meaning and usage. I presume words such as “likelihood” or “likely” fall into this criteria. All of this notwithstanding, I hope this post provided you with a better understanding of what likelihood is, and how it relates to other useful statistical concepts such as maximum likelihood estimation. The topic for our next post is going to be Monte Carlo simulations and methods. If “Monte Carlo” just sounds cool to you, as it did to me when I first came across it, tune in again next week. Catch you up in the next one.",0,0,0,0,1,0,0,0
How lucky was I on my shift?,"At the Yongsan Provost Marshall Office, I receive a wide variety of calls during my shift. Some of them are part of routine communications, such as gate checks or facility operation checks. Others are more spontaneous; fire alarm reports come in from time to time, along with calls from the Korean National Police about intoxicated soldiers who get involved in mutual assault or misdemeanors of the likes. Once, I got a call from the American Red Cross about a suicidal attempt of a soldier off post. All combined, I typically find myself answering about ten to fifteen calls per shift. But yesterday was a special day, a good one indeed, because I received only five calls in total. This not only meant that USAG-Yongsan was safe and sound, but also that I had a relatively light workload. On other days when lawlessness prevails over order, the PMO quickly descends into chaos—patrols get dispatched, the desk sergeant files mountains of paperwork, and I find myself responding to countless phone calls while relaying relevant information to senior officials, first sergeants, and the Korean National Police. So yesterday got me thinking: what is the probability that I get only five calls within a time frame of eight hours, given some estimate of the average number of calls received by the PMO, say 12? How lucky was I? One way we might represent this situation is through a binomial distribution. Simply put, a binomial distribution simulates multiple Bernoulli trials, which are experiments with only two discrete results, such as heads and tails, or more generally, successes and failures. A binomial random variable math expression can be defined as the number of success in math expression repeated trials with probability of success math expression. For example, if we perform ten tosses of a fair coin, the random variable would be the number of heads; math expression would be math expression, and math expression would be math expression. Mathematically, the probability distribution function of a binomial distribution can be written as follows: We can derive this equation by running a simple thought experiment. Let’s say we are tossing a coin ten times. How can we obtain the probability of getting one head and nine tails? To begin with, here is the list of all possible arrangements: Notice that all we had to do was to choose one number math expression that specifies the index of the trial in which a coin toss produced a head. Because there are ten ways of choosing a number from integers math expression to math expression, we got ten different arrangements of the situation satisfying the condition math expression. You might recall that this combinatoric condition can be expressed as math expression, which is the coefficient of the binomial distribution equation. Now that we know that there are ten different cases, we have to evaluate the probability that each of these cases occur, since the total probability math expression, where math expression. Calculating this probability is simple: take the first case, as an example. Assuming independence on each coin toss, we can use multiplication to calculate this probability: Notice that math expression because we assumed the coin was fair. Had it not been fair, we would have different probabilities for math expression and math expression, explained by the relationship that math expression. This is what the binomial PMF is implying: calculating the probability that we get math expression successes in math expression trials requires that we multiply the probability of success math expression by math expression times and the probability of failure math expression by math expression times, because those are the numbers of successful and unsuccessful trials respectively. Now that we have reviewed the concept of binomial distribution, it is time to apply it to our example of phone calls at the Yongsan PMO. Although I do not have an data sheet on me, let’s assume for the sake of convenience that, on average, 12 calls come to the PMO per shift, which is eight hours. Given this information, how can we simulate the situation as a binomial distribution? First, we have to define what constitutes a success in this situation. While there might be other ways to go about this agenda, the most straightforward approach would be to define a phone call as a success. This brings us to the next question: how many trials do we have? Here is where things get a bit more complicated—we don’t really have trials! Notice that this situation is somewhat distinct from coin tosses, as we do not have a clearly defined “trial” or an experiment. Nonetheless, we can approximate the distribution of this random variable by considering each ten-minute blocks as a unit for a single trial, i.e. if a call is received by the PMO between 22:00 and 22:10, then the trial is a success; if not, a failure. Blocking eight hours by ten minutes gives us a total of 48 trials. Because we assumed the average number of phone calls on a single shift to be 12, the probability of success math expression. Let’s simulate this experiment math expression times. We can model this binomial distribution as follows: This code block produces the following output: Under this assumption, we can also calculate how lucky I was yesterday when I only received five calls by plugging in the appropriate values into the binomial PMF function: From a frequentist’s point of view, I would have lazy days like these only 7 times for every thousand days, which is nearly three years! Given that my military service will last only 1.5 years from now, I won’t every have such a lucky day again, at least according to the binomial distribution. But a few glaring problem exists with this mode of analysis. For one, we operated under a rather shaky definition of a trial by arbitrarily segmenting eight hours into ten-minute blocks. If we modify this definition, say, by saying that a single minute comprises an experiment, hence a total of 480 trials, we get a different values for math expression and math expression, which would clearly impact our calculation of math expression. In fact, some weird things happen if we block time into large units, such as an hour—notice how the value of math expression becomes math expression, which is a probabilistic impossibility as math expression should always take values between math expression. Another issue with this model is that a Bernoulli trial does not allow for simultaneous successes. Say, for instance, that within one ten-minute block, we got two calls. However, because the result of a Bernoulli trial is binary, i.e. either a success or a failure, it cannot contain more than one success in unit time. Therefore, binary distribution cannot encode higher dimensions of information, such as two or three simultaneous successes in one trial. These set of complications motivate a new way of modeling phone calls. In the next section, we look at an alternate approach to the problem: the Poisson distribution. Here is some food for thought: what if we divide up unit time into infinitesimally small segments instead of the original ten, such that math expression? This idea is precisely the motivation behind the Poisson distribution. If we divide our time frame of interest into infinite segments, smaller even than microseconds, we can theoretically model multiple successful events, which is something that the binomial distribution could not account for. Intuitively speaking, this approach is akin to modeling a continuous function as infinitely many stepwise functions such that two “adjacent” dots on the graph could be considered as identical points—or, in probabilistic terms, a simultaneous event. And because we have infinitely many trials and only a fixed number of success, this necessarily means that math expression would approach 0. Although this value may seem odd, the argument that the probability of receiving a call at this very instant is 0, since “instant” as a unit of time is infinitely short to have a clearly defined probability. From an algebraic standpoint, math expression is necessary to ensure that math expression, the expected number of success, converges to a real value. Now let’s derive the Poisson formula by tweaking the PMF for a binomial distribution. We commence from this expression: We can substitute math expression for math expression from the definition: Using the definition of combinatorics, Recall that math expression can alternately be defined as math expression. From this definition, it flows that: But then the last term converges to 1 as math expression goes to math expression: We can further simplify the rest of the terms in the limit expression as well. Specifically, math expression collapses to math expression. These terms can be coupled with math expression in the denominator as follows: Putting this all together yields: And we have derived the PMF for the Poisson distribution! We can perform a crude sanity check on this function by graphing it and checking that its maximum occurs at math expression. In this example, we use the numbers we assumed in the PMO phone call example, in which math expression. The code produces the following graph. As expected, the graph peaks at math expression. At a glance, this distribution resembles the binomial distribution we looked at earlier, and indeed that is no coincidence: the Poisson distribution is essentially a special case of binomial distributions whereby the number of trials is literally pushed to the limit. As stated earlier, the binomial distribution can be considered as a very rough approximation of the Poisson distribution, and the accuracy of approximation would be expected to increase as math expression increases. So let me ask the question again: how lucky was I yesterday? The probability distribution function of the Poisson distribution tells us that math expression can be calculated through the following equation: The result given by the Poisson distribution is somewhat larger than that derived from the binomial distribution, which was math expression. This discrepancy notwithstanding, the fact that I had a very lucky day yesterday does not change: I would have days like these once every 100 days, and those days surely don’t come often. But to really calculate how lucky I get for the next 18 months of my life in the military, we need to do a bit more: we need to also take into account the fact that receiving lesser than 5 calls on a shift also constitutes a lucky day. In other words, we need to calculate math expression, as shown below: This calculation can be done rather straightforwardly by plugging in numbers into the Poisson distribution function as demonstrated above. Of course, this is not the most elegant way to solve the problem. We could, for instance, tweak the Poisson distribution function and perform integration. The following is a code block that produces a visualization of what this integral would look like on a graph. Here is the figure produced by executing the code block above. You might notice from the code block that the integrand is not quite the Poisson distribution—instead of a factorial, we have an unfamiliar face, the function. Why was this modification necessary? Recall that integrations can only be performed over smooth and continuous functions, hence the classic example of the absolute value as a non-integrable function. Factorials, unfortunately, also fall into this category of non-integrable functions, because the factorial operation is only defined for integers, not all real numbers. To remedy this deficiency of the factorial, we resort to the gamma function, which is essentially a continuous version of the factorial. Mathematically speaking, the gamma function satisfies the recursive definition of the factorial: Using the gamma distribution function, we can then calculate the area of the shaded region on the figure above. Although I do not present the full calculation process here, the value is approximately equal to that we obtained above, math expression. So to answer the title of this post: about 2 in every 100 days, I will have a chill shift where I get lesser than five calls in eight hours. But all of this aside, I should make it abundantly clear in this concluding section that I like my job, and that I love answering calls on the phone. I can assure you that no sarcasm is involved. If you insist on calculating this integral by hand, I leave that for a mental exercise for the keen reader. Or even better, you can tune back into this blog a few days later to check out my post on the gamma function, where we explore the world of distributions beyond the binomial and the Poisson.",0,0,0,0,1,0,0,1
Gaussian Mixture Models,"We’ve discussed Gaussians a few times on this blog. In particular, recently we explored Gaussian process regression, which is personally a post I really enjoyed writing because I learned so much while studying and writing about it. Today, we will continue our exploration of the Gaussian world with yet another machine learning model that bears the name of Gauss: Gaussian mixture models. After watching yet another inspiring video by mathematicalmonk on YouTube, I meant to write about Gaussian mixture models for quite some time, and finally here it is. I would also like to thank ritvikmath for a great beginner-friendly explanation on GMMs and Expectation Maximization, as well as fiveMinuteStats for a wonderful exposition on the intuition behind the EM algorithm. Without further ado, let’s jump right into it. The motivating idea behind GMMs is that we can model seemingly complicated distributions as a convex combination of Gaussians each defined by different parameters. One visual analogy I found particularly useful is imagining Gaussians as some sort of hill or mountain on a contour map. If we have multiple hills adjacent to one another, we can essentially model the topography of the region as a combination of Gaussians. At peaks, we would see circular contour lines, but where the hills meet, we might see different patterns, most likely circular patterns overlapping with each other. The key point here is that the combination is convex; in other words, the mixing coefficient for each Gaussian should add up to one. If we consider GMM to be a generative model, then we can imagine the generating process as follows: At this point, for clarity’s sake, let’s introduce some notations and concretize what has been elaborated above. First, we define a categorical distribution that will represent the mixing coefficients described above. Let math variable be an math variable-dimensional vector that parametrizes this categorical distribution. In other words, This means that we assume the data to have math variable different clusters. Each math variable is then a mixing coefficient that establishes the convexness of the linear combinations of the underlying Gaussians. Now we can sample the cluster index, denoted as math variable, from the categorical distribution as shown below. Note that we use math variable for the cluster index, since it is considered a latent variable—we don’t observe it directly, yet it is deeply involved in the data generation process. Quite simply, the probability that a data point belongs to the math variableth cluster is represented by math variable. The last step to data generation, as outlined in the bullet points above, is sampling a point from the corresponding Gaussian. where math variable and math variable are the mean and covariance matrices that parameterize the math variableth Gaussian in the mixture model. Now that we have an idea of how GMMs can serve as a generative model that describes the underlying data generation process, let’s think about the marginal probability—that is, the probability that some point math variable in the sample space was generated by the GMM. After all, what we observe is the final output, not the latent variables. Therefore, it would be convenient to be able to come up with some expression for the marginal distribution. We can come up with a simple expression using the law of total probability and marginalization. where math variable can take values between 1 and math variable. Notice that we can thus simplify math variable, as this is simply the categorical distribution for the mixing coefficients. In other words, We can also simplify the condition probability expression, since we already know that it follows a normal distribution. Therefore, we end up with And there we have it, the density function of the Gaussian mixture model! We have a convex combination of math variable different Gaussian PDFs that compose the Gaussian mixture model. In the context of machine learning, the goal is to find the parameters of the model that best describe some given set of data. In the case of GMMs, this is no different. The parameters that we have to estimate are Of course, math variable and math variable are not single vectors or matrices, but math variable collection of such objects. But for notational simplicity, I opted to write them as such as shown above. Given a collection of math variable data points, denoted as math variable, we can now come up with the following expression: math variable denotes the likelihood function, and math variable is a collection of all parameters that define the mixture model. In other words, All this is doing is that we are using the marginal distribution expression we derived earlier and applying that to a situation in which we have math variable data points instead of just one, which is the context we have been operating in so far. We multiply the probabilities given the assumption of independence. From the perspective of maximum likelihood estimation, the goal then would be to maximize this likelihood to find the optimal parameters for math variable and math variable. Before we attempt MLE with the likelihood function, let’s first try to calculate the log likelihood, as this often makes MLE much easier by decoupling products as summations. Let math variable. And now we see a problem: the log is not applied to the inside of the function due to the summation. This is bad news since deriving this expression by math variable will become very tedious. The result is for sure not going to look pretty, let’s try deriving the log likelihood by math variable and set it equal to zero. At least one good news is that, for now, we can safely ignore the outer summation given the linearity of derivatives. We ultimately end up with the expression below: This is not pretty at all, and it seems extremely difficult, if not impossible, to solve analytically for math variable. This is why we can’t approach MLE the way we usually do, by directly calculating derivatives and setting them equal to zero. This is where Expectation Maximization, or EM algorithm comes in. Before we get into the details of what the EM algorithm is, it’s perhaps best to provide a very brief overview of how the EM algorithm works. A very simple way to understand EM is to think of the Gibbs sampler. Simply put, Gibbs sampling is a way of approximating some joint distribution given conditional distributions. The underlying idea was to sample from one distribution and use that sampled result to in turn generate another sample from the next conditional distribution. One might visualize this as a chain of circular dependence: if we obtain a sample from past samples, the new sample can then be used to generate the next sample. Repeat this process until convergence, and we are done. Turns out that the Gibbs sampler can be considered a specific flavor of the EM method. Although I am still far away from fully understanding the inner-workings of the EM algorithm, the underlying idea is clear: given some sort of dependence relationship, much like we saw in the case of Gibbs sampling above, we can generate some sample in one iteration and use that sample in the next. As we will see in this section, such a relationship can clearly be established, which is why EM is so commonly used in the context of Gaussian mixture models. Let’s begin by defining some quantities, the first being the posterior distribution. In other words, given some data, what is the probability that it will belong to a certain class? Using the definition of conditional probability, we can arrive at the following conclusion: This represents the probability that, given some point math variable, the point belongs in the math variableth cluster. With this result, we can now rewrite the MLE calculation that we were performing earlier. Using the new math variable notation, we can thus simplify the result down to We can then simplify this expression to derive an expression for math variable. An important trick is here to use the fact that the covariance matrix is positive semi-definite. Therefore, the covariance matrix plays no role in driving the value down to zero. With some algebraic manipulations, we arrive at Let’s introduce another notational device to simplify the expresison even further. Let Recall that math variable was defined to be the posterior probability that a given point math variable belongs to the math variableth cluster. Then, since we are essentially summing up this quantity across the entire math variable data points in the dataset math variable, we can interpret math variable to effectively be the number of points in the dataset that are assinged to the math variableth cluster. Then, we can now simplify the MLE estimate of the mean as But we can now observe something interesting. Notice that amath variable depend on math variable. In turn, math variable is defined in terms of math variable. This is the very circular dependency that we discussed earlier as we were introducing the EM algorithm and comparing it with the Gibbs sampler. Now it becomes increasingly apparent why the EM algorithm is needed to find a converging solution for the MLE estimates. We can take a similar approach to calculate the MLE of the other two remaining paramters, namely math variable and math variable. The derivation is more complicated since math variable is a matrix; math variable is subject to constraints that apply to any categorical distribution: all elements must be positive and must sum up to one. For my own personal reference and the curious-minded, here is a link to a resource that contains the full derivation. But the fundamental idea is that we would commence from the log likelihood function and derive our way to the solution. The solutions are presented below: So the full picture is now complete: given the inter-dependence of derived quantities, we seek to optimize them using the Expectation Maximization algorithm. Specifically, the EM method works as follows: In today’s post, we took a deep dive into Gaussian mixture models. I find GMMs to be conceptually very intuitive and interesting at the same time. I’m also personally satisfied and glad that I have learned yet another ML/mathematical concept that starts with the word Gaussian. Much like how I felt when learning about Gaussian process regression, now I have an even greater respect for the Gaussian distribution, although I should probably be calling it normal instead, just like everybody else. I’m also happy that I was able to write this blog post in just a single day. Of course, this is in large part due to the fact that I had spent some time a few weeks ago studying this material, but nonetheless I think I’m starting to find the optimal balance between intern dev work and self-studying of math and machine learning. I hope you’ve enjoyed reading this post. In a future post, I will be implementing Gaussian mixture models in Python from scrach. Stay tuned for more!",0,0,1,0,1,0,0,0
Traveling Salesman Problem with Genetic Algorithms,"The traveling salesman problem (TSP) is a famous problem in computer science. The problem might be summarized as follows: imagine you are a salesperson who needs to visit some number of cities. Because you want to minimize costs spent on traveling (or maybe you’re just lazy like I am), you want to find out the most efficient route, one that will require the least amount of traveling. You are given a coordinate of the cities to visit on a map. How can you find the optimal route? The most obvious solution would be the brute force method, where you consider all the different possibilities, calculate the estimated distance for each, and choose the one that is the shortest path. While this is a definite way to solve TSP, the issue with this approach is that it requires a lot of compute—the runtime of this brute force algorithm would be math variable, which is just utterly terrible. In this post, we will consider a more interesting way to approach TSP: genetic algorithms. As the name implies, genetic algorithms somewhat simulate an evolutionary process, in which the principle of the survival of the fittest ensures that only the best genes will have survived after some iteration of evolutionary cycles across a number of generations. Genetic algorithms can be considered as a sort of randomized algorithm where we use random sampling to ensure that we probe the entire search space while trying to find the optimal solution. While genetic algorithms are not the most efficient or guaranteed method of solving TSP, I thought it was a fascinating approach nonetheless, so here goes the post on TSP and genetic algorithms. Before we dive into the solution, we need to first consider how we might represent this problem in code. Let’s take a look at the modules we will be using and the mode of representation we will adopt in approaching TSP. The original, popular TSP requires that the salesperson return to the original starting point destination as well. In other words, if the salesman starts at city A, he has to visit all the rest of the cities until returning back to city A. For the sake of simplicity, however, we don’t enforce this returning requirement in our modified version of TSP. Below are the modules we will be using for this post. We will be using , more specifically a lot of functions from for things like sampling, choosing, or permuting. arrays are also generally faster than using normal Python lists since they support vectorization, which will certainly be beneficial when building our model. For reproducibility, let’s set the random seed to 42. Now we need to consider the question of how we might represent TSP in code. Obviously, we will need some cities and some information on the distance between these cities. One solution is to consider adjacency matrices, somewhat similar to the adjacency list we took a look at on the post on Breadth First and Depth First Search algorithms. The simple idea is that we can construct some matrix that represent distances between cities math variable and math variable such that math variable represents the distance between those two cities. When math variable, therefore, it is obvious that math variable will be zero, since the distance from city math variable to itself is trivially zero. Here is an example of some adjacency matrix. For convenience purposes, we will represent cities by their indices. Now it’s time for us to understand how genetic algorithms work. Don’t worry, you don’t have to be a biology major to understand this; simple intuition will do. The idea is that, we can use some sort of randomized approach to generate an initial population, and motivate an evolutionary cycle such that only superior genes will survive successive iterations. You might be wondering what genes are in this context. Most typically, genes can be thought of as some representation of the solution we are trying to find. In this case, an encoding of the optimal path would be the gene we are looking for. Evolution is a process that finds an optimal solution for survival through competition and mutation. Basically, the genes that have superior traits will survive, leaving offspring into the next generation. Those that are inferior will be unable to find a mate and perish, as sad as it sounds. Then how do these superior or inferior traits occur in the first place? The answer lies in random mutations. The children of one parent will not all have identical genes: due to mutation, which occurs by chance, some will acquire even more superior features that puts them far ahead of their peers. Needless to say, such beneficiaries of positive mutation will survive and leave offspring, carrying onto the next generation. Those who experience adversarial mutation, on the other hand, will not be able to survive. In genetic algorithm engineering, we want to be able to simulate this process over an extended period of time without hard-coding our solution, such that the end result after hundred or thousands of generations will contain the optimal solution. Of course, we can’t let the computer do everything: we still have to implement mutational procedures that define an evolutionary process. But more on that later. First, let’s begin with the simple task of building a way of modeling a population. First, let’s define a class to represent the population. I decided to go with a class-based implementation to attach pieces of information about a specific generation of population to that class object. Specifically, we can have things like to represent the full population, to represent th chosen, selected superior few, to store the score of the best chromosome in the population, to store the best chromosome itself, and , the adjacency matrix that we will be using to calculate the distance in the context of TSP. Here is a little snippet of code that we will be using to randomly generate the first generation of population. Let’s see if this everything works as expected by generating a dummy population. Now we need some function that will determine the fitness of a chromosome. In the context of TSP, fitness is defined in very simple terms: the shorter the total distance, the fitter and more superior the chromosome. Recall that all the distance information we need is nicely stored in . We can calculate the sum of all the distances between two adjacent cities in the chromosome sequence. Next, we evaluate the population. Simply put, evaluation amounts to calculating the fitness of each chromosome in the total population, determining who is best, storing the score information, and returning some probability vector whose each element represents the probability that the th element in the population bag is chosen as a parent. We apply some basic preprocessing to ensure that the worst performing chromosome has absolutely no chance of being selected. When we call , we get a probability vector as expected. From the result, it appears that the last element is the best chromosome; the second chromosome in the population bag is the worst. When we call , notice that we get the last element in the population, as previously anticipated. We can also access the score of the best chromosome. In this case, the distance is said to be 86.25. Note that the lower the score, the better, since these scores represent the total distance a salesman has to travel to visit all the cities. Now, we will select number of parents to be the basis of the next generation. Here, we use a simple roulette model, where we compare the value of the probability vector and a random number sampled from a uniform distribution. If the value of the probability vector is higher, the corresponding chromosome is added to . We repeat this process until we have parents. As expected, we get 4 parents after selecting the parents through . Now is the crucial part: mutation. There are different types of mutation schemes we can use for our model. Here, we use a simple swap and crossover mutation. As the name implies, swap simply involves swapping two elements of a chromosome. For instance, if we have , we might swap the first two elements to end up with . The problem with swap mutation, however, is the fact that swapping is a very disruptive process in the context of TSP. Because each chromosome encodes the order in which a salesman has to visit each city, swapping two cities may greatly impact the final fitness score of that mutated chromosome. Therefore, we also use another form of mutation, known as crossovers. In crossover mutation, we grab two parents. Then, we slice a portion of the chromosome of one parent, and fill the rest of the slots with that of the other parent. When filling the rest of the slots, we need to make sure that there are no duplicates in the chromosome. Let’s take a look at an example. Imagine one parent has and the other has . Let’s also say that slicing a random portion of the first parent gave us . Then, we fill up the rest of the empty indices with the other parent, paying attention to the order in which elements occur. In this case, we would end up with . Let’s see how this works. Now, we wrap the swap and crossover mutation into one nice function to call so that we perform each mutation according to some specified threshold. Let’s test it on . When we call , we end up with the population bag for the next generation, as expected. Now it’s finally time to put it all together. For convenience, I’ve added some additional parameters such as or , but for the most part, a lot of what is being done here should be familiar and straightforward. The gist of it is that we run a simulation of population selection and mutation over generations. The key part is and . Basically, we obtain the children from the mutation and pass it over as the population bag of the next generation in the constructor. Now let’s test it on our TSP example over 20 generations. As generations pass, the fitness score seems to improve, but not by a lot. Let’s try running this over an extended period of time, namely 100 generations. For clarity, let’s also plot the progress of our genetic algorithm by setting to . After something like 30 iterations, it seems like algorithm has converged to the minimum, sitting at around 86.25. Apparently, the best way to travel the cities is to go in the order of . But this was more of a contrived example. We want to see if this algorithm can scale. So let’s write some functions to generate city coordinates and corresponding adjacency matrices. generates number of random city coordinates in the form of a numpy array. Now, we need some functions that will create an adjacency matrix based on the city coordinates. Let’s perform a quick sanity check to see if works as expected. Here, give vertices of a unit square as input to the function. While we’re at it, let’s also make sure that indeed does create city coordinates as expected. Now, we’re finally ready to use these functions to randomly generate city coordinates and use the genetic algorithm to find the optimal path using with the appropriate parameters. Let’s run the algorithm for a few iterations and plot its history. We can see that the genetic algorithm does seems to be optimizing the path as we expect, since the distance metric seems to be decreasing throughout the iteration. Now, let’s actually try plotting the path along with the corresponding city coordinates. Here’s a helper function to print the optimal path. And calling this function, we obtain the following: At a glance, it’s really difficult to see if this is indeed the optimal path, especially because the city coordinates were generated at random. I therefore decided to create a much more contrived example, but with many coordinates, so that we can easily verify whether the path decided on by the algorithm is indeed the optimal path. Namely, we will be arranging city coordinates to lie on a semi-circle, using the very familiar equation Let’s create 100 such fake cities and run the genetic algorithm to optimize the path. If the algorithm does successfully find an optimal path, it will be a single curve from one end of the semi-circle fully connected all the way up to its other end. The algorithm seems to have converged, but the returned does not seem to be the optimal path, as it is not a sorted array from 0 to 99 as we expect. Plotting this result, the fact that the algorithm hasn’t quite found the most optimal solution becomes clearer. This point notwithstanding, it is still worth noting that the algorithm has found what might be referred to as optimal segments: notice that there are some segments of the path that contain consecutive numbers, which is what we would expect to see in the optimal path. An optimal path would look as follows. Comparing the two, we see that the optimal path returned by the genetic algorithm does contain some wasted traveling routes, namely the the chords between certain non-adjacent cities. Nonetheless, a lot of the adjacent cities are connected (hence the use of the aforementioned term, optimal segments). Considering the fact that there are a total of math variable possibilities, the fact that the algorithm was able to narrow it down to a plausible route that beats the baseline is still very interesting. Genetic algorithms belong to a larger group of algorithms known as randomized algorithms. Prior to learning about genetic algorithms, the word “randomized algorithms” seemed more like a mysterious black box. After all, how can an algorithm find an answer to a problem using pseudo-random number generators, for instance? This post was a great opportunity to think more about this naive question through a concrete example. Moreover, it was also interesting to think about the traveling salesman problem, which is a problem that appears so simple and easy, belying the true level of difficulty under the surface. There are many other ways to approach TSP, and genetic algorithms are just one of the many approaches we can take. It is also not the most effective way, as iterating over generations and generations can often take a lot of time. The contrived semi-circle example, for instance, took somewhere around five to ten minutes to fully run on my 13-inch MacBook Pro. Nonetheless, I think it is an interesting way well worth the time and effort spent on implementation. I hope you’ve enjoyed reading this post. Catch you up in the next one!",0,0,0,1,0,0,0,0
Building Neural Network From Scratch,"Welcome back to another episode of “From Scratch” series on this blog, where we explore various machine learning algorithms by hand-coding them from scratch. So far , we have looked at various machine learning models, such as kNN, logistic regression, and naive Bayes. Now is time for an exciting addition to this mix: neural networks. Around last year December, I bought my first book on deep learning, titled Deep Learning from Scratch, by Saito Goki. It was a Korean translation of a book originally published in Japanese by O’Reilly Japan. Many bloggers recommended the book as the go-to introductory textbook on deep learning, some even going as far as to say that it is a must-have. After reading a few pages in, I could see why: as the title claimed, the author used only to essentially recreate deep learning models, ranging from simple vanilla neural networks to convolutional neural networks. As someone who had just started to learn Python, following the book was a lot harder than expected, but it was a worthwhile read indeed. Inspired by that book, and in part in an attempt to test the knowledge I gained from having read that bok, I decided to implement my own rendition of a simple neural network supported by minibatch gradient descent. Let’s jump right into it. The default setup of my Jupyter Notebook, as always: Before we start building our model, we should first prepare some data. Instead of using hand-made dummy data as I had done in some previous posts, I decided to use the library to generate random data points. This approach makes a lot more sense given that neural networks require a lot more input data than do machine learning models. In this particular instance, we will use the function to accomplish this task. Let’s take a look at what our data looks like. As expected, the dataset contains the math variable and math variable coordinates of the points generated by the function. If you haven’t heard about this function before, you might be wondering what all the moons deal is about. Well, if we plot the data points, it will become a lot more obvious. As you can see, the generated points belong to either one of two classes, and together, each class of points seem to form some sort of moon-like shape. Our goal will be to build a neural network that is capable of determining whether a given point belongs to class 0 or class 1. In other words, this is a classic example of a binary classification problem. It is standard practice in any classification problem to convert class labels into one-hot encodeded vectors. The reason why this preprocessing is necessary is that the class number is merely a label that does not carry any meaning. Assume a simple classification problem with 3 labels: 0, 1, and 2. In that context, a class label of 2 is not at all related to adding two data points belonging to class 1, or any arithmatic operation of that kind. To prevent our model from making such arbitrary, unhelpful connections, we convert class labels to one-hot encoded vectors. We could use external libraries such as to invoke the function, but instead let’s just build a function ourselves since this is a relatively simple task. Let’s test the function on the training data. We don’t need the entire data to see that it works, so let’s slice the array to see its first five elements. When we apply to the data, we see that the returned result is a two-dimensional array containing one-hot encoded vectors, as intended. That’s all the data and the preprocessing we will need for now. Activation functions are important aspects of neural networks. In fact, it is what allows neural networks to model nonlinearities in data. As we will see in the next section, a neural network is essentially composed of layers and weights that can be expressed as matrix multiplications. No matter how complex a matrix may be, matrix multiplication is a linear operation, which means that is impossible to model nonlinearities. This is where activation functions kick in: by applying nonlinear transformation to layer outputs, we can make neural networks capable of modeling nonlinearities. This is why deep learning is such a powerful tool: it can be trained to detect nonlinear, complex patterns in data that a human might otherwise be unable to identify. Our vanilla neural network will make use of two activation functions: softmax and ReLU. If you have read my previous post on the Keras functional API, you might recall that we used softmax and ReLU for certain dense layers. Back then, we considered them to be a blackbox without necessarily taking a look at what they do. Let’s explore the details and get our hands dirty today. Mathematically speaking, the softmax function is a function that takes a vector as input and outputs a vector of equal length. Concretely, where Although the formula may appear complex, the softmax function is a lot simpler than it seems. First, note that all math variable entries of the returned vector math variable add up to 1. From here, it is possible to see that the softmax function is useful for ascribing the probability that a sample belongs to one of math variable classes: the math variable-th element of math variable would indicate the probability of the sample belonging to the math variable-th class. Put another way, the index of the largest entry in math variable is the class label number that is most probable. Implementing the softmax function is extremely easy thanks to the vectorized computation made possible through . Presented below is one possible implementation of the softmax function in Python. This particular implementation, however, poses two problems. First, it is susceptible to arithematic overflow. Because computing the softmax function require exponentiation, it is likely for the computer to end up with very large numerical quantities, making calculations unstable. One way to solve this problem is by subtracting values from the exponent. As the calculation shows, adding or subtracting the same value from the exponent of both the numerator and the denominator creates no difference for the output of the softmax function. Therefore, we can prevent numbers from getting too large by subtracting some value from the exponent, thus yielding accurate results from stable computation. We can further improve the softmax function for the purposes of this tutorial by supporting batch computation. By batch, I simply mean multiple inputs in the form of arrays. The function shown above is only able to account for a single vector, presumably given as a list or a one-dimensional numpy array. The implementation below uses a loop to calculate the softmax output of each instance in a matrix input, then returns the result. Note that it also prevents arithematic overflow by subtracting the value of the input array. Let’s test the improved softmax function with a two-dimensional array containing two instances. As expected, the softmax function returns the softmax output applied to each individual instance in the list. Note that the elements of each output instance add up to one, as expected. Another crucial activation function is ReLU, or the rectified linear unit. ReLU is a piece-wise function, and hence introduces nonlinearity, which is one of the purposes of having an activation function in a neural network. The formula for ReLU is extremely simple. If the input value math variable i s greater or equal to zero, the ReLU function outputs the value without modification. However, if math variable is smaller than zero, the returned value is also zero. There are other ways of expressing the ReLU function. One version that is commonly used and thus deserves our attention is written below. Although this appears different from (3), both formulas express the same operation at their core. We can get a better sense of what the function with the help of Python. Assuming that the input is a vector, we can use vectorization to change only the elements in the input vector that are negative to zero, as shown below. Let’s see what the ReLU function looks like by plotting it on the plane. The visualization makes clear the point that ReLU is a piece-wise function that flattens out negative values while leaving positive values unchanged. Now that we have all the ingredients ready, it’s time to build the neural network. Earlier, I said that a neural network can be reduced to matrix multiplication. This is obviously an oversimplification, but there is a degree of truth to that statement. Recall that a single neuron of a neural network can be expressed as a dot product of two vectors, as shown below. Following conventional notation, math variable represents weights; math variable, input data; math variable, bias. Visually, we can imagine the neuron being lit up when the value math variable is large. This is similar to how the human brain works, except that biological neurons are binary in that they either fires on or off; artifical neurons in a network typically take a range of values. If we expand the vector operation in (5), it becomes quickly obvious that we can represent an entire layer of neurons as a product of two matrices. Our simple neural network model can thus be expressed as follows: The equations above represent our simple neural network model composed of two affine layers. The output of the first affine layer, math variable, is modified by a ReLU unit. Then, the output is passed onto the second affine layer, math variable, the output of which is passed onto a softmax unit. The output of the softmax function is the final output of our model. Note that ReLU and softmax are denoted as max and sigma, respectively. The code below is a function that intializes our network. Because our data only has two classes, with each data point containing two entries corresponding to the math variable and math variable coordinates of that point, we set both and arguments to 2 by default. The number of neurons in the affine layers, denoted as , is arbitrarily set to 64. The returns a dictionary that contains all the weights of the model. Note that we need to pay close attention to the dimensionality of our data to ensure that matrix multiplication is possible. We don’t have to worry about the dimensionality of the bias since supports broadcasting by default. Presented below is a visualization of our neural network, created using NN-SVG. Instead of cluttering the diagram by attempting to visualize all 64 neurons, I decided to simplify the picture by assuming that we have 16 neurons in each of the affine layers. But with the power of imagination, I’m sure it’s not so much difficult to see how the picture would change with 64 neurons. Hopefully the visualization gave you a better understanding of what our model looks like. Now that we have a function that creates our model, we are ready to run the model! At this point, our neural network model is only a dictionary that contains matrices of specified sizes, each containing randomly genereated numbers. You might be wondering how a dictionary can be considered a model—after all, a dictionary is merely a data structure, and so is incapable of performing any operations. To make our model to work, therefore, we need a function that performs matrix multiplications and applies activation functions based on the dictionary. The function is precisely such a function that uses the weights stored in our model to return both the intermediary and final outputs, denoted as and respectively. Note that we apply activation functions, such as and when appropriate. This process of deriving an output from an input using a neural network is known as forward propagation. Forward propagation is great and all, but without appropriately trained weights, our model is obviously going to spit out meaningless predictions. The way to go about this is to use the gradient descent algorithm with back propagation. We will discuss more about back propagation in the next subsection, as it is a meaty topic that deserves space of its own. We deal primarily with the former in this section. This is not the first time that we have come across gradient descent on this blog. In fact, we used gradient descent to optimizse our logistic regression model in this post. Recall that the gradient descent algorithm can be summarized as where math variable represents the parameters, or weights, math variable represents the learning rate, and math variable represents the loss function. This is the vanilla gradient descent algorithm, which is also referred to as batch gradient descent. Minibatch gradient descent is similar to gradient descent. The only point of difference is that it calculates the gradient for each minibatch instead of doing so for the entire dataset as does batch gradient descent. The advantage of using a minibatch is that it is computationally lighter and less expensive. Minibatch gradient descent can be considered a happy point of compromise between stochastic and batch gradient descent, which lie on the polar opposite ends of the spectrum. Let’s first take a look at the function, which divides the and into and given a . Internally, the function calls the gradient descent algorithm to update the weights and finally returns the which contains updated parameters based on the training data. As mentioned above, each and are minibatches that will be feeded into our gradient descent function. Note that the function is simply an implementation of equation (7). At the core of the function is the function, which is our implementation of back propagation. This provides a nice point of transition to the next section. Back propagation is a smart way of calculating gradients. There are obviously many ways one might go about gradient calculation. We can simply imagine there being a loss function that is a function of all the thousands of weights and biases making up our neural network, and calculate partial derivatives for each parameter. However, this naive aproach is problematic because it is so computationally expensive. Moreover, if you think about it for a second, you might realize that doing so would result in duplicate computations due to the chain rule. Take the simple example below. If we were to calculate the gradient of the loss function with respect to math variable and math variable, all we need to compute is the gradient of math variable, since that of math variable will naturally be obtained along the way. In other words, computing the gradient simply requires that we start from the very end of the neural network and propagate the gradient values backwards to compute the partial derivatives according to the chain rule. This is what is at the heart of back propagation: in one huge swoop, we can obtain the gradient for all weights and parameters at once instead of having to calculate them individually. For a more detailed explanation of this mechanism, I strongly recommend that you take a look at this excellent blog post written by Christopher Olah. How do we go about back propagation in the case of our model? First, it is necessary to define a loss function. The most commonly used loss function in the context of classification problems is cross entropy, which we explored in this post previously on this blog. For a brief recap, presented below is the formula for calculating cross entropy given a true distribution math variable and a predicted distribution math variable: Our goal is to train our neural network so that is output distribution math variable is as close to math variable as possible. In the case of binary classification, we might alter equation (9) to the following form: The reformulation as shown in equation (10) is the formula for what is known as binary cross entropy. This is the equation that we will be using in the context of our problem, since the dataset we have only contains two class labels of 0 and 1. Now that we have an idea of what the loss function looks like, it’s time to calculate the gradient. Since we are going to be back propagating the gradient, it makes sense to start from the very back of the neural network. Recall that our neural network is structured as follows: The last layer is a softmax unit that receives input math variable to produce output math variable. Our goal, then, is to compute the gradient where math variable and math variable each represent the values taken by the math variableth and math variableth neuron in layers math variable and math variable, respectively. One point of caution is that it is important to consider whether math variable and math variable are equal, as this produces differences in the calculation of the gradient. First consider the case when math variable: When math variable: We see that the gradient is different in the two cases! This is certainly going to important for us when calculating the gradient of math variable, the cross entropy loss function, with respect to math variable. Specifically, we have to consider the two cases separately by dividing up the summation expression into two parts, as shown below: That was a long ride, but in the end, we end up with a very nice expression! This tells us that the gradient of the cross entropy loss function with respect to the second affine layer is simply the size of the error term. In other words, if we expand the result in (13) to apply to the entire matrix of layers, we get This provides a great place for us to start. We can commence from here to find the gradient of the loss function with respect to other layers more further down the neural network. For example, we can calculate the gradient with respect to the weights of the second affine layer as follows: We won’t get into much mathematical details here, but a useful intuition we can use to derive equation (15) is to pay close attention to the dimensionality of data. Note that the dimension of the gradient as a matrix should equal to that of the layer itself. In other words, math variable, so on and so forth. This is because the purpose of gradient computation is to update the matrix of parameters: to perform an element-by-element update with the gradient, it must necessarily be true that the dimensionality of the gradient equals that of the original matrix. Using this observation, it is possible to navigate through the confusion of transposes and left, right matrix multiplication that one might otherwise encounter if they were to approach it without any intuition or heuristics. To expedite this post, I’ll present the result of the gradient calculations for all parameters below. Note that the indicator function, denoted as math variable, is a simple gate function that calculates the gradient of the ReLU unit: It isn’t difficult to see that the indicator function is simply a derivative of the ReLU function as shown in equation (3). Now, it is time to translate our findings into Python. Because our neural network model is represented as a dictionary, I decided to adopt the same data structure for the gradient. Indeed, that is how we designed the function above. The function below is an implementation of back propagation that encapsulates equations (14) through (17). There is a subtlety that I did not discuss previously, which has to do with the bias terms. It may appear as if the gradient of the bias term does not match that of the bias term itself. Indeed, that is a valid observation according to equation (16). The way we go about this is that we add up the elements of the matrix according to columns. This is exactly what we do with the command invoked when computing and , which represent the gradient of the bias terms. With all the complex math behind, here is the code implementation of back propagation. Finally, our model is ready to be trained! Here is a simple function which we can use to train and test our model. Because each iteration can yield a different accuracy, we repeat the experiment multiple times—or specifically, times—to obtain the mean accuracy of our model. We also get a standard deviation of the mean accuracy estimate to see whether or not the performance of the model is reliable and consistent. Let’s test our model with the and data, with batch size set to 10. The mean accuracy of our model is around 95 percent, which isn’t bad for a simple neural network with just two layers. The standard deviation is also reasonably low, indicating that the performance of our model is consistent with little variations. I was almost about to stop here, but then decided that I wanted to express the neural network model as a Python class. After all, that is how actual machine learning and deep learning libraries are implemented. I also decided that it can’t hurt for me to practice object-oriented thinking. So presented in the next section is a nicer, cleaner implementation of a neural network model based off of the functions we designed above. A simple neural network model in just 56 lines of code, ready to be initialized, trained, deployed, and tested! You will see that much of the code is literally just copy and pasted from the original functions we designed above. But just to make sure that everything works fine, let’s try creating a neural network object and use the function to see how well our model performs. I chose 99 as the number of neurons in the affine layers for no reason. In this instance, the accuracy of this model is 95 percent, similar to what we had above. At this point, one question that popped up in my mind was the relationship between the number of neurons and the performance of the neural network model. Intuitively, the more neurons there are, the higher the memory capacity of that model, and thus better the performance. Of course, the larger the number of neurons, the larger the risk of overfitting our model, which can also negatively impact the performance of the neural network. This is conventional wisdom in the land of deep learning. Let’s create a function to plot the performance of a neural network and the number of its neurons. Below is a function that achieves this task. The function receives , , and as arguments. The first two arguments specify the range for the number of neurons that we are interested in. For example, if we set them to 3 and 40, respectively, that means we want to see the accuracy of models with number of neurons ranging from 3 to 40 in a single layer. The argument specifies the number of experiments we want to conduct. This way, we can calculate the mean accuracy, just as we did previously. Let’s call the function to create a plot. The result shows that the performance of the neural network generally increases as the number of neurons increase. We don’t see signs of overfitting, but we know it happens: recall that our neural network model with 99 and 64 hidden neurons hit an accuracy of about 95 percent, whereas the model with only 30 to 40 neurons seem to be outperforming this metric by an accuracy hovering around 98 percent. After having realized this, I considered re-running the function with a different range, but eventuially decided to stop the experiment because running the function took a lot more time than I had expected, even on Google Colab. Creating and training the model takes a long time, especially if we are repeating this process times. For now, the simple observation that the performance seems to increase with more neurons, then fall at one point once overfitting starts to happen, will suffice to satisfy our curiosity. In this post, we built a neural network only using and math. This was a lot more difficult than building other machine learning models from scratch particularly because of the heavy mathematics involved. However, it was definitely worth the challenge becasue completing and writing up this tutorial made me think a lot more about the clockwork of a neural network model. It is easy to think of neural networks as a black box, especially given the sheer ease of creating it. With just , one can build a simple neural network like this one in no time. Indeed, the main reason why I love the Keras functional API so much is that it is so easy to code and deploy a neural network model. However, when we write such models by depending on preexisting libraries, we sometimes grow oblivious to the intricacies the take place under the hood. It is my hope that reading and following along this post gave you a renewed sense of respect for the writers of such libraries, as well as the beauty of neural network models themselves. I hope you enjoyed reading this post. Catch you up in the next one!",1,1,0,1,0,0,0,0
VGG PyTorch Implementation,"In today’s post, we will be taking a quick look at the VGG model and how to implement one using PyTorch. This is going to be a short post since the VGG architecture itself isn’t too complicated: it’s just a heavily stacked CNN. Nonetheless, I thought it would be an interesting challenge. Full disclosure that I wrote the code after having gone through Aladdin Persson’s wonderful tutorial video. He also has a host of other PyTorch-related vidoes that I found really helpful and informative. Having said that, let’s jump right in. We first import the necessary modules. Let’s first take a look at what the VGG architecture looks like. Shown below is a table from the VGG paper. We see that there are a number of different configurations. These configurations typically go by the name of VGG 11, VGG 13, VGG 16, and VGG 19, where the suffix numbers come from the number of layers. Each value of the dictionary below encodes the architecture information for each model. The integer elements represents the out channel of each layer. represents a max pool layer. You will quickly see that the dictionary is just a simple representation of the tabular information above. Now it’s time to build the class that, given some architecture encoding as shown above, can produce a PyTorch model. The basic idea behind this is that we can make use of iteration to loop through each element of the model architecture in list encoding and stack convolutional layers to form a sub-unit of the network. Whenever we encounter , we would append a max pool layer to that stack. This is probably the longest code block I’ve written on this blog, but as you can see, the meat of the code lies in two methods, and . These methods are where all the fun stacking and appending described above takes place. I actually added a little bit of customization to make this model a little more broadly applicable. First, I added batch normalization, which wasn’t in the original paper. Batch normalization is known to stabilize training and improve performance; it wasn’t in the original VGG paper because the batch norm technique hadn’t been introduced back when the paper was published. Also, the model above can actually handle rectangular images, not just square ones. Of course, there still is a constraint, which is that the and parameters must be multiples of 32. Let’s roll out the model architecture by taking a look at VGG19, which is the deepest architecture within the VGG family. If we print the model, we can see the deep structure of convolutions, batch norms, and max pool layers. We can clearly see the two submodules of the network: the convolutional portion and the fully connected portion. Now let’s see if all the dimensions and tensor sizes match up. This quick sanity check can be done by passing in a dummy input. This input represents a 3-channel 224-by-224 image. Passing in this dummy input and checking its shape, we can verify that forward propagation works as intended. And indeed, we get a batched output of size , which is expected given that the input was a batch containing two images. Just for the fun of it, let’s define and see if it is capable of processing rectangular images. Again, we can pass in a dummy input. This time, each image is of size . And we see that the model is able to correctly output what would be a probability distribution after a softmax.",1,0,0,0,0,1,0,0
Neural Style Transfer,"In today’s post, we will take a look at neural style transfer, or NMT for short. NMT is something that I first came across about a year ago when reading Francois Chollet’s Deep Learning with Python book. At that time, I was just getting my feet wet in deep learning with Keras, and I specifically remember myself skipping the chapter on NMT, feeling unprepared and intimidated by the implementation. After a full year, I feel ready to face the challenge, but this time with PyTorch. Let’s get started! Before we get into any specific implementation details, it’s probably helpful to provide some context on how NMT works. Note that, among the many variations of the NMT algorithm, we are going to be discussing the original one first introduced by Gatys et. al. The goal of NMT is simple: given a content image and a style image, transform the content image to have the look and feel of the style image. Below is an example taken from Yunjey’s PyTorch tutorial, which has been an amazing resource so far in my PyTorch journey. One peculiarity in the original NMT algorithm is that, unlike in typical scenarios in which we update the model’s parameters training, in NMT we update the pixel values of the clone of the content image itself to gradually stylize it. There is no “NMT model” that transforms some image; rather, we merely calculate a loss that is the combination of the content loss and style loss, then optimize the image with respect to this combined loss. Given some style image math variable, content image math variable, and a resulting generated image math variable, we can write the expression for the total loss as where math variable is a weight parameter that determines the degree with which we want to prioritize style over content. Intuitively, the more stylized an image, the higher the content loss; the smaller the content loss, the higher the style loss. In a way, these two quantities are somewhat mutually exclusive, which is why we want to use a weight constant to ascribe some level of importance to one over the other. Some variations to this formula include those that include weights for both the style and content terms, such as At the end of the day, both formulations are identical, only scaled by some scalar value. (2) is a special case of (1) where math variable. Thus, we can always go from (1) to (2) simply by multiplying by some constant. For simplicity reasons, we will assume (1) throughout this tutorial. A natural question to ask, then, is how we calculate each of these loss terms. Somehow, these loss terms should be able to capture how different two images are, content-wise or style-wise. This is where feature extractors come into play. Pretrained models, such as the VGG network, have filters that are capable of extracting features from an image. It is known that low level convolutional filters that are closer to the input can extract low-level features such as lines or curves, whereas deeper layers are trained to have activation maps that respond to specific shapes or patterns. Notice that this is in line with what the content loss should be able to encode: the general lines and curves of the image should remain similar, as well as the location or presence of general objects like eyes, nose, or hands, to give some concrete examples. Thus, the content loss is simply the L2 norm of the features extracted from each target layer of some pretrained model math variable. Do not let the notation confuse you. All this means is that we sum over each layers of the pretrained model math variable. For each of these layers, we calculate matrix element-wise L2 norm of the content and generated image features extracted by the math variableth layer of the model. If we sum all of them up, we obtain the value of the content loss. Intuitively, we can think of this as comparing both high level and low level features between the two images. The style loss is somewhat trickier, but not too much. The authors of the original NMT paper used what is called the Gram matrix, sometimes also referred to as the Gramian matrix. The Gram matrix, despite its fancy name, is something that you’ve already seen at some point in your study of linear algebra. Given some matrix math variable, the Gram matrix can be calculated as More strictly speaking, given a set of vectors math variable, a Gram matrix can be calculated such that So how does the Gram matrix encode the stylistic similarities or differences between two images? Before I attempt at an explanation in words, I recommend that you check out this Medium article, which has helped me wrapped my own head around the different dimensions involved in the style loss term. This Medium article has also helped me gain more intuition on why the style loss is the way it is. The motivating idea is that, given an image and a layer in the feature extractor model, the activations each encode information coming from a filter. The resulting feature maps, therefore, contain information about some feature the model has learned, such as the presence of some pattern, shape, or object. By flattening each feature map and constructing a matrix of activations math variable, where math variable is the number of filters and math variable is the width times height of each activation, we can now construct the Gram matrix. Effectively, the Gram matrix is a dot product of each rows of math variable; thus, if some math variableth and math variableth features tend to co-occur, math variable will have a large value. The key here is that the Gram matrix is largely location agnostic; all the information related to locations or positions in the image is lost in the calculation. This is expected, and in some ways desirable, since the style of an image is largely independent from its spatial features. Another key point is that, the style of an image can be thought of as an amalgamation of different combinations of each feature. For instance, Van Gogh’s style of painting is often associated with strong, apparent brush strokes. It is possible to decompose and analyze this style into a set of co-occurring features, such as thick line edges, curves, and so on. So in a sense, the Gram matrix encodes such information at different depths of the pretrained feature extractor, which is why it is fitting to use the Gram matrix for calculating style loss. Concretely, the equation for style loss goes as follows: The style loss is similar to content loss in the sense that it is also a sum of element-wise L2 norms of two matrices. The differences are that we are using the Gram matrix instead of the raw activations themselves, and that we have a scaling constant. But even this constant is a pretty minor change, as I have seen implementations where the style weight was made a trainable parameter as opposed to a fixed scalar. As stated earlier, this tutorial seeks to explain the original NMT algorithm. Subsequent NMT methods use an actual model instead of formulating NMT as an optimization problem in which we modify the generated image itself. The benefit of using an actual model is that it is quicker and more efficient; after all, it takes a lot of time to create a plausible image from some white noise (which is why we are going to use the clone of the content image for this tutorial—but even then, it is still very slow). Now that we have an understanding of how NMT works, let’s get down to the details. Let’s begin by importing necessary modules and handling some configurations for this tutorial. We will be using VGG 19 as our pretrained feature extractor model. We will be using five layers of the network to obtain intermediate representations of the input image. Below is a simple code that lets us achieve this task. In this case, we use the zeroth, fifth, tenth, 19tht, and 28th layers of the model. The output is a list that contains the representations of the input image. It’s time to read, load, and preprocess some images. Below is a simple helper function we will use to read an image from some file directory, then apply any necessary resizing and transformations to the image. Next, let’s define some transformations we will need. The VGG model was trained with a specific transformation configuration, which involves normalizing RGB images according to some mean and standard deviation for each color channel. These values are specified below. We will apply this transformation after loading the image. The application of this transformation will be handled by the function we’ve defined earlier. Later on in the tutorial, we will also need to undo the transformation to obtain a human presentable image. The following operation accomplishes this task. While the numbers might look like they came out of nowhere, it’s actually just a reversal of the operation above. Specifically, given a normalizing operation we can undo this normalization via In other words, the reverse transformation can be summarized as And thus it is not too difficult to derive the values specified in the reverse transformation . Now, let’s actually load the style and content images. We also create a target image. The original way to create the target image would be to generate some white noise, but I decided to copy the content image instead to make things a little easier and expedite the process. Note also that the code has references to some of my local directories; if you want to test this out yourself, make changes as appropriate. Now we are finally ready to solve the optimization problem! The next step would be to generate intermediate representations of each image, then calculate the appropriate loss quantities. Let’s start by defining some values, such as the learning rate, weights, print steps, and others. Below is a helper function that we will be using to save images as we optimize. This will help us see the changes in style as we progress throughout the optimization steps. Finally, this is where all the fun part takes place. For each step, we obtain intermediate representations by triggering a forward pass. Then, for each layer, we calculate the content loss and style loss. The code is merely a transcription of the loss equations as defined above. In particular, calculating the Gram matrix might appear a little bit complicated, but all that’s happening is that we are effectively flattening each activation to make it a single matrix, then calculating the Gram via matrix multiplication with its transpose. We see that style loss decreases quite a bit, whereas the content loss seems to slightly increase with each training step. As stated earlier, it is difficult to optimize on both the content and style, since altering the style of the picture will end up affecting its content in one way or another. However, since our goal is to stylize the target image via NMT, it’s okay to sacrifice a little bit of content while performing NMT, and that’s what is happening here as we can see from the loss values. And here is the result of the transformation! The result is… interesting, and we certainly see that somethings have changed. We see some more interesting texture in the target image, and there appears to be some changes. However, at this point, my laptop was already on fire, and more training did not seem to yield any better results. So I decided to try out other sample implementations of NMT to see how using more advanced NMT algorithms could make things any better. I decided to try out fast neural style transform, which is available on the official PyTorch GitHub repository. Fast NMT is one of the more advanced, recent algorithms that have been studied after the original NMT algorithm, which we’ve implemente above, was introduced. One of the many benefits of fast neural style transfer is that, instead of framing NMT as an optimization problem, FNMT makes it a modeling problem. In this instance, TransformerNet is a pretrained model that can transform images into their stylized equivalents. The code below was borrowed from the PyTorch repository. I decided to try out FNMT on a number of different pictures of myself, just to see how different results would be for each. Here, we loop through the directory and obtain the file path to each content photo. And here are the results! Among the 20 photos that have been stylized, I think some definitely look better than others. In particular, I think the third row looks kind of scary, as it made every photo have red hues all over my face. However, there are definitely ones that look good as well. Overall, FNMT using pretrained models definitely yielded better results than our implementation. Of course, this is expected since the original NMT was not the most efficient algorithm; perhaps we will explore FNMT in a future post. But all in all, I think diving into the mechanics behind NMT was an interesting little project. I hope you’ve enjoyed reading this post. Catch you up in the next one!",1,0,0,0,0,1,0,0
The Gibbs Sampler,"In this post, we will explore Gibbs sampling, a Markov chain Monte Carlo algorithm used for sampling from probability distributions, somewhat similar to the Metropolis-Hastings algorithm we discussed some time ago. MCMC has somewhat of a special meaning to me because Markov chains was one of the first topics that I wrote about here on my blog. It’s been a while since I have posted anything about math or statistics-related, and I’ll admit that I’ve been taking a brief break from these domains, instead working on some personal projects and uping my Python coding skills. This post is going to be a fun, exciting mesh of some Python and math. Without further ado, let’s get started. I remember struggling to understand Metropolis-Hastings a while back. Gibbs sampling, on the other hand, came somewhat very naturally and intuitively to me. This is not because I’ve suddenly grown intelligent over the past couple of months, but because Gibbs sampling is conceptually simpler, at least in my humble opinion. All that is necessary to understand Gibbs sampling is the notion of conditional probability distributions. We know the classic context in which MCMC comes into play in a Bayesian setting: there is some intractable distribution that we wish to sample from. Metropolis-Hastings was one simple way to go about this, and Gibbs sampling provides another method. A feature that makes Gibbs sampling unique is its restrictive context. In order to use Gibbs sampling, we need to have access to information regarding the conditional probabilities of the distribution we seek to sample from. In other words, say we want to sample from some joint probability distribution math variable number of random variables. Let’s denote this distribution as follows: Turns out that the Gibbs sampler is a more specific version of the Metropolis-Hastings algorithm. We can only use the Gibbs sampler in a restricted context: namely, that we have access to conditional probability distributions. You quickly see why the Gibbs sampler can only be used in limited contexts. Nonetheless, when these set of information are available, it is a powerful algorithm with which we can sample from intractable distributions. Let’s see how this works. The gist of the Gibbs sampler is simple: sample from known conditional distributions, and use that resulting value to sample the next random variable from the following conditional probability distribution, ad infinitum. But this is just a lot of words and some needless Latin for fun and flair, so let’s hash out what the sentence really means. Continuing on from our generic example, let’s say we sampled a value from the first conditional probability distribution. We will use a superscript and subscript notation to each denote the iteration and the sequence of random variable. Assume that we start from some random math variable-dimensional vector to start with. Following our notation, this vector would be The superscripts are all 0 since this is the first “sample” we will start off with. Theoretically, it doesn’t matter what these random numbers are—asymptotically speaking, we should still be able to approximate the final distribution, especially if given the fact that we take burn-in into account. On the first iteration, we will begin by sampling from the first probability distribution. Note that we simply used the initial random values for math variable through math variable to sample the first value from a conditional probability distribution. Now, we do the same to sample math variable. Only this time, we can use the result from earlier, namely math variable. We can see how this might help us yield a slightly more convincing result than simply using the random data. We still have to use random values for math variable through math variable since we haven’t sampled from their relevant conditional distributions just yet. However, as we go through all the random variables in order, it becomes obvious that we will no longer be using randomly initialized values at one point. Specifically, on the math variableth iteration, we would expect something like this to happen: math variable can be any number between 1 and math variable, since it is used to represent the math variableth random variable. As we repeat more iterations of sampling, we will eventually end up with a plausible representation of math variable-dimensional vectors, which is what we sought to sample from the intractable distribution! In this section, we will take a look at a very simple example, namely sampling from a bivariate Gaussian distribution. Although we have dealt with math variable-dimensional examples in the algorithm analysis above, for the sake of demonstration, let’s work on a simple example that we can also easily visualize and intuit. For this reason, the bivariate Gaussian distribution is a sensible choice. For this post, I’ll be using , which is a data visualization library built on top of . I’ll simply be using to display a bivariate Gaussian. For reproducibility’s sake, we will also set a random seed. The code for the Gibbs sampler is simple, partially because the distribution we are dealing with is a bivariate Gaussian, not some high-dimensional intractable distribution. This point notwithstanding, the function shows the gist of how Gibbs sampling works. Here, we pass in parameters for the conditional distribution, and start sampling given an initial value corresponding to math variable. As stated earlier, this random value can be chosen arbitrarily. Of course, if we start from a value that is way off, it will take much longer for the algorithm to converge, i.e. we will have to discard a large portion of initially sampled values. This is known as burn-in. In this case, however, we will apply a quick hack and start from a plausible value to begin with, reducing the need for burn-in. We then take turns sampling from the conditional probability distributions using the sampled values, and append to a list to accumulate the result. The more difficult part here is deriving the equation for the conditional probabiilty distributions of the bivariate Gaussian. The full result is available on Wikipedia, but it’s always more interesting and rewarding to derive these results ourselves. But in this section, we will assume that we already know the final result of the derivation and use the formula for now. Note that the two functions are symmetrical, which is expected given that this is a bivariate distribution. These functions simulate a conditional distribution, where given a value of one random variable, we can sample the value of the other. This is the core mechanism by which we will be sampling from the joint probability distribution using the Gibbs sampling algorithm. Let’s initialize the parameters for the distribution and test the sampler. Great! This works as expected. For the purposes of demonstrating the implications of burn-in, let’s define discard the first 100 values that were sampled. Below is the plot of the final resulting distribution based on sampled values using the Gibbs sampler. The result is what we would expect: a bivariate Gaussian. And this is what we end up with if we sample directly from the bivariate Gaussian instead of using the Gibbs sampler. Note that we can do this only because we chose a deliberately simple example; in many other contexts, this would certainly not be the case (if we could sample from a distribution directly, why use the Gibbs sampler in the first place?). Notice the similarity between the result achieved by sampling from the Gibbs sampler and the result produced from direct sampling as shown below. So at this point, we have now empirically checked that Gibbs sampling indeed works: even if we can’t directly sample from the distribution, if we have access to conditional distributions, we can still achieve an asymptotically similar result. Now comes the mathematics portion of deriving the conditional distribution of a multivariate Gaussian, as promised earlier. In this section, we will derive an expression for the conditional distribution of the multivariate Gaussian. This isn’t really relevant to the Gibbs sampling algorithm itself, since the sampler can be used in non-Gaussian contexts as long as we have access to conditional distributions. Nonetheless, deriving this is a good mental exercise that merits some discussion. Just for the sake of quick review, let’s briefly revisit the familiar definition of a conditional probability: In the context of random vectors, we can rewrite this as Of course, if math variable and math variable are scalars, we go back to the familiar bivariate context of our example. In short, deriving the expression for the conditional distribution simply amounts to simplifying the fraction whose denominator is the marginal distribution and the numerator is the joint distribution. Let’s clarify the setup and notation first. We define a math variable-dimensional random vector that follows a multivariate Gaussian distribution, namely math variable. This vector, denoted as math variable, can be split into a math variable-dimensional vector math variable and math variable-dimensional vector math variable in the following fashion: It is apparent that math variable. Similarly, we can split up the covariance matrix math variable in the following fashion where math variable, math variable, math variable. Also, given the symmetric property of the covariance matrix, math variable. The goal is to derive is the conditional distribution, math variable. This derivation was heavily adapted from this source and this thread on Stack Exchange. It is certainly a somewhat lengthy derivation, but there is nothing too conceptually difficult involved—it’s just a lot of algebra and simplifications. We begin from the formula for the multivariate Gaussian: For convenience purposes, let Then, Let Note that this is not a one-to-one correspondence, i.e. math variable. The blocks are only one-to-one insofar as being dimensionally equivalent. Then, using block matrix multiplication, Notice that the final result should be a single scalar given the dimensions of each matrix. Therefore, we can further simply the expression above using the fact that math variable. Specifically, the second and third terms are transposes of each other. Although we simply resorted a convenient substitution in (6), we still need to derive an expression for the inverse of the covariance matrix. Note that the inverse of the covariance matrix can intuitively be understood as the precision matrix. We won’t derive the block matrix inversion formula here. The derivation is just a matter of simply plugging in and substituting one expression for another. For a detailed full derivation, checkout this link or this journal article. To cut to the chase, we end up with Plugging these results back into (8), and with some elided simplification steps, we end up with Note that we can more conveniently express the result in the following fashion: We’re now almost done. Heuristically, we know that the addition in math variable will become a multiplication when plugged back into the original formula for the multivariate Gaussian as shown in (3), using (4). Therefore, if we divide the entire expression by math variable, we will only end up with the term produced by math variable. Using this heuristic, we conclude that Notice that this result is exactly what we have in the function which we used to sample from the conditional distribution. The Gibbs sampler is another very interesting algorithm we can use to sample from complicated, intractable distributions. Although the use case of the Gibbs sampler is somewhat limited due to the fact that we need to be able to access the conditional distribution, it is a powerful algorithm nonetheless. We also discussed the notion of conditional distributions of the multivariate Gaussian in this post. The derivation was not the simplest, and granted we omitted a lot of algebra along the way, but it was a good mental exercise nonetheless. If you are interested in a simpler proof, I highly recommend that you check out the Stack Exchange post I linked above. I hope you enjoyed reading this post. Catch you up in the next one!",0,1,0,0,1,0,0,0
Natural Gradient and Fisher,"In a previous post, we took a look at Fisher’s information matrix. Today, we will be taking a break from the R frenzy and continue our exploration of this topic, meshing together related ideas such as gradient descent, KL divergence, Hessian, and more. The typical formula for batch gradient descent looks something like this: This is the familiar gradient descent algorithm that we know of. While this approach works and certainly makes sense, there are definite limitations; hence the introduction of other more efficient algorithms such as SGD, Adam, and et cetera. However, these algorithms all have one thing in common: they adjust the parameter in the parameter space according to Euclidean distance. In other words, gradient descent essentially looks at regions that are some Euclidean distance away from the current parameter and chooses the direction of steepest descent. This is where the notion of natural gradients come into play: if our goal is to minimize the cost function, which is effectively equivalent to maximizing the likelihood, why not search within the distribution space of the likelihood function instead? After all, this makes more sense since gradient descent in parameter space is likely to be easily perturbed by the mode of parametrization, such as using precision instead of variance in a normal distribution, whereas searching in the distribution space would not be subject to this limitation. So the alternative to this approach would be to search the distribution space and find the distribution that which makes value of the cost function the smallest. This is the motivation behind the notion of a natural gradient. Now you might be wondering how all this has anything to do with the Fisher matrix, which we looked at in the previous post. Well, it turns out there are some deep, interesting questions to be posed and connections to be uncovered. If we’re going to search around the distribution space, one natural question to consider is what distance metric we will use for our search. In case of batch gradient descent, we used Euclidean distance. This made sense since we were simply measuring the distance between two parameters, which are effectively scalars or vector quantities. If we want to search the distribution space, on the other hand, we would have to measure the distance between two probability distributions, one that is defined by the previous parameter and the other defined by the newly found parameter after natural gradient descent. Well, we know one great candidate for this task right off the bat, and that is KL divergence. Recall that KL divergence is a way of quantifying the pseudo-distance between two probability distributions. The formula for KL divergence is shown below. And while we’re at it, let’s throw cross entropy and entropy into the picture as well, both for review and clarity’s sake: For a short, simple review of these concepts, refer to this previous article, or Aurelien Geron’s video on YouTube. In most cases, math variable is the true distribution which we seek to model, while math variable is some more tractable distribution at our disposal. In the classic context of ML, we want to minimize the KL divergence. In this case, however, we’re simply using KL divergence as a means of measuring distance between two parameters in defined within a distribution space. As nicely stated in layman’s term in this Medium article, … instead of “I’ll follow my current gradient, subject to keeping the parameter vector within epsilon distance of the current vector,” you’d instead say “I’ll follow my current gradient, subject to keeping the distribution my model is predicting within epsilon distance of the distribution it was previously predicting” I see this as an intuitive way of nicely summarizing why we’re using KL divergence in searching the distribution space, as opposed to using Euclidean distance in searching the parameter space. Now it’s time for us to connect the dots between KL divergence and Fisher’s matrix. Before we diving right into computations, let’s think about how or why these two concepts might be related at all. One somewhat obvious link is that both quantities deal with likelihood, or to be more precise, log likelihood. Due to the definition of entropy, KL divergence ends up having a log likelihood term, while Fisher’s matrix is the negative expected Hessian of the log likelihood function, or the covariance matrix of Fisher’s score, which is the gradient of the log likelihood. Either way, we know that likelihood is the fundamental bridge connecting the two. Let’s try to compute the KL divergence between math variable and math variable. Conceptually, we can think of math variable as the previous point of the parameter and math variable as the newly updated parameter. In this context, the KL divergence would tell us the effect of one iteration of natural gradient descent. This time, instead of using integral, let’s try to simplify a bit by expressing quantities as expectations. We see the familiar log likelihood term. Given the fact that the Fisher matrix is the negative expected Hessian of the log likelihood, we should be itching to derive this expression twice to get a Hessian out of it. Let’s first obtain the gradient, then get its Jacobian to derive a Hessian. This derivation process was heavily referenced from Agustinus Kristiadi’s blog. Let’s do this one more time to get the Hessian. This conclusion tells us that the curvature of KL divergence is defined by Fisher’s matrix. In hindsight, this is not such a surprising result given that the KL divergence literally had a term for expected log likelihood. Applying the Leibniz rule twice to move the derivative into the integral, we quickly end up with Fisher’s matrix. At this point, you might be wondering about the implications of this conclusion. It’s great that KL divergence and the Fisher matrix are closely related via the Hessian, but what implication does it have for the gradient descent algorithm in distribution space? To answer this question, we first need to perform a quick multivariate second order Taylor expansion on KL divergence. Recall that the simple, generic case of multivariate Taylor expansion looks as follows: This is simply a generalization of the familiar univariate Taylor series approximation we saw earlier. (In most cases, we stop at the second order because computing the third order in the multivariate case requires us to obtain a three-dimensional symmetric tensor. I might write a post on this topic in the future, as I only recently figured this out and found it very amusing.) Continuing our discussion of KL divergence, let’s try to expand the divergence term using Taylor approximation. Here, math variable is small distance in the distribution space defined by KL divergence as the distance metric. This can be a bit obfuscating notation-wise because of the use of math variable as our variable, assuming math variable as a fixed constant, and evaluating the gradient and the Hessian at the point where math variable since we want to approximate the value of KL divergence at the point where where math variable. But really, all that is happening here is that in order to approximate KL divergence, we’re starting at the point where math variable, and using the slope and curvature obtained at that point to approximate the value of KL divergence at distance math variable away. Picturing the simpler univariate situation in the Cartesian plane might help. The bottom line is that the KL divergence is effectively defined by the Fisher matrix. The implication of this is that now, the gradient descent algorithm is subject to the constraint where math variable is some constant. Now, the update rule would be To solve for the argument minima operation, we will resort to the classic method for optimization: Lagrangians. In this case, the Lagrangian would be This immediately follows from using the constraint condition. To make progress, let’s use Taylor approximation again, both on the term for the loss function and the KL divergence. The good news is that we have already derived the expression for the latter. Noting the fact that there are several constants in this expression, we can simplify this into To minimize this expression, we set its gradient equal to zero. Note that we are deriving with respect to math variable. Therefore, We are finally done with our derivation. This equation tells us that the direction of steepest descent is defined by the inverse of the Fisher matrix multiplied by the gradient of the loss function, up to some constant scaling factor. This is different from the vanilla batch gradient descent we are familiar with, which was simply defined as Although the difference seems very minor—after all, all that was changed was the addition of Fisher’s matrix—yet the underlying concept, as we have seen in the derivation, is entirely different. This was definitely a math-heavy post. Even after having written this entire post, I’m still not certain if I have understood the details and subtleties involved in the derivation. And even the details that I understand now will become confusing and ambiguous later when I return back to it. Hopefully I can retain most of what I have learned from this post. Before I close this post, I must give credit to Agustinus Kristiadi, whose blog post was basically the basis of this entire writing. I did look at a few Stack Overflow threads, but the vast majority of what I have written are either distillations or adaptations from their blog. It’s a great resource for understanding the mathematics behind deep learning. I hope you enjoyed reading this blog. See you in the next one!",0,0,1,0,1,0,0,0
Better seq2seq,"In the previous post, we took a look at how to implement a basic sequence-to-sequence model in PyTorch. Today, we will be implementing a small improvement to the previous model. These improvements were suggested in Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, by Cho, et. al. To cut to the chase, the image below, taken from Ben Trevett’s tutorial, encapsulates the enhancement we will be implementing today. In the previous model, the hidden state of the model was a bottleneck: all the information from the encoder was supposed to be compressed into the hidden state, and even that encoder hidden state would have undergone changes as the decoder was unrolled with subsequent target sequences. To reduce this bottleneck and lessen the compression burden on the encoder hidden state, the improved architecture will allow the decoder to gain access to the encoder hidden state at each time step. Moreover, the final classifier output layer in decoder will have access to the original embedding of the target language token as well as the last hidden state of the encoder, represented as math variable in the diagram above. This can be considered a residual connection, since the embedding skips the RNN and directly gets fed into the fully connected layer. Now that we have some idea of what we want to achieve, let’s start coding. Since the setup for this tutorial is identical to that of the previous post, I’ll skip much of the explanation and sanity checks. In the code block below, we load the dataset, then create bucket iterators for each train, validation, and test split. Let’s start with the encoder. The encoder here is actually almost identical to the one we implemented in the previous model. In fact, it is arguably simpler, as we are now using a single GRU layer instead of a two-layered LSTM. The decoder is where all the enhancements are going to take place. Recall the changes we want to make to the previous seq2seq architecture. As you can see, we need to make sure that The first change means that the decoder’s forward method needs to be able to take in the encoder’s hidden states as input. For sake of notational clarity, let’s call those hidden states as “context.” The way we allow the decoder to use its own hidden state as well as the context for computation is that we concatenate the context with its input embeddings. Effectively, we could think of this as creating a new embedding vectors, where the first half comes from actual embeddings of English tokens and the later half comes from the context vector. Another implementation detail not mentioned earlier is the dimension of the last fully connected classifier layer. Since we now concatenate the embedding vector with the hidden state from the GRU, context vector from the encoding, as well as the original embedding vectors, the classifier’s input dimensions are much larger than they were in the previous decoder model. Now it’s time to implement the sequence-to-sequence model. Most of the enhancements were already baked into the decoder, and the fundamental logic through which predictions are generated remain unchanged. Thus, only minimal changes have to be made to the seq2seq model: namely, we need to handle the context vector and pass it to the decoder at every time step. And from here on, the details are exactly identical; the same and functions can be used in the previous post. Since I intended this post to be a simple little exercise as opposed to a fully blown out tutorial, we’ll stop here, but by all means, feel free to experiment more with it. Below are the configurations Ben Trevett used in his tutorial. When I was writing this notebook, I realized that I enjoy thinking about the dimensional details of each tensor being passed around here and there. It is difficult, and sometimes it required me to use dirty print statements to actually log what was happening to each variable, but all in all, I think attention to dimensional detail is definitely something that one should practice and think about when modeling. I hope you enjoyed reading this post. In a future post, we will explore what attention is and how to bake it into a seq2seq model to take it to the next level. Also, happy holidays!",1,0,0,0,0,1,0,0
"Newton-Raphson, Secant, and More","Recently, I ran into an interesting video on YouTube on numerical methods (at this pont, I can’t help but wonder if YouTube can read my mind, but now I digress). It was a channel called numericalmethodsguy, run by a professor of mechanical engineering at the University of Florida. While the videos themselves were recorded a while back in 2009 at just 240p, I found the contents of the video to be very intriguing and easily digestable. His videos did not seem to assume much mathematical knowledge beyond basic high school calculus. After watching a few of his videos, I decided to implement some numerical methods algorithms in Python. Specifically, this post will deal with mainly two methods of solving non-linear equations: the Newton-Raphson method and the secant method. Let’s dive right into it. Before we move on, it’s first necessary to come up with a way of representing equations in Python. For the sake of simplicity, let’s first just consider polynomials. The most obvious, simplest way of representing polynomials in Python is to simply use functions. For example, we can express math variable as However, a downside of this approach is the fact that it’s difficult to directly derive this equation despite the fact that it is nothing more than just a simple polynomial. So instead, we will use a list index-based representation. Namely, the math variableth element of a list represents the coefficient of the math variableth power in a polynomial equation. In other words, math variable would translate into . The is a function that returns a Python function given a list that conforms to this list index representation. Let’s see if this works as expected. math variable, so the function passes our quick sanity test. One useful helper function that I also implemented for the sake of convenience is a array-to-equation parser that translates a list representation into a mathematical expression in Python. This is best demonstrated than explained, so I’ll defer myself to an example. Below is the full definition of the function. At this point, I also thought that it would be useful and interesting to compose a function that translates the string output of into a proper Python function we can use to calculate values. Below is the function that receives as input some parsed output string and returns a corresponding Python function. Now, we can do something like this: Now that we have more than enough tools we can use relating to the list index representation we decided to use to represent polynomials, it’s time to exploit the convenience that this representation affords us to calculate derivatives. Calculating derivatives using the list index representation is extremely easy and convenient: in fact, it can be achieved in just a single line. Let’s test this function with the example we have been using previously. Let’s also use the function to make the final result for human-readable. Seems like the derivative calculation works as expected. In the process, I got a little bit extra and also wrote a function that integrates a function in list index representation format. If we integrate math variable, we end up with math variable, where math variable is the integration constant. Excluding the integration constant, we get a result that is consistent with the function. While it’s great that we can calculate derivatives and integrals, one very obvious drawback of this direct approach is that we cannot deal with non-polynomial functions, such as exponentials or logarithms. Moreover, the list index representation is unable to represent polynomials that include terms whose powers are not positive integers. For these reasons, we will need some other methods of calculating derivatives as well. Hence the motivation for approximation methods, outlined in the section below. If you probe the deepest depths of your memory, somewhere you will recall the following equation, which I’m sure all of us saw in some high school calculus class: This equation, commonly known as the definition of a derivative, is also known as the forward divided difference formula of calculating derivatives. There is another variant, known as the backward divided difference formula: (1) and (2) are almost nearly identical, but the difference lies in which term is subtracted from who. In (1), we go an infinitesimal step forward—hence the math variable—and subtract the value at the point of approximation, math variable. In (2), we go backwards, which is why we get math variable. As math variable approaches 0, (1) and (2) asymptotically gives us identical results. Below is a Python variant of the backward divided difference formula. Some tweaks have been made to the formula for use in the section that follows, but at its core, it’s clear that the function uses the approximation logic we’ve discussed so far. Another variant of the forward and backward divided difference formula is the center divided difference. By now, you might have some intuition as to what this formula is—as the name implies, we somehow use the center point, instead of going forward or backwards from the point of approximation. Here is the formula: Heuristically, this formula also makes sense. We can imagine going both a step forward and backward, then dividing the results by the total of two steps we’ve taken, one in each direction. Shown below is the Python implementation of the center divided difference formula. According to conventional mathematical wisdom, the center divided method normally provides a more robust way of approximating first order derivatives. In this subsection, we discuss why this is the case. Using Taylor expansion, we can approximate the value of math variable as follows, given that math variable goes to 0 under the limit. Notice that we can manipulate (4) to derive the forward divided difference equation in (1). If we move the math variable term to the LHS, then divide both sides by math variable, we end up with Here, we used big-O notation to denote the order of magnitude of the trailing terms. The trailing terms are significant since they are directly related to the accuracy of our approximation. An error term of math variable means that, if we halve the step size, we will also halve the error. This is best understood as a linear relationship between error and the step size. We can conduct a similar mode of analysis with backward divided difference. By symmetry, we can express math variable as If we rearrange (6), we end up with (2). Again, we see that backward divided difference yields linear error, or a trailing term of math variable. Here’s where things get more interesting: in the case of center divided difference, the magnitude of the error term is math variable, meaning that halving the step size decreases the error by four-folds. This is why center divided difference yields much more accurate approximations than forward or backward divided difference. To see this, we subtract (5) from (4), then move some terms, and divide both sides by math variable. Notice that subtracting these two expression results in a lot of term cancellations. Dividing both sides by math variable yields From this result, we now know why the central divided difference method provides more accurate approximations of the derivative than do forward and backward divided difference. This is somewhat related to what we will be doing in the next section, so it’s a good intuition to have throughout when reading the rest of this article. Now that we have these tools for differential calculus, now comes the exciting part: solving non-linear equations. Specifically, we will be taking a look at two numerical methods: the Newton-Raphson method and the secant method. It’s time to put the methods we developed in the preceding sections to use for solving non-linear equations. Specifically, we’ll begin by taking look at a classic algorithm, the Newton-Raphson method. The Newton-Raphson method is one of the many ways of solving non-linear equations. The intuition behind the Newton-Raphson method is pretty straightforward: we can use tangent lines to approximate the x-intercept, which is effectively the root of the equation math variable. Specifically, we begin on some point on the graph, then obtain the tangent line on that point. Then, we obtain the math variable-intercept of that tangent line, and repeat the process we’ve just completed by starting on a point on the graph whose math variable-value is equal to that math variable-intercept. The following image from Wikipedia illustrates this process quite well. (A digression: It’s interesting to see how “function” and “tangent” are written in German—in case you are wondering, I don’t know a word of German.) Mathematically, the Newton-Raphson method can be expressed recursively as follows: Deriving this formula is quite simple. Say we start at a point on the graph, math variable. The tangent line from that point will have a slope of math variable. Therefore, the equation of the tangent line can be expressed as Then, the math variable-intercept can simpy be obtained by finding an math variable value that which makes math variable. Let math variable denote that point. Then, we arrive at the following update rule. Since we will be using math variable as the value for the next iteration, math variable, and now we have the update rule as delineated in (4). Below is an implementation of the Newton-Raphson method in Python. I’ve added some parameters to the function for functionality and customization. is simply some small value we use to decide when to stop the update; if the change in the value of the root is so small that it is not worth the extra compute, we should stop. determines how many iterations we want to continue. If the algorithm is unable to find the root within iterations, it likely means that the function provided does not have a root, or at the very least, the root is not discoverable via the algorithm. Lastly, is a flag that determines whether we return the full update history or simply the last value in the iteration as a single value. One peculiarity that deserves attention is the exception, which occurs in this case if the number of arguments passed into the function does not match. I added this block to take into account the fact that the method and other approximate derivative calculation methods such as have differing numbers of parameters. Let’s see if this actually works by using the example we’ve been reusing thus far, math variable, or and , both of which we have already defined and initialized above. The root seems to be around 2.7. And indeed, if we cube it, we end up with a value extremely close to 20. In other words, we have successfully found the root to math variable. Instead of the direct derivative, , we can also use approximation methods. In the example below, we show that using results in a very similar value (in fact, it is identical in this case, but we need to take into other factors such as numerical stability and overflow which might happen with such high-precision numbers). This result aligns with the earlier observation that center divided difference provides very accurate approximations of the first derivative. Note that the advantage of using is that we can now apply Newton-Raphson to non-polynomial equations that cannot be formulated in list index representation format. For instance, let’s try something like . To verify that this is indeed correct, we can plug back into . Also, given that math variable, we can use some heuristics to realize that the answer must be only ever so slightly larger than 1. Notice that the result is extremely close to zero, suggesting that we have found the correct root. Now that we have seen the robustness of the Newton-Raphson method, let’s take a look at another similar numerical method that uses backward divided difference for derivative approximation. In this section, we will look at the secant method, which is another method for identifying the roots of non-linear equations. Before we get into a description of how this method works, here’s a quick graphic, again from Wikipedia, on how the secant method works. As the name implies, the secant function works by drawing secant lines that cross the function at each iteration. Then, much like the Newton-Raphson method, we find the math variable-intercept of that secant line, find a new point on the graph whose math variable-coordinate corresponds to that intercept, and use the point from the previous iteration to find a new secant line. This process is very intuitively outlined in this video by numericalmethodsguy. The update rule for the secant method can be expressed as We can derive (7) simply by slightly modifying the update rule we saw for Newton-Raphson. Recall that the Newton-Raphson update rule was written as The only modification we need to make to this update rule is to replace math variable with an approximation using the backward divided difference formula. Here, we make a slight modification to (2), specifically by using values from previous iterations. If we plug (8) back into (4), with some algebraic simplifications, we land on (7), the update rule for the secant method. This is left as an exercise for the reader. Now let’s take a look at how we might be able to implement this numerical method in code. Presented below is the method, which follows the same general structure as the function we looked at earlier. The only part that differs is the part where we use backward divided difference, which requires that we look up not only the immediately previous root approximation, but the value prior to that as well. In other words, we need both math variable and math variable to calculate math variable via an iterative update. And here is an obligatory sanity check using our previous example. 2.7 is a familiar value, and indeed it is what was returned by the Newton-Raphson method as well. We confirm that this is indeed the root of the equation. Now that we have looked at both methods, it’s time to make a quick comparison. We will be comparing three different methods: By setting to be , we can obtain a full list of root updates as the three methods begin their quest for the root of the function. We can then see which method converges the quickest. Let’s see how this little experiment turns out. We first begin by importing some dependencies to plot the history of values. Then, we obtain the history for each of the three approaches and plot them as a scatter plot. The result is shown below. You might have to squint your eye to see that (Netwon-Raphson with direct derivatives) and (Newton-Raphson with center divided difference) almost coincide exactly at the same points. I was honestly somewhat surprised by the result: although we had verified the advantage of using center divided difference over forward or backward divided difference via some simple big-O analysis with trailing error terms, I did not expect the two to coincide with such exactitude. Another interesting observation is that the secant method seems to take slightly longer than the Newton-Raphson method. This is probably due to the fact that the secant method uses backward divided difference, and also the fact that it requires two previous at each iteration instead of one. The reason why the first update seems rather ineffective is that the two initial guesses that we fed into the model was probably not such a good starting point. The topic of today’s post was somewhat different from what we had previously dealt with in this blog, but it was an interesting topic for me nonetheless. I had encountered the Newton-Raphson method previously when going down my typical Wikipedia rabbit holes, but it is only today that I feel like I’ve finally got a grasp of the concept. I consider this post to be a start of many more posts on numerical methods to come. I hope you’ve enjoyed reading this post. See you in the next one.",0,0,0,0,0,0,1,0
A Step Up with  Variational Autoencoders,"In a previous post, we took a look at autoencoders, a type of neural network that receives some data as input, encodes them into a latent representation, and decodes this information to restore the original input. Autoencoders are exciting in and of themselves, but things can get a lot more interesting if we apply a bit of twist. In this post, we will take a look at one of the many flavors of the autoencoder model, known as variational autoencoders, or VAE for short. Specifically, the model that we will build in this tutorial is a convolutional variational Autoencoder, since we will be using convolutional layers for better image processing. The model architecture introduced in this tutorial was heavily inspired by the one outlined in François Chollet’s Deep Learning with Python, as well as that from a separate article on the Keras blog. Let’s start by importing the modules necessary for this demonstration. The objective of today’s task is to build an autoencoder model that produces MNIST hand-written digits. The hidden dimension, or the latent space of the model, is going to a random vector living in two-dimensional space. Let’s specify this setup, along with some other miscellaneous configurations, before we proceed with constructing the model architecture. It’s time to build our model… or not quite now. Before we start stacking layers for the encoder and the decoder, we need to define a sampling function that will perform the meat of the variational inference involved in VAE. Let’s start out by taking a look at the sampling function we will use to define one of the layers of the variational Autoencoder network. Simply put, the above below takes as arguments and in the form of a bundled list. As you can guess from the name of the variables, these two parameters refer to the mean and log variance of the random vector living in our predefined latent space. Note that we are assuming a diagonal Gaussian here: in other words, the covariance matrix of the multi-dimensional Gaussian is assumed to be diagonal, meaning that each elements of the vector are independent. If any of this sounds foreign to you, I recommend that you read this post on the Gaussian distribution. Let’s continue our discussion with the sampling function. The goal here is to sample a random vector in the latent space from the distribution specified by the two parameters, mean and log variance. The sampling process can be expressed as follows: where math variable denotes the mean, corresponding to , math variable denotes a tensor of random numbers sampled from the standard normal distribution, and math variable denotes the standard deviation (we will see how this is related to in just a moment). Essentially, the goal here is to use a resampling technique such that we can sample from a standard normal distribution centered around mean 0 and a standard deviation of 1, but consequentially sample from a distribution of math variable living in the latent space. If you are wondering how (1) translates to the return statement, then the following equation might resolve your curiosity. This is the promised elaboration on the relationship between log variance and standard deviation: Therefore, multiplying 0.5 is just a simple algebraic manipulation to morph log variance to standard deviation. The reason why we use log variance instead of just variance or standard deviation is to ensure numerical stability in computation. Now that this part has been cleared, let’s start stacking away layers! Just like the autoencoder, VAEs are composed of two discrete components: the encoder and the decoder. Here, we take a look at the first piece of the puzzle, the encoder network. There are several things to note about this model. First, I decided to use a loop to simplify the process of stacking layers. Instead of repeating the same code over multiple lines, I found this approach to be more succinct and concise. Second, we define a custom layer at the end, shown as , that uses the function we defined earlier. This is the final key that enables us to build an encoder model that receives as input a 28-by-28 image, then output a two-dimensional latent vector representation of that image to pass onto the decoder network. Below is the summary of what our model looks like. Note that the model outputs a total of three quantities: , , and . We need the first two parameters to later sample from the latent distribution; , of course, is needed to train the decoder. The decoder network looks similar to the the encoder, except that much of the architecture is in reverse order. Most notably, we use to undo the convolution done by the encoder. This allows us to effectively scale up the input back to its original dimension, which is what we want to do with a generative model like a VAE. One subtly worth mentioning is the fact that we use a sigmoid activation in the end. This is because we want the pixel values of the output to be between 0 and 1, just as the original input was normalized before it was fed into the encoder network via division by 255. The summary of the decoder network is presented below: Now that we have both the encoder and the decode network fully defined, it’s time to wrap them together into one autoencoder model. This can simply achieved by defining the input as the input of the encoder—the normalized MNIST images—and defining the output as the output of the decoder when fed a latent vector. Concretely, this process might look as follows: Let’s look a the summary of the CVAE. Note that the encoder and the decoder look like individual layers in the grand scheme of the VAE architecture. We have almost everything we need, but there is one crucial step that is missing: compiling the model with an optimizer and a loss function. Normally, defining a loss function is very easy: in most cases, we use pre-made loss functions that are available through the TensorFlow API, such as cross entropy or mean squared error. In the case of variational autoencoders, however, this is not such an easy task: how do we judge the robustness or the effectiveness of the decoder, which is essentially a generative algorithm? Of course, we could stop training once the figures it generates becomes reasonable, i.e. the mock MNIST digits it creates looks compelling to the human eye. However, this is a subjective metric at best, and we can’t expect there to be a ML engineer peering at the screen, looking at the outputs of the decoder per each epoch. To tackle this challenge, we need to dive into some math. Let’s take a look. First, let’s carefully review what our goal is for this task. The motivating idea behind variational autoencoders is that we want to model a specific distribution, namely the distribution of the latent space given some input. As you recall, this latent space is a two dimensional vector modeled as a multivariate diagonal Gaussian. Using Bayes’ theorem, we can express this distribution as follows: By now, it is pretty clear what the problem its: the evidence sitting in the denominator is intractable. Therefore, we cannot directly calculate or derive math variable in its closed form; hence the need for variational inference. The best we can do is to find a distribution math variable that best approximates math variable. How do we find this distribution? Well, we know one handy concept that measures the difference or the pseudo-distance between two distributions, and that is Kullback-Leibler divergence. As we discussed in this post on entropy, KL divergence tells us how different two distributions are. So the goal here would be find a distribution that minimizes the following expression: Using the definition of conditional probability, we can simplify (4) as follows: The trick is to notice that math variable is a constant that can break out of the expectation calculation. Let’s continue by deriving an expression for the evidence term. A useful property to know about KL divergence is the fact that it is always non-negative. We will get into why this is the case in a moment. For now, let’s assume non-negativity to be true and transform (6) into an inequality: The term on the right of the inequality is known as the Evidence Lower Bound, or ELBO for short. Why are we interested in ELBO? First, note that math variable, the evidence, is a constant. Therefore, minimizing KL divergence amounts to maximizing ELBO. This is the key to variational inference: instead of calculating the intractable integral in (3), we can find a distribution math variable that which minimizes KL divergence by maximizing ELBO, which is a tractable operation. Let’s prove why KL divergence is always greater or equal to zero, which is a condition we assumed to be true in the derivation of ELBO above. For the sake of completeness, I present two ways of proving the same property. In the context of probability, Jensen’s inequality can be summarized as follows. Given a convex function math variable, We won’t get into rigorous proofs here, but it’s not difficult to see why this inequality stands with some basic geometric intuition. Due to its bow-like shape, the expected value of a convex function evaluated across a given interval will always be greater or equal to the function evaluated at the expected value of the random variable. How is Jensen’s inequality related to the non-negativity of KL divergence? Let’s return back to the definition of KL divergence. For simplicity and to reduce notational burden, we briefly depart from conditional probabilities math variable and return back to generic distributions math variable and math variable. Notice that the definition of KL divergence itself is an expected value expression. Also, note that math variable is a convex function—math variable itself is concave, but the negative sign flips the concavity the other way. With these observations in mind, we can apply Jensen’s inequality to derive the following: Therefore, we have shown that KL divergence is always greater or equal to zero, which was our end goal. There is another version of a proof that I found a lot more intuitive and easier to follow than the previous approach. This derivation was borrowed from this post. We start from the simple observation that a logarithmic function is always smaller than a linear one. In other words, This is no rocket science, and one can easily verify (11) by simply plotting the two functions on a Cartesian plane. Using (11), we can proceed in a different direction from the definition of KL divergence. Once again, we have shown that KL divergence is positive! Proving this isn’t really necessary in the grand scheme of exploring the mathematics behind VAEs, yet I thought it would help to have this adjunctive section to better understand KL divergence and familiarize ourselves with some standard algebraic manipulations that are frequently invoked in many derivations. Let’s jump back into variational inference and defining the cost function with ELBO. Recall from the setup of our Variational Autoencoder model that we have defined the latent vector as living in two-dimensional space following a multivariate Gaussian distribution. It’s time to apply the ELBO equation to this specific context and derive a closed-form expression of our loss function. Let’s recall the formula for ELBO: After some rearranging, we can decompose ELBO into two terms, one of which is a KL divergence: Now, it’s finally time for us to dive deep into math: let’s unpack the closed form expression in (13). Note that the ELBO expression applies to just about any distribution, but since we chose a multivariate Gaussian to be the base distribution, we will see how it unfolds specifically in this context. Let’s begin by assuming the distribution of our models to be Gaussian. Namely, Because math variable is an approximation of math variable, we naturally assume the same model for the approximate distribution: Now we can derive an expression for the negative KL divergence sitting in the ELBO expression: This may seem like a lot, but it’s really just plugging in the distributions into the definition of KL divergence as an expectation and using some convenient properties of logarithms to perform simple algebraic simplifications. To proceed further, observe that the first term is a constant that can escape out of the expectation: From the definition of variance and expectation, we know that Therefore, we can simplify (17) as follows: Let’s zoom in on the expected value term in (19). Our goal is to use (18) again so that we can flesh out another one half from that term. This can be achieved through some clever algebraic manipulation: But since the the expected value of math variable is constant and that of math variable is zero, We can now plug this simplified expression back into the calculation of KL divergence, in (19): Since we will standardize our input such that math variable and math variable, we can plug these quantities into (22) and show that We are almost done with deriving the expression for ELBO. I say almost, because we still have not dealt with the trailing term in (13): At this point, it is extremely useful to recall the definition of cross entropy, which is generically defined as follows: Therefore, we see that the trailing term in (13) is just a cross entropy between two distributions! This was a circumlocutions journey, but that is enough math we will need for this tutorial. It’s time to get back to coding. All that math was for this simple code snippet shown below: As you can see, this short code snippet shows, in essence, how we can define a compile a model with a custom loss function. In this case, refers to the reconstruction loss, which is the cross entropy term we saw earlier. , as you might expect, simply refers to KL divergence. Notice how there is a multiplying factor in the expression, just like we did when we derived it in the section above. With some keen observations and comparisons, you will easily see that the code is merely a transcription of (13), with some minor differences given dimensionality. One important fact to note is that the gradient descent algorithm, by default, seeks to minimize the loss function. However, we discussed above how the objective of VAE is to maximize ELBO. Therefore, we modify ELBO into a loss function that is to be minimized by defining the loss function as the negative of ELBO. In other words, the cost function math variable is defined as math variable; hence the difference in sign. It’s finally time to test the model. Let’s first begin with data preparation and preprocessing. Now, we should have the training and test set ready to be fed into our network. Next, let’s define a simple callback application using the monitor so that training can be stopped when no substantial improvements are being made to our model. This was included because training a VAE can take some time, and we don’t want to waste computing resources seeing only submarginal increments to the model performance. Training begins! After 14 epochs, training has stopped, meaning that no meaningful improvements were being made. Let’s visualize the representation of the latent space learned by the VAE. Visualizing this representation is easy in this case because we defined the latent space to be two-dimensional; in other words, all points can be plotted on a Cartesian plane. Let’s take a look: This plot shows us how each numbers are distributed across the latent space. Notice that numbers that belong to the same class seem to be generally clustered around each other, although there is a messy region in the middle. This is a reasonable result: while we would expect ones to be fairly easy to distinguish from, say, eights, numbers like zeros and sixes might look very similar, and hence appear mixed as a lump in the fuzzy region in the middle. One cool thing about VAEs is that we can use their learned representation to see how numbers slowly morph and vary across a specified domain. This is why VAEs are considered to be generative models: if we feed the VAE some two-dimensional vector living in the latent space, it will spit out a digit. Whether or not that digit appears convincing depends on the random vector the decoder was provided as input: if the vector is close to the learned mean, math variable, then the result will be convincing; if not, we might see a confusing blob of black and white. Let’s see what exactly is going on in the fuzzy region of the image, because that is apparently where all the digits mingle together and seem indistinguishable from one another. Put differently, if we vary the random vector little by little across that region, we will be able to see how the digit slowly morphs into another number. How cool is that? We were able to get a VAE to show us how one digit can shift across a certain domain of the latent space. This is one of the many cool things we can do with a generative model like a variational autoencoder. In this post, we took a deep dive into the math behind variational autoencoders. It was a long journey, but definitely worth it because it exposed us to many core concepts in deep learning and statistics. At the same time, I found it fascinating to see how a model could learn from a representation to generate numbers, as we saw in the very last figure. In a future post, we will look at generative adversarial networks, or GANs, which might be considered as the pinnacle of generative models and a successor to autoencoders. GANs resemble autoencoders in that it is also composed of two models. One core difference, however, is that in GANs, the two models are in a competing relationship, whereas in autoencoders, the encoder and the decoder play distinct, complementary roles. If any of this sounds exciting, make sure to check out the next post. I hope you enjoyed reading. Catch you up in the next one!",1,0,0,0,0,0,0,0
Fisher Score and Information,"Fisher’s information is an interesting concept that connects many of the dots that we have explored so far: maximum likelihood estimation, gradient, Jacobian, and the Hessian, to name just a few. When I first came across Fisher’s matrix a few months ago, I lacked the mathematical foundation to fully comprehend what it was. I’m still far from reaching that level of knowledge, but I thought I’d take a jab at it nonetheless. After all, I realized that sitting down to write a blog post about some concept forces me to study more, so it is a positive, self-reinforcing cycle. Let’s begin. Fisher’s score function is deeply related to maximum likelihood estimation. In fact, it’s something that we already know–we just haven’t defined it explicitly as Fisher’s score before. First, we begin with the definition of the likelihood function. Assume some dataset math variable where each observation is identically and independently distributed according to a true underlying distribution parametrized by math variable. Given this probability density function math variable, we can write the likelihood function as follows: While it is sometimes the convention that the likelihood function be denoted as math variable, we opt for an alternative notation to reserve math variable for the loss function. To continue, we know that the maximum likelihood estimate of the distribution’s parameter is given by This is the standard drill we already know. The next step, as we all know, is to take the derivative of the term in the argument maxima, set it equal to zero, and voila! We have found the maximum likelihood estimate of the parameter. A quick aside that may become later is the fact that maximizing the likelihood amounts to minimizing the loss function. Now here comes the definition of Fisher’s score function, which really is nothing more than what we’ve done above: it’s just the gradient of the log likelihood function. In other words, we have already been implicitly using Fisher’s score to find the maximum of the likelihood function all along, just without explicitly using the term. Fisher’s score is simply the gradient or the derivative of the log likelihood function, which means that setting the score equal to zero gives us the maximum likelihood estimate of the parameter. An important characteristic to note about Fisher’s score is the fact that the score evaluated the true value of the parameter equals zero. Concretely, this means that given a true parameter math variable, This might seem deceptively obvious: after all, the whole point of Fisher’s score and maximum likelihood estimation is to find a parameter value that would set the gradient equal to zero. This is exactly what I had thought, but there are subtle intricacies taking place here that deserves our attention. So let’s hash out exactly why the expectation of the score with respect to the true underlying distribution is zero. To begin, let’s write out the full expression of the expectation in integral form. If we evaluate this integral at the true parameter, i.e. when math variable, The key part of this derivation is the use of the Leibniz rule, or sometimes known as Feynman’s technique or differentiation under the integral sign. I am most definitely going to write a post detailing in intuitive explanation behind why this operation makes sense in the future, but to prevent unnecessary divergence, for now it suffices to use that rule to show that the expected value of Fisher’s score is zero at the true parameter. Things start to get a little more interesting (and more complicated) as we move onto the discussion of Fisher’s Information Matrix. There are two sides of the coin that we will consider in this discussion: Fisher’s information as understood as the covariance matrix of the score function, and Fisher’s information as understood as a Hessian of the negative log likelihood. The gist of it is that there are two different ways of understanding the same concept, and that they provide intriguing complementary views on the information matrix. Before jumping into anything else, perhaps it’s instructive to review variance, covariance, and the covariance matrix. Here is a little cheat sheet to help you out (and my future self, who will most likely be reviewing this later as well). An intuitive way to think about variance is to consider it as a measure of how far samples are from the mean. We square that quantity to prevent negative values from canceling out positive ones. Covariance is just an extension of this concept applied to a comparison of two random variables instead of one. Here, we consider how two variables move in tandem. And the variance-covariance matrix is simply a matrix that contains information on the covariance of multiple random variables in a neat, compact matrix form. A closed-form expression for the covariance matrix math variable given a random vector math variable, which follows immediately from aforementioned definitions and some linear algebra, looks as follows: Enough of the prologue and review, now we’re ready to start talking about Fisher. The information matrix is defined as the covariance matrix of the score function as a random vector. Concretely, Note that the 0’s follow straight from the earlier observation that math variable. Intuitively, Fisher’s information gives us an estimate of how certain we are about the estimate of the parameter math variable. This can be seen by recognizing the apparent similarity between the definition of the covariance matrix we have defined above and the definition of Fisher’s information. In fact, the variance of the parameter math variable is explained by the inverse of Fisher’s information matrix, and this concept is known as the Cramer-Rao Lower Bound. For the purposes of this post, I won’t get deep into what CRLB is, but there are interesting connections we can make between Fisher’s information, CRLB, and the likelihood, which we will get into later. Because Fisher’s information requires computing the expectation given some probability distribution, it is often intractable. Therefore, given some dataset, often times we use the empirical Fisher as a drop-in substitute for Fisher’s information. The empirical Fisher is defined quite simply as follows: In other words, it is simply an unweighted average of the covariance of the score function for each observed data point. Although this is a subtlety, it helps to clarify nonetheless. Something that may not be immediately apparent yet nonetheless true and very important about Fisher’s information is the fact that it is the negative expected value of the second derivative of the log likelihood. In our multivariate context where math variable is a vector, the second derivative is effectively the Hessian. In other words, You might be wondering how the information matrix can be defined in two says, the covariance and the Hessian. Indeed, this threw me off quite a bit as well, and I struggled to find and understand a good resource that explained why this was the case. Thankfully, Mark Reid’s blog and an MIT lecture contained some very helpful pointers that got me a long way. The derivation is not the easiest, but I’ll try to provide a concise version based on my admittedly limited understanding of this topic. Let’s start from some trivially obvious statements. First, from the definition of a PDF and the derivative operation, we know that Therefore, both the first and second derivative of this function are going to be zero. In multivariate speak, both the gradient and the Hessian are zero vectors and matrices, respectively. Using the Leibniz rule we saw earlier, we can interchange the derivative and come up with the following expressions. Granted, these expressions somewhat muffle the shape of the quantity we are dealing with, namely vectors and matrices, but it is concise and intuitive enough for our purposes. With these statements in mind, let’s now begin the derivation by first taking a look at the Hessian of the score function. From the chain rule, we know that This does not look good at all. However, let’s not fall into despair, since our goal is not to calculate the second derivative or the Hessian itself, but rather its negative expected value. In calculating the expected value, we will be using integrals, which is where the seemingly trivial statements we established earlier come in handy. By linearity of expectation, we can split this expectation up into two pieces. Let’s use integrals to express the first expectation. The good news is that now we see terms canceling out each other. Moreover, from the Leibniz rule and the interchanging of the integral and the derivative, we have shown that the integral in fact evaluates to zero. This ultimately leaves us with Therefore we have established that And we’re done! In this post, we took a look at Fisher’s score and the information matrix. There are a lot of concepts that we can build on from here, such as Cramer Rao’s Lower Bound or natural gradient descent, both of which are interesting concepts at the intersection of machine learning and statistics. Although the derivation is by no means mathematically robust, it nonetheless vindicates a notion that is not necessary apparently obvious, yet makes a lot of intuitive sense in hindsight. I personally found this video by Ben Lambert to be particularly helpful in understanding the connection between likelihood and information. The gist of it is simple: if we consider the Hessian or the second derivative to be indicative of the curvature of the likelihood function, the variance of our estimate of the optimal parameter math variable would be larger if the curvature was smaller, and vice versa. In a sense, the larger the value of the information matrix, the more certain we are about the estimate, and thus the more information we know about the parameter. I hope you enjoyed reading this post. Catch you up on another post, most likely on the Leibniz rule, then natural gradient descent!",0,0,0,0,1,0,0,0
Understanding PageRank,"Google is the most popular search engine in the world. It is so popular that the word “Google” has been added to the Oxford English Dictionary as a proper verb, denoting the act of searching on Google. While Google’s success as an Internet search engine might be attributed to a plethora of factors, the company’s famous PageRank algorithm is undoubtedly a contributing factor behind the stage. The PageRank algorithm is a method by which Google ranks different pages on the world wide web, displaying the most relevant and important pages on the top of the search result when a user inputs an entry. Simply put, PageRank determines which websites are most likely to contain the information the user is looking for and returns the most optimal search result. While the nuts and bolts of this algorithm may appear complicated—and indeed they are—the underlying concept is surprisingly intuitive: the relevance or importance of a page is determined by the number of hyperlinks going to and from the website. Let’s hash out this proposition by creating a miniature version of the Internet. In our microcosm, there are only five websites, represented as nodes on a network graph. Below is a simple representation created using Python and the Networkx package. Running this block results in the following graph:",0,1,0,0,0,0,0,0
"Linear Regression, in Two Ways","If there is one thing I recall most succinctly from my high school chemistry class, it is how to use Excel to draw basic plots. In the eyes of a naive freshman, visualizations seemed to add an air of professionalism. So I would always include a graph of some sort in my lab report, even when I knew they were superfluous. The final icing on the cake? A fancy regression with some r-squared. In today’s post, I want to revisit what used to be my favorite tinkering toy in Excel: regression. More specifically, we’ll take a look at linear regression, which deals with straight lines and planes instead of curved surfaces. Although it sounds simple, the linear regression model is still widely used because it not only provides a clearer picture of obtained data, but can also be used to make predictions based on previous observations. Linear regression is also incredibly simple to implement using existing libraries in programming languages such as Python, as we will later see in today’s post. That was a long prologue—–let’s jump right in. In this section, we will attempt to frame regression in linear algebra terms and use basic matrix operations to derive an equation for the line of best fit. In this section, we will use linear algebra to understand regression. An important theme in linear algebra is orthogonality. How do we determine if two vectors—or more generally, two subspaces—are orthogonal to each other? How do we make two non-orthogonal vectors orthogonal? (Hence Gram-Schmidt.) In our case, we love orthogonality because they are key to deriving the equation for the line of best fit through projection. To see what this means, let’s quickly assume a toy example to work with: assume we have three points, math expression and math expression, as shown below. As we can see, the three points do not form a single line. Therefore, it’s time for some regression. Let’s assume that this line is defined by math expression. The system of equations which we will attempt to solve looks as follows: Or if you prefer the vector-matrix representation as I do, This system, remind ourselves, does not have a solution because we have geometrically observed that no straight line can pass through all three points. What we can do, however, is find a projection of the vector math expression onto matrix math expression so that we can identify a solution that is closest to math expression, which we shall denote as math expression. As you can see, this is where all the linear algebra kicks in. Let’s start by thinking about math expression, the projection of math expression onto math expression. After some thinking, we can convince ourselves that math expression is the component of math expression that lives within the column space of math expression, and that math expression is the error component of math expression that lives outside the column space of math expression. From this, it follows that math expression is orthogonal to math expression, since any non-orthogonal component would have been factored into math expression. Concretely, since the transpose is an alternate representation of the dot product. We can further specify this equation by using the fact that math expression can be expressed as a linear combination of the columns of math expression. In other words, where math expression is the solution to the system of equations represented by math expression. Let’s further unpackage (1) using matrix multiplication. Therefore, We finally have a formula for math expression: Let’s remind ourselves of what math expression is and where we were trying to get at with projection in the context of regression. We started off by plotting three data points, which we observed did not form a straight line. Therefore, we set out to identify the line of best fit by expressing the system of equations in matrix form, math expression, where math expression. But because this system does not have a solution, we ended up modifying the problem to math expression, since this is as close as we can get to solving an otherwise unsolvable system. So that’s where we are with equation (2): a formula for math expression, which contains the parameters that define our line of best fit. Linear regression is now complete. It’s time to put our equation to the test by applying it to our toy data set. Let’s apply (2) in the context of our toy example with three data points to perform a quick sanity check. Calculating the inverse of math expression is going to be a slight challenge, but this process is going to be a simple plug-and-play for the most part. First, let’s remind ourselves of what math expression and math expression are: Let’s begin our calculation: Calculating the inverse, Now, we can put this all together. The final result tells us that the line of best fit, given our data, is Let’s plot this line alongside our toy data to see how the equation fits into the picture. It’s not difficult to see that linear regression was performed pretty well as expected. However, ascertaining the accuracy of a mathematical model with just a quick glance of an eye should be avoided. This point then begs the question: how can we be sure that our calculated line is indeed the best line that minimizes error? To that question, matrix calculus holds the key. We all remember calculus from school. We’re not going to talk much about calculus in this post, but it is definitely worth mentioning that one of the main applications of calculus lies in optimization: how can we minimize or maximize some function, optionally with some constraint? This particular instance of application is particularly pertinent and important in our case, because, if we think about it, the linear regression problem can also be solved with calculus. The intuition behind this approach is simple: if we can derive a formula that expresses the error between actual values of math expression and those predicted by regression, denoted as math expression above, we can use calculus to derive that expression and ultimately locate the global minimum. And that’s exactly what we’re going to do. But before we jump into it, let’s briefly go over some basics of matrix calculus, which is the variant of calculus we will be using throughout. Much like we can derive a function by a variable, say math expression or math expression, loosely speaking, we can derive a function by a matrix. More strictly speaking, this so-called derivative of a matrix is more formally known as the gradient. The reason why we introduced the gradient as a derivative by a matrix is that, in many ways, the gradient in matrix calculus resembles a lot of what we saw with derivatives in single variable calculus. For the most part, this intuition is constructive and helpful, and the few caveats where this intuition breaks down are beyond the purposes of this post. For now, let’s stick to that intuition as we venture into the topic of gradient. As we always like to do, let’s throw out the equation first to see what we’re getting into before anything else. We can represent the gradient of function math expression with respect to matrix math expression is a matrix of partial derivatives, defined as While this formula might seem complicated, in reality, it is just a convenient way of packaging partial derivatives of the function into a compact matrix. Let’s try to understand what this operation entails through a simple dummy example. As you can see, instead of a m-by-n matrix, we have a column vector math expression as an ingredient for a function. But don’t worry: the formula in (3) works for vectors as well, since vectors can be considered as matrices with only a single column. With that in mind, let’s define our function math expression as follows: Great! We see that the math expression is a scalar function that returns some value constructed using the entries of math expression. Equation (3) tells us that the gradient of math expression, then, is simply a matrix of partial derivatives whose dimension equals that of math expression. Concretely, In other words, Notice that this is the single variable calculus equivalent of saying that math expression. This analogue can be extended to other statements in matrix calculus. For instance, where math expression is a symmetric matrix. We can easily verify this statement by performing the calculation ourselves. For simplicity’s sake, let’s say that math expression is a two-by-two matrix, although it could theoretically be any math expression-by-math expression matrix where math expression is some positive integer. Note that we are dealing with square matrices since we casted a condition on math expression that it be symmetrical. Let’s first define math expression and math expression as follows: Then, We can now compute the gradient of this function according to (3): We have not provided an inductive proof as to how the same would apply to math expression-by-math expression matrices, but it should now be fairly clear that math expression, which is the single-variable calculus analogue of saying that math expression. In short, math expression math expression With these propositions in mind, we are now ready to jump back into the linear regression problem. At this point, it is perhaps necessary to remind ourselves of why we went down the matrix calculus route in the first place. The intuition behind this approach was that we can construct an expression for the total error given by the regression line, then derive that expression to find the values of the parameters that minimize the error function. Simply put, we will attempt to frame linear regression as a simple optimization problem. Let’s recall the problem setup from the linear algebra section above. The problem, as we framed it in linear algebra terms, went as follows: given some unsolvable system of equations math expression, find the closest approximations of math expression and math expression, each denoted as math expression and math expression respectively, such that the system is now solvable. We will start from this identical setup with the same notation, but approach it slightly differently by using matrix calculus. The first agenda on the table is constructing an error function. The most common metric for error analysis is mean squared error, or MSE for short. MSE computes the magnitude of error as the squared distance between the actual value of data and that predicted by the regression line. We square the error simply to prevent positive and negative errors from canceling each other out. In the context of our regression problem, where math expression denotes the error function. We can further break this expression down by taking note of the fact that the norm of a vector can be expressed as a product of the vector and its transpose, and that math expression as established in the previous section of this post. Putting these together, Using distribution, we can simplify the above expression as follows: It’s time to take the gradient of the error function, the matrix calculus analogue of taking the derivative. Now is precisely the time when the propositions (4) and (5) we explored earlier will come in handy. In fact, observe that first term in (6) corresponds to case (5); the second term, case (4). The last term can be ignored because it is a scalar term composed of math expression, which means that it will not impact the calculation of the gradient, much like how constants are eliminated during derivation in single-variable calculus. Now, all we have to do is to set the expression above to zero, just like we would do in single variable calculus with some optimization problem. There might be those of you wondering how we can be certain that setting this expression to zero would yield the minimum instead of the maximum. Answering this question requires a bit more math beyond what we have covered here, but to provide a short preview, it turns out that our error function, defined as math expression is a positive definite matrix, which guarantees that the critical point we find by calculating the gradient gives us a minimum instead of a maximum. This statement might sometimes be phrased differently along the lines of convexity, but this topic is better tabled for a separate future post. The key point here is that setting the gradient to zero would tell us when the error is minimized. This is equivalent to Therefore, Now we are done! Just like in the previous section, math expression gives us the parameters for our line of best fit, which is the solution to the linear regression problem. In fact, the keen reader might have already noted that (7) is letter-by-letter identical to formula (2) we derived in the previous section using plain old linear algebra! One the one hand, it just seems surprising and fascinating to see how we end up in the same place despite having taken two disparate approaches to the linear regression problem. But on the other hand, this is what we should have expected all along: no matter what method we use, the underlying thought process behind both modes of approach remain the same. Whether it be through projection or through derivation, we sought to find some parameters, closest to the values we are approximating as much as possible, that would turn an otherwise degenerate system into one that is solvable. Linear regression is a simple model, but I hope this post have done it justice by demonstrating the wealth of mathematical insight that can be gleaned from its derivation.",0,1,0,0,0,0,0,0
Word2vec from Scratch,"In a previous post, we discussed how we can use tf-idf vectorization to encode documents into vectors. While probing more into this topic and geting a taste of what NLP is like, I decided to take a jab at another closely related, classic topic in NLP: word2vec. word2vec is a technique introduced by Google engineers in 2013, popularized by statements such as “king - man + woman = queen.” The gist of it, as you may know, is that we can express words as vectors that encode their semantics in a meaningful way. When I was just getting starting to learn TensorFlow, I came across the embedding layer, which performed exactly this operation: transforming words into vectors. While I thought this process was extremely interesting, I didn’t know about the internals of this structure until today, particularly after reading this wonderful tutorial by Chris McCornick. In this post, we will be implementing word2vec, a popular embedding technique, from scratch with NumPy. Let’s get started! Instead of going over the concepts and implementations separately, let’s jump straight into the whole implementation process and elaborate on what is necessary along the way. In order to create word embeddings, we need some sort of data. Here is a text on machine learning from Wikipedia. I’ve removed some parentheses and citation brackets to make things slightly easier. Since we can’t feed raw string texts into our model, we will need to preprocess this text. The first step, as is the approach taken in many NLP tasks, is to tokenize the text, i.e. splitting the text up into smaller units like words, getting rid of punctuations, and so on. Here is a function that does this trick using regular expressions. Let’s create tokens using the Wikipedia excerpt shown above. The returned object will be a list containing all the tokens in . Another useful operation is to create a map between tokens and indices, and vice versa. In a sense, we are creating a lookup table that allows us to easily convert from words to indices, and indices to words. This will be particularly useful later on when we perform operations such as one-hot encoding. Let’s check if the word-to-index and index-to-word maps have successfully been created. As we can see, the lookup table is a dictionary object containing the relationship between words and ids. Note that each entry in this lookup table is a token created using the function we defined earlier. Now that we have tokenized the text and created lookup tables, we can now proceed to generating the actual training data, which are going to take the form of matrices. Since tokens are still in the form of strings, we need to encode them numerically using one-hot vectorization. We also need to generate a bundle of input and target values, as this is a supervised learning technique. This then begs the question of what the input and target values are going to look like. What is the value that we are trying to approximate, and what sort of input will we be feeding into the model to generate predictions? The answer to these questions and how they tie into word2vec is at the heart of understanding word embeddings—as you may be able to tell, word2vec is not some sort of blackbox magic, but a result of careful training with input and output values, just like any other machine learning task. So here comes the crux of word2vec: we loop through each word (or token) in the sentence. In each loop, we look at words to the left and right of the input word, as shown below. This illustration was taken from this article by Ramzi Karam. In the particular example as shown above, we would generate the following input and prediction pairs part of the training data. Note that the window size is two, which is why we look up to two words to the left and right of the input word. So in a way, we can understand this as forcing the model to understand a rough sense of context—the ability to see which words tend to stick together. In our own example, for instance, we would see a lot of , meaning that the model should be able to capture the close contextual affinity between these two words. Below is the code that generates training data using the algorithm described above. We basically iterate over the tokenized data and generate pairs. One technicality here is that, for the first and last few tokens, it may not be possible to obtain words to the left or right of that input token. In those cases, we simply don’t consider these word pairs and look at only what is feasible without causing s. Also note that we create and separately instead of putting them in tuple form as demonstrated above. This is just for convenience with other matrix operations later on in the post. Below is the definition for , an auxiliary function we used above to combine two objects. Also, here is the code we use to one-hot vectorize tokens. This process is necessary in order to represent each token as a vector, which can then be stacked to create the matrices and . Finally, let’s generate some training data with a window size of two. Let’s quickly check the dimensionality of the data to get a sense of what matrices we are working with. This intuition will become important in particular when training and writing equations for backpropagation in the next section. Both and are matrices with 330 rows and 60 columns. Here, 330 is the number of training examples we have. We would expect this number to have been larger had we used a larger window. 60 is the size of our corpus, or the number of unique tokens we have in the original text. Since we have one-hot encoded both the input and output as 60-dimensional sparse vectors, this is expected. Now, we are finally ready to build and train our embedding network. At this point, you might be wondering how it is that training a neural network that predicts some nearby context word given an input token can be used to embed words into vectors. After all, the output of the network is going to be some probability vector that passed through a softmax layer, not an embedding vector. This is entirely correct, and this is a question that came to my mind as well. However, this is the part that gets the most interesting: the rows of the intermediate weight matrix is the embedding we are looking for! This becomes much more apparent once we consider the dimensions of the weight matrices that compose the model. For simplicity purposes, say we have a total of 5 words in the corpus, and that we want to embed these words as three-dimensional vectors. More specifically, here is the first weight layer of the model: A crucial observation to make is that, because the input is a sparse vector containing one-hot encoded vectors, the weight matrix effectively acts as a lookup table that moves one-hot encoded vectors to dense vectors in a different dimension—more precisely, the row space of the weight matrix. In this particular example, the weight matrix was a transformation of math variable. This is exactly what we want to achieve with embedding: representing words as dense vectors, a step-up from simple one-hot encoding. This process is exactly what embedding is: as we start training this model with the training data generated above, we would expect the row space of this weight matrix to encode meaningful semantic information from the training data. Continuing onwards, here is the second layer that receives as input the embeddings, then uses them to generate a set of outputs. We are almost done. All we now need in the last layer is a softmax layer. When the output is passed into this layer, it is converted into probability vectors whose elements sum up to one. This final output can be considered as context predictions, i.e. which words are likely to be in the window vicinity of the input word. In training—specifically error calculation and backpropagation—we would be comparing this prediction of probability vectors with its true one-hot encoded targets. The error function that we use with softmax is cross entropy, defined as I like to think of this as a dot product of the target vector and the log of the prediction, because that is essentially what the summation is doing. In this alternate formulation, the cross entropy formula can be rewritten as Because math variable a one-hot encoded vector in this case, all the elements in math variable whose entry is zero will have no effect on the final outcome. Indeed, we simply end up taking the negative log of the prediction. Notice that the closer the value of the prediction is to 1, the smaller the cross entropy, and vice versa. This aligns with the behavior we want, since we want the predicted probability to be as close to 1 as possible. So let’s summarize the entire process a little bit. First, embeddings are simply the rows of the first weight matrix, denoted as math variable. Through training and backpropgation, we adjust the weights of math variable, along with the weight matrix in the second layer, denoted as math variable, using cross entropy loss. Overall, our model takes on the following structure: where math variable is the matrix contains the prediction probability vectors. With this in mind, let’s actual start building and train our model. Let’s start implement this model in code. The implementation we took here is extremely similar to the approach we took in this post. For an in-depth review of backpropagation derivation with matrix calculus, I highly recommend that you check out the linked post. The representation we will use for the model is a Python dictionary, whose values are the weight matrices and keys, the name with which we will refer to the weight matrices. In accordance with the nomenclature established earlier, we stick with and to refer to these weights. Let’s specify our model to create ten-dimensional embeddings. In other words, each token will be represented as vectors living in ten-dimensional space. Note that actual models tend to use much higher dimensions, most commonly 300, but for our purposes this is not necessary. Let’s begin with forward propagation. Coding the forward propagation process simply amounts to transcribing the three matrix multiplication equations in (6) into NumPy code. For backpropagation, we will need all the intermediate variables, so we hold them in a dictionary called . However, if we simply want the final prediction vectors only, not the cache, we set to . This is just a little auxiliary feature to make things slightly easier later. We also have to implement the function we used above. Note that this function receives a matrix as input, not a vector, so we will need to slightly tune things up a bit using a simple loop. At this point, we are done with implementing the forward pass. However, before we move on, it’s always a good idea to check the dimensionality of the matrices, as this will provide us with some useful intuition while coding backward propagation later on. The dimensionality of the matrix after passing the first layer, or the embedding layer, is as follows: This is expected, since we want all the 330 tokens in the text to be converted into ten-dimensional vectors. Next, let’s check the dimensionality after passing through the second layer. This time, it is a 330-by-60 matrix. This also makes sense, since we want the output to be sixty dimensional, back to the original dimensions following one-hot encoding. This result can then be passed onto the softmax layer, the result of which will be a bunch probability vectors. Implementing backward propagation is slightly more difficult than forward propagation. However, the good news is that we have already derived the equation for backpropagation given a softmax layer with cross entropy loss in this post, where we built a neural network from scratch. The conclusion of the lengthy derivation was ultimately that given our model Since we know the error, we can now backpropagate it throughout the entire network, recalling basic principles of matrix calculus. If backprop is still confusing to you due to all the tranposes going on, one pro-tip is to think in terms of dimensions. After all, the dimension of the gradient must equal to the dimension of the original matrix. With that in mind, let’s implement the backpropagation function. To keep a log of the value of the error throughout the backpropagation process, I decided to make the final return value of to be the cross entropy loss between the prediction and the target labels. The cross entropy loss function can easily be implemented as follows. Now we’re ready to train and test the model! As we only have a small number of training data—coupled with the fact that the backpropagation algorithm is simple batch gradient descent—let’s just iterate for 50 epochs. While training, we will be caching the value of the cross entropy error function in a list. We can then plot this result to get a better sense of whether the training worked properly. And indeed it seems like we did well! We can thus say with some degree of confidence that the embedding layer has been trained as well. An obvious sanity check we can perform is to see which token our model predicts given the word “learning.” If the model was trained properly, the most likely word should understandably be “machine.” And indeed, when that is the result we get: notice that “machine” is at the top of the list of tokens, sorted by degree of affinity with “learning.” Building and training was fun and all, but our end goal was not to build a neural network; we wanted to get word embeddings. As stated earlier in this post, the key behind word embeddings is that the rows of the first weight matrix is effectively a dense representation of one-hot encoded vectors each corresponding to various tokens in the text dataset. In our example, therefore, the embedding can simply be obtained by But of course, this is not a user-friendly way of displaying the embeddings. In particular, what we want is to be able to input a word through a function and receive as output the embedding vector for that given word. Below is a function that implements this feature. When we test out the word “machine,” we get a dense ten-dimensional vector as expected. And of course, this vector is not a collection of some randomly initialized numbers, but a result of training with context data generated through the sliding window algorithm described above. In other words, these vectors encode meaningful semantic information that tells us which words tend to go along with each other. While this is a relatively simple, basic implementation of word2vec, the underlying principle remains the same nonetheless. The idea is that, we can train a neural network to generate word embeddings in the form of a weight matrix. This is why embedding layers can be trained to generate custom embeddings in popular neural network libraries like TensorFlow or PyTorch. If you end up training word embeddings on large datasets like Wikipedia, you end up with things like word2vec and GloVe, another extremely popular alternative to word2vec. In general, it’s fascinating to think that, with enough data, we can encode enough semantics into these embedding vectors to see relationships such as “king - man + woman = queen.” I hope you’ve enjoyed reading this post. See you in the next one.",1,0,0,1,0,0,0,0
Gamma and Zeta,"Maintaining momentum in writing and self-learning has admittedly been difficult these past few weeks since I’ve started my internship. Normally, I would write one post approximately every four days, but this routine is no longer the norm. To my defense, I’ve been learning a ton about Django and backend operations like querying and routing, and I might write a post about these in the future. But for today, I decided to revisit a topic we’ve previously explored on this blog, partially in the hopes of using nostalgia as positive energy in restarting my internal momentum. I must also note that I meant to write this post for a very long time after watching this video by blackpenredpen whose videos have been a source of mathematical learning and inspiration for me. Let’s talk about the Gamma and Zeta functions. Before we begin the derivation, perhaps it’s a good idea to review what the two greek letter functions are. The Gamma function is written as And we all know that the Gamma function can be seen as an interpolation of the factorial function, since for non-negative integers, the following relationship stands: Note that there is also a variant of the Gamma function, known as the Pi function, which has somewhat of a nicer form: To the mathematically uninformed self, the Pi function seems a lot more tractable and intuitive. Nonetheless, the prevailing function is Euler’s Gamma function instead of Gauss’s Pi function. The reasons for Gamma’s dominance over Pi is discussed extensively in this math overflow thread. At any rate, it’s both interesting and yet also unsurprising to see that these two functions are brainchildren of Gauss and Euler, two names that arguably appear the most in the world of math. The Riemann zeta function is perhaps one of the most famous functions in the world of analysis. It is also sometimes referred to as the Euler-Riemann zeta function, but at this point, prefixing something with “Euler” loses significance since just about everything in mathematics seems to have some Euler prefix in front of it. The Riemann zeta function takes the following form: But this definition, as simple and intuitive as it is, seems to erroneously suggest that the Riemann zeta function is only defined over non-negative integers. This is certainly not the case. In fact, the reason why the Riemann zeta function is so widely studied in mathematics is that its domain ranges over the complex number plane. While we won’t be discussing the complexity of the Riemann zeta function in this regard (no pun intended), it is nonetheless important to consider about how we might calculate, say, math variable. This is where the Gamma function comes in. As hinted earlier, an alternative definition of the Riemann zeta function can be constructed using the Gamma function that takes the Riemann zeta beyond the obvious realm of integers and into the real domain. We first start with a simple change of variables. Specifically, we can substitute math variable for math variable. This means that math variable, using which we can establish the following: With some algebraic implications, we end up with Dividing both sides by math variable, we get All the magic happens when we cast a summation on the entire expression: Notice that now we have the Riemann zeta function on the left hand side. All we have to do is to clean up what is on the right. As it stands, the integral is not particularly tractable; however, we can swap the integral and the summation expression to make progress. I still haven’t figured out the details of when this swapping is possible, which has to do with absolute divergence, but I will be blogging about it in the future once I have a solid grasp of it, as promised before. The expression in the parentheses is just a simple sum of geometric series, which we know how to calculate. Therefore, we obtain To make this integral look more nicer into a form known as the Bose integral, let’s multiply both the numerator and the denominator by math variable. After some cosmetic simplifications, we end up with Putting everything together, now we have derived a nice expression that places both the Riemann zeta and the Gamma functions together: Or, alternatively, a definition of the Riemann zeta in terms of the Gamma: And indeed, with (13), we can evaluate the Riemann zeta function at non-integer points as well. This is also the definition of the Riemann zeta function introduced in Wikipedia. The article also notes, however, that this definition only applies in a limited number of cases. This is because we’ve assumed, in using the summation of the geometric series formula, the fact that math variable. Today’s post was a short yet very interesting piece on the relationship between the Gamma and the Riemann zeta. One thing I think could have been executed better is the depth of the article—for instance, what is the Bose integral and when is it used? I’ve read a few comments on the original YouTube video by blackpenredpen, where people were saying that the Bose integral is used in statistical mechanics and the study of black matter, but discussing that would require so much domain knowledge to cover. Regardless, I think the theoretical aspect of this derivation is interesting nonetheless. One thing I must do is writing a post on divergence and when the interchange of summation and integrals can be performed. I was originally planning to write a much longer article dividing deep into the Gamma and the Beta function as well as their distributions. However, I realized that what I need at this point in time is producing output and reorienting myself back to self-studying blogger mode, perhaps taking a brief hiatus from the grinding intern spending endless hours in Sublime text with Django (of course, I’m doing that because I enjoy and love the dev work). In the end, we all need a healthy balance between many things in life, and self-studying and working are definitely up on that list for me. Hopefully I can find the middle ground that suits me best. I hope you’ve enjoyed reading this post. Catch you up in the next one!",0,0,0,0,0,0,1,0
Understanding the  Leibniz Rule,"Before I begin, I must say that this video by Brian Storey at Olin College is the most intuitive explanation of the Leibniz rule I have seen so far. Granted, my greedy search over the internet space was by no means exhaustive, so I’ve probably missed some other hidden gems here and there. Also, the video is intended as a visual explanation for beginners rather than a robust analytical proof of the Leibniz rule. This point notwithstanding, I highly recommend that you check out the video. This post is going to provide a short, condensed summary of the proof presented in the video, minus the fancy visualization that pen and paper can afford. The Leibniz rule, sometimes referred to as Feynman’s rule or differentiation-under-the-integral-sign-rule, is an interesting, highly useful way of computing complicated integrals. A simple version of the Leibniz rule might be stated as follows: As you can see, what this rule essentially tells us is that integrals and derivatives are interchangeable under mild conditions. We’ve used this rule many times in a previous post on Fisher’s information matrix when computing expected values that involved derivatives. Why is this the case? It turns out that the Leibniz rule can be proved by using the definition of derivatives and some Taylor expansion. Recall that the definition of a derivative can be written as This is something that we’d see straight out of a calculus textbook. As simple as it seems, we can in fact analyze Leibniz’s rule by applying this definition, as shown below: Thus we have shown that, if the limits of integration are constants, we can switch the order of integration and differentiation. But because our quench for knowledge is insatiable, let’s consider the more general case as well: when the limits are not bounded by constant, but rather functions. Specifically, the case we will consider looks as follows. In this case, we see that math variable and math variable are each functions of variable math variable. With some thinking, it is not difficult to convince ourselves that this will indeed introduce some complications that require modifications to our original analysis. Now, not only are we slightly moving the graph of math variable in the math variable axis, we are also shifting the limits of integration such that there is a horizontal shift of the area box in the math variable axis. But fear not, let’s apply the same approach to answer this question. This may appear to be a lot of computation, but all we’ve done is just separating out the integrals while paying attention to the domains of integration. Let’s continue by doing the same for the remaining terms. The first two terms in the limit go away since math variable goes to zero. While the same applies to the fractional terms, one difference is that they are also divided by math variable, which is why they remain. We have simplified quite a bit, but we still have two terms in the limit expression that we’d like to remove. We can do this by applying the definition of the integral. And we’re done! There are other ways of seeing the Leibniz rule, such as by interpreting it as a corollary of the Fundamental Theorem of Calculus and the Chain rule, as outlined in here (by a Professor at Yale!), but I find the geometrically motivated interpretation presented in this article to be the most intuitive. I hope you enjoyed reading this post. Catch you up in the next one!",0,0,0,0,0,0,1,0
k-Nearest Neighbors Algorithm from Scratch,"These days, machine learning and deep neural networks are exploding in importance. These fields are so popular that, unless you’re a cave man, you have probably heard it at least once. The exaggeration not withstanding, there is perhaps no necessity to justify the topic for today’s blog post: exploring a machine learning algorithm by building it from scratch. Apparently, “from scratch” is now a trendy pedagogical methodology employed in many websites and resources that claim to educate their readers about machine learning. To this, I agree: by constructing algorithms from the ground up, one can glean meaningful insights on how machine learning actually works, as opposed to viewing ML as some dark magic that suddenly makes computers intelligible beings. Enough of the prologue, let’s jump right in. As the name implies, the k-nearest neighbors algorithm works by findinng the nearest neighbors of some give data. Then, it looks at the labels of math variable neighboring points to produce a classification prediction. Here, math variable is a parameter that we can tweak to build the KNN model. For instance, let’s say we have a binary classification problem. If we set math variable to 10, the KNN modell will look for 10 nearest points to the data presented. If among the 10 neighbors observed, 8 of them have the label 0 and 2 of them are labeled 1, the KNN algorithm will conclude that the label of the provided data is most likely also going to be 0. As we can see, the KNN algorithm is extremely simple, but if we have enough data to feed it, it can produce some highly accurate predictions. There are still missing pieces to this puzzle, such as how to find the nearest neighbors, but we will explore the specifics of the algorithm on the go as we build the model from scratch. For now, just remember the big picture. Let’s get into the nuts and bolts of the KNN model. Below are the dependencies we will need for this demonstration. One problem we need to start thinking about is how to measure distance between two data points. After all, the implementation of KNN requires that we define some metric to measure the proximity between different points, rank them in order, and sort the list to find math variable nearest neighbors. One way to go about this is to use Euclidean distance, which is defined as follows: It is not difficult to build an implementation of in Python. We can easily achieve this using . Let’s test the functionality of the function using some dummy dataset. This data set was borrowed from Jason Brownlee. Great! As expected, the distance between a point and itself is 0, and other calculated distances also seem reasonable. The next step is to write a function that returns the math variable nearest neighbors of a point given a data set and parameter . There are many ways to implement this, but an example is shown below. First, we enumerate thorugh the data set to calculate all the distances between the given test instance and the data points in the data set. Next, we sort the list. The returned result is a list that contains the indices of the math variable nearest neighbors the algorithm found in the data set. Note that we use the function we wrote above. Let’s see if the code works by testing it on our toy data. The task is to find 5 data points that are closest to the first row of . As expected, the returned list contains the indices of the data points in that are closest to . We can confirm this by looking at the results of the distance calculation we obtained when testing the function. Note that the indices are in order; that is, indice 0 corresponds to the closet neighbor—it is in fact that data point itself—and index 2 refers to the farthest neighbor among the selections. Say we have successfully obtained the list of math variable nearest neighbors. Now what? Well, it’s time to look up the labels of these neighboring data points to see which class is the most prevalent. The KNN model will then conclude that the most prevalent class label is the one that which the data point belongs to. Because this function has to perform more tasks than the functions we wrote earlier, the example code is slightly longer, but here it goes: Basically, the function counts the number of labels of each class and stores the results in a dictionary. Then, it normalizes the values of the dictionary by dividing its values by the total number of data points seen. Although this process is not necessary, it helps us interpret the results in terms of percentages. For example, the example below tells us that approximately 71 percent of the neighbors of are labeled 0; 28 percent are labeled 1. Now we have all the building blocks we need. We can stop here, but let’s nicely wrap all the functions we have build into a single function that we can use to train and test data. In retrospect, we could have built a class instead, but this implementation also works fine, so let’s stick to it for now. Let’s see what the tells us about . Here, we pass on onto the argument because we want to prevent the algorithm from making a prediction based on a data set that contains the data itself; that would defeat the purpose of making a prediction. Let’s see how the model performs. The KNN model rightly predicts that is labeled 0. Great! But we have only been testing our model on a rather dumb data set. Let’s see whether the model works with larger, closer-to-real-life data sets. The iris data set from the UCI machine learning repository is perhaps one of the best known data sets in the field of machine learning. Created by R. A. Fisher, the data set contains 3 classes of 50 instances each, totaling to 150 independent observations of iris plants, specifically Iris Setosa, Iris Versicolour, and Iris Virginica. The feature columns include sepal length, sepal width, petal length, and petal width. Let’s begin by first loading the data set from the library. One preliminary step we might want to take is shuffling the data set and dividing it into a training set and a testing set. As the name implies, a testing set is a set of data we use to test the accuracy of our classification algorithm. A training set, on the other hand, is a data set the KNN model is going to use to make predictions, i.e. it is the data set from which the algorithm will try to find close neighbors. There is a special function already in the library that does all the shuffling and the splitting for us, but in light of the “from scratch” spirit of this post, let’s try to write up the function ourselves. Great! We can now use this function to split the iris data set we have imported by using the following command. Let’s verify that the splitting has successfully been performed by checking the dimensions of the testing set. As we expect, the testing set is a 30-by-4 matrix. In other words, it contains 4 feature columns—the width and length of sepals and petals, as mentioned earlier—and 30 observation of iris plants. We can now use the KNN model we have built to make predictions about these thirty samples. The choice of parameter as 10 was arbitrary. That was very simple. The returned numpy array contains the class labels for each of the thirty observations in the matrix. In other words, the first test data was predicted to belong to class 1; second data, class 0, third data, class 1, and so on. Let’s compare this predicted result with the actual labels. For the most part, it seems like our predicted result is quite similar to the actual labels. But there are some samples that our KNN algorithm missed, such as the 27th data point: although its actual label is 2, our model predicted it to be 1. We can mathematically calculate the accuracy of our model by using the following function. The job of this function is quite simple: it goes through the two lists, element by element, and checks if the two values are identical. See the implementation below. The accuracy of our prediction turns out to be about 97 percent. But can we do better? Recall that we arbitrarily chose math variable to be 10 when we initialized the KNN algorithm. Would accuracy increase if we set the parameter to another number? Let’s try to answer this question by generating a list of accuracy scores for each value of math variable ranging from 1 to 100. We can achieve this by building a function as shown below. Passing 100 to the argument results in a list of accuracy scores. We can go through this list and try to see for which value of math variable accuracy is maximized. But this is a rather tedious job, and things would get quickly out of control if we were to deal with much larger data sets where the value of math variable can be set to much larger numbers. Instead, let’s create a visualization to see how accuracy changes with respect to math variable. The plot shows that accuracy is maximized for many values of math variable, not just 1. Also, we can learn that accuracy does not go beyond the 97 percent we saw earlier, which is a bit of sad news. An interesting insight we can glean, however, is that accuracy seems to drop past some certain thresholds, most notably around 80. One reasonable explanation might be that the model is looking at too many neighbors that it cannot produce a reliable estimate. At any rate, this visualization shows that hyperparameter tuning is an important job of a machine learning engineer—even if the model is great, if the wrong math variable value is used, the model will only demonstrate lackluster performance. This was perhaps the first post where we dealt with a machine learning algorithm. ML is sometimes treated as a black box, where some magic beneath the hood produces desirable results. However, I find exploring these mechanisms a lot more interesting than simply using pre-existing modules and libraries, as important as they may be. Hopefully, this post gave you some idea of how the KNN model works. I plan to post more on machine learning algorithms in the future. However, at the same time, there will be other posts involving the use of popular preexisting libraries to help demonstrate how machine learning models are used in practice; after all, most practitioners don’t build models themselves every time they embark on a project. The bottom line of this pslan is that we find a sweet spot between theory and practice, and eventually become versed at both. Catch you up in the next one. Happy new year!",0,0,1,1,0,0,0,0
The Magic of Euler’s Identity,"At a glance, Euler’s identity is a confusing, mind-boggling mishmash of numbers that somehow miraculously package themselves into a neat, simple form: I remember staring at this identity in high school, trying to wrap my head around the seemingly discordant numbers floating around the equation. Today, I want to share some ideas I have learned since and demonstrate the magic that Euler’s identity can play for us. The classic proof for Euler’s identity flows from the famous Taylor series, a method of expressing any given function in terms of an infinite series of polynomials. I like to understand Taylor series as an approximation of a function through means of differentiation. Recall that a first-order derivative gives the slope of the tangent line at any given point of a function. The second-order derivative provides information regarding the convexity of the function. Through induction, we can convince ourselves that higher order derivatives will convey information about the curvature of the function throughout coordinate system, which is precisely the underlying mechanism behind Taylor’s series. In a more concise notation, we have Notice that math expression is the starting point of our approximation. Therefore, the Taylor series will provide the most accurate estimation of the original function around that point, and the farther we get away from math expression, the worse the approximation will be. For the purpose of our analysis, let’s examine the Taylor polynomials for the following three functions: math expression, and math expression. Recall that the derivative of math expression is math expression, which is precisely what the Taylor series suggests. It is also interesting to see that the Taylor series for math expression is an odd function, while that for math expression is even, which is coherent with the features of their respective original functions. Last but not least, notice that the derivative of Taylor polynomial of math expression gives itself, as it should. Now that we have the Taylor polynomials, proving Euler’s identity becomes a straightforward process of plug and play. Let’s plug math expression into the Taylor polynomial for math expression: Notice that we can separate the terrms with and without math expression: In short, math expression! With this generalized equation in hand, we can plug in math expression into math expression to see Euler’s identity: The classic proof, although fairly straightforward, is not my favorite mode of proving Euler’s identity because it does not reveal any properties about the exponentiation of an imaginary number, or an irrational number for that matter. Instead, I found geometric interpretations of Euler’s formula to be more intuitive and thought-provoking. Below is a version of a proof for Euler’s identity. Let’s start by considering the complex plane. There are two ways of expressing complex numbers on the Argand diagram: points and vectors. One advantage of the vector approach over point representation is that we can borrow some simple concepts from physics to visualize math expression through the former: namely, a trajectory of a point moving along the complex plane with respect to some time parameter math expression. Notice that introducing this new parameter does not alter the fundamental shape or path of the vector math expression; it merely specifies the speed at which the particle is traversing the complex plane. You might recall from high school physics that the velocity vector is a derivative of the position vector with respect to time. In other words, Where math expression is a vector that denotes the position of an object at time math expression. Now, let’s assume that math expression is such a position vector. Then, it follows from the principles of physics that its derivative will be a velocity vector. Therefore, we have What is so special about this velocity vector? For one, we can see that it is a scalar multiple of the original position vector, math expression. Upon closer examination, we might also convince ourselves that this vector is in fact orthogonal to the position vector. This is because multiplying a point or vector by math expression in the complex plane effectively flips the object’s math expression and math expression components, which is precisely what a 90 degree rotation entails. What does it mean to have a trajectory whose instantaneous velocity is perpendicular to that of the position vector? Hint: think of planetary orbits. Yes, that’s right: this relationship is characteristic of circular motions, a type of movement in which an object rotates around a center of axis. The position vector of a circular motion points outward from the center of rotation, and the velocity vector is tangential to the circular trajectory. The implication of this observation is that the trajectory expressed by the vector math expression is essentially that of a circle, with respect to time math expression. More specifically, we see that at math expression, math expression, or math expression, which means that the circle necessarily passes through the point math expression on the complex plane expressed as an Argand graph. From this analysis, we can learn that the trajectory is not just any circle, but a unit circle centered around the origin. But there’s even more! Recall that the velocity vector of the trajectory is a 90-degree rotation of the position vector, i.e. math expression, math expression. Earlier, we concluded that the trajectory expressed by the vector math expression is a unit circle, which necessarily means that math expression for all values of math expression. Then, syllogism tells us that math expression is also one, i.e. the particle on the trajectory moves at unit speed along the unit circle! Now we finally have a full visualization of the position vector. The blue arrow represents the position vector at math expression; green, the velocity vector also at math expression. Why is speed important? Unit speed implies that the particle moves by math expression distance units after math expression time units. Let’s say that math expression time units have passed. Where would the particle be on the trajectory now? After some thinking, we can convince ourselves that it would lie on the point math expression, since the unit circle has a total circumference of math expression. And so we have proved that math expression, Euler’s identity. But we can also go a step further to derive the generalized version of Euler’s identity. Recall that a unit circle can be expressed by the following equation in the Cartesian coordinate system: On the complex plane mapped in polar coordinates, this expression takes on an alternate form: Notice that this contains the same exact information that Euler’s identity provides for us. It expresses: From this geometric interpretation, we can thus conclude that We now know the exact value that math expression represents in the complex number system! Urban legend goes that mathematician Benjamin Peirce famously said the following about Euler’s identity: Gentlemen, that is surely true, it is absolutely paradoxical; we cannot understand it, and we don’t know what it means. But we have proved it, and therefore we know it must be the truth. But contrary to his point of view, Euler’s identity is a lot more than just an interesting, coincidental jumble of imaginary and irrational numbers that somehow churn out a nice, simple integer. In fact, it can be used to better understand fundamental operations such as logarithms and powers. Consider, for example, the value of the following expression: Imaginary powers are difficult to comprehend by heart, and I no make no claims that I do. However, this mind-pulverizing expression starts to take more definite meaning once we consider the generalized form of Euler’s identity, math expression. Let math expression. Then we have Take both sides to the power of i: Interestingly enough, we see that math expression takes on a definitive, real value. We can somewhat intuit this through Euler’s identity, which is basically telling us that there exists some inextricable relationship between real and imaginary numbers. Understood from this point of view, we see that the power operation can be defined in the entire space that is complex numbers. We can also take logarithms of negative numbers. This can simply be shown by starting from Euler’s identity and taking the natural log on both sides. In fact, because math expression is a periodic function around the unit circle, any odd multiple of math expression will give us the same result. While it is true that logarithmic functions are undefined for negative numbers, this proposition is only true in the context of real numbers. Once we move onto the complex plane, what may appear as unintuitive and mind-boggling operations suddenly make mathematical sense. This is precisely the magic of Euler’s identity: the marriage of different numbers throughout the number system, blending them together in such a way that seems so simple, yet so incomprehensibly complex and profound.",0,0,0,0,0,0,1,0
InceptionNet in PyTorch,"In today’s post, we’ll take a look at the Inception model, otherwise known as GoogLeNet. I’ve actually written the code for this notebook in October 😱 but was only able to upload it today due to other PyTorch projects I’ve been working on these past few weeks (if you’re curious, you can check out my projects here and here). I decided to take a brief break and come back to this blog, so here goes another PyTorch model implementation blog post. Let’s jump right into it! First, we import PyTorch and other submodules we will need for this tutorial. Because Inception is a rather big model, we need to create sub blocks that will allow us to take a more modular approach to writing code. This way, we can easily reduce duplicate code and take a bottom-up approach to model design. The module is a simple convolutional layer followed by batch normalization. We also apply a ReLU activation after the batchnorm. Next, we define the inception block. This is where all the fun stuff happens. The motivating idea behind InceptionNet is that we create multiple convolutional branches, each with different kernel (also referred to as filter) sizes. The standard, go-to kernel size is three-by-three, but we never know if a five-by-five might be better or worse. Instead of engaging in time-consuming hyperparameter tuning, we let the model decide what the optimal kernel size is. Specifically, we throw the model three options: one-by-one, three-by-three, and five-by-five kernels, and we let the model figure out how to weigh and process information from these kernels. In the below, you will see that there are indeed various branches, and that the output from these branches are concatenated to produce a final output in the function. Researchers who conceived the InceptionNet architecture decided to add auxiliary classifiers to intermediary layers of the model to ensure that the model actually learns something useful. This was included in InceptionV1; as far as I’m aware, future versions of InceptionNet do not include auxiliary classifiers. Nonetheless, I’ve added it here, just for the fun of it. Now we finally have all the ingredients needed to flesh out the entire model. This is going to a huge model, but the code isn’t too long because we’ve abstracted out many of the building blocks of the model as or . I won’t get into the details here, as the number of parameters are simply from the original paper. As you can see, there are auxiliary classifiers here and there. If the model is training, we get three outputs in total: , , and . When the model is in , however, we only get , as that’s all we need as the final logits to be passed through a softmax function. Let’s see the gigantic beauty of this model. Great. To be honest, I don’t think the output of the print statement is that helpful; all we know is that the model is huge, and that there is a lot of room for error. So let’s conduct a quick sanity check with a dummy input to see if the model works properly. Great! We’ve passed to the model a batch containing two RGB images of size 224-by-224, which is the standard input assumed by the InceptionNet model. We get in return a tensor of shape , which means we got two predictions, as expected. I’m not going to train this model on my GPU-less MacBook, and if you want to use InceptionNet, there are plenty of places to find pretrained models ready to be used right out of the box. However, I still think implementing this model helped me gain a finer grasp of PyTorch. I can say this with full confidence because a full month has passed since I coded out this Jupyter notebook, and I feel a lot more confident in PyTorch than I used to before. I hope you’ve enjoyed reading this blog post. Catch you up in the next one (where I’ll probably post another old notebook that’s been sitting on my computer for a month).",1,0,0,0,0,1,0,0
Logistic Regression Model from Scratch,"This tutorial is a continuation of the “from scratch” series we started last time with the blog post demonstrating the implementation of a simple k-nearest neighbors algorithm. The machine learning model we will be looking at today is logistic regression. If the “regression” part sounds familiar, yes, that is because logistic regression is a close cousin of linear regression—both models are employed in the context of regression problems. Linear regression is used when the estimation parameter is a continuous variable; logistic regression is best suited to tackle binary classification problems. Implementing the logistic regression model is slightly more challenging due to the mathematics involved in gradient descent, but we will make every step explicit throughout the way. Without further ado, let’s get into it. To understand the clockwork behind logistic regression, it is necessary to understand the logistic function. Simply put, the logistic function is a s-shaped curve the squishes real values between positive and negative infinity into the range math variable. This property is convenient from a machine learning perspective because it allows us to perform binary classification. Binary classification is a type of classification problem where we are assigned the task of categorizing data into two groups. For instance, given the dimensions of a patient’s tumor, determine whether the tumor is malignant or benign. Another problem might involve classifying emails as either spam or not spam. We can label spam emails as 1 and non-spam emails as 0, feed the data into a predefined machine learning algorithm, and generate predictions using that model. If the output of an algorithm given some data point is larger than 0.5, it is likely that the given input is a spam; if it is smaller than the 0.5 threshold, chances are the email is not spam. Let’s take a look at the shape of the sigmoid function, which is a special case of the logistic function that we will use throughout this post. To plot the sigmoid function, we need to import some libraries. The sigmoid function is defined as follows: We can express this as a Python function, as demonstrated in the code snippet below. Let’s quickly plot the graph to see what the sigmoid function looks like. As we can see, the sigmoid is a smooth, differentiable function that is bounded between 0 and 1. It is also symmetrical around the point math variable, which is why we can use 0.5 as a threshold for determining the class of a given data point. The logistic regression model uses the sigmoid function to generate predictions, but how exactly does it work? Recall that, in the case of linear regression, our goal was to determine the coefficients of some linear function, specifically Logistic regression is not so different from linear regression. In fact, we can borrow the same notation we used for linear regression to frame logistic regression as follows: In other words, logistic regression can be understood as a process in which our goal is to find the weight coefficients in the equation above the best describe the given data set. Unlike in linear regression, where the predicted value is computed simply by passing the data as arguments into a linear function, logistic regression outputs numbers between 0 and 1, making binary classification possible. However, there is certainly an element of linearity involved, which is part of the reason why both linear and logistic regression models fall under a larger family of models called generalized linear models. Now that we know the basic maths behind logistic regression using the sigmoid function, it’s time to implement it via code. Welcome to the next part of the tutorial, where we start building the actual model from scratch. As always, it’s a good idea to have some dummy data ready for disposal so that we can develop some basic intuition about dimensionality of our data when handling inputs and outputs of our functions. Here is the data we used in the last post on k-nearest neighbors algorithm, slightly modified for the purposes of this post. Let’s start by translating equation (2) into executable code. The idea is that we want to get a dot product of the weight vector and the data vector, then plug the resulting value into the sigmoid function to get some value between 0 and 1. The function shown below exactly performs this task, with the added caveat that it returns a label prediction when the boolean argument is set to ; a raw sigmoid output when the same is set to . Let’s perform a quick sanity check by using some dummy weight vector. Using the coefficients in the list, we can generate predictions for each observation in the . The actual class information is stored in the list. The dummy coefficients are poorly optimized, which is why the predicted class labels do not align well with the actual class labels. This tells us that more tuning is required to update the coefficients and build a robust logistic regression model. But how exactly can we tune our model? Simply eyeballing the actual and predicted labels of our data is probably not going to help us much. To optimize the coefficients to best fit our data, we need to construct some loss function—that is, a function that describes how badly our model is performing. Then, we can optimize the weights of our model by minimizing that loss function, which would mean that our model gradually makes better predictions with each round of optimization. If you recall the previous post on entropy, you will remember that we discussed a concept called cross entropy. In that post, we derived the formula for cross entropy and intuitively understood it as a way of measuring the “distance” between two distributions. This is exactly what we need: a way of quantifying how different the actual and predicted class labels are! Recall the formula for cross entropy: We can consider class labels as a Bernoulli distribution where data that belongs to class 1 has probability 1 of belonging to that class 1 and probability 0 of belonging to class 0, and vice versa for observations in class 0. The logistic regression model will output a Bernoulli distribution, such as math variable, which means that the given input has a 60 percent chance of belonging to class 1; 40 percent to class 0. Applying this to (3), we get: And that is the loss function we will use for logistic regression! The reason why we have two terms, one involving just math variable and another involving math variable is due to the structure of the Bernoulli distribution, which by definition can be written as Now that we have a loss function to work with, let’s build a function that computes cross entropy loss given and using (4). The function returns the average cross entropy over all input data. We use average cross entropy instead of total cross entropy, because it doesn’t make sense to penalize the model for high cross entropy when the input data set was large to begin with. Now what’s next? Since we have a loss function, we need to build an algorithm that will allow us to minimize this cost function. One of the most common methods used to achieve cost minimization is gradient descent. As you might be able to tell, this algorithm has a lot to do with gradients, which can loosely be understood as a fancy way of saying derivatives. Below is an illustration of the gradient descent algorithm in action, sourced from this blog. Basically, what gradient descent does is that it takes the derivative of the loss function with respect to the weight vector every epoch, or iteration, and takes a small step in the opposite direction of that derivative. If you think of this in the context of two dimensions as shown in the illustration, the gradient descent algorithm ends up moving down the parabola, taking little steps each time, until it eventually reaches the global minimum. In mathematical notation, we might express this process as follows: If we were to perform this in vectorized format, where math variable represents a vector containing the weight coefficients of the logistic regression model: The math variable notation is used to denote gradients, an important operation in matrix calculus which we explored in when deriving the normal equation solution to linear regression on this blog. The math variable denotes a hyperparameter known as the learning rate, which essentially determines how big of a step the gradient descent model takes with each iteration. The main takeaway here is the the gradient descent method allows us to find the local minimum of any convex function, no matter how multidimensional or complex. This is an incredibly powerful statement, and it is one that lies at the core of many machine learning algorithms. To implement gradient descent with code, we have to figure out what the gradient descent equation is in the case of logistic regression. To do this, we need a bit of calculus work using the chain rule. Recall that our goal is to compute since we want to calculate the slope of the cross entropy function with respect to the weights vector. For notational convenience, let’s denote the gradient as a derivative: The gradient in (7) can be broken down into distinct components via the chain rule: where So the task of calculating the gradient now boils down to finding the derivative for each of the terms shown in (8). Let’s start with the easiet one, the last term, which is the derivative of math variable with respect to math variable. From (9), we can conclude that The next in line is the derivative of the sigmoid function, which goes as follows: Now we are almost there. The last piece of the puzzle is computing the first term in (8), the derivative of the cross entropy function. Putting this all together into (8), we get: Voila! We have derived an expression for the gradient of the cross entropy loss function. There is one more tiny little step we have to make to concretize this equation, and that is to consider the average of the total gradient, since (13) as it stands applies to only one data observation. Granted, this derivation is not meant to be a rigorous demonstration of mathematical proof, because we glossed over some details concerning matrix transpose, dot products, and dimensionality. Still, it provides a solid basis for the construction of the gradient descent algorithm in code, as shown below. To avoid compensating code readability, I made a stylistic choice of using and to denote the vector of coefficients instead of using for notational consistency. Other than adding some switch optional parameters such as or , the code simply follows the gradient descent algorithm outlined above. Note that equation (6) is expressed via ; equation (14) is expressed by the line . Let’s quickly check that the function works as expected using the dummy data we created earlier. Great! We see that the average cross entropy decreases with more iterations. The returned array contains the coefficients of the logistic regression model, which we can use to now make predictions. We can stop here, but just like we did in the post on k-nearest neighbors, let’s wrap all the functions we have created so far into a single function that represents the logistic regression model. Our model is ready. Time for testing with some real-world data. Let’s import some data from the web. The data we will be looking at is the banknote authentification data set, publicly available on the UCI Machine Learning Repository. This data set contains 1372 observations of bank notes, classified as either authentic or counterfeit. The five features columns of this data set are: Let’s use some modules to import this data set onto our notebook, as shown below. The imported data set was slightly modified in two ways to fit our model. First, I separated the class label data from the data set and stored it as a separate array. Second, I appended s to each observation to create a new column that accounts for intercept approximation. All this means is that we consider our linear model to be where for all available sample observations. This is something that we have been assuming all along throughout the gradient descent derivation process, but had not been stated explicitly to reduce confusion. Just consider it a strategic choice on our part to simplify the model while allowing for the logistic regression model to consider bias. Let’s check the shape of the imported data set to check that the data has been partitioned correctly. Now, it’s time to split the data into training and testing data. To do this, I recycled a function we built earlier in the previous post on k-nearest neighbors algorithm. Using , we can partition the data set into training and testing data. Let’s make 20 percent of observations as testing data and allocate the rest for training. It’s time for some training and prediction generation. Because we did all the work in the previous section, training and predicting can be achieved with just a single line of command. To see how quickly average cross entropy is decreasing, I turned on the as true. This way, we can see how quickly the loss is declining over every 50 epochs. It’s now time to see how well our model has done. Let’s compare , the list that contains the model’s predictions, with , which is essentially the answer key. This is great news. The result shows us that we have correctly predicted 272 values while making wrong predictions in only 2 cases. Let’s systematize this quantity by creating a function that returns how accurate our model is given and . Let’s use this function to test how well our model performed. 99 percent is not a bad estimate at all. One interesting question to consider is how much boost in accuracy we see with each epoch, i.e. what is the bang-per-buck of each iteration cycle? This is an important question to consider because gradient descent is computationally expensive; if we can train our model in just 10 epochs instead of 1000, why not choose the former? To answer this question, let’s plot accuracy against epoch. For fun, I added the learning parameter as an argument to the function as well. Let’s create a plot to see how accuracy changes over 200 epochs, given a learning rate of 0.1. We see that accuracy spikes up on the first 20- epochs or so and quite quickly converges to about 90 percent. Past a certain threshold, the model seems to hover consistently at around the high 90s range, but accuracy still continues to increase ever so slightly with each epoch, though not as quickly as before. If we set the learning rate to a smaller number, we would expect the model to take a lot longer to tune, and indeed this seems to be true: With a much smaller learning rate, the model seems to struggle to achieve high accuracy. However, although there are a lot of uneven spikes, the model still manages to reach a pretty high accuracy score by 200 epochs. This tells us that the success of model training depends a lot on how we set the learning rate; setting an excessively high value for the learning rate might result in overshooting, while a low learning rate might prevent the model from quickly learning from the data and making meaningful progress. Accuracy helps us intuitively understand how well our model is doing, but recall that the main objective of gradient descent is not to maximize accuracy, but to minimize the cross entropy loss function. Therefore, perhaps it makes more sense to evaluate the performance of our logistic regression model by plotting cross entropy. Presented below is a simple function that plots epoch versus cross entropy given a list of learning rates, . Let’s plot cross entropy loss for three different values of : 0.05, 0.1, and 0.5. Just like before, we cap the number of iterations to 200 epochs. The graph shows that the larger the learning rate, the quicker the decrease in cross entropy loss. This result is coherent with what the previous visualizations on accuracy suggested: the higher the learning rate, the quicker the model learns from the training data. In this post, we built the logistic regression model from scratch by deriving an equation for gradient descent on cross entropy given a sigmoid function. In the process, we brought together many useful concepts we explored on this blog previously, such as matrix calculus, cross entropy, and more. It’s always exciting to see when seemingly unrelated concepts come together to form beautiful pictures in unexpected ways, and that is what motivates me to continue my journey down this road. The logistic regression model is simple yet incredibly powerful in the context of binary classification. As we saw earlier with the application of the model to the task of bank notes authentication, the logistic regression model can, when tweaked with the appropriate parameters, make surprisingly accurate predictions given sufficient amount of training data. Of course, the processing of training and tweaking is not always easy because we have to determine some hyperparameters, most notably the learning rate of the gradient descent algorithm, but the fact that logistic regression is a robust model is unchanged nonetheless. Hopefully this post gave you some idea of what happens behind the scene in a regression-based machine learning model. Thanks for reading. See you in the next post, and happy new year!",0,0,1,1,0,0,0,0
MLE and KL Divergence,"These days, I’ve been spending some time trying to read published research papers on neural networks to gain a more solid understanding of the math behind deep learning. This is a rewarding yet also a very challenging endeavor, mostly because I have not studied enough math to really understand all of what is going on. While reading the groundbreaking research paper Wasserstein GAN by Martin Arjovsky, I came across this phrase: … asymptotically, maximum likelihood estimation amounts to minimizing the Kullback-Leibler divergence… I was particularly interested in the last portion of this sentence, that MLE amounts to minimizing KL divergence. We discussed MLE multiple time on this blog, including this introductory post and a related post on MAP. Neither is KL divergence an entirely unfamiliar topic. However, I had not thought about these two concepts together in one setting. In this post, let’s try to hash out what the quote from the paper means. Let’s start with a very quick review of what MLE and KL divergence each are. After all, it’s been a while since I’ve written the linked posts, and for a fruitful, substantive discussion on this topic, it’s necessary to make sure that we have a solid grasp of what MLE and KL divergence are. MLE is a technique used to find the optimal parameter of a distribution that best describes a set of data. To cut to the chase, this statement can be expressed as follows: From here, we can start making assumptions, such as that observations in math variable are i.i.d, which is the assumption that we make to build models such as naïve Bayes, and so on. For now, it suffices to clarify that the goal of maximum likelihood estimation is to find the optimal parameter of a distribution that best captures some given data. KL divergence is a concept that arises from the field of information theory that is also heavily applied in statistics and machine learning. KL divergence is particularly useful because it can be used to measure the dissimilarity between to probability distributions. The familiar equation for KL divergence goes as follows: In Bayesian terms, KL divergence might be used to compare the prior and the posterior distribution, where math variable represents the posterior and math variable, the prior. In machine learning, math variable is often the true distribution which we seek to model, and math variable is the approximation of that true distribution, which is also the prediction generated by the model. Note that KL divergence is not a true measure of distance, since it is asymmetric. In other words, The focus of this post is obviously not on distance metrics, and I plan on writing a separate post devoted to this topic. But as a preview of what is to come, here is an appetizer to get you interested. An alternative to KL divergence that satisfies the condition of symmetry is the Jensen-Shannon Divergence, which is defined as follows: where One can intuit JSD as being a measurement that somewhat averages the two asymmetric quantities of KL divergence. We will revisit JSD in the future when we discuss the mathematics behind GANs. But for now, it suffices to know what KL divergence is and what it measures. Now that we have reviewed the essential concepts that we need, let’s get down to the proof. Let’s start with the statement of the parameter math variable that minimizes the KL divergence between the two distribution math variable and the approximate distribution math variable: Not a lot has happened in this step, except for substituting the math variable expression with its definition as per (2). Observe that in the last derived expression in (4), the term math variable does not affect the argument of the minima, which is why it can safely be omitted to yield the following simplified expression: We can change the argument of the minima operator to the maxima given the negative sign in the expression for the expected value. To proceed further, it is necessary to resort to the Law of Large Numbers, or LLN for short. The law states that the average of samples obtained from a large number of repeated trials should be close to the expected value of that random variable. In other words, the average will approximate the expected value as more trials are performed. More formally, LLN might be stated in the following fashion. Suppose we perform an experiment involving the random variable math variable and repeat it math variable times. Then, we would obtain a set of indecent and identically distributed (i.i.d) samples as shown below: Then, LLN states that A more precise statement of the law uses Chebyshev’s inequality: For the curious, here is the general formulation of Chebyshev’s inequality outside the context of LLN: For the purpose of this post, it is not necessary to go into how Chebyshev’s inequality is derived or what it means. However, it isn’t difficult to see how one might reformulate (8) to derive (7) to prove the Law of Large Numbers. All that the inequality is saying is that no more than a certain fraction of samples can fall outside more than a certain distance away from the mean of the distribution. With this understanding in mind, let’s return to the original problem and wrap up the proof. Let’s apply the Law of Large Numbers to modify the expected value expression sitting in (5): Voila! We have shown that minimizing the KL divergence amounts to finding the maximum likelihood estimate of math variable. This was not the shortest of journeys, but it is interesting to see how the two concepts are related. Indeed, it sort of makes intuitive sense to think that minimizing the distance between the true and approximated distribution is best done through maximum likelihood estimation, which is a technique used to find the parameter of the distribution that best describes given data. I personally find little derivations and proofs like these to be quite interesting, which is why I plan on doing more posts on the mathematics of deep learning and its related concepts in the future. Thanks for reading, and catch you up in the next one.",0,0,0,0,1,0,0,0
Markov Chain and Chutes and Ladders,"In a previous post, we briefly explored the notion of Markov chains and their application to Google’s PageRank algorithm. Today, we will attempt to understand the Markov process from a more mathematical standpoint by meshing it together the concept of eigenvectors. This post was inspired and in part adapted from this source. In linear algebra, an eigenvector of a linear transformation is roughly defined as follows: a nonzero vector that is mapped by a given linear transformation onto a vector that is the scalar multiple of itself This definition, while seemingly abstract and cryptic, distills down into a simple equation when written in matrix form: Here, math expression denotes the matrix representing a linear transformation; math expression, the eignevector; math expression, the scalar value that is multiplied onto the eigenvector. Simply put, an eigenvector math expression of a linear transformation is one that is—allow me to use this term in the loosest sense to encompass positive, negative, and even imaginary scalar values—“stretched” by some factor math expression when the transformation is applied, i.e. multiplied by the matrix math expression which maps the given linear transformation. The easiest example I like to employ to demonstrate this concept is the identity matrix math expression. For the purpose of demonstration, let math expression be an arbritrary vector math expression and math expression the three-by-three identity matrix. Multiplying math expression by math expression produces the following result: The result is unsurprising, but it reveals an interesting way of understanding math expression: identity matrices are a special case of diagonalizable matrices whose eigenvalues are 1. Because the multiplying any arbitrary vector by the identity matrix returns the vector itself, all vectors in the dimensional space can be considered an eigenvector to the matrix math expression, with math expression = 1. A formal way to calculate eigenvectors and eigenvalues can be derived from the equation above. Since math expression is assumed as a nonzero vector, we can deduce that the matrix math expression is a singular matrix with a nontrivial null space. In fact, the vectors in this null space are precisely the eigenvectors that we are looking for. Here, it is useful to recall that the a way to determine the singularity of a matrix is by calculating its determinant. Using these set of observations, we can modify the equation above to the following form: By calculating the determinant of math expression, we can derive the characteristic polynomial, from which we can obtain the set of eigenvectors for math expression representing some linear transformation math expression. Now that we have reviewed some underlying concepts, perhaps it is time to apply our knowledge to a concrete example. Before we move on, I recommend that you check out this post I have written on the Markov process, just so that you are comfortable with the material to be presented in this section. In this post, we turn our attention to the game of Chutes and Ladders, which is an example of a Markov process which demonstrates the property of “memorylessness.” This simply means that the progress of the game depends only on the players’ current positions, not where they were or how they got there. A player might have ended up where they are by taking a ladder or by performing a series of regular dice rolls. In the end, however, all that matters is that the players eventually hit the hundredth cell. To perform a Markov chain analysis on the Chutes and Ladders game, it is first necessary to convert the information presented on the board as a stochastic matrix. How would we go about this process? Let’s assume that we start the game at the math expressionth cell by rolling a dice. There are six possible events, each with probability of math expression. More specifically, we can end up at the index numbers 38, 2, 3, 14, 5, or 6. In other words, at position 0, where math expression and math expression denote the current and next position of the player on the game board, respectively. We can make the same deductions for other cases where math expression. We are thus able to construct a 101-by-101 matrix representing the transition probabilities of our Chutes and Ladders system, where each column represents the system at a different state, i.e. the math expressionth entry of the math expressionth column vector represents the probabilities of moving from cell math expression to cell math expression. To make this more concrete, let’s consider a program that constructs the stochastic matrix , without regards to the chutes and ladders for now. The indexing is key here: for each column, math expressionth rows were assigned the probability of math expression. Let’s say that a player is in the math expressionth cell. Assuming no chutes or ladders, a single roll of a dice will place him at one of the cells from math expression to math expression; hence the indexing as presented above. However, this algorithm has to be modified for bigger or equal to 95. For example if , there are only three probabilities: math expression, math expression, and math expression, each of values math expression, math expression, and math expression respectively. The statements are additional corrective mechanisms to account for this irregularity. So now we’re done with the stochastic matrix! … or not quite. Things get a bit more complicated once we throw the chutes and ladders into the mix. To achieve this, we first build a dictionary containing information on the jump from one cell to another. In this dictionary, the keys correspond to the original position; the values, the index of the cell after the jump, either through a chute or a ladder. For example, represents the first ladder on the game board, which moves the player from the first cell to the thirty eighth cell. To integrate this new piece of information into our code, we need to build a permutation matrix that essentially “shuffles up” the entries of the stochastic matrix in such a way that the probabilities can be assigned to the appropriate entries. For example, does not reflect the fact that getting a 1 on a roll of the dice will move the player up to the thirty eighth cell; it supposes that the player would stay on the first cell. The new permutation matrix would adjust for this error by reordering . For an informative read on the mechanics of permutation, refer to this explanation from Wolfram Alpha. Let’s perform a quick sanity check to verify that contains the right information on the first ladder, namely the entry in the dictionary. Notice the math expression in the math expressionth entry hidden among a haystack of 100 math expressions! This result tells us that is indeed a permutation matrix whose multiplication with will produce the final stochastic vector that correctly enumerates the probabilities encoded into the Chutes and Ladders game board. Here is our final product: We can visualize the stochastic matrix math expression using the package. This produces a visualization of our stochastic matrix. So there is our stochastic matrix! Now that we have a concrete matrix to work with, let’s start by identifying its eigenvectors. This step is key to understanding Markov processes since the eigenvector of the stochastic matrix whose eigenvalue is 1 is the stationary distribution vector, which describes the Markov chain in a state of equilibrium. For an intuitive explanation of this concept, refer to this previous post. Let’s begin by using the package to identify the eigenvalues and eigenvectors of the stochastic matrix. This code block produces the following output: The first entry of this array, which is the value , deserves our attention, as it is the eigenvalue which corresponds to the stationary distribution eigenvector. Since the index of this value is , we can identify its eigenvector as follows: Notice that this eigenvector is a representation of a situation in which the player is in the math expressionth cell of the game board! In other words, it is telling us that once the user reaches the math expressionth cell, they will stay on that cell even after more dice rolls—hence the stationary distribution. On one hand, this information is impractical given that a player who reaches the end goal will not continue the game to go beyond the math expressionth cell. On the other hand, it is interesting to see that the eigenvector reveals information about the structure of the Markov chain in this example. Markov chains like these are referred to as absorbing Markov chains because the stationary equilibrium always involves a non-escapable state that “absorbs” all other states. One might visualize this system as having a loop on a network graph, where it is impossible to move onto a different state because of the circular nature of the edge on the node of the absorbing state. At this point, let’s remind ourselves of the end goal. Since we have successfully built a stochastic matrix, all we have to do is to set some initial starting vector math expression and perform iterative matrix calculations. In recursive form, this statement can be expressed as follows: The math-inclined thinkers in this room might consider the possibility of conducting an eigendecomposition on the stochastic matrix to simply the calculation of matrix powers. There is merit to considering this proposition, although later on we will see that this approach is inapplicable to the current case. Eigendecomposition refers to a specific method of factorizing a matrix in terms of its eigenvalues and eigenvectors. Let’s begin the derivation: let math expression be the matrix of interest, math expression a matrix whose columns are eigenvectors of math expression, and math expression, a matrix whose diagonal entries are the corresponding eigenvalues of math expression. Let’s consider the result of multiplying math expression and math expression. If we view multiplication as a repetition of matrix-times-vector operations, we yield the following result. But recall that math expression are eigenvectors of math expression, which necessarily implies that Therefore, the result of math expression can be rearranged and unpacked in terms of math expression: math expression math expression In short, Therefore, we have math expression, which is the formula for eigendecomposition of a matrix. One of the beauties of eigendecomposition is that it allows us to compute matrix powers very easily. Concretely, Because math expression and math expression nicely cross out, all we have to compute boils down to math expression! This is certainly good news for us, since our end goal is to compute powers of the stochastic matrix to simulate the Markov chain. However, an important assumption behind eigendecomposition is that it can only be performed on nonsingular matrices. Although we won’t go into the formal proofs here, having a full span of independent eigenvectors implies full rank, which is why we must check if the stochastic matrix is singular before jumping into eigendecomposition. Unfortunately, the stochastic matrix is singular because math expression, the number of columns or rows. This implies that our matrix is degenerate, and that the best alternative to eigendecomposition is the singular value decomposition. But for the sake of simplicity, let’s resort to the brute force calculation method instead and jump straight into some statistical analysis. We first write a simple function that simulates the Chutes and Ladders game given a starting position vector . Because a game starts at the math expressionth cell by default, the function includes a default argument on as shown below: Calling this function will give us math expression, which is a 101-by-1 vector whose th entry represents the probability of the player being on the math expressionth cell after a single turn. Now, we can plot the probability distribution of the random variable math expression, which represents the number of turns necessary for a player to end the game. This analysis can be performed by looking at the values of since the last entry of this vector encodes the probability of the player being at the math expressionth cell, i.e. successfully completing the game after rounds. This block produces the following figure: I doubt that anyone would play Chutes and Ladders for this long, but after about 150 rolls of the dice, we can expect with a fair amount of certainty that the game will come to an end. The graph above presents information on cumulative fractions, but we can also look at the graph for marginal probabilities by examining its derivative: And the result: From the looks of it, the maximum of the graph seems to exist somewhere around math expression. To be exact, math expression. This result tells us that we will finish the game in 19 rolls of the dice more often than any other number of turns. We can also use this information to calculate the expected value of the game length. Recall that Or if the probability density function is continuous, In this case, we have a discrete random variable, so we adopt the first formula for our analysis. The formula can be achieved in Python as follows: This result tells us that the typical length of a Chutes and Ladders game is approximately 36 turns. But an issue with using expected value as a metric of analysis is that long games with infinitesimal probabilities are weighted equally to short games of substantial probability of occurrence. This mistreatment can be corrected for by other ways of understanding the distribution, such as median: This function tries to find the point in the cumulative distribution where the value is closest to math expression, i.e. the median of the distribution. The result tells us that about fifty percent of the games end after 29 turns. Notice that this number is smaller than math expression because it discredits more of the long games with small probabilities. The Markov chain represents an in interesting way to analyze systems that are memoryless, such as the one in today’s post, the Chutes and Ladders game. Although it is a simple game, it is fascinating to see just how much information and data can be derived from a simple image of the game board. In a future post, we present another way to approach similar systems, known as Monte Carlo simulations. But that’s for another time. Peace!",0,1,0,0,0,0,0,0
So What are Autoencoders?,"In today’s post, we will take yet another look at an interesting application of a neural network: autoencoders. There are many types of autoencoders, but the one we will be looking at today is the simplest variant, the vanilla autoencoder. Despite its simplicity, however, there is a lot of insight to glean from this example—in fact, it is precisely the simplicity that allows us to better understand how autoencoders work, and potentially extend that understanding to to analyze other flavors of autoencoders, such as variational autoencoder networks which we might see in a future post. Without further ado, let’s get started. We begin by importing all modules and configurations necessary for this tutorial. How do autoencoders work? There are entire books dedicated to this topic, and this post in no way claims to introduce and explore all the fascinating complexities of this model. However, one intuitive way to understand autoencoders is to consider them as, lo and behold, encoders that map complex data points into vectors living in some latent dimension. For example, a 28-by-28 pixel RGB channel image might be compressed into a five-dimensional latent vector. The five numbers composing this vector somehow encodes the core information needed to then decode this vector back into the original 28-by-28 pixel RGB channel image. Of course, some information is inevitably going to be lost—after all, how can five numbers describe the entirety of an image? However, what’s important and fascinating about autoencoders is that, with appropriate training and configuration, they manage to find ways to best compress input data into latent vectors that can be decoded to regenerate a close approximation of the input data. For the purposes of this demonstration, let’s configure the latent dimension of the encoder to be 128 dimensions—in other words, each 28-by-28, single-channel image will be encoded into vectors living in 128 dimensional space. It’s time to build the autoencoder model. In summary, an autoencoder is composed of two components: an encoder and a decoder. The encoder transfers input data into the latent dimension, and the decoder performs the exact reverse: it takes vectors in the latent space and rearranges it to bring it back into its original dimension, which is, in this case, a 28-by-28, single-channel image. The followign code snippet implements this logic using the functional API. Let’s declare the encoder and autoencoder model by invoking the function with the specified image shape and the dimensionality of the latent space. Just to get a sense of what operations are taking place dimensionality-wise, here is a look at the output shapes of the autoencoder model. Notice that the input is of shape , and that the final output is also of the same shape , as expected. Here’s the image of the model for the fancy bells and whistles. Now that the autoencoder model is fully ready, it’s time to see what it can do! Although autoencoders present countless exciting possibilities for application, we will look at a relatively simple use of an autoencoder in this post: denoising. There might be times when the photos we take or image data we use are tarnished by noise—undesired dots or lines that undermine image quality. An autoencoder can be trained to remove these noises fairly easily as we will see in thi post. First, let’s import the MNIST data set for this tutorial. Nothing much exciting is happening below, except for the fact that we are rearranging and preprocessing the dataset so as to maximize training efficiency. Next, we will add noise to the data. Note that the MNIST dataset does not contain noise by default: we will have to artificially and intentionally tarnish the dataset to produce a noisy training set for the autoencoder model. The function precisely performs this function. Using the function, we can create a noisy sample. Note that was set to 0.5, although I’d imagine other values within reasonable range would work equally well as well. Training the model is very simple: the training data is , the noisy dataset, and the predicted label is . Through this configuration, we essentially expect the autoencoder to be able to see noisy images, after which encoding and decoding is performed via a transformation to a latent dimension to ultimately reproduce a pristine image devoid of any noise. For experimental puposes, I tried using the callback on Google Colab. is a platform that gives developers full view of what happens during and after the training process. It makes observing metrics like loss and accuracy a breeze. I highly recommend that you check out this tutorial on how to use and configure this functionality on your notebook. Now that the training is over, what can we do with this autoencoder? Well, let’s see if the autoencoder is now capable of removing noise from tainted image files. But before we jump right into that, let’s first build a simple function that displays images for our convenience. Using the function, we can now display 25 test images that we will feed into the autoencoder. Let’s add noise to the data. Finally, the time has come! The autoencoder will try to “denoise” the contaminated images. Let’s see if it does a good job. Lo and behold, the autoencoder produces pristine images, almost reverting them back to their original state! I find autoencoders interesting for two reasons. First, they can be used to compress images into lower dimensions. Our original image was of size 28-by-28, summing up to a total of 784 pixels. Somehow, the autoencoder finds ways to decompress this image into vectors living in the predefined 128 dimensions. This is interesting in and of itself, since it presents ways that we might be able to compress large files with minimal loss of information. But more importantly, as we have seen in this tutorial, autoencoders can be used to perform certain tasks, such as removing noise from data, and many more. In the next post, we will take a look at a variant of this vanilla autoencoder model, known as variational autoencoders. Variataional autoencoders are a lot more powerful and fascinating because they can actually be used to generate data instead of merely processing them. I hope you enjoyed reading this post. Stay tuned for more!",1,0,0,0,0,0,0,0
"Beta, Bayes, and Multi-armed Bandits","Recently, I fortuitously came across an interesting blog post on the multi-armed bandit problem, or MAB for short. I say fortuitous because the contents of this blog nicely coincided with a post I had meant to write for a very long time: revisiting the Beta distribution, conjugate priors, and all that good stuff. I decided that the MAB would be a refreshing way to discuss this topic. “Bayesian” is a buzz word that statisticians and ML people love anyway, me shamelessly included. In this post, we will start off with a brief introduction into what the MAB problem is, why it is relevant, and how we can use some basic Bayesian analysis with Beta and Bernoulli distributions to derive a nice sampling algorithm, known as Thompson sampling. Let’s dive right into it. The multi-armed bandit problem is a classical gambling setup in which a gambler has the choice of pulling the lever of any one of math variable slot machines, or bandits. The probability of winning for each slot machine is fixed, but of course the gambler has no idea what these probabilities are. To ascertain out the values of these parameters, the gambler must learn through some trial and error. Given this setup, what is the optimal way of going about the problem? One obvious way is to start randomly pulling on some slot machines until they get a rough idea of what the success probabilities are. However, this is obviously not the most systematic approach, and there is not even a clear guideline as to how they should go about pulling these levers. Here is where the Bayesian approach comes in handy. It isn’t difficult to frame this as a Bayesian problem. Given a pull of the slot machine, the result of which we will denote as math variable, a Bernoulli random variable, we can then formulate the problem as follows: And as the classical Bayesian analysis goes, the more data we collect through repeated experiments, the closer the posterior distribution will get to the target distribution. Through this process, we are able to approximate the parameter math variable. Now we have to think about what distributions we want to use for the prior and likelihood. Let’s start with the easier one: the likelihood. The natural choice that makes sense is the binomial distribution. Each pull on the lever can be considered a Bernoulli trial, and as we start exploring with more pulls to accumulate data, we will essentially be sampling from a binomial distribution, with math variable success and math variable failures out of a total of math variable trials. Therefore, more concretely, we can say The more interesting part is the prior distribution. Intuitively, the one might be inclined to say that we need a uniform prior, and indeed that answer would technically be true. However, I qualify with “technically” since the real answer has an added layer of complexity. Having a uniform prior only makes sense on the very first try, when there is no historical data to estimate the parameter from. However, once we start pulling the lever and seeing the results of each consecutive pull, we should be able to build some prior expectation as to what the true value of the parameter is. This suggests that the uniform prior approach is missing some important key pieces. To really drive this point home, let’s discuss more about the notion of conjugate priors, specifically in the context of the Beta and binomial distributions. The notion of conjugate priors is something that recurs extremely often in Bayesian analysis. This is mainly because conjugate priors offer a nice closed-form solution to posterior distribution computation. Simply put, a conjugate prior to some likelihood function is a type of prior that, when multiplied with a given likelihood, produces a posterior distribution that is of the same form as itself. For example, a Beta prior, when multiplied with a binomial likelihood, produces a Beta posterior. This is what we want in Bayesian analysis, since we can now assume that posterior to be our new prior in the next iteration of experiments. In this section, we will build on top of this high level understanding of conjugacy to show that a Beta distribution is indeed a conjugate prior to the binomial likelihood function. On that end, let’s start with a quick definition review of the Beta distribution and the Beta function. The Beta distribution looks as follows: where Comparing (3) and the integral representation of the Beta function in (4), you will see that there is a clear resemblance, and this is no coincidence. In the Beta distribution, the Beta function which appears in the denominator is simply a normalizing constant that ensures that integrating the probability distribution function from 0 to 1 produces 1. The specifics of the Beta function and how it relates to the Gamma function might be a topic for a another post. For now, it suffices to show the general probability function as well as its building blocks, namely the Beta function itself. These will become relevant later when we derive the posterior with a binomial likelihood function to show conjugacy. To prove conjugacy, it suffices to show that multiplying a Beta prior with a binomial likelihood produces a Beta posterior. Roughly speaking, the big picture we will be using is essentially equation (1). For notation consistency, I decided to use math variable for conditional probability and math variable for function parametrization. For a rather pedantic discussion on this notational convention, I recommend that you take a quick look at this cross validated post. Let’s now plug in the Beta and binomial distributions into (5) to see what comes out of it. This is going to be a lot of computation, but the nice part of it is that a lot of terms cancel out each other. We can easily see that there are constant that exist both in the numerator and the denominator. We can pull these constants out of the integral to simplify the expression. These constants include the binomial and the reciprocal Beta normalizing constants. One useful observation to make here is the fact that the numerator itself looks a lot like something we have seen before: the Beta distribution. In fact, you might also realize that the denominator is nothing but just a normalizing constant that ensures that our posterior distribution, when integrated from 0 to 1, integrates up to 1 as the axiom of probability states. We can also see the denominator as the definition of the Beta function. In other words, Therefore, we end up with Notice that offers an extremely nice interpretation: the number of success math variable and failures math variable, which are determined by math variable, yield insight into what the true parameter is via the Beta distribution. This is why the Beta distribution is often referred to as a “probability distribution of probabilities.” I highly recommend that you read this post on cross validated for many intuitive explanations of what the Beta distribution is and how it can be useful, such as in this Bayesian context. Now we know that there is a closed-form solution for Bayesian problems involving conjugate priors and likelihood functions, such as a Beta prior coupled with the binomial likelihood. But we want to be able to interpret the posterior distribution. We might start from rather simple metrics like the mean to get a better idea of what the posterior tells us. Note that we can always generalize such quantities into moments. Given math variable, the expected value of the Beta random variable can be expressed as Proving this is pretty straightforward if we simply use the law of the unconscious statistician. Using the definition of expectation, we can derive the following: Here, we use the Gamma representation of the Beta function. This conclusion gives an even nicer, more intuitive interpretation of the Bayesian update we saw earlier with the Beta prior and binomial likelihood. Namely, given the new posterior we know that sampling from that new posterior will give us a mean value that somewhat resembles the crude approach we would have taken without the prior expectation. Here, the crude approach is referring to the raw frequentist estimate we would have made had we not taken the Bayesian approach to the problem. It is obvious that the two hyperparameters of the prior are acting as an initial weight of sorts, making sure that when little data is available, the prior overshadows observations, but when ample amount of data is collected and available, eventually yields to those observations to estimate the parameter. Now we can turn our attention back to the multi-armed bandit problem. Now we can build on top of our knowledge of the Beta-Binomial update and refine what the frequentist greedy approach. We will also write out some simple functions to simulate the bandit problem and thus demonstrate the effectiveness of the Bayesian approach. Note that a lot of the code in this post was largely inspired by Peter’s blog. Before we get into any modeling, let’s first import the modules we’ll need and set things up. Instead of using the approach outlined in the blog I’ve linked to, I decided to use objects to model bandits. The rationale is that this approach seems to make a little bit more intuitive sense to me. Also, working with Django these days has made me feel more affinity with Python classes. At any rate, let’s go ahead. Now, we can initialize a bandit with some predetermined parameter, . Of course, our goal would be to determine the true value of this parameter through sampling and Bayesian magic. In this case, we have created a fair bandit with a of 0.5. We can also simulate level pulls by repeatedly calling on . Note that this will accumulate the result of each Bernoulli trial in the list object. Notice that we also have . This is an a list object that c Now that we have performed all the basic sanity checks we need, let’s quickly go ahead and create three bandit objects for demonstration purposes. Now we get into the details of how to perform the Bayesian update. More specifically, we’re interested in how we are going to use posterior probabilities to make decisions on which slot machine to pull on. This is where Thompson sampling comes in. In the simple, greedy frequentist approach, we would determine which bandit to pull on given our historical rate of success. If the first slot machine approximately yielded success 60 percent of the time, whereas the second one gave us 40, we would choose the first. Of course, this approach is limited by the fact that, perhaps we only pulled on the second machine 5 times and got only 2 success out of them, whereas we pulled on the first bandit a hundred times and got 60 successes. Maybe it turns out that the second bandit actually has a higher success rate, and that we were simply unlucky those five turns. Thompson sampling remedies this problem by suggesting a different approach: now that we have Bayesian posteriors, we can now directly sample from those posteriors to get an approximation of the parameter values. Since we are sampling from a distribution instead of relying on a point estimate as we do for the greedy approach, this allows for both exploration and exploitation to happen at reasonable frequencies. If a posterior distribution has large variance, this means that we will explore that particular bandit slightly more than others. If a posterior has a large mean—a high success parameter—then we will exploit that machine a bit more to earn more profit, or, in this context, to minimize regret. Before we move on any farther, perhaps’ it’s worth discussing what the term “regret” means in this context. Simply put, regret refers to the amount that we have comparatively lost by making a sub-optimal choice from the get go. Here is a visual diagram I came across on Analytics Vidhya. The maximum reward would obviously be achieved if we pull on the slot machine with the highest success parameter from trial 1. However, this does not happen since the gambler dives into the game without this prior knowledge. Hence, they have to learn what the optimal choice is through exploration and exploitation. It is of course in this learning process that the greedy algorithm or Bayesian analysis with Thompson sampling comes into play. The amount that we have theoretically lost—or, in other words, the extent to which we are far away from the maximum amount we could have earned—is denoted as regret. Thus, to maximize reward is to minimize regret, and vice versa. Now let’s simulate a hundred pulls on the lever using Bayesian analysis using the Beta-Binomial model and Thompson sampling. Nothing much fancy here, all we’re doing is Thompson sampling from the Beta distribution via , then obtaining the index of the bandit with the highest parameter, then pulling the machine that corresponds to that index. We will also keep cumulative track of our results to reproduce the regret diagram shown above. And we’re done with the hundred round of simulations! Hopefully our simulator gambler has made some good choices by following Bayesian update principles, with the Beta-Binomial model and Thompson sampling under their belt. Let’s take a look at the posterior distribution for each bandit. We can easily plot them using , as shown below. Notice that I have also included the uniform prior for reference purposes. The result is as we would expect: the bandit with the highest success parameter of 0.7 seems to have been pulled on the most, which explains why its variance is the smallest out of the bunch. Moreover, the mean of that particular posterior is also close to 0.7, its true value. Notice that the rest of the posteriors also somewhat have this trend, although more uncertainty is reflected into the shape of the distributions via the spread. It is interesting to see how we can go from a uniform prior, or math variable, to almost normal-shaped distributions as we see above. To corroborate our intuition that the best bandit was indeed the most pulled, let’s quickly see the proportion of the pulls through a pie chart. As you can see, there seems to be a positive correlation between the success parameter and the number of pulls. This is certainly good, since we want the gambler to pull on the best bandit the majority of times while avoiding the worse ones as much as possible. This certainly seems to be the case, given that the bandit with the worse parameter—0.1—was pulled the least. Last but not least, let’s revisit the cumulative regret graph introduced earlier. We can draw our own cumulative regret graph by first simulating what would have been the optimal result—in other words, we need to obtain the amount of reward the gambler would have earned had they simply pulled on the best bandit the entire time. Let’s quickly simulate that first. And it turns out that the maximum amount they would have earned, in this particular instance, is 74. I say in this particular instance, since the expected value of the maximum reward is simply 70, given that the highest success parameter is 0.7. This minor detail notwithstanding, we can now use the quantity and the list in order to recreate our own cumulative regret graph, as shown below. It is obvious that the cumulative regret is highest when we start since the current reward is at 0. However, after some trial and error, the gambler starts to figure out which bandit is the best and starts pulling more of those, ultimately ending up at the point that is quite close to the maximum reward, though not quite due to the earlier opportunities that may have been lost due to exploration and sampling. In this post, we took a look at the multi-armed bandit problem and how it relates to Bayesian analysis with the Beta and Binomial distributions. I personally enjoyed writing this post, not only because I hadn’t written in a long time, but also because it helped me revisit some statistics, which is something that I desperately needed to do—I’m spending way too much time dealing with Django and Selenium these days. Time and again, I realize that there is an element of intuitiveness to Bayesian statistics that, though not obvious at first, starts to make more sense as I explore more into that realm. Of course, frequentist statistics offers a quick and easy way of interpreting certain metrics, but the notion of expectation is something that I had thought was complicated and unnecessary at first, but have gradually come to understand, embrace, and even support. Perhaps writing this post has reaffirmed my Bayesian identity as a budding statistician. I hope you’ve enjoyed reading this post. Catch you up in the next one!",0,0,0,0,1,0,0,1
Riemann Zeta and Prime Numbers,"The other day, I came across an interesting article by Chris Henson on the relationship between the Riemann Zeta function and prime numbers. After encountering a similar post on the math Stack Exchange, I thought I’d write an article on the same topic as well, perhaps as sort of a prologue to a previous article posted on this blog, Basel, Zeta, and some more Euler. The code introduced in this blog are adaptations of those written by Chris Henson, so all credits go to the original author. With that said, let’s dive right into it. The Riemann Zeta function is perhaps one of the most deeply studied functions in all of modern mathematics. It is sometimes also referred to as the Euler-Riemann Zeta function, but I will not adhere to this convention not only because it is needlessly long and cumbersome, but also because Euler already has so many constants and functions and theorems bearing his name. Anyhow, the Riemann Zeta function looks as follows: While there are so many layers to explore with this function, one relatively simple and interesting route is to factorize this function. We performed this factorization in the previous post while introducing Euler’s infinite product representation of the Zeta function. Let’s go through this step again, as it is pivotal for our segue into the topic of prime numbers and probability. The idea is that, much like we multiply the ratio to a geometric sequence to calculate its sum, we can multiply terms to the Zeta function to factor out their multiples. For instance, let’s consider the case when we multiply math variable to the Zeta function. Since the Zeta function itself is an infinite series, we can subtract this result from the original Zeta function. This effectively filters or sieves out all terms whose denominator is a multiple of 2, effectively leaving only the odd terms. Concretely, If we repeat this process for all prime numbers, we will eventually be left with the following: where math variable denotes the set of all prime numbers. From this, we can come up with the following alternative representation of the Riemann Zeta function: which can also be expressed as Other than the fact that the factorization of the Riemann Zeta function is satisfying in and of itself, the result in (5) also provides us with an interesting probabilistic interpretation on coprimeness. The intuition is pretty simple: given some random natural number math variable, the probability that a prime math variable will divide math variable is simply math variable. For example, if we come up with some random number, the probability that 2 will divide that number (that the number is even) is 0.5; the probability that the number will be a multiple of 3 is one-thirds. A corollary of this simple analysis is that we can now express the probability that a given random number will be a prime number as follows, using (6): In other words, the reciprocal of the Zeta function tells us the probability that a randomly chosen number will be a prime! The more interesting part of the story is that, we can now extend this single-number example to the case of multiple numbers. The only difference is that, instead of considering whether a single number is prime or not, we will consider the notion of coprimeness, or relative primeness. Imagine we randomly sample math variable numbers, ranging from math variable all the way up to math variable. What is the probability that these math variable numbers are all coprime to each other, i.e. the greatest common divisor for these numbers is 1? With some rumination, it isn’t difficult to convince ourselves that this probability can be expressed as Let’s think for a second why this is the case. The probability that some prime number math variable divides all math variable through math variable is going to be math variable, as dividing each number can be considered an independent event. Therefore, the probability that some prime number math variable does not divide all numbers—i.e. it may divide none or some, but definitely not all—can be expressed as the complement of math variable, or equivalently, math variable. If we apply this analysis to all prime numbers, we end up with (6). Now, let’s simulate the process of random sampling to empirically verify the probabilistic interpretation of the Riemann Zeta function. Before we get into the specifics, below are the dependencies we will need. The first function we will build is one that randomly samples natural numbers from 1 to , and checks if all number pairs within this sample is coprime. For this, we use , which reduces the result of applying to two pairs of numbers in the randomly sampled number list. is a quite common operation in functional programming, and we saw an example of this operation in the context of Spark in a previous post as well. Let’s see this function in action. I’ve added a flag for convenience of testing and demonstration. Let’s toggle this option on for now and see what we get. The GCD of 6, 7, and 1 are 1, so the returned result is as we expect. We also notice that three numbers were returned since . Next, we define a testing function that will simulate multiple runs of the test for us. Because this is a Monte Carlo simulation, to have confidence in our estimate, we need to iterate the sampling process multiple times until we have sufficient amount of data. The parameter determines the number of simulations we will perform. And much like in the previous function, the parameter determines the upper bound of our sampling range. Therefore, the total number of simulations we run will effectively be times. Last but not least, indicates how many numbers we want to sample each time—this parameter is semantically identical to the parameter we saw in the function above. Also, for easier plotting, we will return the domain range object alongside the result of our simulation. The function is just a two-liner—we were able to reduce the number of lines thanks to list comprehension. Now, let’s see if this code works as expected. Here, our experiment consisted of sampling two numbers, starting from range , all the way up to range , with 1000 simulations for each range. Let’s plot the results to get a better idea of what was going on. First, notice that when , the probability that two sampled numbers are coprime is 1. This is unsurprising, since sampling from range simply means that both the sampled numbers were 2—and of course, 2 and 2 are coprimes. However, as the range expands all the way up to 200, we see that the probability of two numbers being coprime drops precipitously, roughly converging to the value of math variable as we expect. Indeed, the dots seem to cluster around the gold line, which represents the value of the Zeta function evaluated at 2. But does this result generalize to cases where we sample more than just two numbers? In other words, if we sample math variable numbers, would the probability that the numbers be coprime approach math variable? Let’s find out. Given a range of values, the function returns a plot for each , which is effectively math variable in our mathematical notation. We see that, for the three values of math variable that were tested—2, 3, and 4—the Zeta function seems to approximate the probability of relative primeness pretty well. Based on our earlier mathematical analysis, we would expect this convergence to get even better as we expand out the range, with an upper bound that is greater than the current 200. While jumping around in Wikipedia, I came across the Dirichlet Eta function, which is a slight variant of the Riemann Zeta function. This function looks as follows: As you can see, this is essentially the alternating version of the Riemann Zeta function. Given this design, we can derive what may appear to be apparent to some yet nonetheless interesting relationship between the Eta and Zeta. Deriving this relationship requires a very similar operation to the sieving or factorizing we performed earlier to derive the probabilistic interpretation of the Zeta function. For a bit of intuition, observe that the Eta function can be split up into what may be referred to as even and odd terms. In other words, The idea is that the even terms are just a multiple of the Zeta function, namely Then, the odd terms can also be seen as the Zeta function minus this multiple: We now have successfully expressed both the even and odd terms of the Eta function in terms of the Zeta function. If we put the two together, we will then be able to express the entirety of the Eta function fully in terms of the Zeta function. To cut to the chase, we get And there we have it, the relationship between the Dirichlet Eta function and the Riemann Zeta function! There are many more interesting things about the Eta function, such as its convergence property, but that is a topic for another post. In this post, we developed an intuition for the implications of the Riemann Zeta function from the perspective of relative primeness and probability. The Zeta function is one of those things in mathematics that appear so simple on the surface, yet is so wonderfully complex and convoluted in the inside. Although we haven’t discussed these other intricacies of the Riemann Zeta function—in particular, its relationship to the Riemann hypothesis, which states that the Zeta function has roots at negative even integers and complex numbers whose real part is math variable—but the approach we took here with prime numbers are probabilities is fascinating in its own right, providing ample food for thought. Many thanks to Chris Henson again for the code and the post. It’s always a lot of fun to mesh mathematics with programming, and I think this is why I enjoyed writing this post. On a related note, I’ve recently gotten into Project Euler, so I might post about some problems now and then as well. I hope you’ve enjoyed reading this post. See you in the next one.",0,0,0,0,0,0,1,0
Dissecting the Gaussian Distribution,"If there is one thing that the field of statistics wouldn’t be complete without, it’s probably normal distributions, otherwise referred to as “the bell curve.” The normal distribution was discovered and studied extensively by Carl Friedrich Gauss, which is why it is sometimes referred to as the Gaussian distribution. We have seen Gaussian distributions before in this blog, specifically on this post on likelihood and probability. However, normal distribution was introduced merely as an example back then. Today, we will put the Gaussian distribution on stage under undivided spotlight. Of course, it is impossible to cover everything about this topic, but it is my goal to use the mathematics we know to derive and understand this distribution in greater detail. Also, it’s just helpful to brush up on some multivariable calculus in a while. Let’s start with the simplest case, the univariate Gaussian distribution. The “univariate” part is just a fancier way of saying that we will dealing be dealing with one-dimensional random variables, i.e. the distribution is going to be plotted on a two-dimensional math expression plane. We make this seemingly trivial distinction to distinguish it from the multivariate Gaussian, which can be plotted on three-dimensional space or beyond. We’ll take a look at the multivariate normal distribution in a later section. For now, let’s derive the univariate case. One of the defining properties of data that are said to be normally distributed when the rate at which the frequencies decrement is proportional to its distance from the mean and the frequencies themselves. Concretely, this statement might be translated as We can separate the variables to achieve the following expression: Integrating both sides yields Let’s get rid of the logarithm by exponentiating both sides. That’s an ugly exponent. But we can make things look better by observing that the constant term math expression can be brought down as a coefficient, since where we make the substitution math expression. Now, the task is to figure out what the constants math expression and math expression are. There is one constraint equation that we have not used yet: the integral of a probability distribution function must converge to 1. In other words, Now we run into a problem. Obviously we cannot calculate this integral as it is. Instead, we need to make a clever substitution. Here’s a suggestion: how about we get rid of the complicated exponential through the substitution Then, it follows that Therefore, the integral in (1) now collapses into Now that looks marginally better. But we have a very dirty constant coefficient at the front. Our natural instinct when we see such a square root expression is to square it. What’s nice about squaring in this case is that the value of the expression is going to stay unchanged at 1. Because the two integrals are independent, i.e. calculating one does not impact the other, we can use two different variables for each integral. For notational convenience, let’s use math expression and math expression. We can combine the two integrals to form an iterated integral of the following form: The term math expression rings a bell, and that bell sounds like circles and therefore polar coordinates. Let’s implement a quick change of variables to move to polar coordinates. Now we have something that we can finally integrate. Using the chain rule in reverse, we get We can consider there to be 1 in the integrand and continue our calculation. The result: From (4), we can express math expression in terms of math expression: After applying the substitution, now our probability density function looks as follows: To figure out what math expression is, let’s try to find the variance of math expression, since we already know that the variance should be equal to math expression. In other words, from the definition of variance, we know that Using (5), we get We can use integration by parts to evaluate this integral. This integral seems complicated, but if we take a closer look, we can see that there is a lot of room for simplification. First, because the rate of decay of an exponential function is faster than the rate of increase of a first-order polynomial, the first term converges to zero. Therefore, we have But since Therefore, Great! Now we know what the constant math expression is: Plugging this expression back into (5), we finally have the equation for the probability distribution function of the univariate Gaussian. And now we’re done! Let’s perform a quick sanity check on (7) by identifying its critical points. Based on prior knowledge, we would expect to find the local maximum at math expression, as this is where the bell curve peaks. If we were to dig a bit deeper into prior knowledge, we would expect the point of inflection to be one standard deviations away from the mean, left and right. Let’s verify if these are actually true. From good old calculus, we know that we can obtain the local extrema by setting the first derivative to zero. We can ignore the constants as they are non-zero. Then, we end up with Because the exponent is always positive, the only way for the expression to evaluate to zero is if This tells us that the local maximum of the univariate Gaussian occurs at the mean of the distribution, as we expect. The inflection point can be obtained by setting the second order derivative of the probability distribution function equal to zero. Luckily, we’re already halfway done with calculating the second order derivative since we’ve already computed the first order derivative above. As we have done above, let’s ignore the constants since they don’t affect the calculation. Because the first exponential term cannot equal zero, we can simplify the equation to Therefore, From this, we can see that the inflection point of the univariate Gaussian is exactly one standard deviation away from the mean. This is one of the many interesting properties of the normal distribution that we can see from the formula for the probability distribution. So far, we’ve looked at the univariate Gaussian, which involved only one random variable math expression. However, what if the random variable in question is a vector that contains multiple random variables? It is not difficult to see that answering this question requires us to think in terms of matrices, which is the go-to method of packaging multiple numbers into neat boxes, known as matrices. Instead of deriving the probability distribution for the multivariate Gaussian from scratch as we did for the univariate case, we’ll build on top of the equation for the univariate Gaussian to provide an intuitive explanation for the multivariate case. In a previous post on linear regression, we took a look at matrix calculus to cover basic concepts such as the gradient. We established some rough intuition by associating various matrix calculus operations and their single-variable calculus analogues. Let’s try to use this intuition as a pivot point to extend the univariate Gaussian model to the multivariate Gaussian. For readability sake, here is the univariate model we have derived earlier. Examining (7), the first observation we might make is that math expression is no longer a coherent expression in the multivariable context. The fix to this is extremely simple: recall that in vector world. Therefore, we can reexpress (7) as This is the result of simply changing the squared term. Continuing, the next subject of our interest would be math expression, as the variance is only strictly defined for one variable, as expressed by its definition below: where math expression is a random variable, which takes a scalar value. This necessarily begs the question: what is the multivariable equivalent of variance? To answer this question, we need to understand covariance and the covariance matrix. To jump right into the answer, the multivariable analogue of variance is covariance, which is defined as Notice that math expression equals variance, which is why we stated earlier that covariance is the multivariate equivalent of variance for univariate quantities. The intuition we can develop from looking at the equation is that covariance measures how far our random variables are from the mean in the math expression and math expression directions. More concretely, covariance is expresses the degree of association between two variables. Simply put, if there is a positive relationship between two variables, i.e. an increase in one variable results in a corresponding increase in the other, the variance will be positive; conversely, if an increase in one variable results in a decrease in the other, covariance will be negative. A covariance of zero signifies that there is no linear relationship between the two variables. At a glance, the concept of covariance bears strong resemblance to the notion of correlation, which also explains the relationship between two variables. Indeed, covariance and correlation are related: in fact, correlation is a function of covariance. The biggest difference between correlation and covariance is that correlation is bounded between -1 and 1, whereas covariance is unbounded. The bottom line is that both correlation and covariance measure the strength of linearity between two variables, with correlation being a normalized version of covariance. At this point in time, one might point out that covariance is not really a multivariate concept since it is defined for only two variables, not three or more. Indeed, the expression math expression is mathematically incoherent. However, covariance can be a multivariate metric since we can express the covariance of any pairs of random variables by constructing what is called the covariance matrix. Simply put, the covariance matrix is a matrix whose elements are the pairwise covariance of two random variables in a random vector. Before we get into the explanation, let’s take a look at the equation for the covariance matrix: where math expression and math expression. This is the matrix analogue of the expression which is an alternate definition of variance. It is natural to wonder why we replaced the squared expression with math expression instead of math expression as we did earlier with the term in the exponent. The simplest answer that covariance is expressed as a matrix, not a scalar value. By dimensionality, math expression produces a single scalar value, whereas math expression creates a matrix of rank one. We can also see why (9) is coherent by unpacking the expected values expression as shown below: Using the linearity of expectation, we can rewrite the equation as Therefore, we end up with which almost exactly parallels the definition of variance, which we might recall is where math expression. The key takeaway is that the covariance matrix constructed from the random vector math expression is the multivariable analogue of variance, which is a function of the random variable math expression. To gain a better idea of what the covariance matrix actually looks like, however, it is necessary to review its structure element-by-element. Here is the brief sketch of the math expression-by-math expression covariance matrix. This might seem complicated, but using the definition of covariance in (8), we can simplify the expression as: Note that the covariance matrix is a symmetric matrix since math expression. More specifically, the covariance matrix is a positive semi-definite matrix. This flows from the definition of positive semi-definiteness. Let math expression be some arbitrary non-zero vector. Then, You might be wondering how (9) ends up as (10). Although this relationship may not be immediately apparent, that the two expressions are identical can be seen by setting the random vector as and performing basic matrix vector multiplication operations. For the sake of brevity, this is left as an exercise for the reader. We now have all the pieces we need to complete the puzzle. Recall that we were trying to derive the probability density function of the multivariate Gaussian by building on top of the formula for the univariate Gaussian distribution. We finished at then moved onto a discussion of variance and covariance. Now that we understand that the covariance matrix is the analogue of variance, we can substitute math expression with math expression, the covariate matrix. Instead of leaving math expression at the denominator, let’s use the fact that to rearrange the expression. This is another example of when the matrix-scalar parallel intuition can come in handy: the scalar multiplicative identity is 1, whereas the equivalent in matrix world is the identity matrix math expression. Therefore, the reciprocal of a matrix can be interpreted as its inverse. From this observation, we can conclude that We are almost done, but not quite. Recall the the constant coefficient of the probability distribution originates from the fact that We have to make some adjustments to the constant coefficient since, in the context of the multivariate Gaussian, the integral translates into While it may not be apparent immediately, it is not hard to accept that the correcting coefficient in this case has to be as there are math expression layers of iterated integrals to evaluate for each math expression through math expression. Instead of the matrix math expression, we use its determinant math expression since we need the coefficient to be a constant, not a matrix term. We don’t go into much detail about the derivation of the constant term; the bottom line is that we want the integral of the probability distribution function over the relevant domain to converge to 1. If we put the pieces of the puzzle back together, we finally have the probability distribution of the multivariate Gaussian distribution: To develop a better intuition for the multivariate Gaussian, let’s take a look at a case of a simple 2-dimensional Gaussian random vector with a diagonal covariance matrix. This example was borrowed from this source. Using the formula for the multivariate Gaussian we derived in (11), we can construct the probability distribution function given math expression, math expression, and math expression. Note that computing math expression, the inverse of the covariance matrix, can be accomplished simply by taking the reciprocal of its diagonal entries since math expression was assumed to be a diagonal matrix. Continuing, In other words, the probability distribution of seeing a random vector math expression given math expression and math expression is equal to the product of the two univariate Gaussians. This result is what we would expect given that math expression. For instance, if math expression and math expression are independent, i.e. observing a value of math expression does not inform us of anything about math expression and vice versa, it would make sense that the possibility of observing a random vector math expression with entries math expression and math expression is merely the product of the independent probabilities of each observing math expression and math expression. This example illustrates the intuitive link between the multivariate and univariate Gaussian distributions. In this post, we took a look at the normal distribution from the perspective of probability distributions. By working from the definition of what constitutes a normal data set, we were able to completely build the probability density function from scratch. The derivation of the multivariate Gaussian was complicated by the fact that we were dealing with matrices and vectors instead of single scalar values, but the matrix-scalar parallel intuition helped us a lot on the way. Note that the derivation of the multivariate Gaussian distribution introduced in this post is not a rigorous mathematical proof, but rather intended as a gentle introduction to the multivariate Gaussian distribution. I hope you enjoyed reading this post on normal distributions. Catch you up in the next one.",0,0,0,0,1,0,0,1
The Math Behind GANs,"Generative Adversarial Networks refer to a family of generative models that seek to discover the underlying distribution behind a certain data generating process. This distribution is discovered through an adversarial competition between a generator and a discriminator. As we saw in an earlier introductory post on GANs, the two models are trained such that the discriminator strives to distinguish between generated and true examples, while the generator seeks to confuse the discriminator by producing data that are as realistic and compelling as possible. In this post, we’ll take a deep dive into the math behind GANs. My primary source of reference is Generative Adversarial Nets by Ian Goodfellow, et al. It is in this paper that Goodfellow first outlined the concept of a GAN, which is why it only makes sense that we commence from the analysis of this paper. Let’s begin! GAN can be seen as an interplay between two different models: the generator and the discriminator. Therefore, each model will have its own loss function. In this section, let’s try to motivate an intuitive understanding of the loss function for each. To minimize confusion, let’s define some notation that we will be using throughout this post. The goal of the discriminator is to correctly label generated images as false and empirical data points as true. Therefore, we might consider the following to be the loss function of the discriminator: Here, we are using a very generic, unspecific notation for math variable to refer to some function that tells us the distance or the difference between the two functional parameters. (If this reminded you of something like cross entropy or Kullback-Leibler divergence, you are definitely on the right track.) We can go ahead and do the same for the generator. The goal of the generator is to confuse the discriminator as much as possible such that it mislabels generated images as being true. The key here is to remember that a loss function is something that we wish to minimize. In the case of the generator, it should strive to minimize the difference between 1, the label for true data, and the discriminator’s evaluation of the generated fake data. A common loss function that is used in binary classification problems is binary cross entropy. As a quick review, let’s remind ourselves of what the formula for cross entropy looks like: In classification tasks, the random variable is discrete. Hence, the expectation can be expressed as a summation. We can simplify this expression even further in the case of binary cross entropy, since there are only two labels: zero and one. This is the math variable function that we have been loosely using in the sections above. Binary cross entropy fulfills our objective in that it measures how different two distributions are in the context of binary classification of determining whether an input data point is true or false. Applying this to the loss functions in (1), We can do the same for (2): Now we have two loss functions with which to train the generator and the discriminator! Note that, for the loss function of the generator, the loss is small if math variable is close to 1, since math variable. This is exactly the sort of behavior we want from a loss function for the generator. It isn’t difficult to see the cogency of (6) with a similar approach. The original paper by Goodfellow presents a slightly different version of the two loss functions derived above. Essentially, the difference between (6) and (8) is the difference in sign, and whether we want to minimize or maximize a given quantity. In (6), we framed the function as a loss function to be minimized, whereas the original formulation presents it as a maximization problem, with the sign obviously flipped. Then, Goodfellow proceeds by framing (8) as a min-max game, where the discriminator seeks to maximize the given quantity whereas the generator seeks to achieve the reverse. In other words, The min-max formulation is a concise one-liner that intuitively demonstrates the adversarial nature of thecompetition between the generator and the discriminator. However, in practice, we define separate loss functions for the generator and the discriminator as we have done above. This is because the gradient of the function math variable is steeper near math variable than that of the function math variable, meaning that trying to maximize math variable, or equivalently, minimizing math variable is going to lead to quicker, more substantial improvements to the performance of the generator than trying to minimize math variable. Now that we have defined the loss functions for the generator and the discriminator, it’s time to leverage some math to solve the optimization problem, i.e. finding the parameters for the generator and the discriminator such that the loss functions are optimized. This corresponds to training the model in practical terms. When training a GAN, we typically train one model at a time. In other words, when training the discriminator, the generator is assumed as fixed. We saw this in action in the previous post on how to build a basic GAN. Let’s return back to the min-max game. The quantity of interest can be defined as a function of math variable and math variable. Let’s call this the value function: In reality, we are more interested in the distribution modeled by the generator than math variable. Therefore, let’s create a new variable, math variable, and use this substitution to rewrite the value function: The goal of the discriminator is to maximize this value function. Through a partial derivative of math variable with respect to math variable, we see that the optimal discriminator, denoted as math variable, occurs when Rearranging (12), we get And this is the condition for the optimal discriminator! Note that the formula makes intuitive sense: if some sample math variable is highly genuine, we would expect math variable to be close to one and math variable to be converge to zero, in which case the optimal discriminator would assign 1 to that sample. On the other hand, for a generated sample math variable, we expect the optimal discriminator to assign a label of zero, since math variable should be close to zero. To train the generator, we assume the discriminator to be fixed and proceed with the analysis of the value function. Let’s first plug in the result we found above, namely (12), into the value function to see what turns out. To proceed from here, we need a little bit of inspiration. Little clever tricks like these are always a joy to look at. If you are confused, don’t worry, you aren’t the only one. Basically, what is happening is that we are exploiting the properties of logarithms to pull out a math variable that previously did not exist. In pulling out this number, we inevitably apply changes to the terms in the expectation, specifically by dividing the denominator by two. Why was this necessary? The magic here is that we can now interpret the expectations as Kullback-Leibler divergence: And it is here that we reencounter the Jensen-Shannon divergence, which is defined as where math variable. This means that the expression in (15) can be expressed as a JS divergence: The conclusion of this analysis is simple: the goal of training the generator, which is to minimize the value function math variable, we want the JS divergence between the distribution of the data and the distribution of generated examples to be as small as possible. This conclusion certainly aligns with our intuition: we want the generator to be able to learn the underlying distribution of the data from sampled training examples. In other words, math variable and math variable should be as close to each other as possible. The optimal generator math variable is thus one that which is able to mimic math variable to model a compelling model distribution math variable. In this post, we took a brief tour of the math behind general adversarial networks. Since the publication of Goodfellow’s work, more GAN models have been introduced and studied by different scholars, such as the Wasserstein GAN or CycleGAN to name just a few. The underlying mathematics for these models are obviously going to be different from what we have seen today, but this is a good starting point nonetheless. I hope you enjoyed reading this post. In the next post, I plan to explore the concept of Fisher information and the Fisher matrix. It is going to be another math-heavy ride with gradients and Hessians, so keep you belts fastened!",1,0,0,0,1,0,0,0
Introduction to seq2seq models,"For a very long time, I’ve been fascinated by sequence-to-sequence models. Give the model a photo as input, it spits out a caption to go along with it; give it some English text, it can translate it into another language. Seq2seq models are also not only widely applicable in different contexts, but it also arguably laid the groundwork for other more advanced models that came after, such as attention and transformers. In studying seq2seq models, I found Ben Trevett’s sequence modeling tutorials extremely helpful. In particular, this post is heavily based off of this notebook. With that cleared up, let’s get started! For this tutorial, we will need to import a number of dependencies, mainly from and . is a library that provides a nice interface to dealing with text-based data in PyTorch. includes a class, which essentially allows us to define some preprocessing steps to be applied on the data. We will be using the dataset, which contains translations of short texts from many languages. In this tutorial, we will be using German and English, so we define preprocessing steps for each language. The preprocessing, as defined below, tells to: If you get errors running this command, make sure that you have downloaded English and German models for spacy. This is required since torchtext internally uses spacy to tokenize the text. We can now prepare the data by calling on the dataset, using the fields we have defined above. Let’s quickly check how many data there are for each train, validation, and test split. It’s also helpful to see what’s actually in each of these splits. Let’s take a look at the very first data in the training set. We see that each example is a dictionary containing English and German sentences. Note also that they have been tokenized; each translation is a list containing words and punctuations. By calling on each field, we can tell torchtext which vocabulary to keep. Internally, this process triggers each field to have , or token to index and , the reverse lookup. We can see that the vocabulary includes not only the words actually in the dataset, but also special tokens such as start, end, and padding tokens. The attribute gives us access to the entire vocabulary in each of the fields. We can see that the German vocabulary is slightly larger than English. Let’s also quickly check the dictionary to see if we can obtain index values. We can see that the start token is the second in the lookup table. Using , we can batch these field datasets into their PyTorch data loader equivalents. Let’s set a batch size and create some iterators. With batching, we get 128 examples at once. The zeroth index of the batch, as can be seen below, is a list containing 128 2’s. This is because 2 is the index that corresponds to the starting token. Since all examples start with , we have 2’s at the zeroth index of the batch. The data preparation steps might felt a little boring, as it’s just interacting with the torchtext API, but it’s necessary nonetheless. Now, it’s finally time for some modeling! A typical sequence-to-sequence model is, by design, composed of an encoder and a decoder. The encoder takes in a sequence as input, processes it to formulate some hidden state, and eventually passes on that hidden state and cell state to the decoder. The underlying assumption here is that the hidden and cell states should be able to encode some long and short term memory of the encoder network. The decoder, by accepting these two states, should have an understanding of the input data. Based on this understanding, it is then trained to output a corresponding sequence. Let’s first take a look at the implementation of the encoder network. It is a simple LSTM model that consists of an embedding layer and an LSTM layer. At the end of the forward pass, we return the hidden and cell states. Note that is the total length of the index of the source language. The decoder is similar to the encoder, but has a little more moving parts. On a high level, the decoder is also an LSTM model like the encoder. In particular, its LSTM layer is configured in such a way that it is able to accept the hidden and cell states from the encoder. One obvious difference is in terms of the vocabulary size, which is now the size of the target language, not the source language. Also, the decoder has a fully connected layer that acts as a classifier. Hence, the size of the output dimension is equal to the vocabulary size of the target language. Now, it’s time to put the two models together in a sequence-to-sequence model. The overall flow of data looks as follows: I resorted to a convenient bullet point listing to summarize everything, but let’s break this down a bit. Within the forward pass of the seq2seq model, the encoder encodes input data, which, in this case, are German sentences. Then, the decoder accepts the hidden and cell states of the encoder, as well as the zeroth index of the target language batch. This zeroth index will simply be a bunch of starting tokens, as we saw earlier. Then, the decoder will generate a prediction using these starting tokens and encoder states. The interesting part comes thereafter. We set some teacher force ratio, which is a number between zero and one. There are two ways through which the decoder can generate the next prediction. Either it can use its own prediction from the previous time step, or, as “teachers,” we can nudge the decoder in the correct direction by telling them what the correct prediction should have been from the previous time step. This teacher guidance is helpful, since at the beginning of training, the model might struggle to generate correct predictions using its own previous predictions. Below is the full implementation of the model. Now that we have a seq2seq model, let’s write some code for the training loop. Below are some preliminary quantities that we will to set up the encoder and decoder models. Let’s also initialize some weights with a uniform distribution. We see that this is quite a big model, with a total of 13898757 trainable parameters. Let’s define the optimizer and criterion for training. Since the model basically outputs logits for a distribution, we can consider it to be a classification problem. Hence we use cross entropy loss, with the minor caveat that we ignore the padding index. For each batch, we obtain the prediction output of the model. There are several details to take care of when calculating the loss. Namely, we need to cut off the first time step of the target and predicted outputs. This is because the output and predictions will look as follows, assuming no extraneous padding: If you examine the model we designed earlier, you will see that starts from 1, meaning that the tensor’s zeroth index will be left untouched as zeros; hence the 0 in math variable. Therefore, we need to slice the tensors and start from the first index. Moreover, we shape the tensors to be two-dimensional since cross entropy expects the predictions to be two-dimensional; the labels, one-dimensional. Last but not least, we clip the gradients to prevent exploding gradients. Next, we evaluate the model. The structure is almost identical to that of the training loop, except that set the model to evaluation mode and execute every forward pass within the statement. Now, we train the model! We save the model weights if the validation loss is higher than the best validation loss prior to the current iteration. Because I wrote all of this code locally, I didn’t train the model. I did run the training loop a few time in Google Colab, but even that took a while. I decided that I’d save my GPU quota for more interesting models later. The end goal of this tutorial was to gain a deeper understanding of how encoder-decoder sequence-to-sequence models are implemented. I remember reading about neural machine translation through the TensorFlow website around a year ago when I was first learning deep learning with Keras, and it’s just great to see that I’ve made enough progress to be able to understand, digest, and model a basic sequence-to-sequence NMT model with PyTorch. But again, the model we have implemented today is extremely simple in terms of its design, and there are many more enhancements we can apply to it. In the coming posts, we will be taking a look at some better, more advanced seq2seq models that implement features like attention. I hope you’ve enjoyed reading this post. Catch you up in the next one!",1,0,0,0,0,1,0,0
Maximum A Posteriori Estimation,"In a previous post on likelihood, we explored the concept of maximum likelihood estimation, a technique used to optimize parameters of a distribution. In today’s post, we will take a look at another technique, known as maximum a posteriori estimation, or MAP for short. MLE and MAP are distinct methods, but they are more similar than different. We will explore the similar mathematical underpinnings behind the methods to gain a better understanding of how distributions can be tweaked to best fit some given data. Let’s begin! Before we jump right into comparing MAP and MLE, let’s refresh our memory on how maximum likelihood estimation worked. Recall that likelihood is defined as In other words, the likelihood of some model parameter math variable given data observations math variable is equal to the probability of seeing math variable given math variable. Thus, likelihood and probability are inevitably related concepts that describe the same landscape, only from different angles. The objective of maximum likelihood estimation, then, is to determine the values for a distribution’s parameters such that the likelihood of observing some given data is maximized under that distribution. In the example in the previous post on likelihoods, we showed that MLE for a normal distribution is equivalent to setting math variable as the sample mean; math variable, sample variance. But this convenient case was specific only to the Gaussian distribution. More generally, maximum likelihood estimation can be expressed as: It is not difficult to see why trying to compute this quantity may not be as easy as it seems: because we are dealing with probabilities, which are by definition smaller than 1, their product will quickly diverge to 0, which might cause arithmetic underflow. Therefore, we typically use log likelihoods instead. Maximizing the log likelihood amounts to maximizing the likelihood function since log is a monotonically increasing function. Finding the maximum could be achieved multiple ways, such as through derivation or gradient descent. As the name suggests, maximum a posteriori is an optimization method that seeks to maximize the posterior distribution in a Bayesian context, which we dealt with in this post. Recall the Bayesian analysis commences from a number of components, namely the prior, likelihood, evidence, and posterior. Concretely, The objective of Bayesian inference is to estimate the posterior distribution, whose probability distribution is often intractable, by computing the product of likelihood and the prior. This process could be repeated multiple times as more data flows in, which is how posterior update can be performed. We saw this mechanism in action with the example of a coin flip, given a binomial likelihood function and a beta prior, which are conjugate distribution pairs. Then what does maximizing the posterior mean in the context of MAP? With some thinking, we can convince ourselves that maximizing the posterior distribution amounts to finding the optimal parameters of a distribution that best describe the given data set. This can be seen by simply interpreting the posterior from a conditional probability point of view: the posterior denotes the probability of the value of the model parameter is math variable given data math variable. Put differently, the value of math variable that maximizes the posterior is the optimal parameter value that best explains the sample observations. This is why at its heart, MAP is not so much different from MLE: although MLE is frequentist while MAP is Bayesian, the underlying objective of the two methods are fundamentally identical. And indeed, this similarity can also be seen through math. And we see that (5) is almost identical to (3), the formula for MLE! The only part where (5) differs is the inclusion of an additional term in the end, the log prior. What does this difference intuitively mean? Simply put, if we specify a prior distribution for the model parameter, the likelihood is no longer just determined by the likelihood of each data point, but also weighted by the specified prior. Consider the prior as an additional “constraint”, construed in a loose sense. The optimal parameter not only has to conform to the given data, but also not deviate too much from the established prior. To get a more intuitive hold of the role that a Bayesian prior plays in MAP, let’s assume the simplest, most uninformative prior we can consider: the uniform distribution. A uniform prior conveys zero beliefs about the distribution of the parameter, i.e. all values of math variable are equally probable. The implication of this decision is that the prior collapses to a constant. Given the nature of the derived MAP formula in (5), constants can safely be ignored as it will not contribute to argument maximization in any way. Concretely, Therefore, in the case of a uniform prior, we see that MAP essentially boils down to MLE! This is an informative result that tells us that, at their core, MLE and MAP seek to perform the same operation. However, MAP, being a Bayesian approach, takes a specified prior into account, whereas the frequenting MLE simply seeks to dabble in data only, as probabilities are considered objective results of repeated infinite trials instead of subjective beliefs as a Bayesian statistician would purport. I hope you enjoyed reading this post. See you in the next one!",0,0,0,0,1,0,0,0
An Introduction to Markov Chain Monte Carlo,"Finally, here is the post that was promised ages ago: an introduction to Monte Carolo Markov Chains, or MCMC for short. It took a while for me to understand how MCMC models work, not to mention the task of representing and visualizing it via code. To add a bit more to the excuse, I did dabble in some other topics recently, such as machine learning models or information theory, which is also partially why this post was delayed quite a bit. Nevertheless, it’s finally here and ready to go. In this post, we will take a look at the Metropolis-Hastings algorithm, the simplest variant among the family of MCMC models. Let’s see what the Bayesian hype is all about. It’s been a while since we last discussed Bayesian inference, so it’s probably a good idea to start with a brief recap. Bayesian statistics commences from Bayes’ theorem, a seminal statement in probability theory that can be expressed as follows Alternatively, Bayes’ theorem can be stated more generally in the context of some partitioned sample space, For simplicity, I omit the equation for the case involving continuous random variables. Simply put, the summation experssion in the denominator would simply be replaced with that involving integration. The power of the proposition underlying Bayes’s theorem really comes into light when we consider it in the context of Bayesian analysis. The objective of Bayesian statistical analysis is to update our beliefs about some probability, known as the posterior, given a preestablished belief, called the prior, and a series of data observations, which might be decomposed into likelihood and evidence. Concretely, This statement is equivalent to where math variable denotes the likelihood function. In plain language, Bayesian statistics operates on the assumption that all probabilities are reflections of subjective beliefs about the distribution of some random variable. A prior expectation or belief we might have about this distribution is referred to as the prior. Then, we can update our prior belief based on sample observations, resulting in a posterior distribution. Roughly speaking, the posterior can be considered as the “average” between the prior and the observed data. This process, which we went over in detail in this post, is at the heart of Bayesian inference, a powerful tool through which data and distributions can be understood. Theoretically, everything should work fine: given some prior and some sample observation data, we should be able to derive a posterior distribution for the random variable of interest. No big deal. Or so it seems. If we take a look again at equation (3), we will realize that there is an evidence term that we have to calculate sitting in the denominator. The formula for evidence can be expressed as follows: Computing this quantity is not as easy as it may appear. Indeed, this is one of the reasons why the Bayesian way of thinking was eschewed for so long by statisticians: prior to the advent of calculators and computers, mathematicians had trouble deriving the closed-form expression for the evidence term with just pen and paper. We might consider options other than direct calculation, such as Monte Carlo approximation or deriving a proportionality experssion by assuming evidence to be a constant. Indeed, the latter is the approach we took in this post on Bayesian inference. Using a beta prior and binomial likelihood, we used the property of conjugacy to derive a formula for the posterior. However, this raises yet another question: what if the prior and likelihood do not have a conjugate relationship? What if we have a very messy prior or complicated likelihood function, so convoluted to the point that calculating the posterior is near impossible? Simple Monte Carlo approximation might not do because of a problem called the curse of dimensionality: the volume of the sample space increases exponentially with the number of dimensions. In high dimensions, the brute force Monte Carlo approach may not be the most appropriate. Markov Chain Monte Carlo seeks to solve this conundrum of posterior derivation in high dimensions sample space. And indeed, it does a pretty good job of solving it. How does Markov Chain Monte Carlo get around the problem outlined above? To see this, we need to understand the two components that comprise Markov Chain Monte Carlo: Markov chains and Monte Carlo methods. We covered the topic of Markov chains on two posts, one on PageRank and the other on the game of chutes and ladders. Nonetheless, some recap would be of help. Wikepedia defines Markov chains as follows: A Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. In other words, a Markov chain is a method of generating a sequence of random variables where the current value of that random variable probabilistically dpends on its prior value. By recursion, this means that the next value of that random variable only depends on its current state. To put this into context, we used Markovian analysis to assess the probability that a user on the internet would move to one site to another in the context of analyzing Google’s PageRank algorithm. Markov chains also popped up when we dealt with chutes and ladders, since the next position of the player in game only depends on their current position on the game board. These examples all demonstrate the Markov property, also known as memorylessness. Later on, we will see how Markov chains come in handy when we decide to “jump” from one number to the next when sampling from the posterior distribution to derive an approximation of the parameter of interest. We also explored Monte Carlo in some detail here on this blog. For those of you who haven’t already, I highly recommend reading the post, as we developed a good intuition of when Monte Carlo simulations can come in handy to deal with tasks of varying difficulty. To cut to the chase, Monte Carlo methods are used to solve intractable problems, or problems that require expensive computing. Instead of systematically deriving a closed-forrm solution, we can alternatively opt for a scheme of random sampling and hope that, with a sufficient sample size, we would eventually be able to derive an approximation of the parameter. Although this seems stupid at first, it is an incredibly powerful approach to solving many problems, including the one presented here involving posterior calculation in Bayesian inference. The Metropolis-Hastings algorithm is one of the first Markov Chain Monte Carlo model that was developed in the late 20th century to simulate particle movement. More advanced MCMC models have been introduced since; however, the Metrapolis-Hastings algorithm still deserves our attention as it demonstrates the basis of how many Markov Chain Monte Carlo models operate. Let’s get into the details of the model. At the heart of Metrapolis-Hastings is the proposal distribution, which we use to simulate the Markov chain random walk part of the model. Setting the parameters for this proposal distribution can be done arbitrarily, i.e. we can set it to be any random numbers. Theoretically, regardless of the parameters of the proposal distribution, the MCMC model would give us the same result after infinite iterations of sampling. In the Metrapolis-Hastings model, the proposal distribution is assumed as normal. Next, we have to decide if the current value of math variable is a value to accept or not. Accepting a randomly sampled value means adding it to our list of historic observations—if we draw a histogram of the entries in this list, it is our hope that we would end up with a close approximation of the posterior distribution. Accepting this value is often referred to as a “jump,” because we can visualize this process as a random walk in the posterior sample space from point math variable to math variable. The question is, how do we decide to jump or not? The answer lies in Bayes’ theorem: If, for example, we should accept the value and perform the jump, because this means that the new proposed parameter does a better job of explaining the data than does the current one. But recall the dilemma we discussed earlier: how do we compute the posterior? After all, the complexity of calcualting evidence was the very reason why scientists came up with MCMC in the first place. Well, here’s a clever trick that might be of use: rearrange (5) in fractional form to get rid of the evidence term in the denominator. In other words, (5) can be reexpressed as which means The evidence term nicely disappears, giving us an expression that we can easily evaluate! This is how Metropolis-Hastings resolves the dilemma of evidence computation—very simple yet some surprisingly effective algebra. But before we move into code, there is something that should be corrected before we move on. In (6), we derived an experssion for the jump condition. The jump condition is not wrong per se, yet a slight modification has to be made to fully capture the gist of Metrapolis-Hastings. The precise jump condition for the sampler goes as follows: where This simply means that we accept the prorposed pararmeter if the quantity calculated in (7) is larger than a random number between 0 and 1 sampled from a uniform distribution. This is why MCMC models involve a form of random walk—while leaving room for somewhat unlikely parameters to be selected, the model samples relatively more from regions of high posterior probability. Now that we have some understanding of how Markov Chain Monte Carlo and the Metropolis-Hastings algorithm, let’s implement the MCMC sampler in Python. As per convention, listed below are the dependencies required for this demonstration. Let’s start by generating some toy data for our analysis. It’s always a good idea to plot the data to get a sense of its shape. The data looks roughly normal. This is because we created the toy data using the function, which generates random numbers from a normal distribution centered around 0. The task for this tutorial, given this data, is going to be estimating the mean of the posterior distribution, assuming we know its standard deviation to be 1. The benefit of working with this dumb example is that we can analytically derive a closed-form exprerssion for the posterior distribution. This is because a normal prior is conjugate with a normal likelihood of known variance, meaning that the posterior distribution for the mean will also turn out to be normal. If you are wondering if this property of conjugacy is relevant to the notion of conjugacy discussed above with Bayesian inference, you are exactly correct: statisticians have a laundry list of distributions with conjugate relationships, accessible on this Wikipedia article. The bottom line is that we can calculate the posterior analytically, which essentially gives us an answer with which we can evaluate our implementation of the Metropolis-Hastings algorithm. The equation for the posterior is presented below. This assumes that the data is normally distributed with known variance and that the prior is normal, representable as For a complete mathematical derivation of (8), refer to this document. As we will see later on, we use (9) to calculate the likelihood and (10) to calculate the prior. For now, however, let’s focus on the analyticial derivation of the posterior. This can be achieved by translating the equation in (8) into code as shown below. Let’s see what the posterior looks like given the prior math variable. For sample observations, we use the toy data set we generated earlier. There we have it, the posterior distribution given sample observations and a normal prior. As expected, we see that the result is normal, a result due to the property of conjugacy. Another observation we might make is that the posterior mean is seems to be slightly larger than 0. This is an expected result given that the mean of the numbers in our toy data set was larger than 0. Because the posterior can be intuited as an “average” between the observed data set and the prior, we would expect the posterior to be centered around a value greater than zero, which is indeed the case. Now that we have a full answer key to our problem, it’s time to build the Metropolis-Hastings sampler from scratch and compare the estimation generated by the sampler with the true analytical posterior we have just derived. Let’s go back to equation (7), which is the crux of the Metropolis-Hastings sampler. To recap, the MCMC sampler works by assuming some value sampled from the proposal distribution, calculating the likelihood and posterior, and seeing if the new proposed value is worth accepting, i.e. if it is worth making a jump in the random walk. All of this sounds pretty abstract when written in words, but it is a simple idea encapsulated by (7). Let’s build the Metropolis-Hastings sampler by implementing the algorithm described above in code as shown below. I looked at Thomas Wiecki’s implementation for reference and modified it to suit the purposes of this post. Although Markov Chain Monte Carlo sounds complicated, really it is achieved by this single block of code. Of course, this code is limited in that is only applicable to a very specific situation, namely the task of deriving the posterior given a normal prior and a normal likelihood with known variance. Nevertheless, we can glean so much insight from this fascinating function. Let’s quickly test the function by making it sample five estimations of the mean parameter. As expected, the sampler starts from the 0, which is the default argument and took a jump at the second sample. After that jump, the sampler rejects the next three values sampled from the proposal distribution, as it stayed dormant at the value 0.17414333. However, with more iterations, we would expect the function to make more jumps, gradually painting a picture of what the posterior should look like. In fact, we can create what is called the trace plot to see which values were sampled by the Metropolis-Hastings algorithm. Trace plots are important because they tell us whether or not our model is well-calibrated, i.e. a sufficient amount of state changes occur. The trace plot contains the trace of 15000 accepted values sampled from the proposal distribution. We see that there are some fluctuations, indicating that state transitions occurred, but also that there seems to be values that the sampler preferred over others. Eyeballing the trace plot, the “comfort zone” seems to be slightly above 0, as we expect. To illustrate the importance of trace plots, let’s see an example involving a bad setup involving a proposal distribution with too small a variance. The trace plot below shows that, although the MCMC model does manage to sample many values, it likes to stay too much in its current state, thus making taking much longer for the sampler to properly estimate the posterior by sampling a wide range of values. The bottom line is that setting the right proposal distribution is important, and that trace plots are a good place to start to check if the proposal distribution is set up properly. Now, it’s time to look at the answer key and see if our sampler has done well. Let’s plot sampled by our Metropolis-Hastings sampler with the analytic posterior to see if they roughly match. Fantastic! Although the estimated posterior is not exactly equal to the analytic posterior, the two are quite similar to each other. We could quantify how similar or different they are by using metrics such as KL divergence, but for simplicity’s sake, let’s contain the post within the realm of Bayes as we have done so far. This goes to show just how useful and powerful Markov Chain Monte Carlo can be: even if a complicated likelihood function in high dimensional space, we would be able to use a similar sampling sequence to estimate the posterior. What’s even more fascinating about Markov Chain Monte Carlo is that, regardless of the value we start off with in the proposal distribution, we will eventually be able to approximate the posterior. This is due to the Markov chain part of MCMC: one of the most interesting properties of Markov chains is that, no matter where we start, we end up in the same stationary distribution. Together, these properties makes MCMC models like Metropolis-Hastings incredible useful for solving intractable problems. is a library made specifically for Bayesian analysis. Of course, it includes functions that implement Markov Chain Monte Carlo models. Although building the Metropolis-Hastings algorithm from scratch was a worthy challenge, we can’t build models from scratch every time we want to conduct from Bayesian analysis involving an intractable posterior, which is why packages like always come in handy. With just a few lines of code, we can perform the exact same operation we performed above. In fact, let’s compare our Metropolis-Hastings sampler with the built-in function in the library. Pretty similar, wouldn’t you say? Markov Chain Monte Carlo is a powerful method with which we can estimate intractable posterior distributions. It is undoubtedly one of the most important tools that a Bayesian statistician should have under their belt. And even if you are frequentist, I still think MCMC models are worth looking at because it’s cool to see just how easily we can estimate a distribution with little to no knowledge about the mathematics involved in calculating the posterior. It’s also fascinating to see how the marriage of two seemingly unrelated concepts that arose out of different contexts–Monte Carlo methods and Markov chains—can produce such a powerful algorithm. In the next post, we will continue our journey down the Bayesian rabbit hole. Perhaps we will start with another application of Bayesian thinking in machine learning. If naive Bayes is your jam, make sure to tune in some other time.",0,0,0,1,0,0,0,0
Fast Gradient Sign Method,"In today’s post, we will take a look at adversarial attacks. Adversarial attacks have become an active field of research in the deep learning community, for reasons quite similar to why information security and cryptography are important fields in the general context of computer science. Adversarial examples are to deep learning models what viruses and malware are for computers. Of course, as is the case with any metaphor or parallel examples, these are simplifications to guide our intuition, not a robust one-to-one correspondence. This notebook was heavily adapted from the PyTorch official tutorial on adversarial example generation. I basically borrowed the model and weights from the tutorial to avoid having to create and train a model from scratch. Let’s get started! Before we jump into the implementation, let’s briefly go over what adversarial attacks are. In particular, we will be looking at one of the earliest methods of adversarial attack, known as the fast gradient sign method, or FGSM for short. FGSM was introduced in the paper Explaining and Harnesing Adversarial Examples, and has gained a lot of traction since. The paper isn’t the easiest, but it’s also not too difficult to follow. Here, I will try to present some details from the paper while not deviating too much from the bigger picture. Given a simple linear classifier where math variable is the weight matrix, we can think of an adversarial example that contains a small, non-perceivable perturbation to the input. Let’s denote the perturbation as math variable. Then, the logits of the classier would be This means that, given a small perturbation math variable, the actual effect of the perturbation on the logits of the classifier is given by math variable. The underlying idea behind FGSM is that we can find some math variable that causes a change that is non-perceivable and ostensibly innocuous to the human eye, yet destructive and adverse enough for the classifier to the extent that its predictions are no longer accurate. Let’s put this into context by considering an example. Say we have some image classifier that receives RBG images as input. Typical RGB images have integer pixel values ranging from 0 to 255. These values are typically preprocessed through division by 255. Hence, the precision of data is limited by this eight-bit bottleneck. This means that, for perturbations below math variable, we should not expect the classifier to output a different prediction. In other words, the addition of math variable should not cause the model to behave any different in the absence of any perturbation. An adversarial example is one that which maximizes the value of math variable to sway the model into making a wrong prediction. Of course, there is a constraint to be placed on math variable; otherwise, we could just apply a huge perturbation to the input image, but then the perturbation could be visible enough to change the ground truth label. Hence, we apply a constraint such that The infinity norm is defined as which, more simply put, means the largest absolute value of the element in the matrix or vector. In this context, it means that the largest magnitude of the element in math variable does not exceed the precision constraint math variable. Then, Goodfellow proceeds to explain the maximum bounds of this perturbation. Namely, given that the maximum bound of the change in activation can be written as where the average magnitude of an element of math variable is given by math variable, and math variable. This tells us that the change in activation given by the perturbation increases linearly with respect to math variable, or the dimension. In other words, in sufficiently high-dimensional contexts, we can expect even a small perturbation capped at math variable to produce a perturbation big enough to render the model susceptible to an adversarial attack. Such perturbed examples as referred to as adversarial examples. The equations above demonstrated that the degree of perturbation increases as dimensionality increases. In other words, we established that creating adversarial examples are possible via infinitesimal perturbations. In this section, let’s dive into the specifics of FGSM. The idea behind FGSM is surprisingly simple: we do opposite of the typical gradient descent in order to maximize the loss, since confusing the model is the end goal of adversarial attack. Therefore, we consider math variable, the model’s input, to be a trainable parameter. Then, we add the gradient to its original input variable to create a perturbation. Mathematically, this can be expressed as follows: where math variable represents the cost function. Then, we can create an adversarial example via This is the crux of the fast gradient sign method: we use the sign of the gradient, multiply it by some small value, and add that perturbation to the original input to create an adversarial example. One way to look at this is in terms of first-order approximation. Recall that In this context, we can consider math variable to be the cost function math variable, which then turns into Then, the goal of an adversarial attack is to maximize the second term in the addition. Since there is an infinity norm constraint on the perturbation, namely with some thinking we can convince ourselves that the perturbed example that maximizes the loss function is given by Let’s import some modules, download and process MNIST data, and declare a model on which to perform an adversarial attack. These are all routine operations, so I’ll drop a lot of the explanation. Below is a tody convolutional neural network model designed to classify MNIST handwritten digits. We load the model and put it into evaluation mode, since we won’t be updating the weights of the model. Since we want to create perturbations for each image, we need to set the batch size to be one. Now, let’s actually implement FGSM. The function below takes an image and some small value epsilon as input. Using the sign of the gradient, we then create a small perturbation to which we add to the image. Note that we clamp the value of the perturbed image to have values between 0 and 1, since that is the valid range for a normalized pixel value. Next, we define a custom loop function that examines each image in the test loader and generates predictions. If the prediction is wrong in the first place without any perturbation, we move on; the point of an adversarial attack is to create an adversarial example from an innocuous example that the model would have otherwise predicted correctly. A key implementation detail is the fact that we set the property of the image to be true so that we can later obtain the gradients for the input. We then back propagate on the loss to obtain the gradient values, then create a perturbed image using the function we have defined above. For demonstration purposes, we store the first few successful adversarial examples in a list. We also calculate the accuracy of the model and return that value as well. Now we’re done! Let’s define some epsilon values and run the test. Intuitively, we would expect the accuracy of the model to go down as the epsilon value increases; a higher epsilon value corresponds to a larger perturbation, which could more adversely impact the model. Through the print statements, we see that our initial predictions are indeed correct. A simple visualization also helps us see how quickly this drop in accuracy happens. Now comes the fun part: showing the adversarial examples. It might seem like there is a lot going on in the code block below, but really, a lot of it is just some grunt work I’ve done to display the examples in a nice grid with headers and labels. Let’s take a look at the adversarial examples that have been generated for each value of epsilon. It’s really interesting to see how our model could confuse some of these examples. Granted, not all handwritings are obvious, and there are definitely ones that could be tricky. However, as the value of epsilon goes up, it appears that even obvious digits can be mistaken by the model to be some other digit that bears no resemblance. This is adversarial attack in action. In today’s post, we discussed adversarial attacks, specifically the fast gradient sign method. Although I tried to get a little technical by going through each equation in the original paper (and even then, I skipped a few), the overarching idea behind FGSM, as I understand it, is surprisingly simple: neural networks are more linear than we think, and we can create gradient-based, small perturbations that can maximize the loss, thus disrupting the model’s behavior. Since FGSM, other more advanced attack methods have been introduced. Nowadays, building robust models that can withstand such attacks are becoming increasingly important. FGSM is an example of a white-box attack method: in this case, we had full access to the gradients and parameters of the model. However, there are also black-box attacks that are performed by an attacker in the absence of knowledge about the model’s parameters. Perhaps we will discuss more of these in the future. I hope you’ve enjoyed reading this post. See you in the next one.",1,0,0,0,0,1,0,0
First Neural Network with Keras,"Lately, I have been on a DataCamp spree after unlocking a two-month free unlimited trial through Microsoft’s Visual Studio Dev Essentials program. If you haven’t already, make sure to check it out, as it offers a plethora of tools, journal subscriptions, and software packages for developers. Anyhow, one of the courses I decided to check out on DataCamp was titled “Introduction to Deep Learning with Python,” which covered basic concepts in deep learning such as forward and backward propagation. The latter half of the tutorial was devoted to the introduction of the Keras API and the implementation of neural networks. I created this notebook immediately after finishing the tutorial for memory retention and self-review purposes. First, we begin by importing the library as well as other affiliated functions in the module. Note that Keras uses TensorFlow as backend by default. The warning in the code block below appears because this notebook was written on Google Colab, which informs users that the platform will be switching over to TensorFlow 2 in the future. As you might be able to guess from one of the imported modules, the objective of the neural network will be to classify hand-written digits. In doing so, we will be dealing with a classic in machine learning literature known as the the MNIST data set, which contains images of hand-written digits from 0 to 9, each hand-labeled by researchers. The variable denotes the total number of class labels available in the classification task, which is 10. specifies the number of iterations the gradient descent algorithm will run for. Let’s begin by loading the data from . Now we have to slightly modify the loaded data so that its dimensions and values are made suitable to be fed into a neural network. Changing the dimensionality of data can be achieved through the function, which takes in the number of rows and columns as its argument. We convert the numbes into type , then normalize it so that its values are all between 0 and 1. Although we won’t get into too much detail as to why normalization is important, an elementary intuition we might develop is that normalization effectively squishes all values into the same bound, making data much more processable. We also implement one-hot encoding on through the function. Let’s quickly check if the necessary adjustments were made by looking up the dimensions of and , respectively. Looks like the data has been reshaped successfully. Now, it’s finally time to get into the nuts and bolts of a neural network. The simplest neural network is the model, which means that every neuron in one layer is connected to all other neurons in the previous layer. Building a simple neural network is extremely easy in a high level API like Keras. The model below has 784 input nodes. The input layer is then connected to a hidden layer with 512 neurons, which is then connected to a second hidden layer with also 512 neurons. Note that the hidden layer uses the function as its activation function. The dropout layers ensure that our model does not overfit to the data. The last output layer has 10 neurons, each corresponding to digits from 0 to 9. The activation fuction of this last layer is the softmax function, which allows us to interpret the final results as a categorical distribution. Let’s double check that the layers have been formed correctly as per our intended design. Everything looks good, which means we are now ready to compile and train our model. Before we do that, however, it is always a good idea to use the module to ensure that gradient descent stops when no substantial weight adjustments are being made to our model. In other words, when the model successfully finds the local minimum (or preferably the global minimum), the will kick in and stop gradient descent from proceeding with further epochs. We are now ready to go! Let’s compile the model by making some configurations, namely the , , and . Simply put, an specifies which flavor of the gradient descent algorithm we want to choose. The simplest version is known as , or the stochastic gradient descent. can be considered an improved version of the stochastic gradient descent in that its learning rate changes depending on the slope of the loss function, defined here as cross entropy. If you recall, cross entropy is basically a measurement of the pseudo-distance between two distributions, i.e. how different two distributions are. But because cross entropy is often not easy to intuitively wrap our minds around, let’s pass the metric to the function, as shown below. It’s time to train the neural network with the training data, and , over a specified number of epochs. As promised, we will use the to stop graident descent from making unnecessary computations down the road. We also specify that and are components of the validation set. Keras shows us how much our neural network improves over each epoch. This is convenient, but can we do better? The answer is a sure yes. Let’s quickly plot a graph to see how model accuracy improves over time, while cross entropy loss decreases with more epochs. As the last step, we might want to save our trained model. This can be achieved with a single line of code. We can load pre-saved models as well. That’s it for today! Obviously there are a lot more we can do with , such as building deeper neural networks or non-sequential models such as CNN or GAN, but these are topics we might look at a later date when I grow more proficient with the Keras API and deep learning in general. For now, consider this to be a gentle introduction to neural networks with Keras. Thanks for reading! Catch you up in the next one.",1,0,1,0,0,0,0,0
PyTorch RNN from Scratch,"In this post, we’ll take a look at RNNs, or recurrent neural networks, and attempt to implement parts of it in scratch through PyTorch. Yes, it’s not entirely from scratch in the sense that we’re still relying on PyTorch autograd to compute gradients and implement backprop, but I still think there are valuable insights we can glean from this implementation as well. For a brief introductory overview of RNNs, I recommend that you check out this previous post, where we explored not only what RNNs are and how they work, but also how one can go about implementing an RNN model using Keras. This time, we will be using PyTorch, but take a more hands-on approach to build a simple RNN from scratch. Full disclaimer that this post was largely adapted from this PyTorch tutorial this PyTorch tutorial. I modified and changed some of the steps involved in preprocessing and training. I still recommend that you check it out as a supplementary material. With that in mind, let’s get started. The task is to build a simple classification model that can correctly determine the nationality of a person given their name. Put more simply, we want to be able to tell where a particular name is from. We will be using some labeled data from the PyTorch tutorial. We can download it simply by typing This command will download and unzip the files into the current directory, under the folder name of . Now that we have downloaded the data we need, let’s take a look at the data in more detail. First, here are the dependencies we will need. We first specify a directory, then try to print out all the labels there are. We can then construct a dictionary that maps a language to a numerical label. We see that there are a total of 18 languages. I wrapped each label as a tensor so that we can use them directly during training. Let’s store the number of languages in some variable so that we can use it later in our model declaration, specifically when we specify the size of the final output layer. Now, let’s preprocess the names. We first want to use to standardize all names and remove any acute symbols or the likes. For example, Once we have a decoded string, we then need to convert it to a tensor so that the model can process it. This can first be done by constructing a mapping, as shown below. We see that there are a total of 59 tokens in our character vocabulary. This includes spaces and punctuations, such as ` .,:;-‘(num_char, 59)(59,)`. We can now build a function that accomplishes this task, as shown below: If you read the code carefully, you’ll realize that the output tensor is of size , which is different from the explanation above. Well, the reason for that extra dimension is that we are using a batch size of 1 in this case. In PyTorch, RNN layers expect the input tensor to be of size . Since every name is going to have a different length, we don’t batch the inputs for simplicity purposes and simply use each input as a single batch. For a more detailed discussion, check out this forum discussion. Let’s quickly verify the output of the function with a dummy input. Now we need to build a our dataset with all the preprocessing steps. Let’s collect all the decoded and converted tensors in a list, with accompanying labels. The labels can be obtained easily from the file name, for example . We could wrap this in a PyTorch class, but for simplicity sake let’s just use a good old loop to feed this data into our model. Since we are dealing with normal lists, we can easily use ’s to separate the training data from the testing data. Let’s see how many training and testing data we have. Note that we used a of 0.1. We will be building two models: a simple RNN, which is going to be built from scratch, and a GRU-based model using PyTorch’s layers. Now we can build our model. This is a very simple RNN that takes a single character tensor representation as input and produces some prediction and a hidden state, which can be used in the next iteration. Notice that it is just some fully connected layers with a sigmoid non-linearity applied during the hidden state computation. We call at the start of every new batch. For easier training and learning, I decided to use to initialize these hidden states. We can now build our model and start training it. I realized that training this model is very unstable, and as you can see the loss jumps up and down quite a bit. Nonetheless, I didn’t want to cook my 13-inch MacBook Pro so I decided to stop at two epochs. Now we can test our model. We could look at other metrics, but accuracy is by far the simplest, so let’s go with that. The model records a 72 percent accuracy rate. This is very bad, but given how simple the models is and the fact that we only trained the model for two epochs, we can lay back and indulge in momentary happiness knowing that the simple RNN model was at least able to learn something. Let’s see how well our model does with some concrete examples. Below is a function that accepts a string as input and outputs a decoded prediction. I don’t know if any of these names were actually in the training or testing set; these are just some random names I came up with that I thought would be pretty reasonable. And voila, the results are promising. The model seems to have classified all the names into correct categories! This is cool and all, and I could probably stop here, but I wanted to see how this custom model fares in comparison to, say, a model using PyTorch layers. GRU is probably not fair game for our simple RNN, but let’s see how well it does. Let’s declare the model and an optimizer to go with it. Notice that we are using a two-layer GRU, which is already one more than our current RNN implementation. The training appeared somewhat more stable at first, but we do see a weird jump near the end of the second epoch. This is partially because I didn’t use gradient clipping for this GRU model, and we might see better results with clipping applied. Let’s see the accuracy of this model. And we get an accuracy of around 80 percent for this model. This is better than our simple RNN model, which is somewhat expected given that it had one additional layer and was using a more complicated RNN cell model. Let’s see how this model predicts given some raw name string. The last one is interesting, because it is the name of a close Turkish friend of mine. The model obviously isn’t able to tell us that the name is Turkish since it didn’t see any data points that were labeled as Turkish, but it tells us what nationality the name might fall under among the 18 labels it has been trained on. It’s obviously wrong, but perhaps not too far off in some regards; at least it didn’t say Japanese, for instance. It’s also not entirely fair game for the model since there are many names that might be described as multi-national: perhaps there is a Russian person with the name of Demirkan. I learned quite a bit about RNNs by implementing this RNN. It is admittedly simple, and it is somewhat different from the PyTorch layer-based approach in that it requires us to loop through each character manually, but the low-level nature of it forced me to think more about tensor dimensions and the purpose of having a division between the hidden state and output. It was also a healthy reminder of how RNNs can be difficult to train. In the coming posts, we will be looking at sequence-to-sequence models, or seq2seq for short. Ever since I heard about seq2seq, I was fascinated by tthe power of transforming one form of data to another. Although these models cannot be realistically trained on a CPU given the constraints of my local machine, I think implementing them themselves will be an exciting challenge. Catch you up in the next one!",1,0,0,1,0,1,0,0
Moments in Statistics,"The word “moment” has many meanings. Most commonly, it connotes a slice of time. In the realm of physics, moment refers to the rotational tendency of some object, similar to how torque measures the change in an object’s angular momentum. As statisticians, however, what we are interested in is what moment means in math and statistics. In this post, we will attempt to shed new light on the topic of probability distributions through moment generating functions, or MGF for short. The mathematical definition of moments is actually quite simple. And of course, we can imagine how the list would continue: the math expressionth moment of a random variable would be math expression. It is worth noting that the first moment corresponds to the mean of the distribution, math expression. The second moment is related to variance, as math expression. The third moment relates to the symmetry of the distribution, or the lack thereof, a quality which goes by the name of skewness. The fourth moment relates to kurtosis, which is a measure of how heavy the tail of a distribution is. Higher kurtosis corresponds to many outliers, while the converse would signify that the distribution contains little deviations. As you can see, the common theme is that the moment contains information about the defining features of a distribution, which is why it is such a convenient way to present information about a distribution. As the name suggests, MGF is a function that generates the moments of a distribution. More specifically, we can calculate the math expressionth moment of a distribution simply by taking the math expressionth derivative of a moment generating function, then plugging in 0 for parameter math expression. We will see what math expression is in a moment when we look at the default formula for MGF. This sounds good and all, but why do we want an MGF in the first place, one might ask. Well, given that moments convey defining properties of a distribution, a moment generating function is basically an all-in-one package that contains every bit of information about the distribution in question. Enough of the abstract, let’s get more specific by taking a look at the mathematical formula for an MGF. If math expression is a continuous random variable, we would take an integral instead. Now, you might be wondering how taking the math expressionth derivative of math expression gives us the math expressionth moment of the distribution for the random variable math expression. To convince ourselves of this statement, we need to start by looking at the Taylor polynomial for the exponential. It’s not difficult to see the coherency of this expression by taking its derivative—the derivative of the polynomial is equal to itself, as we would expect for math expression. From here, we can sufficiently deduce that The coherency of (3) can simply be seen by making the substitution math expression. To continue, now that we have an expression for math expression, we can now calculate math expression, which we might recall is the definition of a moment generating function. where the second equality stands due to linearity of expectation. All the magic happens when we derive this function with respect to math expression. At math expression, all terms in (5) except for the very first one go to zero, leaving us with In other words, deriving the MGF once and plugging in 0 to math expression leaves us with the first moment, as expected. If we derive the function again and do the same, And by induction, we can see how the math expressionth derivative of the MGF at math expression would give us the math expressionth moment of the distribution, math expression. The easiest way to demonstrate the usefulness of MGF is with an example. For fun, let’s revisit a distribution we examined a long time ago on this blog: the Poisson distribution. To briefly recap, the Poisson distribution can be considered as an variation of the binomial distribution where the number of trials, math expression, diverges to infinity, with rate of success defined as math expression. This is why the Poisson distribution is frequently used to model how many random events are likely in a given time frame. Here is the probability distribution of the Poisson distribution. Note that math expression denotes the number of occurrences of the random event in question. The task here is to obtain the mean of the distribution, i.e. to calculate the first moment, math expression. The traditional, no-brainer way of doing this would be to refer to the definition of expected values to compute the sum Computing this sum is not difficult, but it requires some clever manipulations and substitutions. Let’s start by simplifying the factorial in the denominator, and pulling out some expressions out of the sigma. where the third equality stands due to the variant of the Taylor series for the exponential function we looked at earlier: Therefore, we have confirmed that the mean of a Poisson distribution is equal to math expression, which aligns with what we know about the distribution. Another way we can calculate the first moment of the Poisson is by deriving its MGF. This might sound a lot more complicated than just computing the expected value the familiar way demonstrated above, but in fact, MGFs are surprisingly easy to calculate, sometimes even easier than using the definition expectation. Let’s begin by presenting a statement of the MGF. Let’s factor out terms that contain lambda, which is not affected by the summation. Again, we refer to equation (9) to realize that the sigma expression simplifies into an exponential. In other words, From this observation, we can simplify equation (10) as follows: And there is the MGF of the Poisson distribution! All we have to do to obtain the first moment of the Poisson distribution, then, is to derive the MGF once and set math expression to 0. Using the chain rule, At math expression, So we have confirmed again that the mean of a Poisson distribution is equal to math expression. Let’s take another distribution as an example, this time the exponential distribution. We have not looked specifically at the exponential distribution in depth previously, but it is a distribution closely related to the Gamma distribution, which we derived in this post. Specifically, when parameter math expression in a Gamma distribution, it is in effect an exponential distribution. Perhaps we will explore these relationships, along with the Erlang distribution, in a future post. For now, all we have to know is the probability density function of the exponential distribution, which is This time, the task is to obtain the third moment of the distribution, i.e. math expression. But the fundamental approach remains identical: we can either use the definition of expected values to calculate the third moment, or compute the MGF and derive it three times. At a glance, the latter seems a lot more complicated. However, it won’t take long for us to see that sometimes, calculating the MGF is sometimes as easy as, if not easier than, taking the expected values approach. Let’s twist up the order and try the MGF method first. We can pull out the lambda and combine the exponential terms to get This is an easy integral. Let’s proceed with the integration and evaluation sequence: Now, all we have to do is to derive the result in (12) three time and plug in math expression. Although calculating the third order derivative may sound intimidating, it may seem easier in comparison to evaluating the integral which would require us to use integration by parts. In the end, both (12) and (13) are pointing at the same quantity, namely the third moment of the exponential distribution. Perhaps the complexity of calculating either quantity is similar, and the question might just boil down to a matter of preference. However, this example shows that the MGF is a robust method of calculating moments of a distribution, and even more, potentially less computationally expensive than using the brute force method to directly calculate expected values. This was a short post on moments and moment generating functions. Moments was one of these terms that I had come across on Wikipedia or math stackexchange posts, but never had a chance to figure out. Hopefully, this post gave you some intuition behind the notion of moments, as well as how moment generating functions can be used to compute useful properties that explain a distribution. In the next post, we will take a brief break from the world of distributions and discuss some topics in information theory that I personally found interesting. If you would like to dwell on the question like “how do we quantify randomness,” don’t hesitate to tune in again in a few days!",0,0,0,0,1,0,0,1
Convolutional Neural Network with Keras,"Recently, a friend recommended me a book, Deep Learning with Python by Francois Chollet. As an eager learner just starting to fiddle with the Keras API, I decided it was a good starting point. I have just finished the first section of Part 2 on Convolutional Neural Networks and image processing. My impression so far is that the book is more focused on code than math. The apparent advantage of this approach is that it shows readers how to build neural networks very transparently. It’s also a good introduction to many neural network models, such as CNNs or LSTMs. On the flip side, it might leave some readers wondering why these models work, concretely and mathematically. This point notwithstanding, I’ve been enjoying the book very much so far, and this post is a reflection of just that. Today, we will use TensorFlow’s module to build a convolutional neural network for image detection. This code is based on what I have learned from the book, so much credit goes to Deep Learning with Python. I have also looked at Machine Learning Mastery blog for additional reference. Let’s begin! Below are the modules that we will need to import for this demonstration. Note that this Jupyter Notebook was written on Google Colaboratory. The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x. We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the magic: more info. The function loads the CIFAR10 data set from module, then applies some basic preprocessing to make it more usable for deep learning purposes. The CIFAR10 data set contains 50000 training images, each labeled with 1 of 10 classes, ranging from cats, horses, and trucks to airplanes. This is a classic classification task. The preprocessing occurs on two levels: first, the images are normalized so that its pixels takes values between 0 and 1. The training labels are transformed from an integer class label to a one-hot encoded vector. Let’s load the data to proceed with our analysis. Let’s see what the dimensions of the data are. Because the CIFAR10 data set include color images for its training set, we would expect three channels, each corresponding to red, green, and blue (hence RGB). As expected, it appears that the training data is a tensor of four dimensions, while the target labels are 10 dimensional vectors. To get a better idea of what the CIFAR10 data looks like, here is a basic function that will display the images in the data set for us using . We can see that, although the images are very pixelated, it is somewhat possible to make out what each image is showing. Of course, this task is going to be a lot more difficult for our neural network. Now is finally the time to buid the neural network. This network pretty much follows the standard vanilla convolutional neural network model, which typically involves stacking convolution, batch normalization, and pooling layers on top of each other. The dropout layers were also added so as to minimize any potential overfitting. The argument was configured as , named after Kaiming He who found the optimal weight initialization kernel for convolutional layers. The argument ensures that the the feature maps are not downsized too quickly due to repeated applications of convolution and pooling. The layer simply normalizes the tensor returned from the previous layer. There are ongoing research as to what effect batch normalization has on neural networks, but the general consensus is that it helps the model learn more quickly and efficiently. The function returns the predefined sequential model, compiled using the configurations as shown below. Let’s take a look at the summary of the model. The summary shows that this model has 551,466 trainable parameters. The memory capacity of this model is not big, but it is definitely larger than the network we built in the previous post using the Keras API. Now that the model is ready to be deployed, we need to train and test the model. But before that, let’s quickly define a function that will provide us with a visualization of how the model is learning. This function is very similar to the one used in the previous post—all it does it that it plots the model’s accuracy and cross entropy loss with each epoch. This visualization will help us see whether our model is actually learning with each epoch, and whether or not overfitting is occurring at any point in training. The last piece of the puzzle we need is the function. This function is essentially wraps all the functions we have created previously, first by loading the data, training using that data, building a model, training a model, and calling on the function to provide a visualization of the model’s learning curve. One minor tweak I used to spice up this function is the , which basically creates more images for the neural network to train on by slightly modifying existing images in the training set. These modifications involve shifting, zooming, and flipping. When we don’t have enough data to train our model on, using the can be useful. Finally, let’s see how well our model performs! The result shows that the model has learned decently well, with testing accuracy of approximately 80 percent. This is not the best result, but it is certainly not bad, especially given the fact that the images in the dataset, as we have seen above, are very pixelated and sometimes difficult for even humans to decipher and categorize. That’s all for today. It’s fascinating to see how CNNs are capable of perceiving images and categorizing them after appropriate training. But as we all know, the potential of neural networks far extends beyond image classificaiton. In a future post, we might look at more complicated models, such as RNN or LSTMs, to achieve even cooler tasks!",1,0,0,0,0,0,0,0
GAN in PyTorch,"In this blog post, we will be revisiting GANs, or general adversarial networks. This isn’t the first time we’ve seen GANs on this blog: we’ve implemented GANs in Keras, and we have also looked at the mathematics behind GANs. Well, this is somewhat of a repeat of what we’ve done, since all we’re doing here is reimplementing GANs using PyTorch, but I still think it’s worth a revisit. As always, we start by importing the necessary modules in PyTorch, as well as other libraries. Although we’ll be using CPU instead of GPUs, it’s always good to have a object set up so that we can utilize GPUs should we run this script or notebook in a different environment. We’re not going to do anything too fancy (and I learned from experience that vanilla GANs are incredibly hard to train and can produce artifact-ridden results). Let’s keep in simple this time and try to implement a GAN that generates MNIST images. Below are the configurations we will use for our GAN. We can define a simple transformation that converts images to tensors, then applies a standard normalization procedure for easier training. Next, we create a dataset object and a data loader that batches and shuffles post-transformation images for us. You might recall that GANs are composed of two different networks: a discriminator and a generator. The discriminator, also metaphorically referred to as the police, is a classier that tries to determine whether a given data is real or fake, i.e. produced by the generator network. The generator, on the other hand, tries to generate images that are as realistic as possible, and so is referred to as the counterfeiter. Below is our generator network. Although we could have created DCGANs, or deep convolutional adversarial networks, let’s go simple here and just use fully connected layers. Notice that I’ve used the PyTorch API instead of using class-based models. In this particular instance, we won’t have a complicated forward method, so the sequential API will suffice. Next, we create the generator. It is also a sequential model, inside of which are stacks of linear layers with ReLU activations. Before we jump into training, let’s move these networks to the object we’ve created earlier. The interesting part starts here. Notice that we have different optimizers for the discriminator and the generator. This is expected, since we are going to be training two different networks in a different manner. Before we get into the details of training, here is a simple utility function in which we zero the gradients for both the generator and the discriminator. We want the discriminator to be able to distinguish real from fake images. Therefore, the discriminator will have a combined loss: the loss that comes from falsely identifying real images as fake, and the loss that comes from confusing fake, generated images as real ones. For the generator, the loss is actually dependent upon the classifier: the loss comes from the classifier correctly identifying generated images as fake. Let’s see what this means below. And we see that the generator was able to produce somewhat confusing hand-written images of digits! Granted, this is far from perfect, and there are images that look somewhat funky. However, there are also somewhat realistic images here and there, and it is impressive that a simple densely connected network was able to achieve this performance. Had we used CNNs, the result might have been even better. Personally, I enjoyed this short implementation experiment because I was able to see just how easy and intuitive it is to use PyTorch. I’ll come back in the next post with (probably) another PyTorch tutorial. Catch you in the next one!",1,0,0,0,0,1,0,0
"PyTorch, From Data to Modeling","These past few weeks, I’ve been powering through PyTorch notebooks and tutorials, mostly because I enjoyed the PyTorch API so much and found so many of it useful and intuitive. Well, the problem was that I ended up writing something like ten notebooks without ever publishing them on this blog. So really, I’m going over some old notebooks I’ve coded out more than a month ago to finally make it live. That’s enough excuses, let’s get into the basics of PyTorch modeling in this notebook with the CIFAR10 dataset and some basic CNNs. The setup is pretty simple here. We import some modules and functions from PyTorch, as well as to be able to show some basic training plots. One thing I have noticed is that a lot of people do something like which I personally don’t really get, because you can easily just do If you ask me, I think the latter is more elegant and less cluttered (after all, we don’t have to repeat twice). I don’t think the two import statements are functionally different, but if I do figure out any differences, I will make sure to update future notebooks. One of the many nice things about PyTorch is the clean, intuitive API. PyTorch comes with good GPU support, and one of the main ways through which this can be done is by creating a object. Because I am running this notebook on my MacBook Pro, which obviously does not come with Nvidia cuda-enabled graphics cards, the device is set as the CPU. Now, I can “move” tensors and models up to the GPU by doing something like and these statements would allow inference and training to occur within the GPU. And below are some constants I will be using in this notebook. Namely, we will run training for a total of 4 epochs, with a batch size of 32 and a learning rate of 0.001. Now that we have all the things we need, let’s jump into some data preparation and modeling. Another thing I love about PyTorch is the sheer ease with which you can preprocess data. PyTorch makes it incredibly easy to combine and stack multiple transforms to create custom transformations to be applied to the dataset. The easiest way to go about this is to use the method, which looks like this: Here, we are applying to transformations: the first changes the dataset and casts it into PyTorch tensors,, and the second one normalizes the dataset to have a mean of 0.5 and a standard deviation of also 0.5 across all three channels of RGB. How can we apply this transform? Well, we can pass it to initialize the datasets as shown below: Because I already have the CIFAR10 downloaded in the directory of my local, PyTorch does not download the dataset again. We could go with the dataset as-is, but we can use the class to further batch and shuffle the dataset, which we normally want 99 percent of the time. This is as simple as calling and passing in the dataset we want to load. If we loop through the , for instance, we can see that it is giving us a nice batch of 32 photos. Note that the dimensions are in the form of . As for the labels, we get 32 values where each number corresponds to an image. As the last step, let’s just make sure that we know what each of these labels correspond to. The is a tuple of strings that translate label indices to actual strings we can interpret. For example, if we see the label , we know that it denotes , which is . Modeling in PyTorch is the probably the part I love the most. TensorFlow’s sequential API is a great way to start, and PyTorch also provides the same sort of way of building sequential models. However, once you try to build anything that’s more complicated than that, I think PyTorch’s class-based way of approaching modeling makes a lot more intuitive sense and provides more room for experimentation and customization. Before getting into too much detail, below is a very simple CNN we will refer to as an example throughout this post. The first thing you will realize is that the model itself is a class that inherits from . This is a pattern you will see all the time with PyTorch models. is a super class from which we can inherit to build anything from full-fledged models to custom blocks or layers to be used in some other larger model. In the initialization function, we also define a number of layers that will be used in forward propagation. You might be wondering why these have to initialized in the initialization function, as opposed to the forward function itself. While I don’t have a complete, technically cogent explanation to that question, intuitively, we can understand a model’s layers as being components of the model itself. After all, the weights of these layers are adjusted with each iteration or epoch. In that sense, we want the layers to be attached to the model instance itself; hence the OOP design of PyTorch’s model class. In this particular instance, we define a number of convolutional layers, a pooling layer, and two fully connected layers used for classification output. The declaration of the layers themselves are not too different from other frameworks, such as TensorFlow. Also, I’ve written out all the named arguments so that it is immediately clear what each argument is configuring. Once we’ve declared all the necessary components in the initialization function, the next steps to actually churn out forward propagation results given some input. In PyTorch, this is done by defining the function. As you can see above, we basically call on the layers we’ve declared in the initialization function via and pass in any parameters we want. Since this is a very simple CNN, you will see that there is nothing exciting going on; we are simply getting the output of the previous layer and passing that as input into the next. After going through some convolutions and fully connected layers, we can return the result. There are one caveats worth mentioning here, which is the use of . There is a that I could have easily used, and indeed there is an entire discussion on the PyTorch forum on the difference between the two. The bottom line is that they are pretty similar for our purposes. The most salient difference between the two is that the functional approach cannot be used when declaring a sequential model. However, since we are defining a custom forward function, this limitation does not apply. Personally, I prefer the functional because it means that there is one less layer to declare in the initialization function. However, it’s probably better to err on the side of the if you’re not totally sure. Now that we’ve designed and implemented a model, it’s time to train it. This is the part where people might argue that TensorFlow 2 or Keras is superior to PyTorch. In Keras, you can simply call to train the model. However, in PyTorch, this is not necessarily the case, unless you really want to imitate the Keras API and define a function yourself. PyTorch is more low-level in that you need to define a custom training loop manually. However, I actually prefer this low-levelness because it requires me to really think through what is happening in each iteration, namely what the dimension of each batch is, what the model expects to receive as input in the forward computation, and what loss function is appropriate given the output and label. Let’s see what all of this means. First, we begin by actually initializing an instance of the model, a loss function named , and an , which is Adam in this case. A quick note of caution: if you dig into the PyTorch documentation or look at other example classifiers, you will realize, like me, there are two loss functions you can typically use: and , or negative log likelihood loss. The difference between the former and latter is that, while the former applies a softmax function to the output before calculating the actual loss, the latter does not. In our case, since we simply output the raw logits instead of applying a softmax calculation, we need to use the former. Let’s return where we were. Before we jump into training and defining the training loop, it’s always a good idea to see if the output of the model is what you’d expect. In this case, we can simply define some random dummy input and see if the output is correct. Now that we’ve verified the input and output dimensions, we can move onto defining the training loop. Defining the training loop may seem difficult at first, especially if you’re coming from a Keras background, but actually a lot of it is boiler plate, and things are not as difficult as they may seem. First, we define a list to hold the loss values per iteration. We will be using this list for visualization later. The exciting part comes next. For each epoch, we load the images in the . Note that the loader returns a tuple of images and labels, which we can unpack directly within the loop itself. We then move the two objects to , which would be necessary if we were running this one a Cuda-enabled computer. Then, we calculate the loss by calling , the loss function, and append the loss to the list. Note that we have to call since itself is a one-by-one PyTorch tensor. Then comes the important part where we perform backprop. The idea is that we would The three steps correspond to each of the lines in the code above, starting from . As you might be able to guess from the name of the function, we zero the gradients to make sure that we aren’t accumulating gradient values from one iteration to the next. Calling corresponds to calculating the new gradient values, and performs the backprop. The last block of code is simply a convenient print function I’ve written to see the progress of training at certain intervals. As you can see, the loss seems to be decreasing for the most part, although it is jumpy at times. Indeed, plotting makes it clear that the loss has been decreasing, though not entirely in a steady, monotonous fashion. In retrospect, we could have probably added a batch normalization layer to stabilize and expedite training. However, since this post is largely meant as an introduction to PyTorch modeling, not model optimization or design, the example suffices. Now we have finally reached the last step of the development cycle: testing and evaluating the model. This last step will also require us to write a custom loop as we receive batches of data from the object we’ve created above. The good news, however, is that the testing loop is not going to look too much different from the training loop; the only difference will be that we will not be backpropagating per each iteration. We will also be using to make sure that the model is in the evaluation mode, not its default training mode. This ensures that things like batch normalization and dropout work correctly. Let’s talk briefly about the details of this loop. Here, the metric we’re collecting is accuracy. First, we generally see how many correct predictions the model generates. Then, we also see per-class accuracy; that is, whether our model is good at predicting any particular class. This ensures that the model’s performance is balanced throughout all labels. And here is the result! An overall accuracy of 70 percent is definitely not impressive, and we certainly could have done better by building a deeper model, or by using more complex architectures. However, this isn’t the worst performance considering the fact that we only had three convolutional layers. The more important takeaway from this tutorial is how to prepare data, build models, and train and evaluate them through a custom loop. From the tone and style of my writing, it is perhaps immediately clear to you that I am not officially a PyTorch fanboy. Yes, I will admit that I loved Keras for its simplicity, but after having spent more time learning python and DL, I now much prefer the freedom provided by PyTorch’s reasonably abstract API. I hope this notebook provided you with a nice, simple introduction to PyTorch. In the coming notebooks, we will take a deeper dive into implementing models with PyTorch, starting from RNNs all the way up to classic SOTA vision models like InceptionNet, ResNet, and seq2seq models. I can definitely tell you that these are coming, because, as funny as this sounds, I already have all the notetbooks and code ready; I just have to annotate them. I hope you’ve enjoyed reading this post. Catch you up in the next one!",1,0,0,0,0,1,0,0
Complex Fibonacci,"A few days ago, a video popped up in my YouTube suggestions. We all know how disturbingly powerful the YouTube recommendation algorithm is: more than 90 percent of the times, I thoroughly enjoy all suggestions put forth by the mastermind algorithm. This time was no exception: in fact, I enjoyed it so much that I decided to write a short blog post about it. Also a quick plug: if you haven’t checked out Matt Parker’s channel, I highly recommend that you do. Let’s dive right into today’s topic: extending the fibonacci sequence to complex numbers. We all know what the fibonacci sequence looks like, but for formality and notational clarity’s sake, here is what the fibonacci sequence looks like: There are some different conventions as to where the sequence starts. I personally prefer the one that starts from zero, with zero indexing. Here is what I mean: Implementing fibonacci numbers in code is one of the most common exercises that are used to teach concepts such as recursion, memoization, and dynamic programming. This is certainly not the point of today’s post, but here is an obligatory code snippet nonetheless. The code above is the standard fibonacci function as we know it, implemented with simple bare bone recursion. While the code works perfectly fine, there is a fatal problem with this code: it recalculates so many values. Namely, in calling , the program goes all the way up to the math variableth fibonacci number. Then, in the next call of , the program recalculates the same values calculated earlier, up until the math variableth fibonacci number, just one short of the previous one. The classic way to deal with this problem is to use a technique known as memoization. The idea is simple: we keep some sort of memo or cache of values that have already been calculated and store them in some data structure that we don’t have to recalculate values that have already been computed prior. Here is a simple implementation. To see how effective memoization is compared to vanilla recursion, let’s use the module. Comparing this to the test on with memoization, the benefits of caching becomes immediately clear: 3 second and 150 nanoseconds are different by orders of magnitude, and we only asked the functions to calculate the 35th fibonacci number. We can get a sense of how quickly this disparity would grow if we try to calculate something like the 1000th fibonacci number in the sequence. Another perk of using caching as above is that we can now get the full sequence up to the 35th fibonacci number. Although memoization is interesting, it is not the main topic of today’s post. Instead, I want to discuss Binet’s formula, a formula with which we can calculate the math variableth fibonacci number. Binet’s formula states that We can trivially verify that math variable and that math variable. For more robust empirical verification, we will resort to code later. It is worth noting that the quantity in the parenthesis, namely is otherwise referred to as the Golden ratio. Also observe that the other quantity is the negative inverse of the Golden ratio. Let’s take a closer look at why the Binet’s formula makes sense. This is not going to be a rigorous proof or a derivation, but rather an attempt at cursory analysis to provide food for thought. This process was heavily referenced from this Quora post. Intuitively, Binet’s formula has to do with the well-known fact that the ratio between two consecutive fibonacci numbers approaches the Golden ratio as math variable goes to infinity. In this light, we might understand the fibonacci sequence as a geometric sequence with a constant ratio. The goal, then, is to show that the ratio is in fact the Golden ratio. Then, we have the following recurrence relation between consecutive terms. Dividing both sides by math variable, we get This is a simple quadratic equation that we can easily solve. With some precalculus algebra, we get And indeed we start to see the Golden ratio and its negative inverse as solutions to the quadratic. This means that we can express the fibonacci sequence as a linear combinations of these two solutions: Much like solving any difference equations, we have two initial conditions, namely that math variable, math variable. We also trivially know that math variable, but only two conditions suffice to ascertain the value of the coefficients, math variable and math variable. With some algebra, one can verify that Putting these together, we finally get Binet’s formula: An interesting point to note about Binet’s formula is that math variable doesn’t necessarily have to be a non-negative integer as we had previously assumed. In fact, it can be any number: rational, irrational, real, or even complex. The fact that the fibonacci numbers can extend to real number indexing becomes more apparent once we code out the formula. Nothing special at all, this is just a literal transcription of the formula presented above. But now, watch what happens when we try to get the 1.1th fibonacci number, for instance: Lo and behold, we get a complex fibonacci number! I thought this was so fascinating, almost like seeing a magic of some sort. Although I had known about the fibonacci sequence for as long as I can remember, I had never thought about it in continuous terms: in my mind, the fibonacci sequence was, after all, a sequence—a discrete set of numbers adhering to the simple rule that the next number in line is the sum of the previous two. The intriguing part is that, even in this complex fibonacci madness, the simple rule still holds. For instance, You might be wondering why we don’t compare things exactly by means of This is because this equality doesn’t hold due to floating point arithmetic. Therefore, we simply verify equivalence by comparing their magnitude with an arbitrarily small number, . The takeaway from the code snippet is that holds, regardless of whether or not math variable is a non-negative integer. Indeed, Binet’s formula gives us what we might refer to as the interpolation of the fibonacci sequence, in this case extended along the real number line. A corollary of the real number interpolation of the fibonacci sequence via Binet’s formula is that now we can effectively plot the complex fibonacci numbers on the Cartesian plane. Because math variable can be continuous, we would expect some graph to appear, where the math variable-axis represents real numbers, and math variable, the imaginary. This requires a bit of a hack though; note that the result of Binet’s formula is a complex number, or a two-dimensional data point. The input to the function is just a one-dimensional real number. Therefore, we need a way of representing a map from a one-dimensional real number line to a two-dimensional complex plane. This is sort of tricky if you think about it: the normal two-dimensional plane as we know it can only represent a mapping from the math variable-axis to the math variable-axis—in other words, a transformation from one-dimensional space to another one-dimensional space. A three-dimensional math variable-coordinate system, on the other hand, represents a transformation from a two-dimensional space, represented by math variable and math variable, to another one-dimensional space, namely math variable. We aren’t used to going to other way around, where a one-dimensional space is mapped to a two-dimensional space, as is the case here. A simple hack that nonetheless makes a lot of sense in this case is to use the real-number line for two purposes: representing the input dimension, namely the real number line, and one component of the output dimension—the real number portion of the output to Binet’s formula. This admittedly results in a loss of information, since finding the point where math variable won’t give us the math variableth fibonacci number; instead, it will only tell us what the fibonacci number is whose real number component equals math variable. Nonetheless, this is an approach that makes sense since the real number line is a common dimension in both the input and output data. With this in mind, let’s go ahead and try to plot the interpolation of the fibonacci sequence on the complex plane. First, we import the modules we will need. Then, we simply specify the domain on the real number line and generate the fibonacci numbers, separating out the real and imaginary components. Note that is not going to be used for plotting; instead, we use as the math variable-axis, and this is where the loss of temporal information comes in, as mentioned earlier. Now, let’s go ahead and plot it out! And there it is, the full fibonacci sequence, interpolated across the real numbers. When I first saw this pattern in Matt Parker’s video, I was simply in awe, a loss of words. There’s something inexplicably beautiful and wonderful at this pattern, almost as if it was some part of God’s plan. Okay, maybe I’m being too melodramatic about a graph, but there is no denying that this pattern is geometrically interesting and pleasing to the eye. Everything looks so intentional and deliberate. The comments on the aesthetics of the snail shell aside, one point that deserves our attention is what appears to be a straight line. Well, turns out that this is, in fact, not a straight line. The only reason why it appears straight is that the snail pattern overshadows the little vibrations on this portion of the graph. Indeed, zooming in, we see that there is an interesting damping motion going on. This is what the fibonacci sequence would have looked like had we plotted only the positive domain of the real number line. In this post, we took a look at the fibonacci sequence and its interpolation across the real number line. We could go even crazier, as did Matt Parker in his own video, by attempting to interpolate the sequence on the complex number plane, at which point we would now have a mapping from two dimensions to two dimensions, effectively forcing us to think in terms of four dimensions. There is no fast, handy way of drawing or visualizing four dimensions, as we are creatures that are naturally accustomed to three dimensions. There are interesting observations to be made with the full-fledged complex interpolation of the sequence, but I thought this is already interesting as it is nonetheless. Nowadays, I’m reminded of just how many things that I thought I knew well—like the fibonacci sequence—are rife with things to study and rejoice in wonder. More so than the value of understanding something brand new, perhaps the value of intellectual exploration lies in realizing just how ignorant one is, as ironic as it sounds. I didn’t want to end on such a philosophical note, but things have already precipitated contrary to my intentions. Anyhow, I hope you’ve enjoyed reading this post. Catch you up in the next one.",0,0,0,0,0,0,1,0
"0.5!: Gamma Function, Distribution, and More","In a previous post, we looked at the Poisson distribution as a way of modeling the probability of some event’s occurrence within a specified time frame. Specifically, we took the example of phone calls and calculated how lucky I was on the day I got only five calls during my shift, as opposed to the typical twelve. While we clearly established the fact that the Poisson distribution was a more accurate representation of the situation than the binomial distribution, we ran into a problem at the end of the post: how can we derive or integrate the Poisson probability distribution, which is discontinuous? To recap, let’s reexamine the Poisson distribution function: As you can see, this function is discontinuous because of that one factorial term shamelessly flaunting itself in the denominator. The factorial, we might recall, is as an operation is only defined for integers. Therefore, although we can calculate expression such as math expression, we have no idea what the expression math expression evaluates to. Or do we? Here is where the Gamma function kicks in. This is going to be the crux of today’s post. Let’s jump right into it by analyzing the Gamma function, specifically Euler’s integral of the second kind: At a glance, it is not immediately clear as to why this integral is an interpolation of the factorial function. However, if we try to evaluate this expression through integration by parts, the picture becomes clearer: Notice that the first term evaluates to 0. Moreover, the integral term can be expressed in terms of the Gamma function since Applying all the simplifications leave us with Notice that this is a recursive representation of the factorial, since we can further unravel math expression using the same definition. In other words, So it is now clear that the Gamma function is indeed an interpolation of the factorial function. But the Gamma function deserves a bit more attention and analysis than the simple evaluation we have performed above. Specifically, I want to introduce a few more alternative forms of expressing and deriving the Gamma function. There are many ways to approach this subject, and it would be impossible to exhaust through the entire list of possible representations. For the purposes of this post, we look at two forms of the Gamma function I find intriguing. The first version, presented below, is Euler’s definition of the Gamma function as an infinite product. To see how this comes from, we backtrack this equality by dividing the right-hand side by math expression. At this point, we can reduce the fraction by eliminating math expression from both the denominator and the numerator, which leaves us with the following expression: Therefore, we have This is another representation of the Gamma function that is distinct from the integral we saw earlier. The last form that we will see involves some tweaking of the harmonic series, or the simplest case of the Riemann zeta function where math expression. We start from math expression. We derive both sides by math expression to obtain the following: Notice the harmonic series embedded in the expression above. To further simplify this expression, we derive an interpolation of the harmonic series. Let the interpolation be denoted as math expression: We derive both sides by math expression to obtain the following: The expression above is the sum of a geometric series with radius math expression. We integrate both sides by math expression to obtain math expression. We are almost done! Now that we have the expression for the harmonic series, we can plug it back into the original equation on math expression to finish off this derivation. Integrate both sides by math expression to obtain the following: Exponentiate both sides by math expression to remove the logarithm, and we finally get the alternative representation of the Gamma function: So we see that there are many other alternate modes of expressing the same function, math expression. But the truth that remains unchanged is that the Gamma function is essentially a more expansive definition of the factorial that allows for operations on any real numbers. There are many concepts and theories surrounding the Gamma function, such as the Euler-Mascheroni constant, Mellin transformation, and countless many more, but these might be tabled for another discussion as they each deserve a separate post. The Gamma distribution is, just like the binomial and Poisson distribution we saw earlier, ways of modeling the distribution of some random variable math expression. Deriving the probability density function of the Gamma distribution is fairly simple. We start with two parameters, math expression and math expression, using which we can construct a Gamma function. We can divide both sides by math expression to obtain the following expression: We can then apply the substitution math expression to obtain Notice that we can consider the integrand to be a probability distribution function since the result of integration over the prescribed domain yields 1, the total probability. Seen this way, we can finally delineate a definition of the Gamma distribution as follows, with a trivial substitution of parameters. If this derivation involving the Gamma function does not click with you, we can take the alternative route of starting from the functional definition of the Gamma distribution: the Gamma distribution models the waiting time until the occurrence of the math expressionth event in a Poisson process. In other words, given some rate math expression which denotes the rate at which an event occurs, what is the distribution of the waiting time going to look like? The Gamma distribution holds an answer to this question. Let’s jump right into derivation. The first ingredient we need is the equation for the Poisson probability mass function, which we might recall goes as follows: math expression denotes the number of events that occur in unit time, while math expression denotes the rate of success. To generalize this formula by removing the unit time constraint, we can perform a rescaling on math expression to produce the following: where math expression denotes the probability that math expression events occur within time math expression. Notice that setting math expression gives us the unit time version of the formula presented above. The link between the Poisson and Gamma distribution, then, is conveniently established by the fact that the time of the math expressionth arrival is lesser than math expression if more than math expression events happen within the time interval math expression. This proposition can be expressed as an identity in the following form. Notice that the left-hand side is a cumulative distribution function of the Gamma distribution expressed in terms of math expression. Given the derivative relationship between the CDF and PDF, we can obtain the probability distribution function of Gamma by deriving the right-hand side sigma expression with respect to math expression. After some thinking, we can convince ourselves that unraveling the sigma results in a chain reaction wherein adjacent terms nicely cancel one another, ultimately collapsing into a single term, which also happens to be the first term of the expression: Recalling that math expression, we can rewrite the expression as follows: Notice the structural identity between the form we have derived and the equation of the Gamma distribution function introduced above, with parameters math expression, math expression, and math expression. In the language of Poisson, these variables translate to math expression, math expression, and math expression, respectively. To develop and intuition of the Gamma distribution, let’s quickly plot the function. If we determine the values for the parameters math expression and math expression, the term math expression reduces to some constant, say math expression, allowing us to reduce the PDF into the following form: This simplified expression reveals the underlying structure behind the Gamma distribution: a fight for dominance between two terms, one that grows polynomially and the other that decays exponentially. Plotting the Gamma distribution for different values of math expression and math expression gives us a better idea of what this relationship entails. Executing this code block produces the following figure. Notice that, as math expression increases, the Gamma distribution starts to look more like a normal distribution. At math expression, the math expression collapses to 1, resulting in an exponential distribution. A short tangential digression: the exponential distribution is a special case of a Gamma distribution that models the waiting time until the first event in a Poisson process. It can also be considered as the continuous version of the geometric distribution. But maybe more on this later on a separate post. Returning back to the Gamma distribution, we see that altering the math expression also produces a transformative effect on the shape of the Gamma PDF. Specifically, increasing math expression causes the distribution to move to the right along the math expression-axis. This movement occurs because, for every math expression, a larger math expression value results in a decrease in the corresponding value of math expression, which graphically translates to a rightward movement along the axis as shown. We started by looking at the Poisson probability mass function, and started our venture into the Gamma function by pondering the question of interpolating the factorial function. From there, we were able to derive and develop an intuition for the Gamma distribution, which models the waiting time required until the occurrence of the math expressionth event in a Poisson process. This may all sound very abstract because of the theoretical nature of our discussion. So in the posts to follow, we will explore how these distributions can be applied in different contexts. Specifically, we will take a look at Bayesian statistics and inference to demonstrate how distributions can be employed to express prior or posterior probabilities. At the same time, we will also continue our exploration of the distribution world by diving deeper into other probability distributions, such as but not limited to exponential, chi-square, normal, and beta distributions, in no specific order. At the end of the journey, we will see how these distributions are all beautifully interrelated. Catch you up in the next one!",0,0,0,0,1,0,0,1
Gaussian Process Regression,"In this post, we will explore the Gaussian Process in the context of regression. This is a topic I meant to study for a long time, yet was never able to due to the seemingly intimidating mathematics involved. However, after consulting some extremely well-curated resources on this topic, such as Kilian’s lecture notes and UBC lecture videos by Nando de Freitas, I think I’m finally starting to understand what GP is. I highly recommend that you check out these resources, as they are both very beginner friendly and build up each concept from the basics. With that out of the way, let’s get started. Let’s begin by considering the classic setup of a regression problem. The goal of regression is to predict some values given a set of observations, otherwise referred to as a training set. There are of course many variants of the regression problem. For instance, in a previous post we took a look at Bayesian linear regression, where instead of a single point estimate, we tried to derive a distribution of the predicted data at a given test point. Gaussian Processes (GPs) are similar to Bayesian linear regression in that the final result is a distribution which we can sample from. The biggest point of difference between GP and Bayesian regression, however, is that GP is a fundamentally non-parametric approach, whereas the latter is a parametric one. I think this is the most fascinating part about GPs—as we will see later on, GPs do not require us to specify any function or model to fit the data. Instead, all we need to do is to identify the mean and covariance of a multivariate Gaussian that defines the posterior of the GP. All of this sounds too good be true—how can a single multivariate Gaussian distribution be enough for what could potentially be a high-dimensional, complicated regression problem? Let’s discuss some mathematical ideas that enable GP to be so powerful. Gaussians are essentially a black hole of distributions: once a Gaussian, always a Gaussian. For example, we know from a previous post on Gibbs sampling that the conditional of a multivariate Gaussian is also Gaussian. given the setup that and And of course, the marginal of a multivariate Gaussian also produces another Gaussian. This marginalization property can be understood both intuitively by thinking about the implications of viewing the mean and covariance as vectors and matrices, or by taking a direct integral: Lastly and most importantly, we also saw in the post on Bayesian linear regression that the product of two Gaussians is also Gaussian. Like this, a distribution that is Gaussian most likely stays Gaussian, withstanding such operations as marginalization, multiplication, or conditioning. This is a powerful property that we can use to motivate the “Gaussian-ness” behind GP. As stated earlier, GP is non-parametric. Simply put, this means that we don’t have to consider things like the typical math variable in the context of linear regression. Normally, we would start off with something like This is sometimes also written in terms of a weight vector math variable or a function math variable. Here, we also have some Gaussian noise, denoted by math variable: However, since GPs are non-parametric, we do not have to specify anything about the model. How do we remove this consideration? The short answer is that we marginalize out the model from the integral. Let math variable denote the model, math variable the data, math variable the predictions. Normally, an integral like the one above would be intractable without a solution in closed form. Hence, we would have to rely on random sampling methods such as MCMC. However, in this case, we do have a closed form solution, and we know what it looks like: a Gaussian! This means that we uniquely identify the final posterior distribution through GP regression—all we need is the mean and covariance. Let’s start with the easy one first: the mean. The mean is a trivial parameter because we can always normalize the mean to zero by subtracting the mean from the data. Therefore, for simplicity purposes, we assume a zero mean throughout this post. The interesting part lies in the covariance. Recall that the covariance matrix is defined as follows: Roughly speaking, covariance tells us how correlated two entries of the random vector are. This is where I think GPs get really interesting: the key point of GP is to realize that we want to model some smooth function that best fits our data. What does this “smoothness” mean in terms of covariance? The answer is that math variable values that are close to each other must be highly correlated, whereas those that are far apart would have low covariance. In other words, knowing the value of math variable tells us a lot about the value of math variable, whereas it tells us very little about the value of math variable. In short, the closer the values, the higher the covariance. So at the end of the day, all there is to GP regression is to construct this covariance matrix using some distance function. In GPs, these covariance matrices are referred to as kernels. Kernels can be understood as some sort of prior that we impose upon the regression problem. The idea of smoothness noted earlier is one such example of a prior. But it is a general prior that makes a lot of sense, since we normally don’t want stepwise or non-differential functions as the result of regression. But there are hundreds and thousands of kernels out there that each suit different purposes. For the sake of simplicity, however, we will only take a look at one such kernel that relates to smoothness: the squared exponential kernel, often referred to as the RBF kernel. The distance function of the squared exponential kernel looks as follows: We can apply distortions to the RBF function by adding things like coefficients, but for simplicity sake we omit them here. The key takeaway is that the RBF kernel function functions as a distance metric between two points. As an extreme example, let’s consider the case when math variable, the diagonal entries of the covariance matrix, which is effectively the variance along those components. Then, Conversely, when math variable and math variable are extremely different points, We can thus deduce that the distance function returns a value between 0 and 1 that indicates the similarity or closeness between two points. This is exactly the sort of behavior we want for the covariance matrix. In short, the multivariate Gaussian that we will be using for GP regression can simply be summarized as where covariance math variable denotes the kernel matrix. In the RBF kernel function above, we were assuming a function without any noise, namely that math variable. However, once we add in Gaussian noise as we saw above with math variable where math variable, then we need to make adjustments to the kerne to account for added variations. With some thinking, we can persuade ourselves that the only modification that is needed pertains to the diagonal entries of the covariance matrix. This is because math variable only affects variance that exists within the univariate Gaussian for each point on the math variable-axis without affecting the non-diagonal entries, which otherwise pertain to covariance between two points. In other words, the new kernel matrix now becomes This can be seen as a minor correction to the kernel matrix to account for added Gaussian noise. Before we jump straight into code implementation, it’s necessary to discuss the Cholesky decomposition to get some technicality out of the way. The Cholesky decomposition is a specialization of the LDU decomposition, applied to symmetric matrices. The idea is that we can factor a symmetric matrix math variable as Let’s begin by considering the LDU decomposition of the math variable matrix. We know that for symmetric matrices, math variable. This can easily be shown by comparing the LDU decomposition of math variable and math variable respectively: Therefore, we can rewrite the LDU decomposition of A as A nice property of diagonal matrices is that we can easily identify its square, namely, where math variable is a matrix whose diagonal entries are each square root of the corresponding originals in math variable. The tranpose is not necessary since math variable is a diagonal matrix, but we do so for convenience purposes later on in the derivation. Note the trivial case of the identity matrix, whose square root is equal to itself since all diagonal elements take the value of 1 (and math variable). Given this piece of information, what we can now do is to rewrite the factorization of math variable as where math variable. This is the Cholesky decomposition of symmetric matrices—to be more exact, positive semi-definite matrices. The reason why the Cholesky decomposition can only be performed on positive semi-definite matrices becomes apparent when we think about the definition of positive semi-definiteness. Given any non-zero vector math variable, The key takeaway is that, given some positive semi-definite matrix, we can easily factor it into what we might consider to be its square root in the form of math variable. The Cholesky decomposition is extremely useful in the context of sampling. Recall that, in a univariate setting, we can model any normal distribution by simply sampling from a standard normal distribution with zero mean and unit variance: We can extend this simplle idea to the context of multivariate Gaussians. One natural complication, however, is that variance math variable is a matrix in a multivariate setting. Therefore, we would somehow have to find the standard deviation of the Gaussian, or effectively its square root. This is precisely where the Cholesky decomposition comes in handy. We will be using this means of sampling when implementing GP regression in the next section. Let’s put all the pieces together. The crux of GP regression is conditioning. Recall that Here, the setup was that we have some multivariate Gaussian vector math variable. Given some values for a portion of this random vector, namely math variable, we can then derive another multivariate Gaussian for math variable using conditioning. This is exactly what we are trying to do with GP regression. Assuming that the data is normally distributed, given a number of training points and their corresponding math variable values, how can we make predictions at test points? In other words, math variable are the test points; math variable, the training points. Then, we can now establish the following: where math variable denotes the observed values in the training set and the math variables are each components of the kernel matrix for the entire dataset, including both the training and test sets: This partition also means that math variable is the kernel for the training set; math variable, the kernel for the test set. You might be wondering how the generic formula for the conditional distribution morphed into (18). While the notation might obscure their similarity, (18) immediately follows from (1). First, because we assumed zero mean, the term math variable simply collapses into math variable. The same line of reasoning applies to math variable’ hence, the first term disappears from the mean. As for the covariance, a simple comparison is enough to show that the two equations are identical. In a nutshell, GP regression simply amounts to generating a prediction given some training data through conditioning, under the assumption that the underlying function is a infinite-dimensional vector that follows some Gaussian distribution with a kernel acting as its prior. Given this broad conceptual understanding, let’s move onto more concrete implementations. These are the setting we will be using for this post. We set a random seed for reproducibility purposes. Recall that, depsite its beautiful underlying complexity, all there is to GP regression is to identify some conditional Gaussian with a kernel as its covariance. Then, we can simply sample from this conditional distribution to obtain possible models that fit the data. As the first step, let’s implement the RBF kernel. Here, we modify (9) to have an added parameter, math variable, which is a multiplicative constant to the exponent. The function simply uses double iteration to fill each entry of the covariance matrix. Note that and do not have to be identical in length; if their lengths are different, the resulting kernel matrix will simply be rectangular. This is expected given that the components of math variable, namely math variable in (19), will never be square unless the number of test and training points are equal. Now let’s generate some dummy data. In theory, the final function sampled through GP is considered an infinite dimensional vector, but for practical reasons of implementation, the vector in this case will be at most 60 dimensions: ten training points and 50 test points, appended together as one vector. Next, let’s build the kernel with the test points and draw random samples to see what our prior looks like. Recall that sampling can be easily achieved by performing the Cholesky decomposition on the kernel. Let’s plot the ten random samples drawn from the prior. Note that at this point, we have not seen any training data at all. The only stricture imposed on GP vis a vis the kernel is the fact that the function must be smooth, i.e. points that are close to each other in must be highly correlated. Indeed, the sampled data seems to present somewhat smooth curves, although the smoothness is somewhat mitigated by the fact that the model are only vectors of 50 dimensions. However, we would expect them to look even smoother had we augmented the dimensions of the test data to 100 dimensions or more. Next, we need a function from which we generate dummy train data. For the purposes of demonstration, let’s choose a simple sine function. Let’s generate 15 training data points from this function. Note that we are performing a noiseless GP regression, since we did not add any Gaussian noise to . However, we already know how to perform GP with noise, as we discussed earlier how noise only affects the diagonal entries of the kernel. Now it’s time to model the posterior. Recall that the posterior distribution can be expressed as If we use or functions to calculate the inverse of the kernel matrix components, out life would admittedly be easy. However, using inversion is not only typically costly, but also prone to inaccuracy. Therefore, we instead opt for a safer method, namely using . In doing so, we will also be introducing some intermediate variables for clarity. Let’s begin with the expression for the posterior mean math variable, which is math variable. The underlying idea is that we can apply Cholesky decomposition on math variable, and use that as a way to circumvent the need for direct inversion. Let math variable, then Let’s make another substitution, math variable. Then, This means that we can calculate the mean math variable as Similarly, for the covariance math variable, we can introduce an intermediate variable math variable from which we obtain Notice that the final expressions for mean and covariance do not require any form of inversion, which was our end goal for efficient and accurate computation. Let’s transcribe everything back to code. Let refer to math variable (and by the same token refers to math variable). Then, Just to be safe, let’s check that is of the desired shape, namely a vector with 50 entries. Continuing with our computation of the posterior covariance, As expected, is a 50-by-50 matrix. We are now almost done. Since we have computed the mean and covariance , all there is left is to generate samples from this distribution. For that, we resort to Cholesky decomposition again, recalling the idea discussed earlier in (19). Let’s sample a total of 50 samples. now contains 50 samples generated from the posterior. It’s important to keep in mind that these samples are each 50-dimensional vectors—in a sense, they can be considered as “functions”, which is why the Gaussian process is often referred to as sampling functions from a multivariate Gaussian. Let’s plot the final result, alongside the actual function . In red, I’ve also plotted the average of all the 50 samples to see how accurate the result holds up. The model behaves exactly as we would expect: where there is data, we are confident; where there is no data, we are uncertain. Therefore, we see little variation on test points near the data. In sparse regions where there is no training data, the model reflects our uncertainty, which is why we observe variation within the sampled functions. Comparing the region math variable where there is a lot of training data, and math variable where there is little data, this point becomes apparent. Overall, the average of the fifty samples seems to somewhat capture the overall sinusoidal trend present in the training data, notwithstanding the extraneous curvature observed in some regions. My first attempt at understanding Gaussian processes probably dates back to earlier this year, when I obtained an electronic copy of Rasmussen’s Gaussian Process for Machine Learning. I gave up on chapter 1. The book is still far beyond my current level of mathematics, but nonetheless I am glad that I was able to gain at least a cursory understanding of GP regression. I hope you’ve enjoyed reading this post. In a future post, I hope to dive into another topic I’ve not been able to understand back then: Gaussian mixture models. See you in the next one!",0,0,1,1,0,0,0,0
My First GAN,"Generative models are fascinating. It is no wonder that GANs, or General Adversarial Networks, are considered by many to be where future lies for deep learning and neural networks. In this post, we will attempt to create a very simple vanilla GAN using TensorFlow. Specifically, our goal will be to train a neural network that is capable of generating compelling images of ships. Although this is a pretty mundane task, it nonetheless sheds lights on the potential that GAN models hold. Let’s jump right into it. Below are the dependencies and settings we will be using throughout this tutorial. Before we start building the GAN model, it is probably a good idea to define some variables that we will be using to configure the parameters of convolutional layers, namely the dimensionality of the images we will be dealing with, as well as the number of color channels and the size of the latent dimension. Similar to variational autoencoders, GANs are composed of two parts: the generator and the discriminator. As Ian Goodfellow described in the paper where he first put out the notion of a GAN, generators are best understood as counterfeiters of currency, whereas the discriminator is the police trying to distinguish the fake from the true. In other words, a GAN is a two-component model that involves an internal tug-of-war between two adversarial parties, each trying their best to accomplish their mission. As this competition progresses, the generator becomes increasingly better at creating fake images; the discriminator also starts to excel at determining the veracity of a presented image. Enough of theoretical dwellings, let’s begin by defining the generator model. The is a function that returns a generator model according to some set parameters. Let’s take a look at the structure of this network in more detail. Notice that the output of the generator is a batch image of dimensions . This is exactly the same as the , , and information we defined earlier, and that is no coincidence: in order to fool the discriminator, the generator has to generate images that are of the same dimensions as the training images from ImageNet. Now it’s time to complete the GAN by creating a corresponding discriminator, the discerning police officer. The discriminator is essentially a simple binary classier that ascertains whether a given image is true or fake. Therefore, it is no surprise that the final output layer will have one neuron with a sigmoid activation function. Let’s take a more detailed look at the function as shown below. And again, a model summary for convenient reference: Now we have both the discriminator and the generator, but the two are not really connected in the sense that they exist as discrete models lacking any connection between them. What we want to do, however, is to establish some relationship between the generator and the discriminator to complete a GAN, and hence train them in conjunction. This process of putting the pieces together, or adjoining the models, is where I personally find the genius in GAN design. The key takeaway here is that we define and . As you might imagine, the shape of the input is defined by we defined earlier. This is the latent space from which we will sample a random noise vector frame to feed into our GAN. Then, the connection between the generator and the discriminator is effectively established by the statement . All this is saying is that GAN’s output is the evaluation of the generator’s fake image by the discriminator. If the generator does well, it will fool the discriminator and thus output 1; 0 vice versa. Let’s take a look at the code implementation of this logic. Now it’s time to train our model. Let’s first load our dataset. For this, we will be using the images. The dataset contains low resolutions images, so our output is also going to be very rough, but it is a good starting point nonetheless. One hacky thing we do is concatenating the training and testing data. This is because for a GAN, we don’t need to differentiate the two: on the contrary, the more data we have for training, the better. One might suggest that testing data is necessary for the discriminator, which is a valid point, but the end goal here is to build a high performing generator, not the discriminator, so we will gloss over that point for now. For this tutorial, we will be using images of ships, which are labeled as 8. So let’s go ahead and specify that. We see that contains 6000 images, which is more than enough to start training our GAN. To train the GAN, we will define a function. Essentially, this function creates binary labels for real and fake images. Recall that the goal of the discriminator is to successfully discern generated images from real ones. Also recall that to create generated images, the generator needs to sample from a latent dimension. In other words, training will consist of the following steps: These high level abstractions are what implements behind the scenes. There are several subtleties that deserve our attention. First, we fade out the labels ever so slightly to expedite the training process. These are little magic tricks that people have found to work well on GAN training. While I’m not entirely sure about the underlying principle, it most likely comes from the fact that having a smooth manifold is conducive to the training of a neural network. Second, coercing a true label on the GAN essentially trains the generator. Note that we never explicitly address the generator in the function; instead, we only train the discriminator. By coercing a true label on the GAN, we are effectively forcing the generator to produce more compelling images, and penalizing it when it fails to do so. Personally, I find this part to be the genius and beauty of training GANs. Now that we have an idea of what the function accomplishes, let’s use it to start training. The seems to fluctuate a bit, which is not necessarily a good sign but also quite a common phenomenon in GAN training. GANs are notoriously difficult to train, since it requires balancing the performance of the generator and the discriminator in such a way that one does not overpower the other. This is referred to as a min-max game in game theory terms, and finding an equilibrium in such structures are known to be difficult. Let’s take a look at the results now that the iterations are over. The created images are admittedly fuzzy, pixelated, and some even somewhat alien-looking. This point notwithstanding, I find it incredibly fascinating to see that at least some generated images actually resemble ships in the sea. Of particular interest to me are the red ships that appear in and . Given the simplicity of the structure of our network, I would say that this is a successful result. Let’s take a look at the learning curve of the GAN. As you might expect, the loss is very spiky and erratic. This is why it is hard to determine when to stop training a GAN. Of course, there are obvious signs of failure: when the loss of one component starts to get exponentially larger or smaller than its competitor, for instance. However, this did not happen here, so I let the training continue until the specified number of interactions were over. The results, as shown above, suggest that we haven’t failed in our task. In a future post, we will be taking a look at the mathematics behind GANs to really understand what’s happening behind the scenes when we pit the generator against its mortal enemy, the discriminator. See you in the next post!",1,0,0,0,0,0,0,0
PyTorch Tensor Basics,"This is a very quick post in which I familiarize myself with basic tensor operations in PyTorch while also documenting and clarifying details that initially confused me. As you may realize, some of these points of confusion are rather minute details, while others concern important core operations that are commonly used. This document may grow as I start to use PyTorch more extensively for training or model implementation. Let’s get started. There appear to be two ways of specifying the size of a tensor. Using as an example, let’s consider the difference between and It confused me how the two yielded identical results. Indeed, we can even verify that the two tensors are identical via I thought different behaviors would be expected if I passed in more dimensions, plus some additional arguments like , but this was not true. The conclusion of this analysis is that the two ways of specifying the size of a tensor are exactly identical. However, one note of caution is that NumPy is more opinionated than PyTorch and exclusively favors the tuple approach over the unpacked one. The conclusion of this analysis is that either approach is fine; it is perhaps a good idea to stick to one convention and stay consistent with that coding style throughout. Resizing or reshaping a tensor is an incredibly important tensor operation that is used all the time. The interesting thing is that there seems to be many ways of achieving the same behavior. As someone who prefers a more opinionated guideline, this was rather confusing at first. However, here is what I have gathered while sifting through Stack Overflow and PyTorch discussion forums. Let’s first start with a dummy random tensor. (Note that I could have done , as per the conclusion from the section above.) The operation returns a new tensor whose dimensions match those that have been passed into the function as arguments. For example, the snippet below shows how we can reshape into a tensor. One very important detail, however, is that this operation is not in-place. In other words, if we check the size of again, you will realize that it is still a tensor, as was originally initialized. To change itself, we could do Or even better, we can use , which is an in-place operation by design. Notice that, unlike when we called , changes the tensor itself, in-place. In older versions of PyTorch, existed as a non in-place operator. However, in newer versions of PyTorch, this is no longer the case, and PyTorch will complain with an informative deprecation error message. Note that is not an in-place operator, meaning its behavior will largely be identical to that of . PyTorch keeps an internal convention when it comes to differentiating between in-place and copy operations. Namely, functions that end with a are in-place operators. For example, one can add a number to a tensor in-place via , as opposed to the normal , which does not happen in-place. Observe that the addition is not reflected in , indicating that no operations happened in-place. , however, achieves the result without copying and creating a new tensor into memory. is another common function that is used to resize tensors. It has been part of the PyTorch API for quite a long time before was introduced. Without getting into too much technical detail, we can roughly understand view as being similar to in that it is not an in-place operation. However, there are some notable differences. For example, this Stack Overflow post introduces an interesting example: On the other hand, does not run into this error. The difference between the two functions is that, whereas can only be used on contiguous tensors. This SO thread gives a nice explanation of what it means for tensors to be contiguous; the bottom line is that, some operations, such , do not create a completely new tensor, but returns a tensor that shares the data with the original tensor while having different index locations for each element. These tensors do not exist contiguously in memory. This is why calling after a transpose operation raises an error. , on the other hand, does not have this contiguity requirement. This felt somewhat overly technical, and I doubt I will personally ever use over , but I thought it is an interesting detail to take note of nonetheless. Another point of confusion for me was the fact that there appeared to be two different ways of initializing tensors: and . Not only do the two functions look similar, they also practically do the same thing. Upon more observation, however, I realized that there were some differences, the most notable of which was the . seemed to be unable to infer the data type from the input given. On the other hand, was sable to infer the data type from the given input, which was a list of integers. Sure enough, is generally non-configurable, especially when it comes to data types. can accept as a valid argument. The conclusion of this analysis is clear: use instead of . Indeed, this SO post also confirms the fact that should generally be used, as is more of a super class from which other classes inherit. As it is an abstract super class, using it directly does not seem to make much sense. In PyTorch, there are two ways of checking the dimension of a tensor: and . Note that the former is a function call, whereas the later is a property. Despite this difference, they essentially achieve the same functionality. To access one of the elements, we need appropriate indexing. In the case of , it suffices to consider the size as a list, meaning that square bracket syntax can be used. In the case of , indices can directly be passed into as an argument to index individual elements in the size tensor. These past few days, I’ve spent a fair amount of time using PyTorch for basic modeling. One of the main takeaways from that experience is that an intuition on dimensionality and tensor operations in general is a huge plus. This gets especially important for things like batching. One very basic thing I learned–admittedly perhaps too belatedly–is the difference between and as dimensions. Here is a concrete example. This creates a one-dimensional tensor, which is effectively a list. We can check the dimensions of this tensor by calling , which is very similar to how NumPy works. On the other hand, specifying the size as results in a two-dimensional tensor. The simple, barely passing answer to the question of why is two-dimension would be that it has double layered brackets. More exactly speaking, having an additional layer means that it is capable of storing another tensor within it; hence, is living in a dimension that is one above that of . As mentioned earlier, batch dimension is something that becomes very important later on. Some PyTorch layers, most notably RNNs, even have an argument , which accepts a boolean value. If , PyTorch expects the first dimension of the input to be the batch dimension. If , which is the case by default, PyTorch assumes that the first dimension would be the sequence length dimension. A common operation that is used when dealing with inputs is , or its inverse, . Before explaining what these operations perform, let’s just take a look at an example. Let’s start with , the random tensor of size initialized above. If we apply to , we essentially add a new dimension to the 0-th position of ’s shape. As you can see, now there is an additional batch dimension, thus resulting in a tensor whose shape is as opposed to the original . However, of course this operation is not performed in-place, meaning that will still remain unchanged. There are in-place versions of both and though, and that is simply adding a to the end of the function. For example, Equivalently, calling will remove the th dimension of the tensor. By default, is 0. Squeezing and unsqueezing can get handy when dealing with single images, or just single inputs in general. Concatenation and stacking are very commonly used in deep learning. Yet they are also operations that I often had trouble imagining in my head, largely because concatenation can happen along many axes or dimensions. In this section, let’s solidify our understanding of what concatenation really achieves with some dummy examples. With a basic example, we can quickly verify that each tensor is a three-dimensional tensor whose individual elements are two-dimensional tensors of shape . Now, let’s perform the first concatenation along the 0-th dimension, or the batch dimension. We can verify that the concatenation occurred along the 0-th dimension by checking the shape of the resulting tensor. Since we concatenated two tensors each of shape , we would expect the resulting tensor to have the shape of , which is indeed what we got. More generally speaking, we can think that concatenation effectively brought the two elements of each tensor together to form a larger tensor of four elements. I found concatenation along the first and second dimensions to be more difficult to imagine right away. The trick is to mentally draw a connection between the dimension of concatenation and the location of the opening and closing brackets that we should focus on. In the case of the example above, the opening and closing brackets were the outer most ones. In the example below in which we concatenate along the first dimension, the brackets are those that form the boundary of the inner two-dimensional 3-by-4 tensor. Let’s take a look. Notice that the rows of were essentially appended to those of , thus resulting in a tensor whose shape is . For the sake of completeness, let’s also take a look at the very last case, where we concatenate along the last dimension. Here, the brackets of focus are the innermost ones that form the individual one-dimensional rows of each tensor. Therefore, we end up with a “long” tensor whose one-dimensional rows have a total of 8 elements as opposed to the original 4. In this post, we took a look at some useful tensor manipulation operations and techniques. Although I do have some experience using Keras and TensorFlow, I never felt confident in my ability to deal with tensors, as that felt more low-level. PyTorch, on the other hand, provides a nice combination of high-level and low-level features. Tensor operation is definitely more on the low-level side, but I like this part of PyTorch because it forces me to think more about things like input and the model architecture. I will be posting a series of PyTorch notebooks in the coming days. I hope you’ve enjoyed this post, and stay tuned for more!",1,0,0,0,0,1,0,0
A sneak peek at Bayesian Inference,"So far on this blog, we have looked the mathematics behind distributions, most notably binomial, Poisson, and Gamma, with a little bit of exponential. These distributions are interesting in and of themselves, but their true beauty shines through when we analyze them under the light of Bayesian inference. In today’s post, we first develop an intuition for conditional probabilities to derive Bayes’ theorem. From there, we motivate the method of Bayesian inference as a means of understanding probability. Suppose a man believes he may have been affected with a flu after days of fever and coughing. At the nearest hospital, he is offered to undergo a clinical examination that is known to have an accuracy of 90 percent, i.e. it will return positive results to positive cases 90 percent of the time. However, it is also known that the test produces false positives 50 percent of the time. In other words, a healthy, unaffected individual will test positive with a probability of 50 percent. In cases like these, conditional probability is a great way to package and represent information. Conditional probability refers to a measure of the probability of an event occurring, given that another event has occurred. Mathematically, we can define the conditional probability of event math expression given math expression as follows: This equation simple states that the conditional probability of math expression given math expression is the fraction of the marginal probability math expression and the area of intersection between those two events, math expression. This is a highly intuitive restatement of the definition of conditional probability introduced above: given that event math expression has already occurred, conditional probability tells us the probability that event math expression occurs, which is then synonymous to that statement that math expression has occurred. By the same token, we can also define the reverse conditional probability of math expression given math expression through symmetry and substitution. Notice that the numerator stays unchanged since the operation of intersection is commutative. Now let’s develop an intuition for conditional probabilities by applying it to our example of clinical trials and the potentially affected patient. The purported accuracy of the clinical test is 90 percent, which we might express as follows, using the conditional probability notation: By the same token, we can also express the information on false positives as shown below. This conditional probability statement espouses that, given an individual who is not sick, the test returns a false positive 50 percent of the time. Conditional probability provides us with an interesting way to analyze given information. For instance, let math expression be the event that it rains tomorrow, and math expression be the event that it is cloudy at the present moment. Although we are no experts in climatology and weather forecast, common sense tells us that since with the additional piece of information that current weather conditions are cloudy, we are inclined to believe that it will likely rain tomorrow, or in the near future. Like this, conditional probability allows us to update our beliefs on uncertainty given new information, and we will see in the later sections that this is the core idea behind Bayesian inference. Let’s return back to the example of the potential patient with a flu. Shortly afterwards at the hospital, the the man was convinced by the doctor and decided to take the clinical test, the result of which was positive. We cannot assume that the man is sick, however, since the test has a rather high rate of false positives as we saw earlier. In this situation, the parameter that is of interest to us can be expressed as In other words, given a positive test result, what is the probability that the man is actually sick? However, we have no means as of yet to directly answer this question; the two pieces of information we have are that math expression, and that math expression. To calculate the value of math expression, we need Bayes’s theorem to do its trick. Let’s quickly derive Bayes’ theorem using the definition of conditional probabilities delineated earlier. Recall that Multiply math expression and math expression on both sides of (1) and (2) respectively to obtain the following result: Notice that the two equations describe the same quantity, namely math expression. We can use equivalence to put these two equations together in the following form. Equation (3) can be manipulated in the following manner to finally produce a simple form of Bayes’ theorem: We can motivate a more intricate version this rule by modifying the denominator. Given that math expression and math expression are discrete events, we can break down math expression as a union of intersections between math expression and math expression, where math expression represents subsets within event math expression. In concrete form, we can rewrite this as Additionally, we can rewrite the conditional probability math expression in terms of math expression and math expression according to the definition of conditional probability we observed earlier. Applying these alterations to (4) to rewrite math expression produces equation (5): This is the equation of Bayes’ theorem. In simple language, Bayes’ theorem tells us that the conditional probability of some subset math expression given math expression is equal to its relevant fraction within a weighted summation of the conditional probabilities math expression given math expression. Although this equation may seem complicated at a glance, we can develop an intuition for this formula by reminding ourselves of the definition of conditional probabilities, as well as the fact that independent events can be expressed as a union of intersections. At the end of the day, Bayes’ theorem provides a powerful tool through which we can calculate a conditional probability in terms of its reverse, i.e. calculate math expression by utilizing math expression. Why is this important at all? Let’s return back to our example of the potential patient. Recall that the conditional probability of our interest was while the pieces of information we were provided were This is where Bayes’ theorem comes in handy. Notice that we have expressed math expression in terms of math expression and math expression. From a statistics point of view, all we have to do now is conduct a random survey of the population to see the percentage of the demographic infected with the flu. Let’s say that 15 percent of the population has been affected with this flu. Plugging in the relevant value yields Using Bayes’ theorem, we are able to conclude that there is roughly a 24 percent chance that the man who tests positive on this examination is affected by the flu. That seems pretty low given the 90 percent accuracy of the test, doesn’t it? This ostensible discrepancy originates from the fact that the test has a substantial false positive of 50 percent, and also that the vast majority of the population is unaffected by the disease. This means that, if the entire population were to conduct this test, there would be more false positives than there would be true positives; hence the distortion in the value of the conditional probability. But what if the man were to take the same test again? Intuition tells us that the more test he takes, the more confident we can be on whether the man is or is not affected by the disease. For instance, if the man repeats the exam once and receives a positive report, the conditional probability that he is sick given two consecutive positive test results should be higher than the 24 percent we calculated above. We can see this in practice by reapplying Bayes’ theorem with updated information, as shown below: We see that the value of the conditional probability has indeed increased, lending credence to the idea that the man is sick. Like this, Like this, Bayes’ theorem is a powerful tool that can be used to calculate conditional probabilities and to update them continuously through repeated trials. From a Bayesian perspective, we begin with some expectation, or prior probability, that an event will occur. We then update this prior probability by computing conditional probabilities with new information obtained for each trial, the result of which yields a posterior probability. This posterior probability can then be used as a new prior probability for subsequent analysis. In this light, Bayesian statistics offers a new way to compute new information and update our beliefs about an event in probabilistic terms. Bayesian inference is nothing more than an extension of Bayes’ theorem. The biggest difference between the two is that Bayesian inference mainly deals with probability distributions instead of point probabilities. The case of the potential patient we analyzed above was a simple yet illuminating example, but it was limiting in that we assumed all parameters to be simple constants, such as math expression for test accuracy and math expression for false positive frequency. In reality, most statistical estimates exist as probability distributions since there are limitations to our ability to measure and survey data from the population. For example, a simple random sampling of the population might reveal that 15 percent of the sample population is affected with the flu, but this would most likely produce a normal distribution with mean centered around 0.15 instead of a point probability. From a Bayesian standpoint, we would then replace the point probability in our example above with an equation for the distribution, from which we can proceed with the Bayesian analysis of updating our prior with the posterior through repeated testing and computation. Bayes’ theorem, specifically in the context of statistical inference, can be expressed as where math expression stands for observed or measured data, math expression stands for parameters, and math expression stands for some probability distribution. In the language of Bayesian inference, math expression is the posterior distribution for the parameter math expression, math expression is the likelihood function that expresses the likelihood of having parameter math expression given some observed data math expression, math expression is the prior distribution for the parameter math expression, and math expression is evidence, the marginal probability of seeing the data, which is determined by summing or integrating across all possible values of the parameter, weighted by how strongly we believe in those particular values of math expression. Concretely, Notice that this is not so different from the expansion of the denominator we saw with Bayes’ theorem, specifically equation (5). The only difference here is that the integral takes continuous probability density functions into account, as opposed to discrete point probabilities we dealt with earlier. If we temporarily disregard the constants that show up in (6), we can conveniently trim down the equation for Bayesian inference as follows: This idea is not totally alien to us—indeed, this is precisely the insight we gleaned from the example of the potential patient. This statement is also highly intuitive as well. The posterior probability would be some mix of our initial belief, expressed as a prior, and the data newly presented, the likelihood. Bayesian inference, then, can be understood as a procedure for incorporating prior beliefs with evidence in order to derive an updated posterior. What makes Bayesian inference such a powerful technique is that the derived posterior can themselves be used as a prior for subsequent inference conducted with new data. To see Bayesian inference in action, let’s dive into the most classic, beaten-to-death yet nonetheless useful example in probability and statistics: the coin flip. This example was borrowed from the following post. Assume that we have a coin whose fairness is unknown. To be fair, most coins are approximately fair (no pun intended) given the physics of metallurgy and center of mass, but for now let’s assume that we are ignorant of coin’s fairness, or the lack thereof. By employing Bayesian inference, we can update our beliefs on the fairness of the coin as we accumulate more data through repeated coin flips. For the purposes of this post, we will assume that each coin flip is independent of others, i.e. the coin flips are independent and identically distributed. Let’s start by coming up with a model representation of the likelihood function, which we might recall is the probability of having a parameter value of math expression given some data math expression. It is not difficult to see that the best distribution for the likelihood function given the setup of the problem is the binary distribution since each coin flip is a Bernoulli trial. Let math expression denote a random variable that represents the number of tails in math expression coin flips. For convenience purposes, we define 1 to be heads and 0 to be tails. Then, the conditional probability of obtaining math expression heads given a fairness parameter math expression can be expressed as We can perform a quick sanity check on this formula by observing that, when math expression, the probability of observing math expression heads diminishes to 0, unless math expression, in which case the probability becomes 1. This behavior is expected since math expression represents a perfectly biased coin that always shows tails. By symmetry, the same logic applies to a hypothetical coin that always shows heads, and represents a fairness parameter of 1. Now that we have derived a likelihood function, we move onto the next component necessary for Bayesian analysis: the prior. Determining a probability distribution for the prior is a bit more challenging than coming up with the likelihood function, but we do have certain clues as to what characteristics our prior should look possess. First, the domain of the prior probability distribution should be contained within math expression. This is because the range of the fairness parameter math expression is also defined within this range. This constraint immediately tells us that where math expression is represents the probability density function that represents the prior. Recall that some of the other functions we have looked at, namely binomial, Poisson, Gamma, or exponential are all defined within the unclosed interval math expression, making it unsuitable for our purposes. The Beta distribution nicely satisfies this criterion. The Beta distribution is somewhat similar to the Gamma distribution we analyzed earlier in that it is defined by two shape parameters, math expression and math expression. Concretely, the probability density function of the Beta distribution goes as follows: The coefficient, expressed in terms of a fraction of Gamma functions, provides a definition for the Beta function. The derivation of the Beta distribution and its apparent relationship with the Gamma function deserves an entirely separate post devoted specifically to the said topic. For the purpose of this post, an intuitive understanding of this distribution and function will suffice. A salient feature of the Beta distribution that is domain is contained within math expression. This means that, application-wise, the Beta distribution is most often used to model a distribution of probabilities, say the batting average of a baseball player as shown in this post. It is also worth noting that the Beta function, which serves as a coefficient in the equation for the Beta PDF, serves as a normalization constant to ensure that integrating the function over the domain math expression would yield 1 as per the definition of a PDF. To see this, one needs to prove This is left as an exercise for the keen reader. We will revisit this problem in a separate post. Another reason why the Beta distribution is an excellent choice for our prior representation is that it is a conjugate prior to the binomial distribution. Simply put, this means that using the Beta distribution as our prior, combined with a binomial likelihood function, will produce a posterior that also follows a Beta distribution. This fact is crucial for Bayesian analysis. Recall that the beauty of Bayesian inference originates from repeated applicability: a posterior we obtain after a single round of calculation can be used as a prior to perform the next iteration of inference. In order to ensure the ease of this procedure, intuitively it is necessary for the prior and the posterior to take the same form of distribution. Conjugate priors streamline the Bayesian process of updating our priors with posteriors by ensuring that this condition is satisfied. In simple language, mathematicians have found that certain priors go well with certain likelihoods. For instance, a normal prior goes along with a normal likelihood; Gamma prior, Poisson likelihood; Gamma prior, normal likelihood, and so on. Our current combination, Beta prior and binomial likelihood, is also up on this list. To develop some intuition, here is a graphical representation of the Beta function for different values of math expression and math expression. This code block produces the following diagram. Graphically speaking, the larger the value of math expression and math expression, the more bell-shaped it becomes. Also notice that a larger math expression corresponds to a rightward shift, i.e. a head-biased coin; a larger math expression, a tail-oriented one. When math expression and math expression take the same value, the local extrema of the Beta distribution is established at math expression, when the coin is perfectly fair. Now that we have established the usability of the Beta function as a conjugate prior to the binomial likelihood function, let’s finally see Bayesian inference at work. Recall the simplified version of Bayes’ theorem for inference, given as follows: For the prior and the likelihood, we can now plug in the equations corresponding to each distribution to generate a new posterior. Notice that math expression, which stands for data, is now given in the form math expression where math expression denotes the number of heads; math expression, the total number of coin flips. Notice also that constants, such as the combinatorial expression or the reciprocal of the Beta function, can be dropped since we are only establishing a proportional relationship between the left and right hand sides. Further simplifications can be applied: But notice that this expression for the posterior can be encapsulated as a Beta distribution since Therefore, we started from a prior of math expression to end up with a posterior of math expression. This is an incredibly powerful mechanism of updating our beliefs based on presented data. This process also proves that, as purported earlier, the Beta distribution is indeed a conjugate prior of a binomial likelihood function. Now, it’s time to put our theory to the test with concrete numbers. Suppose we start our experiment with completely no expectation as to the fairness of the coin. In other words, the prior would appear to be a uniform distribution, which is really a specific instance of a Beta distribution with math expression. Presented below is a code snippet that simulates 500 coin flips, throughout which we perform five calculations to update our posterior. Executing this code block produces the following figure. This plot shows us the change in our posterior distribution that occurs due to Bayesian update with the processing of each data chunk. Specifically, we perform this Bayesian update after trials. When no coin flips are performed, as shown in the first subplot, the prior follows a uniform distribution as detailed above. As more coin tosses are performed, however, we start to develop an understanding of the fairness of the coin. When we only have a few data points, the more probability there is that we obtain skewed data, which is why the mean estimate of our posterior seems skewed as well. However, with a larger number of trials, the law of large numbers guarantees that we will eventually be able to identify the value of our parameter math expression, which is indeed the case. The key takeaway from this code block is the line . This is all the Bayesian method there is in this updating procedure. Notice that this line of code directly corresponds to the formula for the updated Beta posterior distribution we found earlier, which is math expression refers to , math expression corresponds to , and both math expression and math expression are set to in order to take into account the initial prior which tends to a uniform distribution. An interesting observation we can make about this result is that the variance of the Beta posterior decreases with more trials, i.e. the narrower the distribution gets. This is directly reflective of the fact that we grow increasingly confident about our estimate of the parameter with more tosses of the coin. At the end of the 500th trial, we can conclude that the coin is fair indeed, which is expected given that we simulated the coin flip using the command . If we were to alter the argument for this method, say , then we would expect the final result of the update to reflect the coin’s bias. Bayes’ theorem is a powerful tool that is the basis of Bayesian statistical analysis. Although our example was just a simple coin toss, the sample principle and mechanism can be extended to countless other situations, which is why Baye’s theorem remains highly relevant to this day, especially in the field of machine learning and statistical analysis. Bayesian statistics presents us with an interesting way of understanding probability. The classical way of understanding probability is the frequentist approach, which purports that a probability for an event is the limit of its frequency in infinite trials. In other words, to say that a coin is fair is to say that, theoretically, performing an infinite number of coin flips would result in 50 percent heads and 50 percent tails. However, the Bayesian approach we explored today presents a drastically different picture. In Bayesian statistics, probability is an embodiment of our subjective beliefs about a parameter, such as the fairness of a coin. By performing trials, infinite or not, we gain more information about the parameter of our interest, which affects the posterior probability. Both interpretations of probability are valid, and they help complement each other to help us gain a broader understanding of what the notion of probability entails. I hope this post gave you a better understanding as to why distributions are important—specifically in the context of conjugate priors. In a future post, we will continue our exploration of the Beta distribution introduced today, and connect the dots between Beta, Gamma, and many more distributions in the context of Bayesian statistics. See you in the next one.",0,0,0,0,1,0,0,1
