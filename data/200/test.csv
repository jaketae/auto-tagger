title,body,from_scratch,deep_learning,statistics,linear_algebra,machine_learning,analysis,probability_distribution,pytorch
Better seq2seq,"This can be considered a residual connection, since the embedding skips the RNN and directly gets fed into the fully connected layer. Now that we have some idea of what we want to achieve, let’s start coding. Since the setup for this tutorial is identical to that of the previous post, I’ll skip much of the explanation and sanity checks. In the code block below, we load the  dataset, then create bucket iterators for each train, validation, and test split. Let’s start with the encoder. The encoder here is actually almost identical to the one we implemented in the previous model. In fact, it is arguably simpler, as we are now using a single GRU layer instead of a two-layered LSTM. The decoder is where all the enhancements are going to take place. Recall the changes we want to make to the previous seq2seq architecture.  As you can see, we need to make sure that The first change means that the decoder’s forward method needs to be able to take in the encoder’s hidden states as input. For sake of notational clarity, let’s call those hidden states as “context.",0,1,0,0,0,0,0,1
PyTorch Tensor Basics,"To change  itself, we could do Or even better, we can use , which is an in-place operation by design. Notice that, unlike when we called ,  changes the tensor itself, in-place. In older versions of PyTorch,  existed as a non in-place operator. However, in newer versions of PyTorch, this is no longer the case, and PyTorch will complain with an informative deprecation error message. Note that  is not an in-place operator, meaning its behavior will largely be identical to that of . PyTorch keeps an internal convention when it comes to differentiating between in-place and copy operations. Namely, functions that end with a  are in-place operators. For example, one can add a number to a tensor in-place via , as opposed to the normal , which does not happen in-place. Observe that the addition is not reflected in , indicating that no operations happened in-place. , however, achieves the result without copying and creating a new tensor into memory. is another common function that is used to resize tensors. It has been part of the PyTorch API for quite a long time before  was introduced.",0,1,0,0,0,0,0,1
Neural Style Transfer,"We see that style loss decreases quite a bit, whereas the content loss seems to slightly increase with each training step. As stated earlier, it is difficult to optimize on both the content and style, since altering the style of the picture will end up affecting its content in one way or another. However, since our goal is to stylize the target image via NMT, it’s okay to sacrifice a little bit of content while performing NMT, and that’s what is happening here as we can see from the loss values. And here is the result of the transformation!  The result is… interesting, and we certainly see that somethings have changed. We see some more interesting texture in the target image, and there appears to be some changes. However, at this point, my laptop was already on fire, and more training did not seem to yield any better results. So I decided to try out other sample implementations of NMT to see how using more advanced NMT algorithms could make things any better. I decided to try out fast neural style transform, which is available on the official PyTorch GitHub repository.",0,1,0,0,0,0,0,1
Principal Component Analysis,"Also, since the covariance matrix is by definition symmetric, we can simplify things further to end up with And once again, we have shown that the principal components are the eigenvectors of the covariance matrix. But the procedure outlined above can be used to find only one principal component, that is the eigenvector with the largest eigenvalue. How do we go about searching for multiple eigenvectors? This can be done, once again, with Lagrangians, with the added caveat that we will have more trailing terms in the end. Let’s elaborate on this point further. Here, we assume that we have already obtained the first component, , and our goal is to find the next component, . With induction, we can easily see how this analysis would apply to finding . Simply put, the goal is to maximize  under the constraint that  is orthogonal to  while also satisfying the constraint that it is a unit vector. (In reality, the orthogonality constraint is automatically satisfied since the covariance matrix is symmetric, but we demonstrate this nonetheless.) Therefore, Using Lagrangians, In the last equality, we make a trivial substitution to simplify and get rid of the constant.",0,0,1,1,0,0,0,0
A sneak peek at Bayesian Inference,"In concrete form, we can rewrite this as Additionally, we can rewrite the conditional probability  in terms of  and  according to the definition of conditional probability we observed earlier. Applying these alterations to (4) to rewrite  produces equation (5): This is the equation of Bayes’ theorem. In simple language, Bayes’ theorem tells us that the conditional probability of some subset  given  is equal to its relevant fraction within a weighted summation of the conditional probabilities  given . Although this equation may seem complicated at a glance, we can develop an intuition for this formula by reminding ourselves of the definition of conditional probabilities, as well as the fact that independent events can be expressed as a union of intersections. At the end of the day, Bayes’ theorem provides a powerful tool through which we can calculate a conditional probability in terms of its reverse, i.e. calculate  by utilizing . Why is this important at all? Let’s return back to our example of the potential patient. Recall that the conditional probability of our interest was while the pieces of information we were provided were This is where Bayes’ theorem comes in handy.",0,0,1,0,0,0,1,0
Riemann Zeta and Prime Numbers,"To cut to the chase, we get And there we have it, the relationship between the Dirichlet Eta function and the Riemann Zeta function! There are many more interesting things about the Eta function, such as its convergence property, but that is a topic for another post. In this post, we developed an intuition for the implications of the Riemann Zeta function from the perspective of relative primeness and probability. The Zeta function is one of those things in mathematics that appear so simple on the surface, yet is so wonderfully complex and convoluted in the inside. Although we haven’t discussed these other intricacies of the Riemann Zeta function—in particular, its relationship to the Riemann hypothesis, which states that the Zeta function has roots at negative even integers and complex numbers whose real part is —but the approach we took here with prime numbers are probabilities is fascinating in its own right, providing ample food for thought. Many thanks to Chris Henson again for the code and the post. It’s always a lot of fun to mesh mathematics with programming, and I think this is why I enjoyed writing this post.",0,0,0,0,0,1,0,0
Naive Bayes Model From Scratch,"Here,  refers to a single instance, represented as a vector with  entries; , the corresponding label or class for that instance. Note that  is not a feature matrix, but a single instance. More concretely, Of course,  is just a scalar value. This characterization is very apt in the context of machine learning. The underlying idea is that, given some data with  feature columns, we can derive a probability distribution for the label for that intance. The naive assumption that the naive Bayes classifier makes—now you can guess where that name comes from—is that each of the  variables in the instance vector are independent of one another. In other words, knowing a value for one of the features does not provide us with any information about the values for the other  feature columns. Combining this assumption of independence with Bayes’ theorem, we can now restate (1) as follows: Pretty straight forward. We know that the demominator, which often goes by the name “evidence” in Bayesian inference, is merely a normalizing factor to ensure that the posterior distribution integrates to 1.",1,0,0,0,1,0,0,0
Convex Combinations and MAP,"In PDF terms, this translates to The log prior can simply be derived by casting the logarithmic function to the probability distribution function. Now we are ready to enter the maximization step of the sequence. To calculate the maximum of the posterior distribution, we need to derive the posterior and set the gradient equal to zero. For a more robust analysis, it would be required to show that the second derivative is smaller than 0, which is indeed true in this case. However, for the sake of simplicity of demonstration, we skip that process and move directly to calculating the gradient. Let’s rearrange the final equality in (7). From (8), we can finally derive an expression for . This value of the parameter is one that which maximizes the posterior distribution. And we have derived the MAP estimate for the mean of the univariate Gaussian! Maximum a posteriori analysis is great and all, but what does the final result exactly tell us? While there might be many ways to interpret understand the result as derived in (9), one particular useful intuition to have relates to the concept of convex combinations.",0,0,1,0,0,0,0,0
Fourier Series,"Taylor series is used in countless areas of mathematics and sciences. It is a handy little tool in the mathematicians arsenal that allows us to decompose any function into a series of polynomials, which are fairly easy to work with. Today, we are going to take a brief look at another type of series expansion, known as Fourier series. Note that these concepts are my annotations of Professor Gilbert Strang’s amazing lecture, available on YouTube. The biggest difference between Taylor series and Fourier series is that, unlike Taylor series, whose basic fundamental unit is a polynomial term, the building block of a Fourier series is a trigonometric function, namely one of either sine or cosine. Concretely, a generic formula of a Fourier expansion looks as follows: Personally, I found this formula to be more difficult to intuit than the Taylor series. However, once you understand the underlying mechanics, it’s fascinating to see how periodic wave functions can be decomposed as such. First, let’s begin with an analysis of orthogonality. Commonly, we define to vectors  and  as being orthogonal if That is, if their dot product yields zero.",0,0,1,0,0,1,0,0
Scikit-learn Pipelines with Titanic,"Imputation refers to a technique used to replace missing values. There are many techniques we can use for imputation. From the analysis above, we know that the columns that require imputation are as follows: Let’s first take a look at the data types for each column. Checking data types is necessary both for imputation and general data preprocessing. Specifically, we need to pay attention as to whether a given column encodes categorical or numerical variables. For example, we can’t use the mean to impute categorical variables; instead, something like the mode would make much more sense. The best way to determine whether a variable is categorical or not is simply to use domain knowledge and actually observe the data. Of course, one might use hacky methods like the one below: Although you might think that this is a working hack, this approach is in fact highly dangerous, even in this toy example. For example, consider , which is supposedly a numerical variable of type . However, earlier with , we saw that  is in fact a ordinal variable taking discrete values, one of 1.0, 2.0, and 3.0.",0,0,0,0,1,0,0,0
"PyTorch, From Data to Modeling","The difference between the former and latter is that, while the former applies a softmax function to the output before calculating the actual loss, the latter does not. In our case, since we simply output the raw logits instead of applying a softmax calculation, we need to use the former. Let’s return where we were. Before we jump into training and defining the training loop, it’s always a good idea to see if the output of the model is what you’d expect. In this case, we can simply define some random dummy input and see if the output is correct. Now that we’ve verified the input and output dimensions, we can move onto defining the training loop. Defining the training loop may seem difficult at first, especially if you’re coming from a Keras background, but actually a lot of it is boiler plate, and things are not as difficult as they may seem. First, we define a list to hold the loss values per iteration. We will be using this list for visualization later. The exciting part comes next. For each epoch, we load the images in the .",0,1,0,0,0,0,0,1
"Newton-Raphson, Secant, and More","For example, we can express  as However, a downside of this approach is the fact that it’s difficult to directly derive this equation despite the fact that it is nothing more than just a simple polynomial. So instead, we will use a list index-based representation. Namely, the  th element of a list represents the coefficient of the th power in a polynomial equation. In other words,  would translate into . The  is a function that returns a Python function given a list that conforms to this list index representation. Let’s see if this works as expected. , so the function passes our quick sanity test. One useful helper function that I also implemented for the sake of convenience is a array-to-equation parser that translates a list representation into a mathematical expression in Python. This is best demonstrated than explained, so I’ll defer myself to an example. Below is the full definition of the  function. At this point, I also thought that it would be useful and interesting to compose a function that translates the string output of  into a proper Python function we can use to calculate values.",0,0,0,0,0,1,0,0
Natural Gradient and Fisher,"This is where the notion of natural gradients come into play: if our goal is to minimize the cost function, which is effectively equivalent to maximizing the likelihood, why not search within the distribution space of the likelihood function instead? After all, this makes more sense since gradient descent in parameter space is likely to be easily perturbed by the mode of parametrization, such as using precision instead of variance in a normal distribution, whereas searching in the distribution space would not be subject to this limitation. So the alternative to this approach would be to search the distribution space and find the distribution that which makes value of the cost function the smallest. This is the motivation behind the notion of a natural gradient. Now you might be wondering how all this has anything to do with the Fisher matrix, which we looked at in the previous post. Well, it turns out there are some deep, interesting questions to be posed and connections to be uncovered. If we’re going to search around the distribution space, one natural question to consider is what distance metric we will use for our search.",0,0,1,0,1,0,0,0
"PyTorch, From Data to Modeling","Now that we’ve designed and implemented a model, it’s time to train it. This is the part where people might argue that TensorFlow 2 or Keras is superior to PyTorch. In Keras, you can simply call  to train the model. However, in PyTorch, this is not necessarily the case, unless you really want to imitate the Keras API and define a  function yourself. PyTorch is more low-level in that you need to define a custom training loop manually. However, I actually prefer this low-levelness because it requires me to really think through what is happening in each iteration, namely what the dimension of each batch is, what the model expects to receive as input in the forward computation, and what loss function is appropriate given the output and label. Let’s see what all of this means. First, we begin by actually initializing an instance of the model, a loss function named , and an , which is Adam in this case. A quick note of caution: if you dig into the PyTorch documentation or look at other example classifiers, you will realize, like me, there are two loss functions you can typically use:  and , or negative log likelihood loss.",0,1,0,0,0,0,0,1
A Brief Introduction to Recurrent Neural Networks,"There are a lot more advanced recurrent neural networks that have complicated internal cell structures to better emulate human memory, in a sense. The biggest difference between a simple recurrent neural network and an LSTM is that LSTMs have a unique parameter known as the carrier that encodes an additional layer of information about the state of the cell. I might write a separate post devoted to the intricacies of the LSTM, but if you’re an avid reader who’s itching to know more about it right away, I highly recommend this excellent post by Christiopher Olah. For now, let’s just say that LSTMs represent a huge improvement over conventional RNNs, and that we can implement them in  by simply calling the  layer as shown below: Because LSTM layers take a lot longer to train than others, and because the representational capacity of a single LSTM layer is higher than that of others, I decided to use only one LSTM layer instead of two. Let’s initialize this model to take a better look.  The last model we will create is a convnet, which we explored on this previous post on image classification.",0,1,0,0,0,0,0,0
k-Nearest Neighbors Algorithm from Scratch,"If among the 10 neighbors observed, 8 of them have the label 0 and 2 of them are labeled 1, the KNN algorithm will conclude that the label of the provided data is most likely also going to be 0. As we can see, the KNN algorithm is extremely simple, but if we have enough data to feed it, it can produce some highly accurate predictions. There are still missing pieces to this puzzle, such as how to find the nearest neighbors, but we will explore the specifics of the algorithm on the go as we build the model from scratch. For now, just remember the big picture. Let’s get into the nuts and bolts of the KNN model. Below are the dependencies we will need for this demonstration. One problem we need to start thinking about is how to measure distance between two data points. After all, the implementation of KNN requires that we define some metric to measure the proximity between different points, rank them in order, and sort the list to find  nearest neighbors.",1,0,0,0,1,0,0,0
Likelihood and Probability,"In other words, the likelihood function answers the question: provided some list of observed or sampled data , what is the likelihood that our parameter of interest takes on a certain value ? One measurement we can use to answer this question is simply the probability density of the observed value of the random variable at that distribution. In mathematical notation, this idea might be transcribed as: At a glance, likelihood seems to equal probability—after all, that is what the equation (1) seems to suggest. But first, let’s clarify the fact that  is probability density, not probability. Moreover, the interpretation of probability density in the context of likelihood is different from that which arises when we discuss probability; likelihood attempts to explain the fit of observed data by altering the distribution parameter. Probability, in contrast, primarily deals with the question of how probable the observed data is given some parameter . Likelihood and probability, therefore, seem to ask similar questions, but in fact they approach the same phenomenon from opposite angles, one with a focus on the parameter and the other on data. Let’s develop more intuition by analyzing the difference between likelihood and probability from a graphical standpoint.",0,0,1,0,0,0,0,0
Stirling Approximation,"For those of you who are feeling rusty on the Poisson distribution as I was, here is a simple explanation on the Poisson—specifically, its mean and variance. By the virtue of the definition of the parameter, it should be fairly clear why :  is a rate parameter that indicates how many events occur within a window of unit time. The expected calculation can easily be shown using Taylor expansion: Next, we prove that the variance of a Poisson random variable defined by parameter  is equal to . Let  be a Poisson random variable. Then, Then, using the definition of variance, we know that From this, we are once again reminded of the defining property of the Poisson, which is that both the mean and variance of a Poisson random variable is defined by the parameter . Let’s tie this back to our original discussion of the Central Limit Theorem. CLT states that, even if a distribution of a random variable is not normal, the distribution of the sums of these random variables will approximate a normal distribution.",0,0,1,0,0,1,1,0
Bayesian Linear Regression,"In many real-world cases, this process can be intractable, but because we are dealing with two Gaussian distributions, the property of conjugacy ensures that this problem is not only tractable, but also that the resulting posterior would also be Gaussian. Although this may not be immediately apparent, observe that the exponent is a quadratic that follows the form after making appropriate substitutions Therefore, we know that the posterior for  is indeed Gaussian, parameterized as follows: Let’s try to obtain the MAP estimate of of , i.e. simplify  Notice the similarity with the MLE estimate, which is the solution to normal equation, which I otherwise referred to as vanilla linear regression: This is no coincidence: in a previous post on MAP and MLE, we observed that the MAP and MLE become identical when we have a uniform prior. In other words, the only cause behind the divergence between MAP and MLE is the existence of a prior distribution. We can thus consider the additional term in (10) absent in (11) as a vestige of the prior we defined for .",0,0,0,1,0,0,0,0
Bayesian Linear Regression,"What do those sampled values mean for us in the context of linear regression? Well, let’s plot some sampled lines using the  function conveniently made available through the  library.  We see that the gray lines, sampled by , all seem to be a good estimate of the true regression line, colored in gold. We might also notice that the sampled regression lines seem to stay below the true regression line for smaller values of . This is because we have more samples beneath the true regression line that we have above it. Bayesian linear regression is able to account for such variations in data and uncertainty, which is a huge advantage over the simple MLE linear regression method. The true power of Bayesian linear regression might be summarized as follows: instead of returning just a single line using the MLE weight estimate of data, Bayesian linear regression models the entire data set to create a distribution of linear functions so to speak, allowing us to sample from that distribution to obtain sample linear regression lines. This is an approach that makes much more sense, since it allows us to take into account the uncertainty in our linear regression estimate.",0,0,0,1,0,0,0,0
MLE and KL Divergence,"After all, it’s been a while since I’ve written the linked posts, and for a fruitful, substantive discussion on this topic, it’s necessary to make sure that we have a solid grasp of what MLE and KL divergence are. MLE is a technique used to find the optimal parameter of a distribution that best describes a set of data. To cut to the chase, this statement can be expressed as follows: From here, we can start making assumptions, such as that observations in  are i.i.d, which is the assumption that we make to build models such as naïve Bayes, and so on. For now, it suffices to clarify that the goal of maximum likelihood estimation is to find the optimal parameter of a distribution that best captures some given data. KL divergence is a concept that arises from the field of information theory that is also heavily applied in statistics and machine learning. KL divergence is particularly useful because it can be used to measure the dissimilarity between to probability distributions.",0,0,1,0,0,0,0,0
On Expectations and Integrals,"Consider a constant random variable (I know it sounds oxymoronic, but the idea is that the random variable takes only one value and that value only). In this case, we can imagine the probability density function as a literal spike in the sense that the PDF will peak at  and be zero otherwise. The cumulative density function will thus exhibit a discontinuous jump from zero to 1 at . And by the same line of logic, it is easy to see that the expected value of this random variable is , as expected. Although this is a rather boring example in that the expectation of a constant is of course the constant itself, it nonetheless demonstrates the potential applications of Riemann-Stieltjes. I hope you enjoyed reading this post. Lately, I have been busy working on some interesting projects. There is a lot of blogging and catching up to do, so stay posted for exciting updates to come!.",0,0,1,0,0,0,0,0
A Step Up with  Variational Autoencoders,"The decoder network looks similar to the the encoder, except that much of the architecture is in reverse order. Most notably, we use  to undo the convolution done by the encoder. This allows us to effectively scale up the input back to its original dimension, which is what we want to do with a generative model like a VAE. One subtly worth mentioning is the fact that we use a sigmoid activation in the end. This is because we want the pixel values of the output to be between 0 and 1, just as the original input was normalized before it was fed into the encoder network via division by 255. The summary of the decoder network is presented below: Now that we have both the encoder and the decode network fully defined, it’s time to wrap them together into one autoencoder model. This can simply achieved by defining the input as the input of the encoder—the normalized MNIST images—and defining the output as the output of the decoder when fed a latent vector. Concretely, this process might look as follows: Let’s look a the summary of the CVAE.",0,1,0,0,0,0,0,0
Gaussian Process Regression,"The underlying idea is that we can apply Cholesky decomposition on , and use that as a way to circumvent the need for direct inversion. Let , then Let’s make another substitution, . Then, This means that we can calculate the mean  as Similarly, for the covariance , we can introduce an intermediate variable  from which we obtain Notice that the final expressions for mean and covariance do not require any form of inversion, which was our end goal for efficient and accurate computation. Let’s transcribe everything back to code. Let  refer to  (and by the same token  refers to ). Then, Just to be safe, let’s check that  is of the desired shape, namely a vector with 50 entries. Continuing with our computation of the posterior covariance, As expected,  is a 50-by-50 matrix. We are now almost done. Since we have computed the mean  and covariance , all there is left is to generate samples from this distribution. For that, we resort to Cholesky decomposition again, recalling the idea discussed earlier in (19). Let’s sample a total of 50 samples. now contains 50 samples generated from the posterior.",1,0,0,0,1,0,0,0
Word2vec from Scratch,"In a previous post, we discussed how we can use tf-idf vectorization to encode documents into vectors. While probing more into this topic and geting a taste of what NLP is like, I decided to take a jab at another closely related, classic topic in NLP: word2vec. word2vec is a technique introduced by Google engineers in 2013, popularized by statements such as “king - man + woman = queen.” The gist of it, as you may know, is that we can express words as vectors that encode their semantics in a meaningful way. When I was just getting starting to learn TensorFlow, I came across the embedding layer, which performed exactly this operation: transforming words into vectors. While I thought this process was extremely interesting, I didn’t know about the internals of this structure until today, particularly after reading this wonderful tutorial by Chris McCornick. In this post, we will be implementing word2vec, a popular embedding technique, from scratch with NumPy. Let’s get started! Instead of going over the concepts and implementations separately, let’s jump straight into the whole implementation process and elaborate on what is necessary along the way.",1,1,0,0,0,0,0,0
A Brief Introduction to Recurrent Neural Networks,"An interesting point to note about the learning curve of this model is that the test data accuracy and loss seem to stay roughly stagnant despite the progression of epochs. To better understand this phenomena, we probably have to run more trials with more data over longer iterations than we have done in this tutorial. This point notwithstanding, it is interesting to see how a single layer LSTM network can outperform a stacked RNN network.  The last up on this list is the one-dimensional convolutional neural network. The convent produced very remarkable results in this experiment, especially given its extremely short training time. Recurrent neural networks typically take a lot of time to train—even when they are not stacked—because each neuron is defined by a rather complicated operation involving many parameters, such as states, carriage, and so on. Convents, on the other hand, are relatively simpler, and thus take noticeably shorter to train and deploy. This tutorial demonstrates that convents can perform as well as simple recurrent networks to establish a baseline performance metric.  In this post, we briefly introduced and explored the concept of recurrent neural networks, how they work, and how to build them using the  functional API.",0,1,0,0,0,0,0,0
So What are Autoencoders?,"Next, we will add  noise to the data. Note that the MNIST dataset does not contain noise by default: we will have to artificially and intentionally tarnish the dataset to produce a noisy training set for the autoencoder model. The  function precisely performs this function. Using the  function, we can create a noisy sample. Note that  was set to 0.5, although I’d imagine other values within reasonable range would work equally well as well. Training the model is very simple: the training data is , the noisy dataset, and the predicted label is . Through this configuration, we essentially expect the autoencoder to be able to see noisy images, after which encoding and decoding is performed via a transformation to a latent dimension to ultimately reproduce a pristine image devoid of any noise. For experimental puposes, I tried using the  callback on Google Colab.  is a platform that gives developers full view of what happens during and after the training process. It makes observing metrics like loss and accuracy a breeze. I highly recommend that you check out this tutorial on how to use and configure this functionality on your notebook.",0,1,0,0,0,0,0,0
A PyTorch Primer,"Also, working with Django has somewhat helped me grasp the idea of classes more easily, which certainly helped me take in class-based concepts in PyTorch more easily. I distinctively remember people saying that PyTorch is more object-oriented compared to TensorFlow, and I might express agreement to that statement after having gone through the extreme basics of PyTorch. In the upcoming articles, I hope to use PyTorch to build more realistic models, preferrably in the domain of NLP, as that seems to be where PyTorch’s comparative advantage stands out the most compared to TensorFlow. Of course, this is not to say that I don’t like TensorFlow anymore, or that PyTorch is not an appropriate module to use in non-NLP contexts: I think each of them are powerful libraries of their own that provide a unique set of functionalities for the user. And being bilinguial—or even a polyglot, if you can use things like Caffe perhaps—in the DL module landscape will certainly not hurt at all. I hope you’ve enjoyed this article. Catch you up in the next one!.",0,1,0,0,0,0,0,1
"Newton-Raphson, Secant, and More","Notice that subtracting these two expression results in a lot of term cancellations. Dividing both sides by  yields From this result, we now know why the central divided difference method provides more accurate approximations of the derivative than do forward and backward divided difference. This is somewhat related to what we will be doing in the next section, so it’s a good intuition to have throughout when reading the rest of this article. Now that we have these tools for differential calculus, now comes the exciting part: solving non-linear equations. Specifically, we will be taking a look at two numerical methods: the Newton-Raphson method and the secant method. It’s time to put the methods we developed in the preceding sections to use for solving non-linear equations. Specifically, we’ll begin by taking look at a classic algorithm, the Newton-Raphson method. The Newton-Raphson method is one of the many ways of solving non-linear equations. The intuition behind the Newton-Raphson method is pretty straightforward: we can use tangent lines to approximate the x-intercept, which is effectively the root of the equation . Specifically, we begin on some point on the graph, then obtain the tangent line on that point.",0,0,0,0,0,1,0,0
InceptionNet in PyTorch,"The standard, go-to kernel size is three-by-three, but we never know if a five-by-five might be better or worse. Instead of engaging in time-consuming hyperparameter tuning, we let the model decide what the optimal kernel size is. Specifically, we throw the model three options: one-by-one, three-by-three, and five-by-five kernels, and we let the model figure out how to weigh and process information from these kernels. In the  below, you will see that there are indeed various branches, and that the output from these branches are concatenated to produce a final output in the  function. Researchers who conceived the InceptionNet architecture decided to add auxiliary classifiers to intermediary layers of the model to ensure that the model actually learns something useful. This was included in InceptionV1; as far as I’m aware, future versions of InceptionNet do not include auxiliary classifiers. Nonetheless, I’ve added it here, just for the fun of it. Now we finally have all the ingredients needed to flesh out the entire model. This is going to a huge model, but the code isn’t too long because we’ve abstracted out many of the building blocks of the model as  or .",0,1,0,0,0,0,0,1
(Attempt at) Knowledge Distillation,"One technicality we have not touched upon earlier is the fact that we need to apply some scaling to the KL divergence loss. The gradient of the  term, compared to , is smaller by a factor of . Thus, we need to multiply  to correct this difference in magnitude. Let’s also write a training loop for just training the model as is. We will essentially conduct a controlled experiment where we compare the result of knowledge distillation and vanilla training. And here is a simple evaluation function we will use to check the performance of each model at the end of training. Since we are merely evaluating the model, we don’t need to keep gradients. First, we train the teacher model. After 10 epochs, the teacher model manages to get an accuracy of around 98 percent. Next, we train the student model using the typical training scheme without knowledge distillation. Using this method, the student model manages to get an accuracy of 98 percent; in fact, 0.1 percentage point higher than that of the teacher model. At this point, I already knew that something was off, but I decided to continue with the experiment.",0,1,0,0,0,0,0,1
Word2vec from Scratch,"” If the model was trained properly, the most likely word should understandably be “machine.” And indeed, when that is the result we get: notice that “machine” is at the top of the list of tokens, sorted by degree of affinity with “learning.” Building and training was fun and all, but our end goal was not to build a neural network; we wanted to get word embeddings. As stated earlier in this post, the key behind word embeddings is that the rows of the first weight matrix is effectively a dense representation of one-hot encoded vectors each corresponding to various tokens in the text dataset. In our example, therefore, the embedding can simply be obtained by But of course, this is not a user-friendly way of displaying the embeddings. In particular, what we want is to be able to input a word through a function and receive as output the embedding vector for that given word. Below is a function that implements this feature. When we test out the word “machine,” we get a dense ten-dimensional vector as expected.",1,1,0,0,0,0,0,0
Likelihood and Probability,"Combining these two results, we would expect the maximum likelihood distribution to follow  where  =  and  =  in our code. And that concludes today’s article on (maximum) likelihood. This post was motivated from a rather simple thought that came to my mind while overhearing a conversation that happened at the PMO office. Despite the conceptual difference between probability and likelihood, people will continue to use employ these terms interchangeably in daily conversations. From a mathematician’s point of view, this might be unwelcome, but the vernacular rarely strictly aligns with academic lingua. In fact, it’s most often the reverse; when jargon or scholarly terms get diffused with everyday language, they often transform in meaning and usage. I presume words such as “likelihood” or “likely” fall into this criteria. All of this notwithstanding, I hope this post provided you with a better understanding of what likelihood is, and how it relates to other useful statistical concepts such as maximum likelihood estimation. The topic for our next post is going to be Monte Carlo simulations and methods. If “Monte Carlo” just sounds cool to you, as it did to me when I first came across it, tune in again next week.",0,0,1,0,0,0,0,0
PyTorch Tensor Basics,"Without getting into too much technical detail, we can roughly understand view as being similar to  in that it is not an in-place operation. However, there are some notable differences. For example, this Stack Overflow post introduces an interesting example: On the other hand,  does not run into this error. The difference between the two functions is that, whereas  can only be used on contiguous tensors. This SO thread gives a nice explanation of what it means for tensors to be contiguous; the bottom line is that, some operations, such , do not create a completely new tensor, but returns a tensor that shares the data with the original tensor while having different index locations for each element. These tensors do not exist contiguously in memory. This is why calling  after a transpose operation raises an error. , on the other hand, does not have this contiguity requirement. This felt somewhat overly technical, and I doubt I will personally ever use  over , but I thought it is an interesting detail to take note of nonetheless. Another point of confusion for me was the fact that there appeared to be two different ways of initializing tensors:  and .",0,1,0,0,0,0,0,1
Understanding the  Leibniz Rule,"But fear not, let’s apply the same approach to answer this question. This may appear to be a lot of computation, but all we’ve done is just separating out the integrals while paying attention to the domains of integration. Let’s continue by doing the same for the remaining terms. The first two terms in the limit go away since  goes to zero. While the same applies to the fractional terms, one difference is that they are also divided by , which is why they remain. We have simplified quite a bit, but we still have two terms in the limit expression that we’d like to remove. We can do this by applying the definition of the integral. And we’re done! There are other ways of seeing the Leibniz rule, such as by interpreting it as a corollary of the Fundamental Theorem of Calculus and the Chain rule, as outlined in here (by a Professor at Yale!), but I find the geometrically motivated interpretation presented in this article to be the most intuitive. I hope you enjoyed reading this post. Catch you up in the next one!.",0,0,0,0,0,1,0,0
The Exponential Family,"This is all great, but there is still an unanswered question lingering in the air: what is the MLE estimate of the parameter  ? This moment is precisely when equation (25) comes in handy. Recall that Therefore, Finally, we have arrived at our destination: We finally know how to calculate the parameter under which the likelihood of observing given data is maximized. The beauty of this approach is that it applies to all probability distributions that belong to the exponential family because our analysis does not depend on which distribution is in question; we started from the canonical form of the exponential family to derive a set of generic equations. This is the convenience of dealing with the exponential family: because they are all defined by the same underlying structure, the MLE equations hold general applicability. In this post, we explored the exponential family of distributions, which I flippantly ascribed the title “The Medici of Probability Distributions.",0,0,1,0,0,0,1,0
Traveling Salesman Problem with Genetic Algorithms,"For convenience purposes, we will represent cities by their indices. Now it’s time for us to understand how genetic algorithms work. Don’t worry, you don’t have to be a biology major to understand this; simple intuition will do. The idea is that, we can use some sort of randomized approach to generate an initial population, and motivate an evolutionary cycle such that only superior genes will survive successive iterations. You might be wondering what genes are in this context. Most typically, genes can be thought of as some representation of the solution we are trying to find. In this case, an encoding of the optimal path would be the gene we are looking for. Evolution is a process that finds an optimal solution for survival through competition and mutation. Basically, the genes that have superior traits will survive, leaving offspring into the next generation. Those that are inferior will be unable to find a mate and perish, as sad as it sounds. Then how do these superior or inferior traits occur in the first place? The answer lies in random mutations.",1,0,0,0,0,0,0,0
Fisher Score and Information,"And the variance-covariance matrix is simply a matrix that contains information on the covariance of multiple random variables in a neat, compact matrix form. A closed-form expression for the covariance matrix  given a random vector , which follows immediately from aforementioned definitions and some linear algebra, looks as follows: Enough of the prologue and review, now we’re ready to start talking about Fisher. The information matrix is defined as the covariance matrix of the score function as a random vector. Concretely, Note that the 0’s follow straight from the earlier observation that . Intuitively, Fisher’s information gives us an estimate of how certain we are about the estimate of the parameter . This can be seen by recognizing the apparent similarity between the definition of the covariance matrix we have defined above and the definition of Fisher’s information. In fact, the variance of the parameter  is explained by the inverse of Fisher’s information matrix, and this concept is known as the Cramer-Rao Lower Bound. For the purposes of this post, I won’t get deep into what CRLB is, but there are interesting connections we can make between Fisher’s information, CRLB, and the likelihood, which we will get into later.",0,0,1,0,0,0,0,0
How lucky was I on my shift?,"At the Yongsan Provost Marshall Office, I receive a wide variety of calls during my shift. Some of them are part of routine communications, such as gate checks or facility operation checks. Others are more spontaneous; fire alarm reports come in from time to time, along with calls from the Korean National Police about intoxicated soldiers who get involved in mutual assault or misdemeanors of the likes. Once, I got a call from the American Red Cross about a suicidal attempt of a soldier off post. All combined, I typically find myself answering about ten to fifteen calls per shift. But yesterday was a special day, a good one indeed, because I received only five calls in total. This not only meant that USAG-Yongsan was safe and sound, but also that I had a relatively light workload. On other days when lawlessness prevails over order, the PMO quickly descends into chaos—patrols get dispatched, the desk sergeant files mountains of paperwork, and I find myself responding to countless phone calls while relaying relevant information to senior officials, first sergeants, and the Korean National Police.",0,0,1,0,0,0,1,0
The Magic of Euler’s Identity,"At a glance, Euler’s identity is a confusing, mind-boggling mishmash of numbers that somehow miraculously package themselves into a neat, simple form: I remember staring at this identity in high school, trying to wrap my head around the seemingly discordant numbers floating around the equation. Today, I want to share some ideas I have learned since and demonstrate the magic that Euler’s identity can play for us. The classic proof for Euler’s identity flows from the famous Taylor series, a method of expressing any given function in terms of an infinite series of polynomials. I like to understand Taylor series as an approximation of a function through means of differentiation. Recall that a first-order derivative gives the slope of the tangent line at any given point of a function. The second-order derivative provides information regarding the convexity of the function. Through induction, we can convince ourselves that higher order derivatives will convey information about the curvature of the function throughout coordinate system, which is precisely the underlying mechanism behind Taylor’s series. In a more concise notation, we have Notice that  is the starting point of our approximation.",0,0,0,0,0,1,0,0
Demystifying Entropy (And More),"This is an important distinction to make, because entropy is essentially a probabilistically weighted average of all the random events that a random variable can take. In other words, entropy is defined as the weighted average of information given by each random event: For the continuous case, we would use an integral instead. Say  denotes the random variable of interest in a fair coin toss. Then, we are most interested in how much bits, on average, we would need to encode information generated from the distribution of this random variable. Using (3), we can easily answer this question by calculating the follows: This tells us that the entropy involved in a fair coin toss is 1 bit, i.e, on average, we only need a single digit of binary number to encode information given by a fair coin toss. But how might this number change for a biased coin? We would not expect the entropy of the random variable given by the result of a biased coin to be 1. For example, consider a coin that always lands on heads.",0,0,1,0,0,0,0,0
A sneak peek at Bayesian Inference,"Although our example was just a simple coin toss, the sample principle and mechanism can be extended to countless other situations, which is why Baye’s theorem remains highly relevant to this day, especially in the field of machine learning and statistical analysis. Bayesian statistics presents us with an interesting way of understanding probability. The classical way of understanding probability is the frequentist approach, which purports that a probability for an event is the limit of its frequency in infinite trials. In other words, to say that a coin is fair is to say that, theoretically, performing an infinite number of coin flips would result in 50 percent heads and 50 percent tails. However, the Bayesian approach we explored today presents a drastically different picture. In Bayesian statistics, probability is an embodiment of our subjective beliefs about a parameter, such as the fairness of a coin. By performing trials, infinite or not, we gain more information about the parameter of our interest, which affects the posterior probability. Both interpretations of probability are valid, and they help complement each other to help us gain a broader understanding of what the notion of probability entails.",0,0,1,0,0,0,1,0
Gaussian Process Regression,"For the sake of simplicity, however, we will only take a look at one such kernel that relates to smoothness: the squared exponential kernel, often referred to as the RBF kernel. The distance function of the squared exponential kernel looks as follows: We can apply distortions to the RBF function by adding things like coefficients, but for simplicity sake we omit them here. The key takeaway is that the RBF kernel function functions as a distance metric between two points. As an extreme example, let’s consider the case when , the diagonal entries of the covariance matrix, which is effectively the variance along those components. Then, Conversely, when  and  are extremely different points, We can thus deduce that the distance function returns a value between 0 and 1 that indicates the similarity or closeness between two points. This is exactly the sort of behavior we want for the covariance matrix. In short, the multivariate Gaussian that we will be using for GP regression can simply be summarized as where covariance  denotes the kernel matrix. In the RBF kernel function above, we were assuming a function without any noise, namely that .",1,0,0,0,1,0,0,0
Traveling Salesman Problem with Genetic Algorithms,"Here, we use a simple roulette model, where we compare the value of the probability vector and a random number sampled from a uniform distribution. If the value of the probability vector is higher, the corresponding chromosome is added to . We repeat this process until we have  parents. As expected, we get 4 parents after selecting the parents through . Now is the crucial part: mutation. There are different types of mutation schemes we can use for our model. Here, we use a simple swap and crossover mutation. As the name implies, swap simply involves swapping two elements of a chromosome. For instance, if we have , we might swap the first two elements to end up with . The problem with swap mutation, however, is the fact that swapping is a very disruptive process in the context of TSP. Because each chromosome encodes the order in which a salesman has to visit each city, swapping two cities may greatly impact the final fitness score of that mutated chromosome. Therefore, we also use another form of mutation, known as crossovers. In crossover mutation, we grab two parents.",1,0,0,0,0,0,0,0
InceptionNet in PyTorch,"I’m not going to train this model on my GPU-less MacBook, and if you want to use InceptionNet, there are plenty of places to find pretrained models ready to be used right out of the box. However, I still think implementing this model helped me gain a finer grasp of PyTorch. I can say this with full confidence because a full month has passed since I coded out this Jupyter notebook, and I feel a lot more confident in PyTorch than I used to before. I hope you’ve enjoyed reading this blog post. Catch you up in the next one (where I’ll probably post another old notebook that’s been sitting on my computer for a month).",0,1,0,0,0,0,0,1
Fast Gradient Sign Method,"In other words, the addition of  should not cause the model to behave any different in the absence of any perturbation. An adversarial example is one that which maximizes the value of  to sway the model into making a wrong prediction. Of course, there is a constraint to be placed on ; otherwise, we could just apply a huge perturbation to the input image, but then the perturbation could be visible enough to change the ground truth label. Hence, we apply a constraint such that The infinity norm is defined as which, more simply put, means the largest absolute value of the element in the matrix or vector. In this context, it means that the largest magnitude of the element in  does not exceed the precision constraint . Then, Goodfellow proceeds to explain the maximum bounds of this perturbation. Namely, given that the maximum bound of the change in activation can be written as where the average magnitude of an element of  is given by , and . This tells us that the change in activation given by the perturbation increases linearly with respect to , or the dimension.",0,1,0,0,0,0,0,1
"Newton-Raphson, Secant, and More","We confirm that this is indeed the root of the equation. Now that we have looked at both methods, it’s time to make a quick comparison. We will be comparing three different methods: By setting  to be , we can obtain a full list of root updates as the three methods begin their quest for the root of the function. We can then see which method converges the quickest. Let’s see how this little experiment turns out. We first begin by importing some dependencies to plot the history of values. Then, we obtain the history for each of the three approaches and plot them as a scatter plot. The result is shown below.  You might have to squint your eye to see that  (Netwon-Raphson with direct derivatives) and  (Newton-Raphson with center divided difference) almost coincide exactly at the same points. I was honestly somewhat surprised by the result: although we had verified the advantage of using center divided difference over forward or backward divided difference via some simple big-O analysis with trailing error terms, I did not expect the two to coincide with such exactitude.",0,0,0,0,0,1,0,0
Markov Chain and Chutes and Ladders,"For example,  does not reflect the fact that getting a 1 on a roll of the dice will move the player up to the thirty eighth cell; it supposes that the player would stay on the first cell. The new permutation matrix  would adjust for this error by reordering . For an informative read on the mechanics of permutation, refer to this explanation from Wolfram Alpha. Let’s perform a quick sanity check to verify that  contains the right information on the first ladder, namely the entry  in the  dictionary. Notice the  in the th entry hidden among a haystack of 100 s! This result tells us that  is indeed a permutation matrix whose multiplication with  will produce the final stochastic vector that correctly enumerates the probabilities encoded into the Chutes and Ladders game board. Here is our final product: We can visualize the stochastic matrix  using the  package. This produces a visualization of our stochastic matrix. So there is our stochastic matrix! Now that we have a concrete matrix to work with, let’s start by identifying its eigenvectors.",0,0,0,1,0,0,0,0
A Step Up with  Variational Autoencoders,"This can be achieved through some clever algebraic manipulation: But since the the expected value of  is constant and that of  is zero, We can now plug this simplified expression back into the calculation of KL divergence, in (19): Since we will standardize our input such that  and , we can plug these quantities into (22) and show that We are almost done with deriving the expression for ELBO. I say almost, because we still have not dealt with the trailing term in (13): At this point, it is extremely useful to recall the definition of cross entropy, which is generically defined as follows: Therefore, we see that the trailing term in (13) is just a cross entropy between two distributions! This was a circumlocutions journey, but that is enough math we will need for this tutorial. It’s time to get back to coding. All that math was for this simple code snippet shown below: As you can see, this short code snippet shows, in essence, how we can define a compile a model with a custom loss function. In this case,  refers to the reconstruction loss, which is the cross entropy term we saw earlier.",0,1,0,0,0,0,0,0
The Exponential Family,"” This is obviously my poor attempt at an intellectual joke, to which many of you might cringe, but I personally think it somewhat captures the idea that many probability distributions that we see on the textbook are, in fact, surprisingly more related than we might think. At least to me, it wasn’t obvious from the beginning that the exponential and the Bernoulli distributions shared the same structure, not to mention the wealth of other distributions that belong to the exponential family.  Also, the convenient factorization is what allowed us to perform an MLE estimation, which is an important concept in statistics with wide ranging applications. This post in no way claims to give a full, detailed view of the exponential family, but hopefully it gave you some understanding of what it is and why it is useful. In the next post, we will take a look at maximum a posteriori estimation and how it relates to the concept of convex combinations. Stay tuned for more.",0,0,1,0,0,0,1,0
A Step Up with  Variational Autoencoders,"Note that the encoder and the decoder look like individual layers in the grand scheme of the VAE architecture. We have almost everything we need, but there is one crucial step that is missing: compiling the model with an optimizer and a loss function. Normally, defining a loss function is very easy: in most cases, we  use pre-made loss functions that are available through the TensorFlow API, such as cross entropy or mean squared error. In the case of variational autoencoders, however, this is not such an easy task: how do we judge the robustness or the effectiveness of the decoder, which is essentially a generative algorithm? Of course, we could stop training once the figures it generates becomes reasonable, i.e. the mock MNIST digits it creates looks compelling to the human eye. However, this is a subjective metric at best, and we can’t expect there to be a ML engineer peering at the screen, looking at the outputs of the decoder per each epoch. To tackle this challenge, we need to dive into some math. Let’s take a look. First, let’s carefully review what our goal is for this task.",0,1,0,0,0,0,0,0
Principal Component Analysis,"Let’s define this transformation as , and the matrix corresponding to the decoding . In other words, PCA makes a number of assumptions to simplify this problem. The most important assumption is that each column of  is orthogonal to each other. As we will see later in an alternate derivation with statistics, this has to do with the notion of covariance. Another restriction is that the columns of  must have a Euclidean norm of one. This constraint is necessary for us to find a unique matrix  that achieves compression—otherwise, we could have any multiples, leading to an infinite number of such matrices. We make one more convenient assumption about the given data points, . That is,  is assumed to have a mean of zero, i.e. . If this is not the case, we can easily perform standardization by subtracting the mean from the data. With this setup in mind, let’s finally start the derivation. As said earlier, the goal of PCA is to compress data (and be able to uncompress it) with as little loss of information as possible.",0,0,1,1,0,0,0,0
"PyTorch, From Data to Modeling","Indeed, plotting  makes it clear that the loss has been decreasing, though not entirely in a steady, monotonous fashion.  In retrospect, we could have probably added a batch normalization layer to stabilize and expedite training. However, since this post is largely meant as an introduction to PyTorch modeling, not model optimization or design, the example suffices. Now we have finally reached the last step of the development cycle: testing and evaluating the model. This last step will also require us to write a custom loop as we receive batches of data from the  object we’ve created above. The good news, however, is that the testing loop is not going to look too much different from the training loop; the only difference will be that we will not be backpropagating per each iteration. We will also be using  to make sure that the model is in the evaluation mode, not its default training mode. This ensures that things like batch normalization and dropout work correctly. Let’s talk briefly about the details of this loop. Here, the metric we’re collecting is accuracy. First, we generally see how many correct predictions the model generates.",0,1,0,0,0,0,0,1
A Brief Introduction to Recurrent Neural Networks,"Luckily, there isn’t going to be much preprocessing involved since we will be using a dataset available from the  library, the IMBD movie reviews dataset. The dataset contains 25,000 movie reviews from IMDB, labeled as either positive or negative. Each review is encoded as a sequence of integers according to a consistent encoding scheme. In other words, each integer corresponds to a unique word in the vocabulary of the dataset. More specifically, the integer to which a word is mapped corresponds to the frequency with which the word appears, i.e. the word encoded as 10 corresponds to the 10th most frequent word in the data. We will apply some very basic preprocessing on the dataset so that we can feed it into our model. Specifically, we will preprocess the dataset such that only a  number of most frequently occurring words are considered. This step will weed out words that occur very infrequently, thus decreasing the amount of noise from the network’s perspective. Next, we will apply padding to the dataset so that all reviews are of length . This means that longer reviews will be truncated, whereas shorter reviews will be padded with zero entries.",0,1,0,0,0,0,0,0
Naive Bayes Model From Scratch,"Note that instead of using the for-loop approach used in previous posts, this function is more vectorized, making computation less expensive. The shorter code is also an added benefit. The accuracy of our from-scratch model is 97 percent, which is not bad for a start. Let’s see if the  model in  outperforms our hand-coded model. The accuray score yielded by  is exactly identical to that achieved by our model! Looks like the  model in scikit-learn does exactly what our model does, at least juding from the metric of accuracy. This is surprising, but since we basically followed the Bayesian line of reasoning to buid our model, which is what naive Bayes really is all about, perhaps this is not as astonishing as it seems. In this post, we built the Gaussian naive Bayes model from scratch. In the process, we reviewed key concepts such as Bayesian inference and maximum a posteriori estimation, both of which are key statistical concepts used in many subdomains of machine learning. Hopefully through this tutorial, you gained a better understanding of how Gaussian mathematics and Bayesian thinking can be used in the context of classification.",1,0,0,0,1,0,0,0
"0.5!: Gamma Function, Distribution, and More","In a previous post, we looked at the Poisson distribution as a way of modeling the probability of some event’s occurrence within a specified time frame. Specifically, we took the example of phone calls and calculated how lucky I was on the day I got only five calls during my shift, as opposed to the typical twelve. While we clearly established the fact that the Poisson distribution was a more accurate representation of the situation than the binomial distribution, we ran into a problem at the end of the post: how can we derive or integrate the Poisson probability distribution, which is discontinuous? To recap, let’s reexamine the Poisson distribution function: As you can see, this function is discontinuous because of that one factorial term shamelessly flaunting itself in the denominator. The factorial, we might recall, is as an operation is only defined for integers. Therefore, although we can calculate expression such as , we have no idea what the expression  evaluates to. Or do we? Here is where the Gamma function kicks in. This is going to be the crux of today’s post.",0,0,1,0,0,0,1,0
Revisiting Basel with Fourier,"While a lot of this is just simple calculus and algebra, I nonetheless find it fascinating how the Basel problem can be approached from so many different angles—hence my renewed respect for Euler and other mathematicians who wrestled with this problem hundreds of years ago. As simple as it appears, there are so many different techniques and modes of analysis we can use to approach the problem. It was nice exercise and review of some calculus techniques. I’ve been digging more into the curious interchange of integral and summation recently, and when this operation is allowed, if at all. Turns out that this problem is slightly more complicated than it appears and requires some understanding of measure theory, which I had tried getting into a few months ago without much fruition. Hopefully this time, I’ll be able to figure something out, or at the very least gain some intuition on this operation. I hope you’ve enjoyed reading this post. Catch you up in the next one.",0,0,0,0,0,1,0,0
Word2vec from Scratch,"We would expect this number to have been larger had we used a larger window. 60 is the size of our corpus, or the number of unique tokens we have in the original text. Since we have one-hot encoded both the input and output as 60-dimensional sparse vectors, this is expected. Now, we are finally ready to build and train our embedding network. At this point, you might be wondering how it is that training a neural network that predicts some nearby context word given an input token can be used to embed words into vectors. After all, the output of the network is going to be some probability vector that passed through a softmax layer, not an embedding vector. This is entirely correct, and this is a question that came to my mind as well. However, this is the part that gets the most interesting: the rows of the intermediate weight matrix is the embedding we are looking for! This becomes much more apparent once we consider the dimensions of the weight matrices that compose the model.",1,1,0,0,0,0,0,0
First Neural Network with Keras,"Everything looks good, which means we are now ready to compile and train our model. Before we do that, however, it is always a good idea to use the  module to ensure that gradient descent stops when no substantial weight adjustments are being made to our model. In other words, when the model successfully finds the local minimum (or preferably the global minimum), the  will kick in and stop gradient descent from proceeding with further epochs. We are now ready to go! Let’s compile the model by making some configurations, namely the , , and . Simply put, an  specifies which flavor of the gradient descent algorithm we want to choose. The simplest version is known as , or the stochastic gradient descent.  can be considered an improved version of the stochastic gradient descent in that its learning rate changes depending on the slope of the loss function, defined here as cross entropy. If you recall, cross entropy is basically a measurement of the pseudo-distance between two distributions, i.e. how different two distributions are. But because cross entropy is often not easy to intuitively wrap our minds around, let’s pass the  metric to the  function, as shown below.",0,1,0,0,1,0,0,0
How lucky was I on my shift?,"So let me ask the question again: how lucky was I yesterday? The probability distribution function of the Poisson distribution tells us that  can be calculated through the following equation: The result given by the Poisson distribution is somewhat larger than that derived from the binomial distribution, which was . This discrepancy notwithstanding, the fact that I had a very lucky day yesterday does not change: I would have days like these once every 100 days, and those days surely don’t come often. But to really calculate how lucky I get for the next 18 months of my life in the military, we need to do a bit more: we need to also take into account the fact that receiving lesser than 5 calls on a shift also constitutes a lucky day. In other words, we need to calculate , as shown below: This calculation can be done rather straightforwardly by plugging in numbers into the Poisson distribution function as demonstrated above. Of course, this is not the most elegant way to solve the problem. We could, for instance, tweak the Poisson distribution function and perform integration.",0,0,1,0,0,0,1,0
Revisiting Basel with Fourier,"In the last post, we revisited the Riemann Zeta function, which we had briefly introduced in another previous post on Euler’s take on the famous Basel problem. It seems like math is my jam nowadays, so I decided to write another post on this topic—but this time, with some slightly different takes. In this post, we will explore an alternative way of solving the Basel problem using Fourier series expansion, and also discuss a alternative representations of the Basel problem in integral form. For the integral representations, I’m directly referencing Flammable Maths, a YouTube channel that I found both entertaining and informative. Let’s get started. First, let’s recall what the Basel problem is. The problem is quite simple: the goal is to obtain the value of an infinite series, namely This seems like an innocuous, straightforward problem. One can easily prove, for instance, the fact that this series converges using integral approximation. However, to obtain the value of this series is a lot more difficult than it appears—it is no coincidence that this problem remained unsolved for years until Euler came along.",0,0,0,0,0,1,0,0
"Beta, Bayes, and Multi-armed Bandits","Namely, given the new posterior we know that sampling from that new posterior will give us a mean value that somewhat resembles the crude approach we would have taken without the prior expectation. Here, the crude approach is referring to the raw frequentist estimate we would have made had we not taken the Bayesian approach to the problem. It is obvious that the two hyperparameters of the prior are acting as an initial weight of sorts, making sure that when little data is available, the prior overshadows observations, but when ample amount of data is collected and available, eventually yields to those observations to estimate the parameter. Now we can turn our attention back to the multi-armed bandit problem. Now we can build on top of our knowledge of the Beta-Binomial update and refine what the frequentist greedy approach. We will also write out some simple functions to simulate the bandit problem and thus demonstrate the effectiveness of the Bayesian approach. Note that a lot of the code in this post was largely inspired by Peter’s blog. Before we get into any modeling, let’s first import the modules we’ll need and set things up.",0,0,1,0,0,0,1,0
"Basel, Zeta, and some more Euler","At this point, it doesn’t even surprise us to know that Euler applied this line of thinking to calculate the value of the sum of higher order inverses. An interesting corollary of Euler’s solution to the Basel problem is the Wallis product, which is a representation of the quantity  as an infinite product, as shown below: It seems mathematically unintuitive to say that an irrational number such as  can be expressed as a product of fractions, which is a way of representing rational numbers. However, we can verify the soundness of the Wallis product by substituting  for  in (1): Taking the reciprocal of this entire expression reveals the Wallis product. The Basel problem is a specific case of the Riemann zeta function, whose general form can be written as follows. A small digression: when , the zeta function converges to a value known as Apéry’s constant, eponymously named after the French mathematician who proved its irrationality in the late 20th century. Beyond the field of analytics and pure math, the zeta function is widely applied in fields such as physics and statistics. Perhaps we will explore these topics in the future. So back to the zeta function.",0,0,0,0,0,1,0,0
Bayesian Linear Regression,"For any regression problem, we first need a data set. Let  denote this pre-provided data set, containing  entries where each entry contains an -dimensional vector and a corresponding scalar. Concretely, where The goal of Bayesian linear regression is to find the predictive posterior distribution for . This is where the difference between Bayesian linear regression and the normal equation method becomes most apparent. Whereas vanilla linear regression only gives us a single point estimate given an input vector, Bayesian linear regression gives an entire distribution. For the purposes of our demonstration, we will define the predictive posterior to take the following form as shown below, with precision  pre-given. Precision is simply the reciprocal of variance and is commonly used as an alternative way of parametrizing Gaussian distributions. In other words, we assume the model Our goal will be to derive a posterior for this distribution by performing Bayesian inference on , which corresponds to the slope of the linear regression equation, where  denotes noise and randomness in the data, thus affecting our final prediction. To begin Bayesian inference on parameter , we need to specify a prior. Our uninformed prior will look as follows.",0,0,0,1,0,0,0,0
Dissecting the Gaussian Distribution,"At this point in time, one might point out that covariance is not really a multivariate concept since it is defined for only two variables, not three or more. Indeed, the expression  is mathematically incoherent. However, covariance can be  a multivariate metric since we can express the covariance of any pairs of random variables by constructing what is called the covariance matrix. Simply put, the covariance matrix is a matrix whose elements are the pairwise covariance of two random variables in a random vector. Before we get into the explanation, let’s take a look at the equation for the covariance matrix: where  and . This is the matrix analogue of the expression which is an alternate definition of variance. It is natural to wonder why we replaced the squared expression with  instead of  as we did earlier with the term in the exponent. The simplest answer that covariance is expressed as a matrix, not a scalar value. By dimensionality,  produces a single scalar value, whereas  creates a matrix of rank one.",0,0,1,0,0,0,1,0
Demystifying Entropy (And More),"Calculating the product of these ratios will tell us which distribution is more likely given all available data points. In other words, A technique we saw when we were exploring the topic of likelihood maximization was log likelihood. Log likelihoods are useful because we can reexpress products as sums, using the property of logs. (7) makes sense, but it weighs all likelihood ratios equally. In reality, most samples are not equiprobable; some values are more likely than others, unless in the context of uniform distributions. To account for this, let’s reframe (7) as an expected values calculation, i.e. give different weight to each likelihood ratio depending on the probability of observing that data point. Let’s make (8) look better by unpacking the fraction sitting in the log function as a subtraction of two terms. The final key is to realize the secret that In other words, we have derived the mathematical definition of KL divergence! The mathematical definition of cross entropy can simply be derived by plugging in (10) into (5).",0,0,1,0,0,0,0,0
Scikit-learn Pipelines with Titanic,"In such cases, using this library to visualize where missing values occur is a good idea, as this is an additional dimension of information that calling  wouldn’t be able to reveal. Now that we have a rough sense of where missing values occur, we need to decide from one of few choices: Indeed, in this case, we will go ahead and drop the  attribute. This choice becomes more obvious when we compute the percentage of null values. This shows us that 77 percent of the rows have missing  attribute values. Given this information, it’s probably a bad idea to try and impute these values. We opt to drop it instead. Correlation (or somewhat equivalently, covariance) is a metric that we always care about, since ultimately the goal of ML engineering is to use a set of input features to generate a prediction. Given this context, we don’t want to feed our model useless information that lacks value; instead, we only want to feed into the model highly correlated, relevant, and informative features.",0,0,0,0,1,0,0,0
Naive Bayes Model From Scratch,"So we can discard this piece of information and distill (2) down even farther: Equation (3) tells us that it is possible to calculate the probability of instance  belonging to class  systematically. Why is this important? The simple answer is that we can use (3) to train the naive Bayes classifier. Say we know that for a particular instance , the label is . Then, we have to find the distribution for each feature such that we can maximize . Does this ring any bells? Yes—it is maximum a posteriori estimation! In other words, our goal would be to maximize the posterior distribution for each training instance so that we can eventually build a model that would output the most likely label that the testing instance belongs to. In other words, our training scheme can be summarized as: But this is all to abstract. Let’s get into the details by implementing the naive Bayes classifer from scratch. Before we proceed, however, I must tell you that there are many variations of the naive Bayes classifer.",1,0,0,0,1,0,0,0
"0.5!: Gamma Function, Distribution, and More","After some thinking, we can convince ourselves that unraveling the sigma results in a chain reaction wherein adjacent terms nicely cancel one another, ultimately collapsing into a single term, which also happens to be the first term of the expression: Recalling that , we can rewrite the expression as follows: Notice the structural identity between the form we have derived and the equation of the Gamma distribution function introduced above, with parameters , , and . In the language of Poisson, these variables translate to , , and , respectively. To develop and intuition of the Gamma distribution, let’s quickly plot the function. If we determine the values for the parameters  and , the term  reduces to some constant, say , allowing us to reduce the PDF into the following form: This simplified expression reveals the underlying structure behind the Gamma distribution: a fight for dominance between two terms, one that grows polynomially and the other that decays exponentially. Plotting the Gamma distribution for different values of  and  gives us a better idea of what this relationship entails. Executing this code block produces the following figure.",0,0,1,0,0,0,1,0
Markov Chain and Chutes and Ladders,"But an issue with using expected value as a metric of analysis is that long games with infinitesimal probabilities are weighted equally to short games of substantial probability of occurrence. This mistreatment can be corrected for by other ways of understanding the distribution, such as median: This function tries to find the point in the cumulative distribution where the value is closest to , i.e. the median of the distribution. The result tells us that about fifty percent of the games end after 29 turns. Notice that this number is smaller than  because it discredits more of the long games with small probabilities. The Markov chain represents an in interesting way to analyze systems that are memoryless, such as the one in today’s post, the Chutes and Ladders game. Although it is a simple game, it is fascinating to see just how much information and data can be derived from a simple image of the game board. In a future post, we present another way to approach similar systems, known as Monte Carlo simulations. But that’s for another time. Peace!.",0,0,0,1,0,0,0,0
"Linear Regression, in Two Ways","This analogue can be extended to other statements in matrix calculus. For instance, where  is a symmetric matrix. We can easily verify this statement by performing the calculation ourselves. For simplicity’s sake, let’s say that  is a two-by-two matrix, although it could theoretically be any -by- matrix where  is some positive integer. Note that we are dealing with square matrices since we casted a condition on  that it be symmetrical. Let’s first define  and  as follows: Then, We can now compute the gradient of this function according to (3): We have not provided an inductive proof as to how the same would apply to -by- matrices, but it should now be fairly clear that , which is the single-variable calculus analogue of saying that . In short, 
 With these propositions in mind, we are now ready to jump back into the linear regression problem. At this point, it is perhaps necessary to remind ourselves of why we went down the matrix calculus route in the first place.",0,0,0,1,0,0,0,0
Fisher Score and Information,"Because Fisher’s information requires computing the expectation given some probability distribution, it is often intractable. Therefore, given some dataset, often times we use the empirical Fisher as a drop-in substitute for Fisher’s information. The empirical Fisher is defined quite simply as follows: In other words, it is simply an unweighted average of the covariance of the score function for each observed data point. Although this is a subtlety, it helps to clarify nonetheless. Something that may not be immediately apparent yet nonetheless true and very important about Fisher’s information is the fact that it is the negative expected value of the second derivative of the log likelihood. In our multivariate context where  is a vector, the second derivative is effectively the Hessian. In other words, You might be wondering how the information matrix can be defined in two says, the covariance and the Hessian. Indeed, this threw me off quite a bit as well, and I struggled to find and understand a good resource that explained why this was the case. Thankfully, Mark Reid’s blog and an MIT lecture contained some very helpful pointers that got me a long way.",0,0,1,0,0,0,0,0
Word2vec from Scratch,"Note that actual models tend to use much higher dimensions, most commonly 300, but for our purposes this is not necessary. Let’s begin with forward propagation. Coding the forward propagation process simply amounts to transcribing the three matrix multiplication equations in (6) into NumPy code. For backpropagation, we will need all the intermediate variables, so we hold them in a dictionary called . However, if we simply want the final prediction vectors only, not the cache, we set  to . This is just a little auxiliary feature to make things slightly easier later. We also have to implement the  function we used above. Note that this function receives a matrix as input, not a vector, so we will need to slightly tune things up a bit using a simple loop. At this point, we are done with implementing the forward pass. However, before we move on, it’s always a good idea to check the dimensionality of the matrices, as this will provide us with some useful intuition while coding backward propagation later on.",1,1,0,0,0,0,0,0
An Introduction to Markov Chain Monte Carlo,"The trace plot below shows that, although the MCMC model does manage to sample many values, it likes to stay too much in its current state, thus making taking much longer for the sampler to properly estimate the posterior by sampling a wide range of values. The bottom line is that setting the right proposal distribution is important, and that trace plots are a good place to start to check if the proposal distribution is set up properly.  Now, it’s time to look at the answer key and see if our sampler has done well. Let’s plot  sampled by our Metropolis-Hastings sampler with the analytic posterior to see if they roughly match.  Fantastic! Although the estimated posterior is not exactly equal to the analytic posterior, the two are quite similar to each other. We could quantify how similar or different they are by using metrics such as , but for simplicity’s sake, let’s contain the post within the realm of Bayes as we have done so far.",1,0,0,0,0,0,0,0
Fisher Score and Information,"In calculating the expected value, we will be using integrals, which is where the seemingly trivial statements we established earlier come in handy. By linearity of expectation, we can split this expectation up into two pieces. Let’s use integrals to express the first expectation. The good news is that now we see terms canceling out each other. Moreover, from the Leibniz rule and the interchanging of the integral and the derivative, we have shown that the integral in fact evaluates to zero. This ultimately leaves us with Therefore we have established that And we’re done! In this post, we took a look at Fisher’s score and the information matrix. There are a lot of concepts that we can build on from here, such as Cramer Rao’s Lower Bound or natural gradient descent, both of which are interesting concepts at the intersection of machine learning and statistics. Although the derivation is by no means mathematically robust, it nonetheless vindicates a notion that is not necessary apparently obvious, yet makes a lot of intuitive sense in hindsight. I personally found this video by Ben Lambert to be particularly helpful in understanding the connection between likelihood and information.",0,0,1,0,0,0,0,0
On Expectations and Integrals,"You might be wondering why the Riemann-Stieltjes integral is necessary in the first place. After all, the definition of expectation we already know by heart should be enough, shouldn’t it? To answer this question, consider the following  function: This cumulative mass function is obviously discontinuous since it is a step-wise function. This also means that it is not differentiable; hence, we cannot use the definition of expectation that we already know. However, this does not mean that the random variable  does not have an expected value. In fact, it is possible to calculate the expectation using the Riemann-Stieltjes integral quite easily, despite the discontinuity! The integral we wish to calculate is the following: Therefore, we should immediately start visualizing splitting up the domain of integration, the real number line, into infinitesimal pieces. Each box will be of height  and width . In the context of the contrived example, this definition makes the calculation extremely easy, since   equals zero in all locations but the jumps where the discontinuities occur. In other words, We can easily extend this idea to calculating things like variance or other higher moments. A more realistic example might be the Dirac delta function.",0,0,1,0,0,0,0,0
Building Neural Network From Scratch,"The most commonly used loss function in the context of classification problems is cross entropy, which we explored in this post previously on this blog. For a brief recap, presented below is the formula for calculating cross entropy given a true distribution  and a predicted distribution : Our goal is to train our neural network so that is output distribution  is as close to  as possible. In the case of binary classification, we might alter equation (9) to the following form: The reformulation as shown in equation (10) is the formula for what is known as binary cross entropy. This is the equation that we will be using in the context of our problem, since the dataset we have only contains two class labels of 0 and 1. Now that we have an idea of what the loss function looks like, it’s time to calculate the gradient. Since we are going to be back propagating the gradient, it makes sense to start from the very back of the neural network. Recall that our neural network is structured as follows: The last layer is a softmax unit that receives input  to produce output .",1,1,0,1,0,0,0,0
BLEU from scratch,"This value, however, is clipped by , which is the maximum number of occurrence of that n-gram in any one of the reference sentences. In other words, for each reference, we count the number of occurrence of that n-gram and take the maximum value among them. This can seem very confusing, but hopefully it’s clearer once you read the code. Here is my implementation using . Notice that we use a  in order to remove redundancies.  corresponds to ;  corresponds to . Using this modified metric, we can see that the  is now penalized quite a lot through the clipping mechanism. But there are still problems that modified precision doesn’t take into account. Consider the following example translation. To us, it’s pretty obvious that  is a bad translation. Although some of the key words might be there, the order in which they are arranged violates English syntax. This is the limitation of using unigrams for precision analysis. To make sure that sentences are coherent and read fluently, we now have to introduce the notion of n-grams, where  is larger than 1. This way, we can preserve some of the sequential encoding in reference sentences and make better comparison.",1,1,0,0,0,0,0,0
Convex Combinations and MAP,"The second equality is due to proportionality, whereby  is independent of  and thus can be removed from the argmax operation. The fourth equality is due to the monotonically increasing nature of the logarithmic function. We always love using logarithms to convert products to sums, because sums are almost always easier to work with than products, especially when it comes to integration or differentiation. If any of these points sounds confusing or unfamiliar, I highly recommend that you check out my articles on MAP and MLE. To proceed, we have to derive concrete mathematical expressions for the log likelihood and the log prior. Recall the formula for the univariate Gaussian that describes our data: Then, from (1), we know that the likelihood function is simply going to be a product of the univariate Gaussian distribution. More specifically, the log likelihood is going to be the sum of the logs of the Gaussian probability distribution function. There is the log likelihood function! All we need now is the log prior. Recall that the prior is a normal distribution centered around mean  with standard deviation of 1.",0,0,1,0,0,0,0,0
Recommendation Algorithm with SVD,"The plot is shown below:  With some alteration of the viewing angle, now we see through visualization that Movies 2 and 3 are close, as we had expected from the original ratings matrix . This is an interesting result, and it shows just how powerful singular value decomposition is at extracting important patterns from given data sets. Now that we understand what SVD does for us, it’s time to code our recommender function that uses distance calculation to output movie recommendations. In this post, we will be using the dot product as a means of determining distance, although other metrics such as Euclidean distance would suit our purposes as well. An advantage of using the dot product is that it is computationally less expensive and easy to achieve with code, as shown below. The  function recommends an  number of movies given that the user rated  highly. For example, let’s say some user really liked Movie 2 and is looking for two more movies that are similar to Movie 2. Then, we can simply call the function above by passing in appropriate arguments as follows.",1,0,0,1,0,0,0,0
Introduction to tf-idf,"” I know that I use this word a lot before writing down equations or formulas, just for the sake of notational clarity. However, the word itself carries little information on what the post is about. The same goes for other words, such as “example,” “however,” and so on. So term frequency only doesn’t really tell us much; instead, we want to pay attention to words that occur frequently in a given document, but doesn’t appear a lot in others—such words are most likely to be unique keywords that potentially capture the gist of that document. Given this analysis, it isn’t difficult to see why tf-idf is designed the way it is. Although we give priority weight to words with high term frequency, we discount words that appear frequently across all documents by dividing tf by idf, or inverse document frequency. In short, document frequency tells us how frequently a given word appears throughout all documents; the inverse is the reciprocal of that quantity. In practice, we often apply a logarithm to prevent the idf score from exploding. Also, we add some smoothing to prevent division by zero.",1,0,0,0,0,0,0,0
Demystifying Entropy (And More),"If bits sounds similar to bytes or gigabytes we use for storage, you’re exactly on the right path. In fact, the relationship between bit and byte is established directly by the fact that where  denotes bits and  denotes bytes. This is why we use bytes to represent the amount of disk storage in computers, for instance. It is also worth mentioning that the alternative name for bits is Shannons, named eponymously after the mathematician who pioneered the field of information theory, as mentioned above. Now that we have some idea of what information is and how we can quantify it using binary numbers in bits, it’s time to get into the math. Information can be calculated through the formula where  is the information need to express the random event , and  is the probability that event  occurs, i.e. . There are different versions of this formula, such as the one that uses Euler’s constant as the log base instead of 2. Whereas the unit of information as measured through (1) is in bits, that calculated through (2) as shown below is in the unit of nats.",0,0,1,0,0,0,0,0
Wonders of Monte Carlo,"Our goal here is to simulate this motion many times to estimate the probability that the man would successfully end up in the restroom to go about his business. This example was borrowed from this post by Zacharia Miller. Before we start typing up some code, let’s first lay down the ground rules of this simulation. First, we assume that the pub is modeled as a ten-by-ten grid, the bottom-left point defined as  and the top-right . The drunkard will start his walk at his table, represented by the coordinate . For each walk, function will generate a random number to determine the direrction of his movement. The magnitude of each walk is 1 by default. Beforer a walk is performed, we will invoke another function to check if his movements are legal, i.e. whether he stepped out of the boundary of the pub. If his moves are legal, we continue with the movement; if not, we stop and assume that the trial has yielded a failure. The goal of this random walk is to end up in the top-right portion of the pub, a square defined by coordinates , and .",0,0,1,0,0,0,0,0
A Brief Introduction to Recurrent Neural Networks,"Neural networks are powerful models that can be used to identify complex hidden patterns in data. There are many types of neural networks, two of which we have seen already on this blog: the vanilla, feed-forward neural network and convolutional neural networks, often abbreviated as convnets. Today, we will add a third kind to this exciting mix: recurrent neural networks, or RNNs. Let’s take a brief conceptual look at how recurrent neural networks work, then implement a toy RNN to see how it compares to other models on the IMDB movie reviews dataset. I heavily borrowed my examples from Deep Learning with Python by François Chollet and the tutorial on text classification available from the official TensorFlow documentation. Recurrent neural networks, as the name implies, refer to neural network models that contain some sort of internal looping structure that simulates a flow of information. A good way to conceptualize this loop is to think of something like  loops, where a certain operation is performed repeatedly for a specified number of cycles. Given these pieces of information, we might ask ourselves two questions.",0,1,0,0,0,0,0,0
A PyTorch Primer,"If we try to call  on any of the other intermediate variables, such as  or , PyTorch will complain. Let’s try to understand the result of this computation. Let  denote the final  tensor. Since we called , and since  has a total of four elements, we can write out our dummy calculations mathematically in the following fashion: Using partial differentiation to obtain the gradients, Since , Since  is just an arbitrary, non-specific index out of a total of four, we can easily see that the same applies for all other indices, and hence we will end up with a matrix whose all four entries take the value of 4.5, as PyTorch has rightly computed. We can go even a step farther and declare custom operations. For example, here’s a dummy implementation of the ReLU function. Let’s talk about the  method first. Note that it takes in two argument parameters:  and . As you might have guessed,  is simply the value that the function will be provided with. The  can simply be thought of as a cache where we can store vectors or matrices to be used during backpropagation. In this case, we store the  by calling  method.",0,1,0,0,0,0,0,1
Introduction to seq2seq models,"For a very long time, I’ve been fascinated by sequence-to-sequence models. Give the model a photo as input, it spits out a caption to go along with it; give it some English text, it can translate it into another language. Seq2seq models are also not only widely applicable in different contexts, but it also arguably laid the groundwork for other more advanced models that came after, such as attention and transformers. In studying seq2seq models, I found Ben Trevett’s sequence modeling tutorials extremely helpful. In particular, this post is heavily based off of this notebook. With that cleared up, let’s get started! For this tutorial, we will need to import a number of dependencies, mainly from  and .  is a library that provides a nice interface to dealing with text-based data in PyTorch. includes a  class, which essentially allows us to define some preprocessing steps to be applied on the data. We will be using the  dataset, which contains translations of short texts from many languages. In this tutorial, we will be using German and English, so we define preprocessing steps for each language.",0,1,0,0,0,0,0,1
Traveling Salesman Problem with Genetic Algorithms,"As the name implies, genetic algorithms somewhat simulate an evolutionary process, in which the principle of the survival of the fittest ensures that only the best genes will have survived after some iteration of evolutionary cycles across a number of generations. Genetic algorithms can be considered as a sort of randomized algorithm where we use random sampling to ensure that we probe the entire search space while trying to find the optimal solution. While genetic algorithms are not the most efficient or guaranteed method of solving TSP, I thought it was a fascinating approach nonetheless, so here goes the post on TSP and genetic algorithms. Before we dive into the solution, we need to first consider how we might represent this problem in code. Let’s take a look at the modules we will be using and the mode of representation we will adopt in approaching TSP. The original, popular TSP requires that the salesperson return to the original starting point destination as well. In other words, if the salesman starts at city A, he has to visit all the rest of the cities until returning back to city A.",1,0,0,0,0,0,0,0
Scikit-learn Pipelines with Titanic,"The idea is that one-hot encoding all categorical variables may very well lead to an unmanageable number of columns, thus causing one to flounder in the curse of dimensionality. A quick fix, then, is to apply PCA or some other dimensionality reduction technique onto the results of one-hot encoding. Back to the implementation, note that we can look inside the individual components of  by simply treating it as an iterable, much like a list or tuple. For example, Next, we need to do something similar for numerical variables. Only this time, we wouldn’t be one-hot encoding the data; instead, what we want to do is to apply some scaling, such as normalization or standardization. Recently in one of Andreas Mueller’s lectures on YouTube, I learned about the , which uses median and IQR instead of mean and standard deviation as does the . This makes the  a superior choice in the presence of outliers. Let’s try using it here. Now that we have the two pipelines for numeric and categorical columns, now it’s time to put them together into one nice package, then apply the process over the entire dataframe.",0,0,0,0,1,0,0,0
An Introduction to Markov Chain Monte Carlo,"The question is, how do we decide to jump or not? The answer lies in Bayes’ theorem: If, for example, we should accept the value and perform the jump, because this means that the new proposed parameter does a better job of explaining the data than does the current one. But recall the dilemma we discussed earlier: how do we compute the posterior? After all, the complexity of calcualting evidence was the very reason why scientists came up with MCMC in the first place. Well, here’s a clever trick that might be of use: rearrange (5) in fractional form to get rid of the evidence term in the denominator. In other words, (5) can be reexpressed as which means The evidence term nicely disappears, giving us an expression that we can easily evaluate! This is how Metropolis-Hastings resolves the dilemma of evidence computation—very simple yet some surprisingly effective algebra. But before we move into code, there is something that should be corrected before we move on. In (6), we derived an experssion for the jump condition. The jump condition is not wrong per se, yet a slight modification has to be made to fully capture the gist of Metrapolis-Hastings.",1,0,0,0,0,0,0,0
"Newton-Raphson, Secant, and More","Some tweaks have been made to the formula for use in the section that follows, but at its core, it’s clear that the function uses the approximation logic we’ve discussed so far. Another variant of the forward and backward divided difference formula is the center divided difference. By now, you might have some intuition as to what this formula is—as the name implies, we somehow use the center point, instead of going forward or backwards from the point of approximation. Here is the formula: Heuristically, this formula also makes sense. We can imagine going both a step forward and backward, then dividing the results by the total of two steps we’ve taken, one in each direction. Shown below is the Python implementation of the center divided difference formula. According to conventional mathematical wisdom, the center divided method normally provides a more robust way of approximating first order derivatives. In this subsection, we discuss why this is the case. Using Taylor expansion, we can approximate the value of  as follows, given that  goes to 0 under the limit. Notice that we can manipulate (4) to derive the forward divided difference equation in (1).",0,0,0,0,0,1,0,0
Riemann Zeta and Prime Numbers,"The only difference is that, instead of considering whether a single number is prime or not, we will consider the notion of coprimeness, or relative primeness. Imagine we randomly sample  numbers, ranging from  all the way up to . What is the probability that these  numbers are all coprime to each other, i.e. the greatest common divisor for these numbers is 1? With some rumination, it isn’t difficult to convince ourselves that this probability can be expressed as Let’s think for a second why this is the case. The probability that some prime number  divides all  through  is going to be , as dividing each number can be considered an independent event. Therefore, the probability that some prime number  does not divide all numbers—i.e. it may divide none or some, but definitely not all—can be expressed as the complement of , or equivalently, . If we apply this analysis to all prime numbers, we end up with (6). Now, let’s simulate the process of random sampling to empirically verify the probabilistic interpretation of the Riemann Zeta function. Before we get into the specifics, below are the dependencies we will need.",0,0,0,0,0,1,0,0
The Gibbs Sampler,"Now, we do the same to sample . Only this time, we can use the result from earlier, namely . We can see how this might help us yield a slightly more convincing result than simply using the random data. We still have to use random values for  through  since we haven’t sampled from their relevant conditional distributions just yet. However, as we go through all the random variables in order, it becomes obvious that we will no longer be using randomly initialized values at one point. Specifically, on the th iteration, we would expect something like this to happen:  can be any number between 1 and , since it is used to represent the th random variable. As we repeat more iterations of sampling, we will eventually end up with a plausible representation of -dimensional vectors, which is what we sought to sample from the intractable distribution! In this section, we will take a look at a very simple example, namely sampling from a bivariate Gaussian distribution. Although we have dealt with -dimensional examples in the algorithm analysis above, for the sake of demonstration, let’s work on a simple example that we can also easily visualize and intuit.",0,0,1,1,0,0,0,0
PyTorch Tensor Basics,"In the case of , it suffices to consider the size as a list, meaning that square bracket syntax can be used. In the case of , indices can directly be passed into as an argument to index individual elements in the size tensor. These past few days, I’ve spent a fair amount of time using PyTorch for basic modeling. One of the main takeaways from that experience is that an intuition on dimensionality and tensor operations in general is a huge plus. This gets especially important for things like batching. One very basic thing I learned–admittedly perhaps too belatedly–is the difference between  and  as dimensions. Here is a concrete example. This creates a one-dimensional tensor, which is effectively a list. We can check the dimensions of this tensor by calling , which is very similar to how NumPy works. On the other hand, specifying the size as  results in a two-dimensional tensor. The simple, barely passing answer to the question of why  is two-dimension would be that it has double layered brackets.",0,1,0,0,0,0,0,1
The Math Behind GANs,"Hence, the expectation can be expressed as a summation. We can simplify this expression even further in the case of binary cross entropy, since there are only two labels: zero and one. This is the  function that we have been loosely using in the sections above. Binary cross entropy fulfills our objective in that it measures how different two distributions are in the context of binary classification of determining whether an input data point is true or false. Applying this to the loss functions in (1), We can do the same for (2): Now we have two loss functions with which to train the generator and the discriminator! Note that, for the loss function of the generator, the loss is small if  is close to 1, since . This is exactly the sort of behavior we want from a loss function for the generator. It isn’t difficult to see the cogency of (6) with a similar approach. The original paper by Goodfellow presents a slightly different version of the two loss functions derived above. Essentially, the difference between (6) and (8) is the difference in sign, and whether we want to minimize or maximize a given quantity.",0,1,1,0,0,0,0,0
BLEU from scratch,"The fact that unigrams are a poor way of evaluating translations becomes immediately clear once we plot the  in n-grams against modified precision.  As you can see, precision score decreases as  gets higher. This makes sense: a larger  simply means that the window of comparison is larger. Unless whole phrases co-occur in the translation and reference sentences—which is highly unlikely—precision will be low. People have generally found that a suitable  value lies somewhere around 1 and 4. As we will see later, packages like  use what is known as cumulative 4-gram BLEU score, or BLEU-4. The good news is that our current implementation is already able to account for different  values. This is because we wrote a handy little function, . By passing in different values to , we can deal with different n-grams. Now we’re almost done. The last example to consider is the following translation: This is obviously a bad translation. However, due to the way modified precision is currently being calculated, this sentence will likely earn a high score. To prevent this from happening, we need to apply what is known as brevity penalty.",1,1,0,0,0,0,0,0
Understanding the  Leibniz Rule,"Why is this the case? It turns out that the Leibniz rule can be proved by using the definition of derivatives and some Taylor expansion. Recall that the definition of a derivative can be written as This is something that we’d see straight out of a calculus textbook. As simple as it seems, we can in fact analyze Leibniz’s rule by applying this definition, as shown below: Thus we have shown that, if the limits of integration are constants, we can switch the order of integration and differentiation. But because our quench for knowledge is insatiable, let’s consider the more general case as well: when the limits are not bounded by constant, but rather functions. Specifically, the case we will consider looks as follows. In this case, we see that  and  are each functions of variable . With some thinking, it is not difficult to convince ourselves that this will indeed introduce some complications that require modifications to our original analysis. Now, not only are we slightly moving the graph of  in the  axis, we are also shifting the limits of integration such that there is a horizontal shift of the area box in the  axis.",0,0,0,0,0,1,0,0
