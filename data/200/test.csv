title,body,analysis,probability_distribution,machine_learning,from_scratch,tensorflow,statistics,deep_learning,linear_algebra
k-Nearest Neighbors Algorithm from Scratch,"The iris data set from the UCI machine learning repository is perhaps one of the best known data sets in the field of machine learning. Created by R. A. Fisher, the data set contains 3 classes of 50 instances each, totaling to 150 independent observations of iris plants, specifically Iris Setosa, Iris Versicolour, and Iris Virginica. The feature columns include sepal length, sepal width, petal length, and petal width. Let’s begin by first loading the data set from the  library. One preliminary step we might want to take is shuffling the data set and dividing it into a training set and a testing set. As the name implies, a testing set is a set of data we use to test the accuracy of our classification algorithm. A training set, on the other hand, is a data set the KNN model is going to use to make predictions, i.e. it is the data set from which the algorithm will try to find close neighbors.",0,0,1,1,0,0,0,0
The Math Behind GANs,"Generative Adversarial Networks refer to a family of generative models that seek to discover the underlying distribution behind a certain data generating process. This distribution is discovered through an adversarial competition between a generator and a discriminator. As we saw in an earlier introductory post on GANs, the two models are trained such that the discriminator strives to distinguish between generated and true examples, while the generator seeks to confuse the discriminator by producing data that are as realistic and compelling as possible. In this post, we’ll take a deep dive into the math behind GANs. My primary source of reference is Generative Adversarial Nets by Ian Goodfellow, et al. It is in this paper that Goodfellow first outlined the concept of a GAN, which is why it only makes sense that we commence from the analysis of this paper. Let’s begin! GAN can be seen as an interplay between two different models: the generator and the discriminator. Therefore, each model will have its own loss function. In this section, let’s try to motivate an intuitive understanding of the loss function for each. To minimize confusion, let’s define some notation that we will be using throughout this post.",0,0,0,0,0,1,1,0
Markov Chain and Chutes and Ladders,"There are six possible events, each with probability of . More specifically, we can end up at the index numbers 38, 2, 3, 14, 5, or 6. In other words, at position 0, where  and  denote the current and next position of the player on the game board, respectively. We can make the same deductions for other cases where . We are thus able to construct a 101-by-101 matrix representing the transition probabilities of our Chutes and Ladders system, where each column represents the system at a different state, i.e. the th entry of the th column vector represents the probabilities of moving from cell  to cell . To make this more concrete, let’s consider a program that constructs the stochastic matrix , without regards to the chutes and ladders for now. The indexing is key here: for each column, th rows were assigned the probability of . Let’s say that a player is in the th cell. Assuming no chutes or ladders, a single roll of a dice will place him at one of the cells from  to ; hence the indexing as presented above. However, this algorithm has to be modified for  bigger or equal to 95.",0,0,0,0,0,0,0,1
Gaussian Process Regression,"The Cholesky decomposition is extremely useful in the context of sampling. Recall that, in a univariate setting, we can model any normal distribution by simply sampling from a standard normal distribution with zero mean and unit variance: We can extend this simplle idea to the context of multivariate Gaussians. One natural complication, however, is that variance  is a matrix in a multivariate setting. Therefore, we would somehow have to find the standard deviation of the Gaussian, or effectively its square root. This is precisely where the Cholesky decomposition comes in handy. We will be using this means of sampling when implementing GP regression in the next section. Let’s put all the pieces together. The crux of GP regression is conditioning. Recall that Here, the setup was that we have some multivariate Gaussian vector . Given some values for a portion of this random vector, namely , we can then derive another multivariate Gaussian for  using conditioning. This is exactly what we are trying to do with GP regression.",0,0,1,1,0,0,0,0
"Beta, Bayes, and Multi-armed Bandits","Each pull on the lever can be considered a Bernoulli trial, and as we start exploring with more pulls to accumulate data, we will essentially be sampling from a binomial distribution, with  success and  failures out of a total of  trials. Therefore, more concretely, we can say The more interesting part is the prior distribution. Intuitively, the one might be inclined to say that we need a uniform prior, and indeed that answer would technically be true. However, I qualify with “technically” since the real answer has an added layer of complexity. Having a uniform prior only makes sense on the very first try, when there is no historical data to estimate the parameter from. However, once we start pulling the lever and seeing the results of each consecutive pull, we should be able to build some prior expectation as to what the true value of the parameter is. This suggests that the uniform prior approach is missing some important key pieces. To really drive this point home, let’s discuss more about the notion of conjugate priors, specifically in the context of the Beta and binomial distributions.",0,1,0,0,0,1,0,0
Fisher Score and Information,"In calculating the expected value, we will be using integrals, which is where the seemingly trivial statements we established earlier come in handy. By linearity of expectation, we can split this expectation up into two pieces. Let’s use integrals to express the first expectation. The good news is that now we see terms canceling out each other. Moreover, from the Leibniz rule and the interchanging of the integral and the derivative, we have shown that the integral in fact evaluates to zero. This ultimately leaves us with Therefore we have established that And we’re done! In this post, we took a look at Fisher’s score and the information matrix. There are a lot of concepts that we can build on from here, such as Cramer Rao’s Lower Bound or natural gradient descent, both of which are interesting concepts at the intersection of machine learning and statistics. Although the derivation is by no means mathematically robust, it nonetheless vindicates a notion that is not necessary apparently obvious, yet makes a lot of intuitive sense in hindsight. I personally found this video by Ben Lambert to be particularly helpful in understanding the connection between likelihood and information.",0,0,0,0,0,1,0,0
The Exponential Family,"A simple hack that we almost always use when dealing with maximum likelihood, therefore, is to apply a log transformation to calculate the log likelihood, since the logarithm is a monotonically increasing function. In other words, What does the log likelihood look like? Well, all we have to do is to apply a log function to (16), which yields the following result. Maximizing the log liklihood can be achieved by setting the gradient to zero, as the gods of calculus would tell us. As you might recall from a previous post on some very basic matrix calculus, the gradient is simply a way of packaging derivatives in a multivariate context, typically involving vectors. If any of this sounds unfamilar, I highly recommend that you check out the linked post. We can compute the partial derivative of the log likelihood function with respect to  as shown below. Observe that the last term in (18) is eliminated because it is a constant with respect to . This is a good starting point, but we still have no idea how to derive the log of . To go about this problem, we have to derive an expression for .",0,1,0,0,0,1,0,0
The Gibbs Sampler,"We know the classic context in which MCMC comes into play in a Bayesian setting: there is some intractable distribution that we wish to sample from. Metropolis-Hastings was one simple way to go about this, and Gibbs sampling provides another method. A feature that makes Gibbs sampling unique is its restrictive context. In order to use Gibbs sampling, we need to have access to information regarding the conditional probabilities of the distribution we seek to sample from. In other words, say we want to sample from some joint probability distribution  number of random variables. Let’s denote this distribution as follows: Turns out that the Gibbs sampler is a more specific version of the Metropolis-Hastings algorithm. We can only use the Gibbs sampler in a restricted context: namely, that we have access to conditional probability distributions. You quickly see why the Gibbs sampler can only be used in limited contexts. Nonetheless, when these set of information are available, it is a powerful algorithm with which we can sample from intractable distributions. Let’s see how this works.",0,0,0,0,0,1,0,1
A Simple Autocomplete Model,"Then, the model will output a prediction vector, which is then passed onto  given a specified . We will finally have a prediction that is 1 character. Then, we incorporate that one character prediction into the original 60-character data we started with. We slice the new augmented data set from  to end up with another prediction. We would then slice the data set from, you guessed it,  and repeat the process as outlined above. When we iterate through this cycle many times, we would eventually end up with some generated text. Below is the  function that implements the iteration process. We’re almost done! To get a better sense of what impact temperature has on the generation of text, let’s quickly write up a  function that will allow us to generate text for differing values of . The time has come: let’s test our model for four different temperature values from 0.3 to 1.2, evenly spaced. We will make our model go through 1000 iterations to make sure that we have a long enough text to read, analyze, and evaluate. For the sake of readability, I have reformatted the output result in markdown quotations. Generated text at temperature 0.",0,0,0,0,1,0,1,0
Recommendation Algorithm with SVD,"Of course, we could think of an alternate implementation of this algorithm that makes use of the  matrix instead of , but that would be a slightly different recommendation system that uses past user’s movie ratings as information to predict whether or not the particular individual would like a given movie. As we can see, SVD can be used in countless ways in the domain of recommendation algorithms, which goes to show how powerful it is as a tool for data analysis. In today’s post, we dealt primarily with singular value decomposition and its application in the context of recommendation systems. Although the system we built in this post is extremely simple, especially in comparison to the complex models that companies use in real-life situations, nonetheless our exploration of SVD is valuable in that we started from the bare basics to build our own model. What is even more fascinating is that many recommendation systems involve singular value decomposition in one way or another, meaning that our exploration is not detached from the context of reality. Hopefully this post has given you some intuition behind how recommendation systems work and what math is involved in those algorithms.",0,0,0,1,0,0,0,1
"0.5!: Gamma Function, Distribution, and More","Notice that, as  increases, the Gamma distribution starts to look more like a normal distribution. At , the  collapses to 1, resulting in an exponential distribution. A short tangential digression: the exponential distribution is a special case of a Gamma distribution that models the waiting time until the first event in a Poisson process. It can also be considered as the continuous version of the geometric distribution. But maybe more on this later on a separate post. Returning back to the Gamma distribution, we see that altering the  also produces a transformative effect on the shape of the Gamma PDF. Specifically, increasing  causes the distribution to move to the right along the -axis. This movement occurs because, for every , a larger  value results in a decrease in the corresponding value of , which graphically translates to a rightward movement along the axis as shown. We started by looking at the Poisson probability mass function, and started our venture into the Gamma function by pondering the question of interpolating the factorial function.",0,1,0,0,0,1,0,0
First Neural Network with Keras,"Everything looks good, which means we are now ready to compile and train our model. Before we do that, however, it is always a good idea to use the  module to ensure that gradient descent stops when no substantial weight adjustments are being made to our model. In other words, when the model successfully finds the local minimum (or preferably the global minimum), the  will kick in and stop gradient descent from proceeding with further epochs. We are now ready to go! Let’s compile the model by making some configurations, namely the , , and . Simply put, an  specifies which flavor of the gradient descent algorithm we want to choose. The simplest version is known as , or the stochastic gradient descent.  can be considered an improved version of the stochastic gradient descent in that its learning rate changes depending on the slope of the loss function, defined here as cross entropy. If you recall, cross entropy is basically a measurement of the pseudo-distance between two distributions, i.e. how different two distributions are. But because cross entropy is often not easy to intuitively wrap our minds around, let’s pass the  metric to the  function, as shown below.",0,0,1,0,1,0,1,0
Gaussian Mixture Models,"From the perspective of maximum likelihood estimation, the goal then would be to maximize this likelihood to find the optimal parameters for  and . Before we attempt MLE with the likelihood function, let’s first try to calculate the log likelihood, as this often makes MLE much easier by decoupling products as summations. Let . And now we see a problem: the log is not applied to the inside of the function due to the summation. This is bad news since deriving this expression by  will become very tedious. The result is for sure not going to look pretty, let’s try deriving the log likelihood by  and set it equal to zero. At least one good news is that, for now, we can safely ignore the outer summation given the linearity of derivatives. We ultimately end up with the expression below: This is not pretty at all, and it seems extremely difficult, if not impossible, to solve analytically for . This is why we can’t approach MLE the way we usually do, by directly calculating derivatives and setting them equal to zero. This is where Expectation Maximization, or EM algorithm comes in.",0,0,1,0,0,1,0,0
Scikit-learn Pipelines with Titanic,"is not quite as bad in that it doesn’t create and test all possible models that distinct combinations of hyperparameters can yield: instead, it relies on a randomized algorithm to perform a search of the hyperparameter space. This is why  is a lot quicker than , with marginal sacrifices in model performance. Let’s see how we might be able to perform hyperparameter search given a pipeline like the one we have built above. The parameter space we are searching for here is by no means exhaustive, but it covers a fair amount of ground. Of course, we can go crazy with randomized search, basically shoving Scikit-learn with every possible configuration and even running a grid search instead. However, that would take an extreme amount of time and computing resources. Therefore, it is important to consider which features are potentially the most important and zoom into these deciding parameters for hypterparameter optimization. The search took a good five to ten minutes, which is a fair amount of time. Let’s take a look at its results. We can also take a look at the best parameters that were found.",0,0,1,0,0,0,0,0
A Step Up with  Variational Autoencoders,"Now that this part has been cleared, let’s start stacking away layers! Just like the autoencoder, VAEs are composed of two discrete components: the encoder and the decoder. Here, we take a look at the first piece of the puzzle, the encoder network. There are several things to note about this model. First, I decided to use a  loop to simplify the process of stacking layers. Instead of repeating the same code over multiple lines, I found this approach to be more succinct and concise. Second, we define a custom layer at the end, shown as , that uses the  function we defined earlier. This is the final key that enables us to build an encoder model that receives as input a 28-by-28 image, then output a two-dimensional latent vector representation of that image to pass onto the decoder network. Below is the summary of what our model looks like. Note that the model outputs a total of three quantities: , , and . We need the first two parameters to later sample from the latent distribution; , of course, is needed to train the decoder.",0,0,0,0,1,0,1,0
Scikit-learn Pipelines with Titanic,"In its inception, this post was conceived of as a simple introduction to ’s pipelines, but it eventually ballooned up into a somewhat comprehensive rundown of a little Kaggle project. I hope to do a lot more of these in the coming days, just because I think there is immense value in mastering ML, although DL sounds a lot cooler. I hope you enjoyed reading this post. Catch you up in the next one!.",0,0,1,0,0,0,0,0
A Brief Introduction to Recurrent Neural Networks,"Luckily, there isn’t going to be much preprocessing involved since we will be using a dataset available from the  library, the IMBD movie reviews dataset. The dataset contains 25,000 movie reviews from IMDB, labeled as either positive or negative. Each review is encoded as a sequence of integers according to a consistent encoding scheme. In other words, each integer corresponds to a unique word in the vocabulary of the dataset. More specifically, the integer to which a word is mapped corresponds to the frequency with which the word appears, i.e. the word encoded as 10 corresponds to the 10th most frequent word in the data. We will apply some very basic preprocessing on the dataset so that we can feed it into our model. Specifically, we will preprocess the dataset such that only a  number of most frequently occurring words are considered. This step will weed out words that occur very infrequently, thus decreasing the amount of noise from the network’s perspective. Next, we will apply padding to the dataset so that all reviews are of length . This means that longer reviews will be truncated, whereas shorter reviews will be padded with zero entries.",0,0,0,0,1,0,1,0
Markov Chain and Chutes and Ladders,"Unfortunately, the stochastic matrix is singular because , the number of columns or rows. This implies that our matrix is degenerate, and that the best alternative to eigendecomposition is the singular value decomposition. But for the sake of simplicity, let’s resort to the brute force calculation method instead and jump straight into some statistical analysis. We first write a simple function that simulates the Chutes and Ladders game given a starting position vector . Because a game starts at the th cell by default, the function includes a default argument on  as shown below: Calling this function will give us , which is a 101-by-1 vector whose th entry represents the probability of the player being on the th cell after a single turn. Now, we can plot the probability distribution of the random variable , which represents the number of turns necessary for a player to end the game. This analysis can be performed by looking at the values of  since the last entry of this vector encodes the probability of the player being at the th cell, i.e. successfully completing the game after  rounds.",0,0,0,0,0,0,0,1
PyTorch Tensor Basics,"This is a very quick post in which I familiarize myself with basic tensor operations in PyTorch while also documenting and clarifying details that initially confused me. As you may realize, some of these points of confusion are rather minute details, while others concern important core operations that are commonly used. This document may grow as I start to use PyTorch more extensively for training or model implementation. Let’s get started. There appear to be two ways of specifying the size of a tensor. Using  as an example, let’s consider the difference between and It confused me how the two yielded identical results. Indeed, we can even verify that the two tensors are identical via I thought different behaviors would be expected if I passed in more dimensions, plus some additional arguments like , but this was not true. The conclusion of this analysis is that the two ways of specifying the size of a tensor are exactly identical. However, one note of caution is that NumPy is more opinionated than PyTorch and exclusively favors the tuple approach over the unpacked one.",0,0,0,0,0,0,1,0
Introduction to tf-idf,"Let’s test in on our dummy dataset to see if we get the count of each tokenized word. This is precisely what we need to calculate idf. Recall the formula for calculating idf As noted earlier, the intuition behind idf was that important keywords probably appear only in specific relevant documents, whereas generic words of comparatively lesser importance appear throughout all documents. We transcribe (4) into code as follows: Now, we have the idf vectors for the nine terms in the dummy dataset. At this point, all there is left to do is to multiply the term frequencies with their corresponding idf scores. This is extremely easy, since we are essentially performing a dot product of the tf and idf vectors for each document. As a final step, we normalize the result to ensure that longer documents do not overshadow shorter ones. Normalizing is pretty simple, so we’ll assume that we have a function  that does the job for now. Before we test the code, we obviously need to implement . This can simply done by obtaining the sum of the L2 norm of each vector, then dividing each element by that constant.",0,0,0,1,0,0,0,0
MLE and KL Divergence,"These days, I’ve been spending some time trying to read published research papers on neural networks to gain a more solid understanding of the math behind deep learning. This is a rewarding yet also a very challenging endeavor, mostly because I have not studied enough math to really understand all of what is going on. While reading the groundbreaking research paper Wasserstein GAN by Martin Arjovsky, I came across this phrase: … asymptotically, maximum likelihood estimation amounts to minimizing the Kullback-Leibler divergence… I was particularly interested in the last portion of this sentence, that MLE amounts to minimizing KL divergence. We discussed MLE multiple time on this blog, including this introductory post and a related post on MAP. Neither is KL divergence an entirely unfamiliar topic. However, I had not thought about these two concepts together in one setting. In this post, let’s try to hash out what the quote from the paper means. Let’s start with a very quick review of what MLE and KL divergence each are.",0,0,0,0,0,1,0,0
Word2vec from Scratch,"As we can see, the lookup table is a dictionary object containing the relationship between words and ids. Note that each entry in this lookup table is a token created using the  function we defined earlier. Now that we have tokenized the text and created lookup tables, we can now proceed to generating the actual training data, which are going to take the form of matrices. Since tokens are still in the form of strings, we need to encode them numerically using one-hot vectorization. We also need to generate a bundle of input and target values, as this is a supervised learning technique. This then begs the question of what the input and target values are going to look like. What is the value that we are trying to approximate, and what sort of input will we be feeding into the model to generate predictions? The answer to these questions and how they tie into word2vec is at the heart of understanding word embeddings—as you may be able to tell, word2vec is not some sort of blackbox magic, but a result of careful training with input and output values, just like any other machine learning task.",0,0,0,1,0,0,1,0
Markov Chain and Chutes and Ladders,"For the purpose of demonstration, let  be an arbritrary vector  and  the three-by-three identity matrix. Multiplying  by  produces the following result: The result is unsurprising, but it reveals an interesting way of understanding : identity matrices are a special case of diagonalizable matrices whose eigenvalues are 1. Because the multiplying any arbitrary vector by the identity matrix returns the vector itself, all vectors in the dimensional space can be considered an eigenvector to the matrix , with  = 1. A formal way to calculate eigenvectors and eigenvalues can be derived from the equation above. Since  is assumed as a nonzero vector, we can deduce that the matrix  is a singular matrix with a nontrivial null space. In fact, the vectors in this null space are precisely the eigenvectors that we are looking for. Here, it is useful to recall that the a way to determine the singularity of a matrix is by calculating its determinant. Using these set of observations, we can modify the equation above to the following form: By calculating the determinant of , we can derive the characteristic polynomial, from which we can obtain the set of eigenvectors for  representing some linear transformation .",0,0,0,0,0,0,0,1
Bayesian Linear Regression,"In many real-world cases, this process can be intractable, but because we are dealing with two Gaussian distributions, the property of conjugacy ensures that this problem is not only tractable, but also that the resulting posterior would also be Gaussian. Although this may not be immediately apparent, observe that the exponent is a quadratic that follows the form after making appropriate substitutions Therefore, we know that the posterior for  is indeed Gaussian, parameterized as follows: Let’s try to obtain the MAP estimate of of , i.e. simplify  Notice the similarity with the MLE estimate, which is the solution to normal equation, which I otherwise referred to as vanilla linear regression: This is no coincidence: in a previous post on MAP and MLE, we observed that the MAP and MLE become identical when we have a uniform prior. In other words, the only cause behind the divergence between MAP and MLE is the existence of a prior distribution. We can thus consider the additional term in (10) absent in (11) as a vestige of the prior we defined for .",0,0,0,0,0,0,0,1
My First GAN,"Now it’s time to complete the GAN by creating a corresponding discriminator, the discerning police officer. The discriminator is essentially a simple binary classier that ascertains whether a given image is true or fake. Therefore, it is no surprise that the final output layer will have one neuron with a sigmoid activation function. Let’s take a more detailed look at the  function as shown below. And again, a model summary for convenient reference:  Now we have both the discriminator and the generator, but the two are not really connected in the sense that they exist as discrete models lacking any connection between them. What we want to do, however, is to establish some relationship between the generator and the discriminator to complete a GAN, and hence train them in conjunction. This process of putting the pieces together, or adjoining the models, is where I personally find the genius in GAN design. The key takeaway here is that we define  and . As you might imagine, the shape of the input is defined by  we defined earlier. This is the latent space from which we will sample a random noise vector frame to feed into our GAN.",0,0,0,0,1,0,1,0
Moments in Statistics,"The easiest way to demonstrate the usefulness of MGF is with an example. For fun, let’s revisit a distribution we examined a long time ago on this blog: the Poisson distribution. To briefly recap, the Poisson distribution can be considered as an variation of the binomial distribution where the number of trials, , diverges to infinity, with rate of success defined as . This is why the Poisson distribution is frequently used to model how many random events are likely in a given time frame. Here is the probability distribution of the Poisson distribution. Note that  denotes the number of occurrences of the random event in question. The task here is to obtain the mean of the distribution, i.e. to calculate the first moment, . The traditional, no-brainer way of doing this would be to refer to the definition of expected values to compute the sum Computing this sum is not difficult, but it requires some clever manipulations and substitutions. Let’s start by simplifying the factorial in the denominator, and pulling out some expressions out of the sigma.",0,1,0,0,0,1,0,0
Gamma and Zeta,"Regardless, I think the theoretical aspect of this derivation is interesting nonetheless. One thing I must do is writing a post on divergence and when the interchange of summation and integrals can be performed. I was originally planning to write a much longer article dividing deep into the Gamma and the Beta function as well as their distributions. However, I realized that what I need at this point in time is producing output and reorienting myself back to self-studying blogger mode, perhaps taking a brief hiatus from the grinding intern spending endless hours in Sublime text with Django (of course, I’m doing that because I enjoy and love the dev work). In the end, we all need a healthy balance between many things in life, and self-studying and working are definitely up on that list for me. Hopefully I can find the middle ground that suits me best. I hope you’ve enjoyed reading this post. Catch you up in the next one!.",1,0,0,0,0,0,0,0
Dissecting LSTMs,"Thus, it is no surprise that the following two equations are structured the way they are: First, we see the familiar forward pass, a familiar structure we have seen earlier. Borrowing the analogy we established in the previous post,  is a filter that decides which information to use and drop. The raw material that we pass into this filter is in fact the cell state, processed by a  activation function. This is also a familiar structure we saw earlier in the information update sequence of the network. Only this time, we use the cell state to generate output. This makes sense somewhat intuitively, since the cell state is essentially the memory of the network, and hence to generate output would require the use of this memory. Of course, this should not be construed so literally since what ultimately happens during backpropagation is entirely up to the network, and at that point we simply lay back and hope for the network to learn the best. This point notwithstanding, I find this admittedly coarse heuristic to be nonetheless useful in intuiting the clockwork behind LSTMs.",0,0,0,0,0,0,1,0
Gaussian Mixture Models,"At peaks, we would see circular contour lines, but where the hills meet, we might see different patterns, most likely circular patterns overlapping with each other. The key point here is that the combination is convex; in other words, the mixing coefficient for each Gaussian should add up to one. If we consider GMM to be a generative model, then we can imagine the generating process as follows: At this point, for clarity’s sake, let’s introduce some notations and concretize what has been elaborated above. First, we define a categorical distribution that will represent the mixing coefficients described above. Let  be an -dimensional vector that parametrizes this categorical distribution. In other words, This means that we assume the data to have  different clusters. Each  is then a mixing coefficient that establishes the convexness of the linear combinations of the underlying Gaussians. Now we can sample the cluster index, denoted as , from the categorical distribution as shown below. Note that we use  for the cluster index, since it is considered a latent variable—we don’t observe it directly, yet it is deeply involved in the data generation process.",0,0,1,0,0,1,0,0
An Introduction to Markov Chain Monte Carlo,"After that jump, the sampler rejects the next three values sampled from the proposal distribution, as it stayed dormant at the value 0.17414333. However, with more iterations, we would expect the function to make more jumps, gradually painting a picture of what the posterior should look like. In fact, we can create what is called the trace plot to see which values were sampled by the Metropolis-Hastings algorithm. Trace plots are important because they tell us whether or not our model is well-calibrated, i.e. a sufficient amount of state changes occur.  The trace plot contains the trace of 15000 accepted values sampled from the proposal distribution. We see that there are some fluctuations, indicating that state transitions occurred, but also that there seems to be values that the sampler preferred over others. Eyeballing the trace plot, the “comfort zone” seems to be slightly above 0, as we expect. To illustrate the importance of trace plots, let’s see an example involving a bad setup involving a proposal distribution with too small a variance.",0,0,0,1,0,0,0,0
"Linear Regression, in Two Ways","This analogue can be extended to other statements in matrix calculus. For instance, where  is a symmetric matrix. We can easily verify this statement by performing the calculation ourselves. For simplicity’s sake, let’s say that  is a two-by-two matrix, although it could theoretically be any -by- matrix where  is some positive integer. Note that we are dealing with square matrices since we casted a condition on  that it be symmetrical. Let’s first define  and  as follows: Then, We can now compute the gradient of this function according to (3): We have not provided an inductive proof as to how the same would apply to -by- matrices, but it should now be fairly clear that , which is the single-variable calculus analogue of saying that . In short, 
 With these propositions in mind, we are now ready to jump back into the linear regression problem. At this point, it is perhaps necessary to remind ourselves of why we went down the matrix calculus route in the first place.",0,0,0,0,0,0,0,1
Demystifying Entropy (And More),"What is information? Warren Weaver, who popularized Shannon’s works and together developed the field of information theory, pointed out that information is not related to what is said, but what could be said. This element of uncertainty involved in one’s degree of freedom is what makes the notion of information inseparable from probability and randomness. As Ian Goodfellow put it in Deep Learning, The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred. In other words, a low probability event expresses a lot of information, while a high probability event expresses low information as its occurrence provides little information of value to the informed. Put differently, rare events require more information to represent than common ones. Consider, for example, how we might represent the amount of information involved in a fair coin toss. We know for a fact that where  and  denote the events that the coin lands on heads and tails, respectively.",0,0,0,0,0,1,0,0
A Step Up with  Variational Autoencoders,"A useful property to know about KL divergence is the fact that it is always non-negative. We will get into why this is the case in a moment. For now, let’s assume non-negativity to be true and transform (6) into an inequality: The term on the right of the inequality is known as the Evidence Lower Bound, or ELBO for short. Why are we interested in ELBO? First, note that , the evidence, is a constant. Therefore, minimizing KL divergence amounts to maximizing ELBO. This is the key to variational inference: instead of calculating the intractable integral in (3), we can find a distribution  that which minimizes KL divergence by maximizing ELBO, which is a tractable operation. Let’s prove why KL divergence is always greater or equal to zero, which is a condition we assumed to be true in the derivation of ELBO above. For the sake of completeness, I present two ways of proving the same property. In the context of probability, Jensen’s inequality can be summarized as follows. Given a convex function , We won’t get into rigorous proofs here, but it’s not difficult to see why this inequality stands with some basic geometric intuition.",0,0,0,0,1,0,1,0
The Magic of Euler’s Identity,"But we have proved it, and therefore we know it must be the truth. But contrary to his point of view, Euler’s identity is a lot more than just an interesting, coincidental jumble of imaginary and irrational numbers that somehow churn out a nice, simple integer. In fact, it can be used to better understand fundamental operations such as logarithms and powers. Consider, for example, the value of the following expression: Imaginary powers are difficult to comprehend by heart, and I no make no claims that I do. However, this mind-pulverizing expression starts to take more definite meaning once we consider the generalized form of Euler’s identity, . Let . Then we have Take both sides to the power of i: Interestingly enough, we see that  takes on a definitive, real value. We can somewhat intuit this through Euler’s identity, which is basically telling us that there exists some inextricable relationship between real and imaginary numbers. Understood from this point of view, we see that the power operation can be defined in the entire space that is complex numbers. We can also take logarithms of negative numbers.",1,0,0,0,0,0,0,0
Naive Bayes Model From Scratch,"Note that instead of using the for-loop approach used in previous posts, this function is more vectorized, making computation less expensive. The shorter code is also an added benefit. The accuracy of our from-scratch model is 97 percent, which is not bad for a start. Let’s see if the  model in  outperforms our hand-coded model. The accuray score yielded by  is exactly identical to that achieved by our model! Looks like the  model in scikit-learn does exactly what our model does, at least juding from the metric of accuracy. This is surprising, but since we basically followed the Bayesian line of reasoning to buid our model, which is what naive Bayes really is all about, perhaps this is not as astonishing as it seems. In this post, we built the Gaussian naive Bayes model from scratch. In the process, we reviewed key concepts such as Bayesian inference and maximum a posteriori estimation, both of which are key statistical concepts used in many subdomains of machine learning. Hopefully through this tutorial, you gained a better understanding of how Gaussian mathematics and Bayesian thinking can be used in the context of classification.",0,0,1,1,0,0,0,0
The Gibbs Sampler,"It is certainly a somewhat lengthy derivation, but there is nothing too conceptually difficult involved—it’s just a lot of algebra and simplifications. We begin from the formula for the multivariate Gaussian: For convenience purposes, let Then, Let Note that this is not a one-to-one correspondence, i.e. . The blocks are only one-to-one insofar as being dimensionally equivalent. Then, using block matrix multiplication, Notice that the final result should be a single scalar given the dimensions of each matrix. Therefore, we can further simply the expression above using the fact that . Specifically, the second and third terms are transposes of each other. Although we simply resorted a convenient substitution in (6), we still need to derive an expression for the inverse of the covariance matrix. Note that the inverse of the covariance matrix can intuitively be understood as the precision matrix. We won’t derive the block matrix inversion formula here. The derivation is just a matter of simply plugging in and substituting one expression for another. For a detailed full derivation, checkout this link or this journal article.",0,0,0,0,0,1,0,1
An Introduction to Markov Chain Monte Carlo,"The question is, how do we decide to jump or not? The answer lies in Bayes’ theorem: If, for example, we should accept the value and perform the jump, because this means that the new proposed parameter does a better job of explaining the data than does the current one. But recall the dilemma we discussed earlier: how do we compute the posterior? After all, the complexity of calcualting evidence was the very reason why scientists came up with MCMC in the first place. Well, here’s a clever trick that might be of use: rearrange (5) in fractional form to get rid of the evidence term in the denominator. In other words, (5) can be reexpressed as which means The evidence term nicely disappears, giving us an expression that we can easily evaluate! This is how Metropolis-Hastings resolves the dilemma of evidence computation—very simple yet some surprisingly effective algebra. But before we move into code, there is something that should be corrected before we move on. In (6), we derived an experssion for the jump condition. The jump condition is not wrong per se, yet a slight modification has to be made to fully capture the gist of Metrapolis-Hastings.",0,0,0,1,0,0,0,0
Building Neural Network From Scratch,"Inspired by that book, and in part in an attempt to test the knowledge I gained from having read that bok, I decided to implement my own rendition of a simple neural network supported by minibatch gradient descent. Let’s jump right into it. The default setup of my Jupyter Notebook, as always: Before we start building our model, we should first prepare some data. Instead of using hand-made dummy data as I had done in some previous posts, I decided to use the  library to generate random data points. This approach makes a lot more sense given that neural networks require a lot more input data than do machine learning models. In this particular instance, we will use the  function to accomplish this task. Let’s take a look at what our data looks like. As expected, the dataset contains the  and  coordinates of the points generated by the  function. If you haven’t heard about this function before, you might be wondering what all the moons deal is about. Well, if we plot the data points, it will become a lot more obvious.",0,0,0,1,0,0,1,1
Word2vec from Scratch,"One technicality here is that, for the first and last few tokens, it may not be possible to obtain words to the left or right of that input token. In those cases, we simply don’t consider these word pairs and look at only what is feasible without causing s. Also note that we create  and  separately instead of putting them in tuple form as demonstrated above. This is just for convenience with other matrix operations later on in the post. Below is the definition for , an auxiliary function we used above to combine two  objects. Also, here is the code we use to one-hot vectorize tokens. This process is necessary in order to represent each token as a vector, which can then be stacked to create the matrices  and . Finally, let’s generate some training data with a window size of two. Let’s quickly check the dimensionality of the data to get a sense of what matrices we are working with. This intuition will become important in particular when training and writing equations for backpropagation in the next section. Both  and  are matrices with 330 rows and 60 columns. Here, 330 is the number of training examples we have.",0,0,0,1,0,0,1,0
A Simple Autocomplete Model,"However, text embedding is insuitable for this task since our goal is to build a character-level text generation model. In other words, our model is not going to generate word predictions; instead, it will spit out a character each prediction cycle. Therefore, we will use an alternative technique, namely mapping each character to an integer value. This isn’t as elegant as text embedding or even one-hot encoding but for a character-level analysis, it should work fine. The  function takes a string text data as input and returns a list of training data, each of length , sampled every  characters. It also returns the training labels and a hash table mapping characters to their respective integer encodings. Let’s perform a quick sanity check to see if the function works as expected. Specifying  to 60 means that each instance in the training data will be 60 consecutive characters sampled from the text data every  characters. The result tells us that we have a total of 200278 training instances, which is probably plenty to train, test, and validate our model. The result also tells us that there are 57 unique characters in the text data.",0,0,0,0,1,0,1,0
Complex Fibonacci,"The code above is the standard fibonacci function as we know it, implemented with simple bare bone recursion. While the code works perfectly fine, there is a fatal problem with this code: it recalculates so many values. Namely, in calling , the program goes all the way up to the th fibonacci number. Then, in the next call of , the program recalculates the same values calculated earlier, up until the th fibonacci number, just one short of the previous one. The classic way to deal with this problem is to use a technique known as memoization. The idea is simple: we keep some sort of memo or cache of values that have already been calculated and store them in some data structure that we don’t have to recalculate values that have already been computed prior. Here is a simple implementation. To see how effective memoization is compared to vanilla recursion, let’s use the  module. Comparing this to the  test on  with memoization, the benefits of caching becomes immediately clear: 3 second and 150 nanoseconds are different by orders of magnitude, and we only asked the functions to calculate the 35th fibonacci number.",1,0,0,0,0,0,0,0
Gaussian Mixture Models,"But the fundamental idea is that we would commence from the log likelihood function and derive our way to the solution. The solutions are presented below: So the full picture is now complete: given the inter-dependence of derived quantities, we seek to optimize them using the Expectation Maximization algorithm. Specifically, the EM method works as follows: In today’s post, we took a deep dive into Gaussian mixture models. I find GMMs to be conceptually very intuitive and interesting at the same time. I’m also personally satisfied and glad that I have learned yet another ML/mathematical concept that starts with the word Gaussian. Much like how I felt when learning about Gaussian process regression, now I have an even greater respect for the Gaussian distribution, although I should probably be calling it normal instead, just like everybody else. I’m also happy that I was able to write this blog post in just a single day. Of course, this is in large part due to the fact that I had spent some time a few weeks ago studying this material, but nonetheless I think I’m starting to find the optimal balance between intern dev work and self-studying of math and machine learning.",0,0,1,0,0,1,0,0
Building Neural Network From Scratch,"We can commence from here to find the gradient of the loss function with respect to other layers more further down the neural network. For example, we can calculate the gradient with respect to the weights of the second affine layer as follows: We won’t get into much mathematical details here, but a useful intuition we can use to derive equation (15) is to pay close attention to the dimensionality of data. Note that the dimension of the gradient as a matrix should equal to that of the layer itself. In other words, , so on and so forth. This is because the purpose of gradient computation is to update the matrix of parameters: to perform an element-by-element update with the gradient, it must necessarily be true that the dimensionality of the gradient equals that of the original matrix. Using this observation, it is possible to navigate through the confusion of transposes and left, right matrix multiplication that one might otherwise encounter if they were to approach it without any intuition or heuristics. To expedite this post, I’ll present the result of the gradient calculations for all parameters below.",0,0,0,1,0,0,1,1
Gaussian Process Regression,"In a nutshell, GP regression simply amounts to generating a prediction given some training data through conditioning, under the assumption that the underlying function is a infinite-dimensional vector that follows some Gaussian distribution with a kernel acting as its prior. Given this broad conceptual understanding, let’s move onto more concrete implementations. These are the setting we will be using for this post. We set a random seed for reproducibility purposes. Recall that, depsite its beautiful underlying complexity, all there is to GP regression is to identify some conditional Gaussian with a kernel as its covariance. Then, we can simply sample from this conditional distribution to obtain possible models that fit the data. As the first step, let’s implement the RBF kernel. Here, we modify (9) to have an added parameter, , which is a multiplicative constant to the exponent. The  function simply uses double iteration to fill each entry of the covariance matrix. Note that  and  do not have to be identical in length; if their lengths are different, the resulting kernel matrix will simply be rectangular.",0,0,1,1,0,0,0,0
BLEU from scratch,"Recently, I joined the Language, Information, and Learning at Yale lab, led by Professor Dragomir Radev. Although I’m still in what I would consider to be the incipient stages of ML/DL/NLP studies—meaning it will take time for me to be able to actively participate in an contribute to research and publications—I think it will be a great learning experience from which I can glean valuable insight into what research at Yale looks like. One of the first projects I was introduced to at the lab is domain-independent table summarization. As the name implies, the goal is to train a model such that it can extract some meaningful insight from the table and produce a human-readable summary. Members are the lab seem to be making great progress in this project, and I’m excited to see where it will go. In the meantime, I decided to write a short post on BLEU, a metric that I came across while reading some of the survey papers related to this topic. Let’s dive into it. Before going into code and equations, a high-level overview of what BLEU is might be helpful here.",0,0,0,1,0,0,1,0
The Magic of Euler’s Identity,"Why is speed important? Unit speed implies that the particle moves by  distance units after  time units. Let’s say that  time units have passed. Where would the particle be on the trajectory now? After some thinking, we can convince ourselves that it would lie on the point , since the unit circle has a total circumference of . And so we have proved that , Euler’s identity. But we can also go a step further to derive the generalized version of Euler’s identity. Recall that a unit circle can be expressed by the following equation in the Cartesian coordinate system: On the complex plane mapped in polar coordinates, this expression takes on an alternate form: Notice that this contains the same exact information that Euler’s identity provides for us. It expresses: From this geometric interpretation, we can thus conclude that We now know the exact value that  represents in the complex number system! Urban legend goes that mathematician Benjamin Peirce famously said the following about Euler’s identity: Gentlemen, that is surely true, it is absolutely paradoxical; we cannot understand it, and we don’t know what it means.",1,0,0,0,0,0,0,0
Principal Component Analysis,"This is when PCA comes in: with PCA, we can figure out which dimensions are the most important and apply a transformation to compress that data into lower dimensions, making it a lot more tractable and easier to work with. And in case you’re still wondering, principal components refer to those new extracted dimensions used to newly represent data! Let’s derive PCA with some good old linear algebra tricks. I used Ian Goodfellow’s Deep Learning and a lecture slide from Columbia references for this post. The setup of a classic PCA problem might be summarized as follows. Suppose we have a dataset of  points, each living in -dimensional space. In other words, 
where Our goal is to find a way to compress the data into lower dimensional space  where . We might imagine this as a transformation, i.e. the objective is to find a transformation So that applying  will yield a new vector  living in lower dimensional space. We can also imagine there being a reverse transformation or a decoding function  that achieves Because PCA is in essence a linear transformation, it is most natural to express and understand it as a matrix.",0,0,0,0,0,1,0,1
Convolutional Neural Network with Keras,"The  function returns the predefined sequential model, compiled using the configurations as shown below. Let’s take a look at the summary of the model. The summary shows that this model has 551,466 trainable parameters. The memory capacity of this model is not big, but it is definitely larger than the network we built in the previous post using the Keras API. Now that the model is ready to be deployed, we need to train and test the model. But before that, let’s quickly define a function that will provide us with a visualization of how the model is learning. This function is very similar to the one used in the previous post—all it does it that it plots the model’s accuracy and cross entropy loss with each epoch. This visualization will help us see whether our model is actually learning with each epoch, and whether or not overfitting is occurring at any point in training. The last piece of the puzzle we need is the  function.",0,0,0,0,1,0,1,0
How lucky was I on my shift?,"In fact, some weird things happen if we block time into large units, such as an hour—notice how the value of  becomes , which is a probabilistic impossibility as  should always take values between . Another issue with this model is that a Bernoulli trial does not allow for simultaneous successes. Say, for instance, that within one ten-minute block, we got two calls. However, because the result of a Bernoulli trial is binary, i.e. either a success or a failure, it cannot contain more than one success in unit time. Therefore, binary distribution cannot encode higher dimensions of information, such as two or three simultaneous successes in one trial. These set of complications motivate a new way of modeling phone calls. In the next section, we look at an alternate approach to the problem: the Poisson distribution. Here is some food for thought: what if we divide up unit time into infinitesimally small segments instead of the original ten, such that ? This idea is precisely the motivation behind the Poisson distribution.",0,1,0,0,0,1,0,0
A sneak peek at Bayesian Inference,"But what if the man were to take the same test again? Intuition tells us that the more test he takes, the more confident we can be on whether the man is or is not affected by the disease. For instance, if the man repeats the exam once and receives a positive report, the conditional probability that he is sick given two consecutive positive test results should be higher than the 24 percent we calculated above. We can see this in practice by reapplying Bayes’ theorem with updated information, as shown below: We see that the value of the conditional probability has indeed increased, lending credence to the idea that the man is sick. Like this, Like this, Bayes’ theorem is a powerful tool that can be used to calculate conditional probabilities and to update them continuously through repeated trials. From a Bayesian perspective, we begin with some expectation, or prior probability, that an event will occur. We then update this prior probability by computing conditional probabilities with new information obtained for each trial, the result of which yields a posterior probability. This posterior probability can then be used as a new prior probability for subsequent analysis.",0,1,0,0,0,1,0,0
Logistic Regression Model from Scratch,"The data we will be looking at is the banknote authentification data set, publicly available on the UCI Machine Learning Repository. This data set contains 1372 observations of bank notes, classified as either authentic or counterfeit. The five features columns of this data set are: Let’s use some modules to import this data set onto our notebook, as shown below. The imported data set was slightly modified in two ways to fit our model. First, I separated the class label data from the data set and stored it as a separate  array. Second, I appended s to each observation to create a new column that accounts for intercept approximation. All this means is that we consider our linear model to be where for all available sample observations. This is something that we have been assuming all along throughout the gradient descent derivation process, but had not been stated explicitly to reduce confusion. Just consider it a strategic choice on our part to simplify the model while allowing for the logistic regression model to consider bias. Let’s check the shape of the imported data set to check that the data has been partitioned correctly.",0,0,1,1,0,0,0,0
Riemann Zeta and Prime Numbers,"And much like in the previous  function, the  parameter determines the upper bound of our sampling range. Therefore, the total number of simulations we run will effectively be  times. Last but not least,  indicates how many numbers we want to sample each time—this parameter is semantically identical to the  parameter we saw in the  function above. Also, for easier plotting, we will return the  domain range object alongside the result of our simulation. The function is just a two-liner—we were able to reduce the number of lines thanks to list comprehension. Now, let’s see if this code works as expected. Here, our experiment consisted of sampling two numbers, starting from range , all the way up to range , with 1000 simulations for each range. Let’s plot the results to get a better idea of what was going on.  First, notice that when , the probability that two sampled numbers are coprime is 1. This is unsurprising, since sampling from range  simply means that both the sampled numbers were 2—and of course, 2 and 2 are coprimes.",1,0,0,0,0,0,0,0
BLEU from scratch,"BLEU, which stands for Bilingual Evaluation Understudy, is an metric that was introduced to quantitatively evaluate the quality of machine translations. The motivation is clear: as humans, we are able to get an intuitive sense of whether or not a given translation is accurate and of high quality; however, it is difficult to translate this arbitrary linguistic intuition to train NLP models to produce better translations. This is where BLEU comes to the rescue. The way BLEU works is simple. Given some candidate translation of a sentence and a group of reference sentences, we use a bag-of-word approach to see how many occurences of BOWs co-occur in both the translation and reference sentences. BOW is a simple yet highly effective way of ensuring that the machine translation contains key phrases or words that reference translations also contain. In other words, BLEU compares candidate translations with human-produced, annotated reference translations and compares how many hits there are in the candidate sentence. The more BOW hits there are, the better the translation. Of course, there are many more details that go beyond this.",0,0,0,1,0,0,1,0
Convolutional Neural Network with Keras,"Recently, a friend recommended me a book, Deep Learning with Python by Francois Chollet. As an eager learner just starting to fiddle with the Keras API, I decided it was a good starting point. I have just finished the first section of Part 2 on Convolutional Neural Networks and image processing. My impression so far is that the book is more focused on code than math. The apparent advantage of this approach is that it shows readers how to build neural networks very transparently. It’s also a good introduction to many neural network models, such as CNNs or LSTMs. On the flip side, it might leave some readers wondering why these models work, concretely and mathematically. This point notwithstanding, I’ve been enjoying the book very much so far, and this post is a reflection of just that. Today, we will use TensorFlow’s  module to build a convolutional neural network for image detection. This code is based on what I have learned from the book, so much credit goes to Deep Learning with Python. I have also looked at Machine Learning Mastery blog for additional reference. Let’s begin! Below are the modules that we will need to import for this demonstration.",0,0,0,0,1,0,1,0
"0.5!: Gamma Function, Distribution, and More","The first version, presented below, is Euler’s definition of the Gamma function as an infinite product. To see how this comes from, we backtrack this equality by dividing the right-hand side by . At this point, we can reduce the fraction by eliminating  from both the denominator and the numerator, which leaves us with the following expression: Therefore, we have This is another representation of the Gamma function that is distinct from the integral we saw earlier. The last form that we will see involves some tweaking of the harmonic series, or the simplest case of the Riemann zeta function where . We start from . We derive both sides by  to obtain the following: Notice the harmonic series embedded in the expression above. To further simplify this expression, we derive an interpolation of the harmonic series. Let the interpolation be denoted as : We derive both sides by  to obtain the following: The expression above is the sum of a geometric series with radius . We integrate both sides by  to obtain .",0,1,0,0,0,1,0,0
Traveling Salesman Problem with Genetic Algorithms,"Here, we use a simple roulette model, where we compare the value of the probability vector and a random number sampled from a uniform distribution. If the value of the probability vector is higher, the corresponding chromosome is added to . We repeat this process until we have  parents. As expected, we get 4 parents after selecting the parents through . Now is the crucial part: mutation. There are different types of mutation schemes we can use for our model. Here, we use a simple swap and crossover mutation. As the name implies, swap simply involves swapping two elements of a chromosome. For instance, if we have , we might swap the first two elements to end up with . The problem with swap mutation, however, is the fact that swapping is a very disruptive process in the context of TSP. Because each chromosome encodes the order in which a salesman has to visit each city, swapping two cities may greatly impact the final fitness score of that mutated chromosome. Therefore, we also use another form of mutation, known as crossovers. In crossover mutation, we grab two parents.",0,0,0,1,0,0,0,0
Dissecting the Gaussian Distribution,"If there is one thing that the field of statistics wouldn’t be complete without, it’s probably normal distributions, otherwise referred to as “the bell curve.” The normal distribution was discovered and studied extensively by Carl Friedrich Gauss, which is why it is sometimes referred to as the Gaussian distribution. We have seen Gaussian distributions before in this blog, specifically on this post on likelihood and probability. However, normal distribution was introduced merely as an example back then. Today, we will put the Gaussian distribution on stage under undivided spotlight. Of course, it is impossible to cover everything about this topic, but it is my goal to use the mathematics we know to derive and understand this distribution in greater detail. Also, it’s just helpful to brush up on some multivariable calculus in a while. Let’s start with the simplest case, the univariate Gaussian distribution. The “univariate” part is just a fancier way of saying that we will dealing be dealing with one-dimensional random variables, i.e. the distribution is going to be plotted on a two-dimensional  plane. We make this seemingly trivial distinction to distinguish it from the multivariate Gaussian, which can be plotted on three-dimensional space or beyond.",0,1,0,0,0,1,0,0
Building Neural Network From Scratch,"The visualization makes clear the point that ReLU is a piece-wise function that flattens out negative values while leaving positive values unchanged. Now that we have all the ingredients ready, it’s time to build the neural network. Earlier, I said that a neural network can be reduced to matrix multiplication. This is obviously an oversimplification, but there is a degree of truth to that statement. Recall that a single neuron of a neural network can be expressed as a dot product of two vectors, as shown below. Following conventional notation,  represents weights; , input data; , bias. Visually, we can imagine the neuron being lit up when the value  is large.  This is similar to how the human brain works, except that biological neurons are binary in that they either fires on or off; artifical neurons in a network typically take a range of values. If we expand the vector operation in (5), it becomes quickly obvious that we can represent an entire layer of neurons as a product of two matrices. Our simple neural network model can thus be expressed as follows: The equations above represent our simple neural network model composed of two affine layers.",0,0,0,1,0,0,1,1
Revisiting Basel with Fourier,"Then, we get With a very small bit of algebra, we end up with And there we have it, the value of ! It’s interesting to see how all this came out of the fourier series of . In this section, we will be taking a look at some interesting representations of the Basel problem, mysteriously packaged in integrals. At a glance, it’s somewhat unintuitive to think that an infinite summation problem can be stated as an integral in exact terms; however, the translation from summation to integrals are not out of the blue. Using things like Taylor series, it is in fact possible to show that the Basel problem can be stated as an integral. For instance, consider this integral One thing I am starting to realize these past few days is that some of these integrals are extremely difficult despite being deceptively simple in their looks. This is a good example. To get started, we might consider making a quick change of variables, namely . This will effectively get rid of the rather messy-looking denominator sitting in the fraction. To make further progress, at this point let’s consider the Taylor series expansion of .",1,0,0,0,0,0,0,0
Logistic Regression Model from Scratch,"Now, it’s time to split the data into training and testing data. To do this, I recycled a function we built earlier in the previous post on k-nearest neighbors algorithm. Using , we can partition the data set into training and testing data. Let’s make 20 percent of observations as testing data and allocate the rest for training. It’s time for some training and prediction generation. Because we did all the work in the previous section, training and predicting can be achieved with just a single line of command. To see how quickly average cross entropy is decreasing, I turned on the  as true. This way, we can see how quickly the loss is declining over every 50 epochs. It’s now time to see how well our model has done. Let’s compare , the list that contains the model’s predictions, with , which is essentially the answer key. This is great news. The result shows us that we have correctly predicted 272 values while making wrong predictions in only 2 cases. Let’s systematize this quantity by creating a  function that returns how accurate our model is given  and . Let’s use this function to test how well our model performed.",0,0,1,1,0,0,0,0
Fourier Series,"This is the beauty of expansion techniques like the Fourier and Taylor: as counterintuitive as it may seem, these tools tell us that any function can be approximated through an infinite summation, even if the original function may not resemble the building block of the expansion technique at all at a glance.",1,0,0,0,0,1,0,0
MLE and KL Divergence,"Then, we would obtain a set of indecent and identically distributed (i.i.d) samples as shown below: Then, LLN states that A more precise statement of the law uses Chebyshev’s inequality: For the curious, here is the general formulation of Chebyshev’s inequality outside the context of LLN: For the purpose of this post, it is not necessary to go into how Chebyshev’s inequality is derived or what it means. However, it isn’t difficult to see how one might reformulate (8) to derive (7) to prove the Law of Large Numbers. All that the inequality is saying is that no more than a certain fraction of samples can fall outside more than a certain distance away from the mean of the distribution. With this understanding in mind, let’s return to the original problem and wrap up the proof. Let’s apply the Law of Large Numbers to modify the expected value expression sitting in (5): Voila! We have shown that minimizing the KL divergence amounts to finding the maximum likelihood estimate of . This was not the shortest of journeys, but it is interesting to see how the two concepts are related.",0,0,0,0,0,1,0,0
Riemann Zeta and Prime Numbers,"The only difference is that, instead of considering whether a single number is prime or not, we will consider the notion of coprimeness, or relative primeness. Imagine we randomly sample  numbers, ranging from  all the way up to . What is the probability that these  numbers are all coprime to each other, i.e. the greatest common divisor for these numbers is 1? With some rumination, it isn’t difficult to convince ourselves that this probability can be expressed as Let’s think for a second why this is the case. The probability that some prime number  divides all  through  is going to be , as dividing each number can be considered an independent event. Therefore, the probability that some prime number  does not divide all numbers—i.e. it may divide none or some, but definitely not all—can be expressed as the complement of , or equivalently, . If we apply this analysis to all prime numbers, we end up with (6). Now, let’s simulate the process of random sampling to empirically verify the probabilistic interpretation of the Riemann Zeta function. Before we get into the specifics, below are the dependencies we will need.",1,0,0,0,0,0,0,0
k-Nearest Neighbors Algorithm from Scratch,"However, I find exploring these mechanisms a lot more interesting than simply using pre-existing modules and libraries, as important as they may be. Hopefully, this post gave you some idea of how the KNN model works. I plan to post more on machine learning algorithms in the future. However, at the same time, there will be other posts involving the use of popular preexisting libraries to help demonstrate how machine learning models are used in practice; after all, most practitioners don’t build models themselves every time they embark on a project. The bottom line of this pslan is that we find a sweet spot between theory and practice, and eventually become versed at both. Catch you up in the next one. Happy new year!.",0,0,1,1,0,0,0,0
"Newton-Raphson, Secant, and More","Recently, I ran into an interesting video on YouTube on numerical methods (at this pont, I can’t help but wonder if YouTube can read my mind, but now I digress). It was a channel called numericalmethodsguy, run by a professor of mechanical engineering at the University of Florida. While the videos themselves were recorded a while back in 2009 at just 240p, I found the contents of the video to be very intriguing and easily digestable. His videos did not seem to assume much mathematical knowledge beyond basic high school calculus. After watching a few of his videos, I decided to implement some numerical methods algorithms in Python. Specifically, this post will deal with mainly two methods of solving non-linear equations: the Newton-Raphson method and the secant method. Let’s dive right into it. Before we move on, it’s first necessary to come up with a way of representing equations in Python. For the sake of simplicity, let’s first just consider polynomials. The most obvious, simplest way of representing polynomials in Python is to simply use functions.",1,0,0,0,0,0,0,0
"Beta, Bayes, and Multi-armed Bandits","This is going to be a lot of computation, but the nice part of it is that a lot of terms cancel out each other. We can easily see that there are constant that exist both in the numerator and the denominator. We can pull these constants out of the integral to simplify the expression. These constants include the binomial and the reciprocal Beta normalizing constants. One useful observation to make here is the fact that the numerator itself looks a lot like something we have seen before: the Beta distribution. In fact, you might also realize that the denominator is nothing but just a normalizing constant that ensures that our posterior distribution, when integrated from 0 to 1, integrates up to 1 as the axiom of probability states. We can also see the denominator as the definition of the Beta function. In other words, Therefore, we end up with Notice that offers an extremely nice interpretation: the number of success  and failures , which are determined by , yield insight into what the true parameter is via the Beta distribution. This is why the Beta distribution is often referred to as a “probability distribution of probabilities.",0,1,0,0,0,1,0,0
Moments in Statistics,"Hopefully, this post gave you some intuition behind the notion of moments, as well as how moment generating functions can be used to compute useful properties that explain a distribution. In the next post, we will take a brief break from the world of distributions and discuss some topics in information theory that I personally found interesting. If you would like to dwell on the question like “how do we quantify randomness,” don’t hesitate to tune in again in a few days!.",0,1,0,0,0,1,0,0
"Beta, Bayes, and Multi-armed Bandits","Given this setup, what is the optimal way of going about the problem? One obvious way is to start randomly pulling on some slot machines until they get a rough idea of what the success probabilities are. However, this is obviously not the most systematic approach, and there is not even a clear guideline as to how they should go about pulling these levers. Here is where the Bayesian approach comes in handy. It isn’t difficult to frame this as a Bayesian problem. Given a pull of the slot machine, the result of which we will denote as , a Bernoulli random variable, we can then formulate the problem as follows: And as the classical Bayesian analysis goes, the more data we collect through repeated experiments, the closer the posterior distribution will get to the target distribution. Through this process, we are able to approximate the parameter . Now we have to think about what distributions we want to use for the prior and likelihood. Let’s start with the easier one: the likelihood. The natural choice that makes sense is the binomial distribution.",0,1,0,0,0,1,0,0
Introduction to tf-idf,"As always, this post is going to take a hands-on approach by demonstrating a simple way of implementing tf-idf vectorization from scratch. Let’s get started. tf-idf stands for term frequency-inverse document frequency. This is all there is to it—in fact, the formula for tf-idf can simply be expressed as where  denotes a single term; , a singe document, and , a collection of documents. So simply put, tf-idf is simply a product of the term frequency, denoted above as , and inverse document frequency, . All there is left, then, is to figure out what term frequency and inverse document frequency are. Without much explanation, you can probably guess what term frequency is: it simply indicates how frequently a word appeared in a given document. For example, if there were a total of 3 distinct words in a document (very short, I know), then each of the three words would have a tf score of . Put differently, the sum of the tf vector for each document should sum to one.",0,0,0,1,0,0,0,0
"Basel, Zeta, and some more Euler","In the later segment of his life, Euler found a way to express the zeta function as, you guessed it, an infinite product. This time, however, Euler did not rely on Taylor polynomials. Instead, he employed a more general approach to the problem. It is here that we witness Euler’s clever manipulation of equations again. We commence from the zeta function, whose terms are enumerated below. Much like how we multiply the ratio to a geometric sequence to calculate its sum, we adopt a similar approach by multiplying the second term,  to the entire expression. This operations yields By subtracting this modified zeta function from the original, we derive the following expression below. Now, we only have what might be considered as the odd terms of the original zeta function. We then essentially repeat the operation we have performed so far, by multiplying the expression by  and subtracting the result from the odd-term zeta function. It is not difficult to see that iterating through this process will eventually yield Euler’s product identity for the zeta function. The key to understanding this identity is that only prime numbers will appear as a component of the product identity.",1,0,0,0,0,0,0,0
Traveling Salesman Problem with Genetic Algorithms,"As the name implies, genetic algorithms somewhat simulate an evolutionary process, in which the principle of the survival of the fittest ensures that only the best genes will have survived after some iteration of evolutionary cycles across a number of generations. Genetic algorithms can be considered as a sort of randomized algorithm where we use random sampling to ensure that we probe the entire search space while trying to find the optimal solution. While genetic algorithms are not the most efficient or guaranteed method of solving TSP, I thought it was a fascinating approach nonetheless, so here goes the post on TSP and genetic algorithms. Before we dive into the solution, we need to first consider how we might represent this problem in code. Let’s take a look at the modules we will be using and the mode of representation we will adopt in approaching TSP. The original, popular TSP requires that the salesperson return to the original starting point destination as well. In other words, if the salesman starts at city A, he has to visit all the rest of the cities until returning back to city A.",0,0,0,1,0,0,0,0
Building Neural Network From Scratch,"We don’t need the entire data to see that it works, so let’s slice the  array to see its first five elements. When we apply  to the data, we see that the returned result is a two-dimensional array containing one-hot encoded vectors, as intended. That’s all the data and the preprocessing we will need for now. Activation functions are important aspects of neural networks. In fact, it is what allows neural networks to model nonlinearities in data. As we will see in the next section, a neural network is essentially composed of layers and weights that can be expressed as matrix multiplications. No matter how complex a matrix may be, matrix multiplication is a linear operation, which means that is impossible to model nonlinearities. This is where activation functions kick in: by applying nonlinear transformation to layer outputs, we can make neural networks capable of modeling nonlinearities. This is why deep learning is such a powerful tool: it can be trained to detect nonlinear, complex patterns in data that a human might otherwise be unable to identify. Our vanilla neural network will make use of two activation functions: softmax and ReLU.",0,0,0,1,0,0,1,1
A sneak peek at Bayesian Inference,"This is all the Bayesian method there is in this updating procedure. Notice that this line of code directly corresponds to the formula for the updated Beta posterior distribution we found earlier, which is  refers to ,  corresponds to , and both  and  are set to  in order to take into account the initial prior which tends to a uniform distribution. An interesting observation we can make about this result is that the variance of the Beta posterior decreases with more trials, i.e. the narrower the distribution gets. This is directly reflective of the fact that we grow increasingly confident about our estimate of the parameter with more tosses of the coin. At the end of the 500th trial, we can conclude that the coin is fair indeed, which is expected given that we simulated the coin flip using the command . If we were to alter the argument for this method, say , then we would expect the final result of the update to reflect the coin’s bias. Bayes’ theorem is a powerful tool that is the basis of Bayesian statistical analysis.",0,1,0,0,0,1,0,0
A PyTorch Primer,"I’ve always been a fan of TensorFlow, specifically , for its simplicity and ease of use in implementing algorithms and building models. Today, I decided to give PyTorch a try. It is my understanding that TensorFlow is more often used in coporate production environments, whereas PyTorch is favored by academics, especially those in the field of NLP. I thought it would be an interesting idea to give it a try, so here is my first go at it. Note that the majority of the code shown here are either borrowed from or are adaptations of those available on the PyTorch website, which is full of rich content and tutorials for beginners. Of course, basic knowledge of DL and Python would be helpful, but otherwise, it is a great place to start. Let’s dive right in! Like TensorFlow, PyTorch is a scientific computing library that makes use of GPU computing power to acceleration calculations. And of course, it can be used to create neural networks. In this section, we will take a look at how automatic differentiation works in PyTorch.",0,0,0,0,0,0,1,0
"Basel, Zeta, and some more Euler","At this point, it doesn’t even surprise us to know that Euler applied this line of thinking to calculate the value of the sum of higher order inverses. An interesting corollary of Euler’s solution to the Basel problem is the Wallis product, which is a representation of the quantity  as an infinite product, as shown below: It seems mathematically unintuitive to say that an irrational number such as  can be expressed as a product of fractions, which is a way of representing rational numbers. However, we can verify the soundness of the Wallis product by substituting  for  in (1): Taking the reciprocal of this entire expression reveals the Wallis product. The Basel problem is a specific case of the Riemann zeta function, whose general form can be written as follows. A small digression: when , the zeta function converges to a value known as Apéry’s constant, eponymously named after the French mathematician who proved its irrationality in the late 20th century. Beyond the field of analytics and pure math, the zeta function is widely applied in fields such as physics and statistics. Perhaps we will explore these topics in the future. So back to the zeta function.",1,0,0,0,0,0,0,0
Demystifying Entropy (And More),"For the purposes of this post, we will be using equation (1) instead of two. This is primarily because we will be using the binary number analogy to build an intuition for information computation. Let’s quickly create a visualization that shows the relationship between probability and information in bits. As equation (1) describes this relationship quite concisely, let’s try plotting it on a graph.  So that’s how we calculate randomness in a random event—the amount of information that is needed to represent randomness as probability. If you think about it for a second, this is a very intuitive definition of randomness: the more random and infrequent an event is, the more information would be required to represent it. With this in mind, now we move onto the bigger picture: entropy in the context of random variables. In the previous section, we looked at how random events can be represented as information in bits. What’s important here was that we were dealing with isolated random events instead of random variables. For example, in the fair coin toss example, we dealt with information involved with  and , not the binomial random variable  itself.",0,0,0,0,0,1,0,0
PyTorch Tensor Basics,"The conclusion of this analysis is that either approach is fine; it is perhaps a good idea to stick to one convention and stay consistent with that coding style throughout. Resizing or reshaping a tensor is an incredibly important tensor operation that is used all the time. The interesting thing is that there seems to be many ways of achieving the same behavior. As someone who prefers a more opinionated guideline, this was rather confusing at first. However, here is what I have gathered while sifting through Stack Overflow and PyTorch discussion forums. Let’s first start with a dummy random tensor. (Note that I could have done , as per the conclusion from the section above.) The  operation returns a new tensor whose dimensions match those that have been passed into the function as arguments. For example, the snippet below shows how we can reshape  into a  tensor. One very important detail, however, is that this operation is not in-place. In other words, if we check the size of  again, you will realize that it is still a  tensor, as was originally initialized.",0,0,0,0,0,0,1,0
Dissecting LSTMs,"Therefore, for the sake of demonstration, we will only deal with  and . Let’s start with the easier of the two, . Recall that As we have done earlier, let’s introduce an intermediate variable, , and try deriving the gradient for that variable. Note that with this substitution, . Now we can move onto deriving the expressions for the gradient of the actual parameters, starting with . This is extremely simple since  and  are defined by a linear relationship. The next in line is . This is also very simple, since all we need to do is to consider one instance of matrix multiplication. where, given a concatenation operator , Now we are done! The gradient for the rest of the parameters, such as  or  look almost exactly the same as  and  respectively, and not without reason: as we have noted above, what I conveniently called the filter-and-raw-material structure of LSTM gates remain consistent across the forget, input, and output gates. Therefore, we can apply the same chain rule to arrive at the same expressions. However, there is one more caveat that requires our last bit of attention, and that is the gradient for .",0,0,0,0,0,0,1,0
So What are Autoencoders?,"Let’s declare the encoder and autoencoder model by invoking the  function with the specified image shape and the dimensionality of the latent space. Just to get a sense of what operations are taking place dimensionality-wise, here is a look at the output shapes of the autoencoder model. Notice that the input is of shape , and that the final output is also of the same shape , as expected. Here’s the image of the model for the fancy bells and whistles.  Now that the autoencoder model is fully ready, it’s time to see what it can do! Although autoencoders present countless exciting possibilities for application, we will look at a relatively simple use of an autoencoder in this post: denoising. There might be times when the photos we take or image data we use are tarnished by noise—undesired dots or lines that undermine image quality. An autoencoder can be trained to remove these noises fairly easily as we will see in thi post. First, let’s import the MNIST data set for this tutorial. Nothing much exciting is happening below, except for the fact that we are rearranging and preprocessing the dataset so as to maximize training efficiency.",0,0,0,0,1,0,1,0
Traveling Salesman Problem with Genetic Algorithms,"Namely, we will be arranging city coordinates to lie on a semi-circle, using the very familiar equation Let’s create 100 such fake cities and run the genetic algorithm to optimize the path. If the algorithm does successfully find an optimal path, it will be a single curve from one end of the semi-circle fully connected all the way up to its other end.  The algorithm seems to have converged, but the returned  does not seem to be the optimal path, as it is not a sorted array from 0 to 99 as we expect. Plotting this result, the fact that the algorithm hasn’t quite found the most optimal solution becomes clearer. This point notwithstanding, it is still worth noting that the algorithm has found what might be referred to as optimal segments: notice that there are some segments of the path that contain consecutive numbers, which is what we would expect to see in the optimal path.  An optimal path would look as follows.  Comparing the two, we see that the optimal path returned by the genetic algorithm does contain some wasted traveling routes, namely the the chords between certain non-adjacent cities.",0,0,0,1,0,0,0,0
Recommendation Algorithm with SVD,"With this setup, we start from the definition of eigenvectors and eigenvalues: If we apply transpose on both sides, We can legally multiply both sides by , which results in the following: However, since , Furthermore, we can use the fact that the eigenvalue corresponding to  is . Then, Since , the only way for (5) to make sense is if —and this is exactly what we have been trying to show. Since  and  are two distinct eigenvectors of the symmetric matrix , we have successfully shown that any two eigenvectors of  will be orthogonal, i.e. their dot product is going to be zero. Let’s start by assuming that  has some non-zero eigenvector  whose corresponding eigenvalue is . Then, we have If we left multiply both sides by , we get By the definition of an eigenvector, it is not difficult to see that  has an eigenvector  whose corresponding eigenvalue is . In short, the reason why SVD works is that the eigenvalue matrix  can be obtained either way by performing an eigendecomposition of the matrix  or .",0,0,0,1,0,0,0,1
Markov Chain and Chutes and Ladders,"Let’s begin the derivation: let  be the matrix of interest,  a matrix whose columns are eigenvectors of , and , a matrix whose diagonal entries are the corresponding eigenvalues of . Let’s consider the result of multiplying  and . If we view multiplication as a repetition of matrix-times-vector operations, we yield the following result. But recall that  are eigenvectors of , which necessarily implies that Therefore, the result of  can be rearranged and unpacked in terms of : 
 In short, Therefore, we have , which is the formula for eigendecomposition of a matrix. One of the beauties of eigendecomposition is that it allows us to compute matrix powers very easily. Concretely, Because  and  nicely cross out, all we have to compute boils down to ! This is certainly good news for us, since our end goal is to compute powers of the stochastic matrix to simulate the Markov chain. However, an important assumption behind eigendecomposition is that it can only be performed on nonsingular matrices. Although we won’t go into the formal proofs here, having a full span of independent eigenvectors implies full rank, which is why we must check if the stochastic matrix is singular before jumping into eigendecomposition.",0,0,0,0,0,0,0,1
Understanding PageRank,"Google is the most popular search engine in the world. It is so popular that the word “Google” has been  as a proper verb, denoting the act of searching on Google. While Google’s success as an Internet search engine might be attributed to a plethora of factors, the company’s famous PageRank algorithm is undoubtedly a contributing factor behind the stage.  is a method by which Google ranks different pages on the world wide web, displaying the most relevant and important pages on the top of the search result when a user inputs an entry. Simply put, PageRank determines which websites are most likely to contain the information the user is looking for and returns the most optimal search result. While the nuts and bolts of this algorithm may appear complicated—and indeed they are—the underlying concept is surprisingly intuitive: the relevance or importance of a page is determined by the number of hyperlinks going to and from the website. Let’s hash out this proposition by creating a miniature version of the Internet. In our microcosm, there are only five websites, represented as nodes on a network graph. Below is a simple representation created using Python and the .",0,0,0,0,0,0,0,1
Riemann Zeta and Prime Numbers,"This function looks as follows: As you can see, this is essentially the alternating version of the Riemann Zeta function. Given this design, we can derive what may appear to be apparent to some yet nonetheless interesting relationship between the Eta and Zeta. Deriving this relationship requires a very similar operation to the sieving or factorizing we performed earlier to derive the probabilistic interpretation of the Zeta function. For a bit of intuition, observe that the Eta function can be split up into what may be referred to as even and odd terms. In other words, The idea is that the even terms are just a multiple of the Zeta function, namely Then, the odd terms can also be seen as the Zeta function minus this multiple: We now have successfully expressed both the even and odd terms of the Eta function in terms of the Zeta function. If we put the two together, we will then be able to express the entirety of the Eta function fully in terms of the Zeta function.",1,0,0,0,0,0,0,0
Likelihood and Probability,"it better explains the data  since , which is larger than . To sum up, likelihood is something that we can say about a distribution, specifically the parameter of the distribution. On the other hand, probabilities are quantities that we ascribe to individual data. Although these two concepts are easy to conflate, and indeed there exists an important relationship between them explained by Bayes’ theorem, yet they should not be conflated in the world of mathematics. At the end of the day, both of them provide interesting ways to analyze the organic relationship between data and distributions. Maximum likelihood estimation, or MLE in short, is an important technique used in many subfields of statistics, most notably Bayesian statistics. As the name suggests, the goal of maximum likelihood estimation is to find the parameters of a distribution that maximizes the probability of observing some given data . In other words, we want to find the optimal way to fit a distribution to the data. As our intuition suggests, MLE quickly reduces into an optimization problem, the solution of which can be obtained through various means, such as Newton’s method or gradient descent.",0,0,0,0,0,1,0,0
Gaussian Process Regression,"This marginalization property can be understood both intuitively by thinking about the implications of viewing the mean and covariance as vectors and matrices, or by taking a direct integral: Lastly and most importantly, we also saw in the post on Bayesian linear regression that the product of two Gaussians is also Gaussian. Like this, a distribution that is Gaussian most likely stays Gaussian, withstanding such operations as marginalization, multiplication, or conditioning. This is a powerful property that we can use to motivate the “Gaussian-ness” behind GP. As stated earlier, GP is non-parametric. Simply put, this means that we don’t have to consider things like the typical  in the context of linear regression. Normally, we would start off with something like This is sometimes also written in terms of a weight vector  or a function . Here, we also have some Gaussian noise, denoted by : However, since GPs are non-parametric, we do not have to specify anything about the model. How do we remove this consideration? The short answer is that we marginalize out the model from the integral. Let  denote the model,  the data,  the predictions.",0,0,1,1,0,0,0,0
Scikit-learn Pipelines with Titanic,"So hacky methods must not be used in isolation; at the very least, they need to be complemented with some form of human input. Let’s try to use a simple pipeline to deal with missing values in some categorical variables. This is going to be our first sneak peak at how pipelines are declared and used. Here, we have declared a three-step pipeline: an imputer, one-hot encoder, and principal component analysis. How this works is fairly simple: the imputer looks for missing values and fills them according to the strategy specified. There are many strategies to choose from, such as most constant or most frequent. Then, we one-hot encode the categorical variables since most machine learning models cannot accept non-numerical values as input. The last PCA step might seem extraneous. However, as discussed in this Stack Overflow thread, the judicious combination of one-hot plus PCA can seldom be beat by other encoding schemes. PCA finds the linear overlap, so will naturally tend to group similar features into the same feature. I don’t have enough experience to attest to the veracity of this claim, but mathematically or statistically speaking, this proposition seems valid.",0,0,1,0,0,0,0,0
Building Neural Network From Scratch,"As expected, the softmax function returns the softmax output applied to each individual instance in the list. Note that the elements of each output instance add up to one, as expected. Another crucial activation function is ReLU, or the rectified linear unit. ReLU is a piece-wise function, and hence introduces nonlinearity, which is one of the purposes of having an activation function in a neural network. The formula for ReLU is extremely simple. If the input value  i s greater or equal to zero, the ReLU function outputs the value without modification. However, if  is smaller than zero, the returned value is also zero. There are other ways of expressing the ReLU function. One version that is commonly used and thus deserves our attention is written below. Although this appears different from (3), both formulas express the same operation at their core. We can get a better sense of what the function with the help of Python. Assuming that the input is a  vector, we can use vectorization to change only the elements in the input vector that are negative to zero, as shown below. Let’s see what the ReLU function looks like by plotting it on the plane.",0,0,0,1,0,0,1,1
"Newton-Raphson, Secant, and More","Below is the  function that receives as input some parsed output string and returns a corresponding Python function. Now, we can do something like this: Now that we have more than enough tools we can use relating to the list index representation we decided to use to represent polynomials, it’s time to exploit the convenience that this representation affords us to calculate derivatives. Calculating derivatives using the list index representation is extremely easy and convenient: in fact, it can be achieved in just a single line. Let’s test this function with the  example we have been using previously. Let’s also use the  function to make the final result for human-readable. Seems like the derivative calculation works as expected. In the process, I got a little bit extra and also wrote a function that integrates a function in list index representation format. If we integrate , we end up with , where  is the integration constant. Excluding the integration constant, we get a result that is consistent with the  function. While it’s great that we can calculate derivatives and integrals, one very obvious drawback of this direct approach is that we cannot deal with non-polynomial functions, such as exponentials or logarithms.",1,0,0,0,0,0,0,0
The Math Behind GANs,"Hence, the expectation can be expressed as a summation. We can simplify this expression even further in the case of binary cross entropy, since there are only two labels: zero and one. This is the  function that we have been loosely using in the sections above. Binary cross entropy fulfills our objective in that it measures how different two distributions are in the context of binary classification of determining whether an input data point is true or false. Applying this to the loss functions in (1), We can do the same for (2): Now we have two loss functions with which to train the generator and the discriminator! Note that, for the loss function of the generator, the loss is small if  is close to 1, since . This is exactly the sort of behavior we want from a loss function for the generator. It isn’t difficult to see the cogency of (6) with a similar approach. The original paper by Goodfellow presents a slightly different version of the two loss functions derived above. Essentially, the difference between (6) and (8) is the difference in sign, and whether we want to minimize or maximize a given quantity.",0,0,0,0,0,1,1,0
Complex Fibonacci,"We can get a sense of how quickly this disparity would grow if we try to calculate something like the 1000th fibonacci number in the sequence. Another perk of using caching as above is that we can now get the full sequence up to the 35th fibonacci number. Although memoization is interesting, it is not the main topic of today’s post. Instead, I want to discuss Binet’s formula, a formula with which we can calculate the th fibonacci number. Binet’s formula states that We can trivially verify that  and that . For more robust empirical verification, we will resort to code later. It is worth noting that the quantity in the parenthesis, namely is otherwise referred to as the Golden ratio. Also observe that the other quantity is the negative inverse of the Golden ratio. Let’s take a closer look at why the Binet’s formula makes sense. This is not going to be a rigorous proof or a derivation, but rather an attempt at cursory analysis to provide food for thought. This process was heavily referenced from this Quora post.",1,0,0,0,0,0,0,0
My First GAN,"So let’s go ahead and specify that. We see that  contains 6000 images, which is more than enough to start training our GAN. To train the GAN, we will define a  function. Essentially, this function creates binary labels for real and fake images. Recall that the goal of the discriminator is to successfully discern generated images from real ones. Also recall that to create generated images, the generator needs to sample from a latent dimension. In other words, training will consist of the following steps: These high level abstractions are what  implements behind the scenes. There are several subtleties that deserve our attention. First, we fade out the labels ever so slightly to expedite the training process. These are little magic tricks that people have found to work well on GAN training. While I’m not entirely sure about the underlying principle, it most likely comes from the fact that having a smooth manifold is conducive to the training of a neural network. Second, coercing a true label on the GAN essentially trains the generator. Note that we never explicitly address the generator in the function; instead, we only train the discriminator.",0,0,0,0,1,0,1,0
Wonders of Monte Carlo,"For convenience purposes, let’s center this circle and square both at the origin. Next, we generate a series of random coordinates within the region of the square. Then, we count the percentage of dots that fall within the area of the cricle. Using a simple formula of proportions, we can calculate the area of the circle, through which we can then estimate the value of . Before we get into the specifics of this algorithm, let’s see hwo this plays out in code. Now that we have the function ready, let’s try calling it with some input parameters. Below, we perform our little crude Monte Carlo simulation with a hundred randomly generated data points. Invoking the  function returns the value of the estimation. The returned result is not abysmal, but we clearly can do a lot better. The reason behind this spotty estimation can be checked by drawing the plot of the Monte Carlo simulation, as shown below.  As we can see, ten samples simply aren’t enough to really cover the entire area of the plane or the circle.",0,0,0,0,0,1,0,0
