title,body,from_scratch,deep_learning,statistics,linear_algebra,machine_learning,analysis,probability_distribution,pytorch
Natural Gradient and Fisher,"In a previous post, we took a look at Fisher’s information matrix. Today, we will be taking a break from the R frenzy and continue our exploration of this topic, meshing together related ideas such as gradient descent, KL divergence, Hessian, and more. The typical formula for batch gradient descent looks something like this: This is the familiar gradient descent algorithm that we know of. While this approach works and certainly makes sense, there are definite limitations; hence the introduction of other more efficient algorithms such as SGD, Adam, and et cetera. However, these algorithms all have one thing in common: they adjust the parameter in the parameter space according to Euclidean distance. In other words, gradient descent essentially looks at regions that are some Euclidean distance away from the current parameter and chooses the direction of steepest descent.",0,0,1,0,1,0,0,0
Dissecting the Gaussian Distribution,"Although this relationship may not be immediately apparent, that the two expressions are identical can be seen by setting the random vector as and performing basic matrix vector multiplication operations. For the sake of brevity, this is left as an exercise for the reader. We now have all the pieces we need to complete the puzzle. Recall that we were trying to derive the probability density function of the multivariate Gaussian by building on top of the formula for the univariate Gaussian distribution. We finished at then moved onto a discussion of variance and covariance. Now that we understand that the covariance matrix is the analogue of variance, we can substitute  with , the covariate matrix. Instead of leaving  at the denominator, let’s use the fact that to rearrange the expression. This is another example of when the matrix-scalar parallel intuition can come in handy: the scalar multiplicative identity is 1, whereas the equivalent in matrix world is the identity matrix . Therefore, the reciprocal of a matrix can be interpreted as its inverse. From this observation, we can conclude that We are almost done, but not quite.",0,0,1,0,0,0,1,0
Gaussian Mixture Models,"As we will see in this section, such a relationship can clearly be established, which is why EM is so commonly used in the context of Gaussian mixture models. Let’s begin by defining some quantities, the first being the posterior distribution. In other words, given some data, what is the probability that it will belong to a certain class? Using the definition of conditional probability, we can arrive at the following conclusion: This represents the probability that, given some point , the point belongs in the th cluster. With this result, we can now rewrite the MLE calculation that we were performing earlier. Using the new  notation, we can thus simplify the result down to We can then simplify this expression to derive an expression for . An important trick is here to use the fact that the covariance matrix is positive semi-definite. Therefore, the covariance matrix plays no role in driving the value down to zero. With some algebraic manipulations, we arrive at Let’s introduce another notational device to simplify the expresison even further. Let Recall that  was defined to be the posterior probability that a given point  belongs to the th cluster.",0,0,1,0,1,0,0,0
Principal Component Analysis,"We know that  is a row vector since its dot product with  should be possible dimensionally speaking. Then, we know that the gradient with respect to  should give each of the elements of , but in column vector format—hence the need for a transpose. In general, the rule of thumb is that the gradient of a scalar with respect to a vector or a matrix should return a vector or matrix of the same dimension. Recall from (1) that reconstruction can be achieved by applying compression followed by a decoding operation: Since we know that  is just  and  is  by definition, we can express (1) in a different way. In retrospect, this is somewhat intuitive since  can roughly be thought of as a pseudo-orthonormal matrix—pseudo since there is no guarantee that it is a square matrix. Now, all that is left is to find the matrix . The way to go about this is to reconsider (3), the notion of minimizing data loss, given our findings in (6). In other words, Instead of considering a single observation, here we consider the design matrix in its entirety. Note that  is a design matrix whose rows correspond to a single observation.",0,0,1,1,0,0,0,0
How lucky was I on my shift?,"This is what the binomial PMF is implying: calculating the probability that we get  successes in  trials requires that we multiply the probability of success  by  times and the probability of failure  by  times, because those are the numbers of successful and unsuccessful trials respectively. Now that we have reviewed the concept of binomial distribution, it is time to apply it to our example of phone calls at the Yongsan PMO. Although I do not have an  data sheet on me, let’s assume for the sake of convenience that, on average, 12 calls come to the PMO per shift, which is eight hours. Given this information, how can we simulate the situation as a binomial distribution? First, we have to define what constitutes a success in this situation. While there might be other ways to go about this agenda, the most straightforward approach would be to define a phone call as a success.",0,0,1,0,0,0,1,0
Word2vec from Scratch,"When the output is passed into this layer, it is converted into probability vectors whose elements sum up to one. This final output can be considered as context predictions, i.e. which words are likely to be in the window vicinity of the input word. In training—specifically error calculation and backpropagation—we would be comparing this prediction of probability vectors with its true one-hot encoded targets. The error function that we use with softmax is cross entropy, defined as I like to think of this as a dot product of the target vector and the log of the prediction, because that is essentially what the summation is doing. In this alternate formulation, the cross entropy formula can be rewritten as Because  a one-hot encoded vector in this case, all the elements in  whose entry is zero will have no effect on the final outcome. Indeed, we simply end up taking the negative log of the prediction. Notice that the closer the value of the prediction is to 1, the smaller the cross entropy, and vice versa. This aligns with the behavior we want, since we want the predicted probability to be as close to 1 as possible.",1,1,0,0,0,0,0,0
Logistic Regression Model from Scratch,"For instance, given the dimensions of a patient’s tumor, determine whether the tumor is malignant or benign. Another problem might involve classifying emails as either spam or not spam. We can label spam emails as 1 and non-spam emails as 0, feed the data into a predefined machine learning algorithm, and generate predictions using that model. If the output of an algorithm given some data point is larger than 0.5, it is likely that the given input is a spam; if it is smaller than the 0.5 threshold, chances are the email is not spam. Let’s take a look at the shape of the sigmoid function, which is a special case of the logistic function that we will use throughout this post. To plot the sigmoid function, we need to import some libraries. The sigmoid function is defined as follows: We can express this as a Python function, as demonstrated in the code snippet below. Let’s quickly plot the graph to see what the sigmoid function looks like.  As we can see, the sigmoid is a smooth, differentiable function that is bounded between 0 and 1.",1,0,0,0,1,0,0,0
How lucky was I on my shift?,"How can we obtain the probability of getting one head and nine tails? To begin with, here is the list of all possible arrangements: Notice that all we had to do was to choose one number  that specifies the index of the trial in which a coin toss produced a head. Because there are ten ways of choosing a number from integers  to , we got ten different arrangements of the situation satisfying the condition . You might recall that this combinatoric condition can be expressed as , which is the coefficient of the binomial distribution equation. Now that we know that there are ten different cases, we have to evaluate the probability that each of these cases occur, since the total probability , where . Calculating this probability is simple: take the first case,  as an example. Assuming independence on each coin toss, we can use multiplication to calculate this probability: Notice that  because we assumed the coin was fair. Had it not been fair, we would have different probabilities for  and , explained by the relationship that .",0,0,1,0,0,0,1,0
"Basel, Zeta, and some more Euler","Indeed, we know that this series converges to a real value. We also know that integration would give us a rough approximation. However, how can evaluate this series with exactitude? Euler’s solution, simple and elegant, demonstrates his genius and acute heuristics. Euler begins his exposition by analyzing the Taylor expansion of the sine function, which goes as follows: Dividing both sides by , we obtain the following: Now it’s time to follow Euler’s amazing intuition. If we take a close look at the equation , we can convince ourselves that its solutions will adhere to the form , where  is a non-zero integer between . This is expected given the periodic behavior of the sine function and its intercepts with the -axis. Given these zeros, we can then reconstruct the original function  as an infinite product using the fundamental theorem of algebra, or more specifically, Weierstrass factorization. Let’s try to factor out the coefficient of the  term through some elementary induction.",0,0,0,0,0,1,0,0
Likelihood and Probability,"Using the property in (3), we can simplify the equation above: To find the maximum of this function, we can use a bit of calculus. Specifically, our goal is to find a parameter that which makes the first derivative of the log likelihood function to equal 0. To find the optimal mean parameter , we derive the log likelihood function with respect to  while considering all other variables as constants. From this, it follows that Rearranging this equation, we are able to obtain the final expression for the optimal parameter  that which maximizes the likelihood function: As part 2 of the trilogy, we can also do the same for the other parameter of interest in the normal distribution, namely the standard deviation denoted by . We can simplify this equation by multiplying both sides by . After a little bit of rearranging, we end up with Finally, we have obtained the parameter values for the mean and variance of a normal distribution that maximizes the likelihood of our data.",0,0,1,0,0,0,0,0
Building Neural Network From Scratch,"Instead of cluttering the diagram by attempting to visualize all 64 neurons, I decided to simplify the picture by assuming that we have 16 neurons in each of the affine layers. But with the power of imagination, I’m sure it’s not so much difficult to see how the picture would change with 64 neurons.  Hopefully the visualization gave you a better understanding of what our model looks like. Now that we have a function that creates our model, we are ready to run the model! At this point, our neural network model is only a dictionary that contains matrices of specified sizes, each containing randomly genereated numbers. You might be wondering how a dictionary can be considered a model—after all, a dictionary is merely a data structure, and so is incapable of performing any operations. To make our model to work, therefore, we need a function that performs matrix multiplications and applies activation functions based on the dictionary. The  function is precisely such a function that uses the weights stored in our model to return both the intermediary and final outputs, denoted as  and  respectively.  Note that we apply activation functions, such as  and  when appropriate.",1,1,0,1,0,0,0,0
Gaussian Process Regression,"However, we would expect them to look even smoother had we augmented the dimensions of the test data to 100 dimensions or more. Next, we need a function from which we generate dummy train data. For the purposes of demonstration, let’s choose a simple sine function. Let’s generate 15 training data points from this function. Note that we are performing a noiseless GP regression, since we did not add any Gaussian noise to . However, we already know how to perform GP with noise, as we discussed earlier how noise only affects the diagonal entries of the kernel. Now it’s time to model the posterior. Recall that the posterior distribution can be expressed as If we use  or  functions to calculate the inverse of the kernel matrix components, out life would admittedly be easy. However, using inversion is not only typically costly, but also prone to inaccuracy. Therefore, we instead opt for a safer method, namely using . In doing so, we will also be introducing some intermediate variables for clarity. Let’s begin with the expression for the posterior mean , which is .",1,0,0,0,1,0,0,0
Markov Chain and Chutes and Ladders,"For the purpose of demonstration, let  be an arbritrary vector  and  the three-by-three identity matrix. Multiplying  by  produces the following result: The result is unsurprising, but it reveals an interesting way of understanding : identity matrices are a special case of diagonalizable matrices whose eigenvalues are 1. Because the multiplying any arbitrary vector by the identity matrix returns the vector itself, all vectors in the dimensional space can be considered an eigenvector to the matrix , with  = 1. A formal way to calculate eigenvectors and eigenvalues can be derived from the equation above. Since  is assumed as a nonzero vector, we can deduce that the matrix  is a singular matrix with a nontrivial null space. In fact, the vectors in this null space are precisely the eigenvectors that we are looking for. Here, it is useful to recall that the a way to determine the singularity of a matrix is by calculating its determinant. Using these set of observations, we can modify the equation above to the following form: By calculating the determinant of , we can derive the characteristic polynomial, from which we can obtain the set of eigenvectors for  representing some linear transformation .",0,0,0,1,0,0,0,0
Neural Style Transfer,"The Gram matrix, despite its fancy name, is something that you’ve already seen at some point in your study of linear algebra. Given some matrix , the Gram matrix can be calculated as More strictly speaking, given a set of vectors , a Gram matrix can be calculated such that So how does the Gram matrix encode the stylistic similarities or differences between two images? Before I attempt at an explanation in words, I recommend that you check out this Medium article, which has helped me wrapped my own head around the different dimensions involved in the style loss term. This Medium article has also helped me gain more intuition on why the style loss is the way it is. The motivating idea is that, given an image and a layer in the feature extractor model, the activations each encode information coming from a filter. The resulting feature maps, therefore, contain information about some feature the model has learned, such as the presence of some pattern, shape, or object.",0,1,0,0,0,0,0,1
Building Neural Network From Scratch,"Note that the indicator function, denoted as , is a simple gate function that calculates the gradient of the ReLU unit: It isn’t difficult to see that the indicator function is simply a derivative of the ReLU function as shown in equation (3). Now, it is time to translate our findings into Python. Because our neural network model is represented as a dictionary, I decided to adopt the same data structure for the gradient. Indeed, that is how we designed the  function above. The  function below is an implementation of back propagation that encapsulates equations (14) through (17). There is a subtlety that I did not discuss previously, which has to do with the bias terms. It may appear as if the gradient of the bias term does not match that of the bias term itself. Indeed, that is a valid observation according to equation (16). The way we go about this is that we add up the elements of the matrix according to columns. This is exactly what we do with the  command invoked when computing  and , which represent the gradient of the bias terms.",1,1,0,1,0,0,0,0
Logistic Regression Model from Scratch,"As we saw earlier with the application of the model to the task of bank notes authentication, the logistic regression model can, when tweaked with the appropriate parameters, make surprisingly accurate predictions given sufficient amount of training data. Of course, the processing of training and tweaking is not always easy because we have to determine some hyperparameters, most notably the learning rate of the gradient descent algorithm, but the fact that logistic regression is a robust model is unchanged nonetheless. Hopefully this post gave you some idea of what happens behind the scene in a regression-based machine learning model. Thanks for reading. See you in the next post, and happy new year!.",1,0,0,0,1,0,0,0
Bayesian Linear Regression,"MAP versus MLE is a recurring theme that appears throughout the paradigmatic shift from frequentist to Bayesian, so it merits discussion. Now that we have a posterior distribution for  which we can work with, it’s time to derive the predictive distribution. We go about this by marginalizing  using the property of conditional probability, as illustrated below. This may seem like a lot, but most of it was simple calculation and distributing vectors over parentheses. It’s time to use the power of conjugacy again to extract a normal distribution out of the equation soup. Let’s complete the square of the exponent according to the Gaussian form after making the appropriate substitutions Again, observing this is not a straightforward process, especially if we had no idea what the final distribution is going to look like. However, given that the resulting predictive posterior will take a Gaussian form, we can backtrack using this knowledge to obtain the appropriate substitution parameters in (13). Continuing, where the last equality stands because we can pull out terms unrelated to  by considering them as constants.",0,0,0,1,0,0,0,0
Dissecting LSTMs,"This relationship becomes a bit more apparent when we graph the two functions side by side.  Let’s refer to the definition of  to derive an expression for its derivative (no pun intended). Now comes the more complicated part. Thankfully, we’ve already done something very similar in the past when we were building a vanilla neural network from scratch. For instance, we know from this post that, given a cross entropy loss function, the gradient of the softmax layer can simply be calculated by subtracting the predicted probability from the true distribution. In other words, given an intermediate variable we know that Note that  is denoted as  in the code segment. Since we have this information, now it’s just a matter of back propagating the gradients to the lower segments of the acyclic graph that defines the neural network. Given (8), it only makes sense to continue with the next parameter, . The transpose or the order in which the terms are multiplied may be confusing, but with just some scratch work on paper, it isn’t difficult to verify these gradients by checking their dimensions. The equation for  is even simpler, since there is no matrix multiplication involved.",0,1,0,0,0,0,0,0
Demystifying Entropy (And More),"The other day, my friend and I were talking about our mutual friend Jeremy. “He’s an oddball,” my friend Sean remarked, to which I agreed. Out of nowhere, Jeremy had just told us that he would not be coming back to Korea for the next three years. “He is just about the most random person I know.” And both of us, being aspiring statistics majors, began wondering: is there a systematic way of measuring randomness? It is from here that we went down the rabbit hole of Google and Wikipedia search. I ended up landing on entropy land, which is going to be the topic for today’s post. It’s a random post on the topic of randomness. To begin our discussion of randomness, let’s take a look at how scientists measure the randomness involved in natural phenomena, such as particle movement. If you are a chemist or physicist, you might be familiar with the concept of entropy, one of the core elements of thermodynamics and a topic that recurs throughout many subfields of natural sciences.",0,0,1,0,0,0,0,0
Recommendation Algorithm with SVD,"This decomposition structure is similar to that of eigendecomposition, and this is no coincidence: in fact, formula (1) can simply be shown by performing an eigendecomposition on  and . Let’s begin by calculating the first case, , assuming formiula (1). This process looks as follows: The last equality stands since the inverse of an orthogonal matrix is equal to its transpose. Substituting  for , equation (2) simplifies to And we finally have what we have seen with eigendecomposition: a matrix of independent vectors equal to the rank of the original matrix, a diagonal matrix, and an inverse. Indeed, what we have in (3) is an eigendecomposition of the matrix . Intuitively speaking, because matrix  is not necessarily square, we calculate  to make it square, then perform the familiar eigendecomposition. Note that we have orthogonal eigenvectors in this case because  is a symmetric matrix—more specifically, positive semi-definite. We won’t get into this subtopic too much, but we will explore a very simple proof for this property, so don’t worry. For now, let’s continue with our exploration of the SVD formula by turning our attention from matrix —a factor of eigendecomposition on —to the matrix .",1,0,0,1,0,0,0,0
Recommendation Algorithm with SVD,"Singular value decomposition can intuitively be thought of as a square root version of eigendecomposition, since essentially  and  are all derivatives that come from the “square” of a matrix, the two transpose multiples. This intuition also aligns with the fact that  is a diagonal matrix containing the square roots of eigenvalues of the transpose products. With these in mind, let’s get ready to build the recommendation model. In this optional section, we take a look at two mathematical propositions we referenced while motivating the SVD formula: first, that symmetric matrices have orthogonal eigenvectors; second, that  and  have identical non-zero eigenvalues. The proof for both of these statements are simple, but feel free to gloss over this section if you just want to see SVD at work instead of the mathematical details behind singular value decomposition. Let  be some symmetric matrix, i.e. . Also assume that  has two distinct eigenvectors,  and  with corresponding eigenvalues  and .",1,0,0,1,0,0,0,0
Introduction to tf-idf,"Also note that published modules use sparse representations to minimize computational load, as we will later see with scikit-learn. Now it’s time to implement the first step: calculating term frequency. In Python, this simply amounts to looping through each document, creating a tf vector per iteration, and making sure that they are normalized as frequencies at the very end. In creating tf vectors for each document, we will be referencing the word-to-index mapping in our corpus. Let’s see what we get for the four documents in our dummy example. Due to floating point arithmetic, the decimals don’t look the most pleasing to the eye, but it’s clear that normalization has been performed as expected. Also note that we get 4 vectors of length 9 each, as expected. Next, it’s time to implement the idf portion of the vectorization process. In order to calculate idf, we first need a total count of each number in the entire document collection. A module that is perfect for this job is , which accepts as input an iterable and outputs a dictionary-like object whose values represent the count of each key.",1,0,0,0,0,0,0,0
(Attempt at) Knowledge Distillation,"If we show the model a 6, chances are, it will accurately predict that the number is a 6. No big deal here. The interesting part occurs when we look at its logits. After a softmax activation, we might expect to see something like  We can see that the network correctly outputs 6 as the most probably digit by assigning it the largest logit. However, this is not the end of the story: we also see that the network has assigned a somewhat high probability to other numbers, such as 0 and 9. If you think about it, it kind of makes sense: given the shape of the digits, one would reasonably expect a 6 to look like 0, or vice versa, depending on the handwriting, and ditto the 9. Such implicit knowledge the model learns about the dataset is referred to as black knowledge. The goal of knowledge distillation is to be able to help the student model learn not only the true labels from the dataset, but also this dark knowledge that the larger teacher model has about the data. This is why we train the student model on the logits of the teacher model.",0,1,0,0,0,0,0,1
"Linear Regression, in Two Ways","The intuition behind this approach is simple: if we can derive a formula that expresses the error between actual values of  and those predicted by regression, denoted as  above, we can use calculus to derive that expression and ultimately locate the global minimum. And that’s exactly what we’re going to do. But before we jump into it, let’s briefly go over some basics of matrix calculus, which is the variant of calculus we will be using throughout. Much like we can derive a function by a variable, say  or , loosely speaking, we can derive a function by a matrix. More strictly speaking, this so-called derivative of a matrix is more formally known as the gradient. The reason why we introduced the gradient as a derivative by a matrix is that, in many ways, the gradient in matrix calculus resembles a lot of what we saw with derivatives in single variable calculus. For the most part, this intuition is constructive and helpful, and the few caveats where this intuition breaks down are beyond the purposes of this post. For now, let’s stick to that intuition as we venture into the topic of gradient.",0,0,0,1,0,0,0,0
"Linear Regression, in Two Ways","The final result tells us that the line of best fit, given our data, is Let’s plot this line alongside our toy data to see how the equation fits into the picture.  It’s not difficult to see that linear regression was performed pretty well as expected. However, ascertaining the accuracy of a mathematical model with just a quick glance of an eye should be avoided. This point then begs the question: how can we be sure that our calculated line is indeed the best line that minimizes error? To that question, matrix calculus holds the key. We all remember calculus from school. We’re not going to talk much about calculus in this post, but it is definitely worth mentioning that one of the main applications of calculus lies in optimization: how can we minimize or maximize some function, optionally with some constraint? This particular instance of application is particularly pertinent and important in our case, because, if we think about it, the linear regression problem can also be solved with calculus.",0,0,0,1,0,0,0,0
Recommendation Algorithm with SVD,"However, perhaps this is due to the fact that all entries of this matrix have pretty small values, and it is difficult to see how the difference between Movie 2 and 3 compares to, say, the distance between Movies 1 and 4. Perhaps we should scale this in terms of relative distances or plot it on a three dimensional space, which is exactly what we are going to in a moment. Before we jump into visualizations, however, let’s deal with the elephant in the room first: is it okay to simply chop off a few dimensions to reduce a high dimensional image to fit into three-dimensional space? To answer this question, let’s check the  matrix for this particular instance of singular value decomposition. Note that we already have the first three values of  in our hands given that  in our instantiation of singular value decomposition. The information we lose pertains to the last two values, given by  and . These values are smaller in order of magnitude compared to, for instance, the largest value of , which is . This supports the idea that the information we lose amid dimensionality reduction is minimal.",1,0,0,1,0,0,0,0
Logistic Regression Model from Scratch,"In mathematical notation, we might express this process as follows: If we were to perform this in vectorized format, where  represents a vector containing the weight coefficients of the logistic regression model: The  notation is used to denote gradients, an important operation in matrix calculus which we explored in when deriving the normal equation solution to linear regression on this blog. The  denotes a hyperparameter known as the learning rate, which essentially determines how big of a step the gradient descent model takes with each iteration. The main takeaway here is the the gradient descent method allows us to find the local minimum of any convex function, no matter how multidimensional or complex. This is an incredibly powerful statement, and it is one that lies at the core of many machine learning algorithms. To implement gradient descent with code, we have to figure out what the gradient descent equation is in the case of logistic regression. To do this, we need a bit of calculus work using the chain rule. Recall that our goal is to compute since we want to calculate the slope of the cross entropy function with respect to the weights vector.",1,0,0,0,1,0,0,0
Word2vec from Scratch,"In order to create word embeddings, we need some sort of data. Here is a text on machine learning from Wikipedia. I’ve removed some parentheses and citation brackets to make things slightly easier. Since we can’t feed raw string texts into our model, we will need to preprocess this text. The first step, as is the approach taken in many NLP tasks, is to tokenize the text, i.e. splitting the text up into smaller units like words, getting rid of punctuations, and so on. Here is a function that does this trick using regular expressions. Let’s create tokens using the Wikipedia excerpt shown above. The returned object will be a list containing all the tokens in . Another useful operation is to create a map between tokens and indices, and vice versa. In a sense, we are creating a lookup table that allows us to easily convert from words to indices, and indices to words. This will be particularly useful later on when we perform operations such as one-hot encoding. Let’s check if the word-to-index and index-to-word maps have successfully been created.",1,1,0,0,0,0,0,0
"Newton-Raphson, Secant, and More","Recall that the Newton-Raphson update rule was written as The only modification we need to make to this update rule is to replace  with an approximation using the backward divided difference formula. Here, we make a slight modification to (2), specifically by using values from previous iterations. If we plug (8) back into (4), with some algebraic simplifications, we land on (7), the update rule for the secant method. This is left as an exercise for the reader. Now let’s take a look at how we might be able to implement this numerical method in code. Presented below is the  method, which follows the same general structure as the  function we looked at earlier. The only part that differs is the part where we use backward divided difference, which requires that we look up not only the immediately previous root approximation, but the value prior to that as well. In other words, we need both  and  to calculate  via an iterative update. And here is an obligatory sanity check using our previous example. 2.7 is a familiar value, and indeed it is what was returned by the Newton-Raphson method as well.",0,0,0,0,0,1,0,0
My First GAN,"Then, the connection between the generator and the discriminator is effectively established by the statement . All this is saying is that GAN’s output is the evaluation of the generator’s fake image by the discriminator. If the generator does well, it will fool the discriminator and thus output 1; 0 vice versa. Let’s take a look at the code implementation of this logic.  Now it’s time to train our model. Let’s first load our dataset. For this, we will be using the  images. The dataset contains low resolutions images, so our output is also going to be very rough, but it is a good starting point nonetheless. One hacky thing we do is concatenating the training and testing data. This is because for a GAN, we don’t need to differentiate the two: on the contrary, the more data we have for training, the better. One might suggest that testing data is necessary for the discriminator, which is a valid point, but the end goal here is to build a high performing generator, not the discriminator, so we will gloss over that point for now. For this tutorial, we will be using images of ships, which are labeled as 8.",0,1,0,0,0,0,0,0
"Beta, Bayes, and Multi-armed Bandits","It is of course in this learning process that the greedy algorithm or Bayesian analysis with Thompson sampling comes into play. The amount that we have theoretically lost—or, in other words, the extent to which we are far away from the maximum amount we could have earned—is denoted as regret. Thus, to maximize reward is to minimize regret, and vice versa. Now let’s simulate a hundred pulls on the lever using Bayesian analysis using the Beta-Binomial model and Thompson sampling. Nothing much fancy here, all we’re doing is Thompson sampling from the Beta distribution via , then obtaining the index of the bandit with the highest parameter, then pulling the machine that corresponds to that index. We will also keep cumulative track of our results to reproduce the regret diagram shown above. And we’re done with the hundred round of simulations! Hopefully our simulator gambler has made some good choices by following Bayesian update principles, with the Beta-Binomial model and Thompson sampling under their belt. Let’s take a look at the posterior distribution for each bandit. We can easily plot them using , as shown below. Notice that I have also included the uniform prior for reference purposes.",0,0,1,0,0,0,1,0
A sneak peek at Bayesian Inference,"But what if the man were to take the same test again? Intuition tells us that the more test he takes, the more confident we can be on whether the man is or is not affected by the disease. For instance, if the man repeats the exam once and receives a positive report, the conditional probability that he is sick given two consecutive positive test results should be higher than the 24 percent we calculated above. We can see this in practice by reapplying Bayes’ theorem with updated information, as shown below: We see that the value of the conditional probability has indeed increased, lending credence to the idea that the man is sick. Like this, Like this, Bayes’ theorem is a powerful tool that can be used to calculate conditional probabilities and to update them continuously through repeated trials. From a Bayesian perspective, we begin with some expectation, or prior probability, that an event will occur. We then update this prior probability by computing conditional probabilities with new information obtained for each trial, the result of which yields a posterior probability. This posterior probability can then be used as a new prior probability for subsequent analysis.",0,0,1,0,0,0,1,0
Likelihood and Probability,"To get started, recall the that This is the good old definition of probability as defined for a continuous random varriable , given some probability density function  with parameter . Graphically speaking, we can consider probability as the area or volume under the probability density function, which may be a curve, plane, or a hyperplane depending on the dimensionality of our context. Unlike probability, likelihood is best understood as a point estimate on the PDF. Imagine having two disparate distributions with distinct parameters. Likelihood is an estimate we can use to see which of these two distributions better explain the data we have in our hands. Intuitively, the closer the mean of the distribution is to the observed data point, the more likely the parameters for the distribution would be. We can see this in action with a simple line of code. This code block creates two distributions of different parameters,  and . Then, we assume that a sample of value 1 is observed. Then, we can compare the likelihood of the two parameters given this data by comparing the probability density of the data for each of the two distributions. In this case,  seems more likely, i.e.",0,0,1,0,0,0,0,0
Neural Style Transfer,"Fast NMT is one of the more advanced, recent algorithms that have been studied after the original NMT algorithm, which we’ve implemente above, was introduced. One of the many benefits of fast neural style transfer is that, instead of framing NMT as an optimization problem, FNMT makes it a modeling problem. In this instance, TransformerNet is a pretrained model that can transform images into their stylized equivalents. The code below was borrowed from the PyTorch repository. I decided to try out FNMT on a number of different pictures of myself, just to see how different results would be for each. Here, we loop through the directory and obtain the file path to each content photo. And here are the results!  Among the 20 photos that have been stylized, I think some definitely look better than others. In particular, I think the third row looks kind of scary, as it made every photo have red hues all over my face. However, there are definitely ones that look good as well. Overall, FNMT using pretrained models definitely yielded better results than our implementation.",0,1,0,0,0,0,0,1
Gaussian Mixture Models,"From the perspective of maximum likelihood estimation, the goal then would be to maximize this likelihood to find the optimal parameters for  and . Before we attempt MLE with the likelihood function, let’s first try to calculate the log likelihood, as this often makes MLE much easier by decoupling products as summations. Let . And now we see a problem: the log is not applied to the inside of the function due to the summation. This is bad news since deriving this expression by  will become very tedious. The result is for sure not going to look pretty, let’s try deriving the log likelihood by  and set it equal to zero. At least one good news is that, for now, we can safely ignore the outer summation given the linearity of derivatives. We ultimately end up with the expression below: This is not pretty at all, and it seems extremely difficult, if not impossible, to solve analytically for . This is why we can’t approach MLE the way we usually do, by directly calculating derivatives and setting them equal to zero. This is where Expectation Maximization, or EM algorithm comes in.",0,0,1,0,1,0,0,0
Riemann Zeta and Prime Numbers,"The first function we will build is one that randomly samples  natural numbers from 1 to , and checks if all number pairs within this sample is coprime. For this, we use , which reduces the result of applying  to two pairs of numbers in the randomly sampled number list.  is a quite common operation in functional programming, and we saw an example of this operation in the context of Spark in a previous post as well. Let’s see this function in action. I’ve added a  flag for convenience of testing and demonstration. Let’s toggle this option on for now and see what we get. The GCD of 6, 7, and 1 are 1, so the returned result is  as we expect. We also notice that three numbers were returned since . Next, we define a testing function that will simulate multiple runs of the  test for us. Because this is a Monte Carlo simulation, to have confidence in our estimate, we need to iterate the sampling process multiple times until we have sufficient amount of data. The  parameter determines the number of simulations we will perform.",0,0,0,0,0,1,0,0
A Step Up with  Variational Autoencoders,"Due to its bow-like shape, the expected value of a convex function evaluated across a given interval will always be greater or equal to the function evaluated at the expected value of the random variable.  How is Jensen’s inequality related to the non-negativity of KL divergence? Let’s return back to the definition of KL divergence. For simplicity and to reduce notational burden, we briefly depart from conditional probabilities  and return back to generic distributions  and . Notice that the definition of KL divergence itself is an expected value expression. Also, note that  is a convex function— itself is concave, but the negative sign flips the concavity the other way. With these observations in mind, we can apply Jensen’s inequality to derive the following: Therefore, we have shown that KL divergence is always greater or equal to zero, which was our end goal. There is another version of a proof that I found a lot more intuitive and easier to follow than the previous approach. This derivation was borrowed from this post. We start from the simple observation that a logarithmic function is always smaller than  a linear one.",0,1,0,0,0,0,0,0
Moments in Statistics,"As you can see, the common theme is that the moment contains information about the defining features of a distribution, which is why it is such a convenient way to present information about a distribution. As the name suggests, MGF is a function that generates the moments of a distribution. More specifically, we can calculate the th moment of a distribution simply by taking the th derivative of a moment generating function, then plugging in 0 for parameter . We will see what  is in a moment when we look at the default formula for MGF. This sounds good and all, but why do we want an MGF in the first place, one might ask. Well, given that moments convey defining properties of a distribution, a moment generating function is basically an all-in-one package that contains every bit of information about the distribution in question. Enough of the abstract, let’s get more specific by taking a look at the mathematical formula for an MGF. If  is a continuous random variable, we would take an integral instead. Now, you might be wondering how taking the th derivative of  gives us the th moment of the distribution for the random variable .",0,0,1,0,0,0,1,0
Introduction to tf-idf,"This is because the way we built ordinal indexing in corpus is probably different from how scikit-learn implements it internally. This point notwithstanding, it’s clear that the values of each vectors are identical, disregarding the fact that the result produced by our algorithm has more decimal points due to floating point arithmetic. This was a short introductory post on tf-idf vectors. When I first heard about tf-idf vectors from a friend studying computational linguistics, I was intimidated. However, now that I have a project I want to complete, namely an auto-tagging and classification NLP model, I’ve mustered more courage and motivation to continue my study the basics of NLP. I hope you’ve enjoyed reading this post. Catch you up in the next one! (Yes, this is a trite ending comment I use in almost every post, so the idf scores for the words in these two sentences are going to be very low.).",1,0,0,0,0,0,0,0
Complex Fibonacci,"The code above is the standard fibonacci function as we know it, implemented with simple bare bone recursion. While the code works perfectly fine, there is a fatal problem with this code: it recalculates so many values. Namely, in calling , the program goes all the way up to the th fibonacci number. Then, in the next call of , the program recalculates the same values calculated earlier, up until the th fibonacci number, just one short of the previous one. The classic way to deal with this problem is to use a technique known as memoization. The idea is simple: we keep some sort of memo or cache of values that have already been calculated and store them in some data structure that we don’t have to recalculate values that have already been computed prior. Here is a simple implementation. To see how effective memoization is compared to vanilla recursion, let’s use the  module. Comparing this to the  test on  with memoization, the benefits of caching becomes immediately clear: 3 second and 150 nanoseconds are different by orders of magnitude, and we only asked the functions to calculate the 35th fibonacci number.",0,0,0,0,0,1,0,0
Scikit-learn Pipelines with Titanic,"In today’s post, we will explore ways to build machine learning pipelines with Scikit-learn. A pipeline might sound like a big word, but it’s just a way of chaining different operations together in a convenient object, almost like a wrapper. This abstracts out a lot of individual operations that may otherwise appear fragmented across the script. I also personally think that Scikit-learn’s ML pipeline is very well-designed. So here is a brief introduction to ML pipelines is Scikit-learn. For the purposes of this tutorial, we will be using the classic Titanic dataset, otherwise known as the course material for Kaggle 101. I’m still trying to get my feet into Kaggle, so it is my hope that this tutorial will also help those trying to break into data science competitions. First, let’s import the modules and datasets needed for this tutorial. Scikit-learn is the go-to library for machine learning in Python. It contains not only data loading utilities, but also imputers, encoders, pipelines, transformers, and search tools we will need to find the optimum model for the task. Let’s load the dataset using . Let’s observe the data by calling .",0,0,0,0,1,0,0,0
MLE and KL Divergence,"Then, we would obtain a set of indecent and identically distributed (i.i.d) samples as shown below: Then, LLN states that A more precise statement of the law uses Chebyshev’s inequality: For the curious, here is the general formulation of Chebyshev’s inequality outside the context of LLN: For the purpose of this post, it is not necessary to go into how Chebyshev’s inequality is derived or what it means. However, it isn’t difficult to see how one might reformulate (8) to derive (7) to prove the Law of Large Numbers. All that the inequality is saying is that no more than a certain fraction of samples can fall outside more than a certain distance away from the mean of the distribution. With this understanding in mind, let’s return to the original problem and wrap up the proof. Let’s apply the Law of Large Numbers to modify the expected value expression sitting in (5): Voila! We have shown that minimizing the KL divergence amounts to finding the maximum likelihood estimate of . This was not the shortest of journeys, but it is interesting to see how the two concepts are related.",0,0,1,0,0,0,0,0
PyTorch Tensor Basics,"Not only do the two functions look similar, they also practically do the same thing. Upon more observation, however, I realized that there were some differences, the most notable of which was the .  seemed to be unable to infer the data type from the input given. On the other hand,  was sable to infer the data type from the given input, which was a list of integers. Sure enough,  is generally non-configurable, especially when it comes to data types. can accept  as a valid argument. The conclusion of this analysis is clear: use  instead of . Indeed, this SO post also confirms the fact that  should generally be used, as  is more of a super class from which other classes inherit. As it is an abstract super class, using it directly does not seem to make much sense. In PyTorch, there are two ways of checking the dimension of a tensor:  and . Note that the former is a function call, whereas the later is a property. Despite this difference, they essentially achieve the same functionality. To access one of the  elements, we need appropriate indexing.",0,1,0,0,0,0,0,1
First Neural Network with Keras,"Let’s quickly check if the necessary adjustments were made by looking up the dimensions of  and , respectively. Looks like the data has been reshaped successfully. Now, it’s finally time to get into the nuts and bolts of a neural network. The simplest neural network is the  model, which means that every neuron in one layer is connected to all other neurons in the previous layer. Building a simple neural network is extremely easy in a high level API like Keras. The model below has 784 input nodes. The input layer is then connected to a hidden layer with 512 neurons, which is then connected to a second hidden layer with also 512 neurons. Note that the hidden layer uses the  function as its activation function. The dropout layers ensure that our model does not overfit to the data. The last output layer has 10 neurons, each corresponding to digits from 0 to 9. The activation fuction of this last layer is the softmax function, which allows us to interpret the final results as a categorical distribution. Let’s double check that the layers have been formed correctly as per our intended design.",0,1,0,0,1,0,0,0
Maximum A Posteriori Estimation,"Concretely, The objective of Bayesian inference is to estimate the posterior distribution, whose probability distribution is often intractable, by computing the product of likelihood and the prior. This process could be repeated multiple times as more data flows in, which is how posterior update can be performed. We saw this mechanism in action with the example of a coin flip, given a binomial likelihood function and a beta prior, which are conjugate distribution pairs. Then what does maximizing the posterior mean in the context of MAP? With some thinking, we can convince ourselves that maximizing the posterior distribution amounts to finding the optimal parameters of a distribution that best describe the given data set. This can be seen by simply interpreting the posterior from a conditional probability point of view: the posterior denotes the probability of the value of the model parameter is  given data . Put differently, the value of  that maximizes the posterior is the optimal parameter value that best explains the sample observations. This is why at its heart, MAP is not so much different from MLE: although MLE is frequentist while MAP is Bayesian, the underlying objective of the two methods are fundamentally identical.",0,0,1,0,0,0,0,0
Convolutional Neural Network with Keras,"Note that this Jupyter Notebook was written on Google Colaboratory. The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x. We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the  magic: more info. The  function loads the CIFAR10 data set from  module, then applies some basic preprocessing to make it more usable for deep learning purposes. The CIFAR10 data set contains 50000 training images, each labeled with 1 of 10 classes, ranging from cats, horses, and trucks to airplanes. This is a classic classification task. The preprocessing occurs on two levels: first, the images are normalized so that its pixels takes values between 0 and 1. The training labels are transformed from an integer class label to a one-hot encoded vector. Let’s load the data to proceed with our analysis. Let’s see what the dimensions of the data are. Because the CIFAR10 data set include color images for its training set, we would expect three channels, each corresponding to red, green, and blue (hence RGB). As expected, it appears that the training data is a tensor of four dimensions, while the target labels are 10 dimensional vectors.",0,1,0,0,0,0,0,0
A sneak peek at Bayesian Inference,"In this light, Bayesian statistics offers a new way to compute new information and update our beliefs about an event in probabilistic terms. Bayesian inference is nothing more than an extension of Bayes’ theorem. The biggest difference between the two is that Bayesian inference mainly deals with probability distributions instead of point probabilities. The case of the potential patient we analyzed above was a simple yet illuminating example, but it was limiting in that we assumed all parameters to be simple constants, such as  for test accuracy and  for false positive frequency. In reality, most statistical estimates exist as probability distributions since there are limitations to our ability to measure and survey data from the population. For example, a simple random sampling of the population might reveal that 15 percent of the sample population is affected with the flu, but this would most likely produce a normal distribution with mean centered around 0.15 instead of a point probability.",0,0,1,0,0,0,1,0
k-Nearest Neighbors Algorithm from Scratch,"The iris data set from the UCI machine learning repository is perhaps one of the best known data sets in the field of machine learning. Created by R. A. Fisher, the data set contains 3 classes of 50 instances each, totaling to 150 independent observations of iris plants, specifically Iris Setosa, Iris Versicolour, and Iris Virginica. The feature columns include sepal length, sepal width, petal length, and petal width. Let’s begin by first loading the data set from the  library. One preliminary step we might want to take is shuffling the data set and dividing it into a training set and a testing set. As the name implies, a testing set is a set of data we use to test the accuracy of our classification algorithm. A training set, on the other hand, is a data set the KNN model is going to use to make predictions, i.e. it is the data set from which the algorithm will try to find close neighbors.",1,0,0,0,1,0,0,0
(Attempt at) Knowledge Distillation,"To cut to the chase, the loss function in knowledge distillation is a combined loss of weighted averages: the cross entropy between the student model outputs and labels, as well as the Kullback–Leibler divergence between the temperature-adjusted student and teacher model logits. More concisely, where  denotes the softmax function,  denotes softmax prime with adjusted temperature,  denotes the raw logits of the teacher model, and  is a weighted average parameter. While this might seem like a lot, in reality it is just the weighted average of the normal cross entropy loss that we would use in typical training, combined with the KL divergence loss between the temperature-adjusted softmax outputs of the student and teacher model. Intuitively, we can understand the KL divergence as an additional form of regularization that forces the student network to not only train on hard ground-truth labels, but also the dark knowledge coming from the outputs of the teacher model. Now that we have an overall idea of how knowledge distillation works, let’s start implementing knowledge distillation training. Before we move on, full disclaimer that this experiment did not go as I had expected.",0,1,0,0,0,0,0,1
A Simple Autocomplete Model,"You might remember back in the old days when autocomplete was just terrible. The suggestions provided by autocomplete would be useless if not downright stupid—I remember that one day when I intended to type “Gimme a sec,” only to see my message get edited into “Gimme a sex” by the divine touches of autocomplete. On the same day, the feature was turned off on my phone for the betterment of the world. Now, times have changed. Recently, I decided to give autocorrect a chance on my iPhone. Surprisingly, I find myself liking autocomplete more than hating it, especially now that the weather is getting colder by each day: when my frost-numbed finger tips touch on the wrong places of the phone screen to produce words that aren’t really words, iPhone’s autocomplete somehow magically reads my mind to rearrange all that inscrutable alphabet soup into words that make actual, coherent sense. Sometimes, it’s so good at correcting my typos that I intentionnally make careless mistakes on the keyboard just to see how far it can go. One of the obvious reasons behind such drastic improvements in autocomplete functionality is the development of deep neural networks.",0,1,0,0,0,0,0,0
"PyTorch, From Data to Modeling","As you can see above, we basically call on the layers we’ve declared in the initialization function via  and pass in any parameters we want. Since this is a very simple CNN, you will see that there is nothing exciting going on; we are simply getting the output of the previous layer and passing that as input into the next. After going through some convolutions and fully connected layers, we can return the result. There are one caveats worth mentioning here, which is the use of . There is a  that I could have easily used, and indeed there is an entire discussion on the PyTorch forum on the difference between the two. The bottom line is that they are pretty similar for our purposes. The most salient difference between the two is that the functional approach cannot be used when declaring a sequential model. However, since we are defining a custom forward function, this limitation does not apply. Personally, I prefer the functional because it means that there is one less layer to declare in the initialization function. However, it’s probably better to err on the side of the  if you’re not totally sure.",0,1,0,0,0,0,0,1
"Beta, Bayes, and Multi-armed Bandits","However, after some trial and error, the gambler starts to figure out which bandit is the best and starts pulling more of those, ultimately ending up at the point that is quite close to the maximum reward, though not quite due to the earlier opportunities that may have been lost due to exploration and sampling. In this post, we took a look at the multi-armed bandit problem and how it relates to Bayesian analysis with the Beta and Binomial distributions. I personally enjoyed writing this post, not only because I hadn’t written in a long time, but also because it helped me revisit some statistics, which is something that I desperately needed to do—I’m spending way too much time dealing with Django and Selenium these days. Time and again, I realize that there is an element of intuitiveness to Bayesian statistics that, though not obvious at first, starts to make more sense as I explore more into that realm. Of course, frequentist statistics offers a quick and easy way of interpreting certain metrics, but the notion of expectation is something that I had thought was complicated and unnecessary at first, but have gradually come to understand, embrace, and even support.",0,0,1,0,0,0,1,0
Demystifying Entropy (And More),"In a typical scenario, we might have a true probability distribution  that we are trying to model, and our deep learning algorithm might produce some approximate probability distribution . We might evaluate the effectiveness of our model by calculating the distance between  and . Seen in this light, cross entropy can be interpreted as a target cost function to be minimized. Here is the equation that explains the relationship between entropy, cross entropy, and KL divergence. where  denotes cross entropy; ,entropy, and the last term, KL divergence. Now, let’s try to understand what each of them means. KL divergence has many interpretations. One possible definition of KL divergence is that it measures the average number of extra information content required to represent a message with distribution  instead of . In Machine Learning: A Probabilistic Perspective, Kevin P. Murphy describes KL divergence as follows: … the KL divergence is the average number of extra bits needed to encode the data, due to the fact that we used distribution  to encode the data instead of the true distribution . Put differently, KL divergence is the amount of information that is lost when  is used to approximate .",0,0,1,0,0,0,0,0
Understanding the  Leibniz Rule,"Before I begin, I must say that this video by Brian Storey at Olin College is the most intuitive explanation of the Leibniz rule I have seen so far. Granted, my greedy search over the internet space was by no means exhaustive, so I’ve probably missed some other hidden gems  here and there. Also, the video is intended as a visual explanation for beginners rather than a robust analytical proof of the Leibniz rule. This point notwithstanding, I highly recommend that you check out the video. This post is going to provide a short, condensed summary of the proof presented in the video, minus the fancy visualization that pen and paper can afford. The Leibniz rule, sometimes referred to as Feynman’s rule or differentiation-under-the-integral-sign-rule, is an interesting, highly useful way of computing complicated integrals. A simple version of the Leibniz rule might be stated as follows: As you can see, what this rule essentially tells us is that integrals and derivatives are interchangeable under mild conditions. We’ve used this rule many times in a previous post on Fisher’s information matrix when computing expected values that involved derivatives.",0,0,0,0,0,1,0,0
Scikit-learn Pipelines with Titanic,"So hacky methods must not be used in isolation; at the very least, they need to be complemented with some form of human input. Let’s try to use a simple pipeline to deal with missing values in some categorical variables. This is going to be our first sneak peak at how pipelines are declared and used. Here, we have declared a three-step pipeline: an imputer, one-hot encoder, and principal component analysis. How this works is fairly simple: the imputer looks for missing values and fills them according to the strategy specified. There are many strategies to choose from, such as most constant or most frequent. Then, we one-hot encode the categorical variables since most machine learning models cannot accept non-numerical values as input. The last PCA step might seem extraneous. However, as discussed in this Stack Overflow thread, the judicious combination of one-hot plus PCA can seldom be beat by other encoding schemes. PCA finds the linear overlap, so will naturally tend to group similar features into the same feature. I don’t have enough experience to attest to the veracity of this claim, but mathematically or statistically speaking, this proposition seems valid.",0,0,0,0,1,0,0,0
A Simple Autocomplete Model,"Let’s run this for 50 epochs, just to give our model enough time to explore the loss function and settle on a good minimum. As I was training this model on Google Colab, I noticed that training even this simple model took a lot of time. Therefore, I decided that it is a good idea to probably save the trained model—in the worst case scenario that poor network connection suddenly caused the Jupyter kernel to die, saving a saved model file would be of huge help since I can continue training again from there. Saving the model on Google Colab requires us to import a simple module, . The process is very simple. To load the model, we can simply call the command below. Let’s take a look at the loss curve of the model. We can simply look at the value of the loss function as printed throughout the training scheme, but why not visualize it if we can?  As expected, the loss decreases throughout each epoch. The reason I was not paticularly worried about overfitting was that we had so much data to work with, especially in comparison with the relatively constrained memory capacity of our one-layered model.",0,1,0,0,0,0,0,0
Complex Fibonacci,"Intuitively, Binet’s formula has to do with the well-known fact that the ratio between two consecutive fibonacci numbers approaches the Golden ratio as  goes to infinity. In this light, we might understand the fibonacci sequence as a geometric sequence with a constant ratio. The goal, then, is to show that the ratio is in fact the Golden ratio. Then, we have the following recurrence relation between consecutive terms. Dividing both sides by , we get This is a simple quadratic equation that we can easily solve. With some precalculus algebra, we get And indeed we start to see the Golden ratio and its negative inverse as solutions to the quadratic. This means that we can express the fibonacci sequence as a linear combinations of these two solutions: Much like solving any difference equations, we have two initial conditions, namely that , . We also trivially know that , but only two conditions suffice to ascertain the value of the coefficients,  and . With some algebra, one can verify that Putting these together, we finally get Binet’s formula: An interesting point to note about Binet’s formula is that  doesn’t necessarily have to be a non-negative integer as we had previously assumed.",0,0,0,0,0,1,0,0
Wonders of Monte Carlo,"This versatility is why MC method is such a powerful tool in the statistician’s arsenal. In today’s post, we will attempt to solve various bite-sized tasks using MC methods. These tasks will be of varying difficulty, but taken together, they will collectively demonstrate the useful applications of MC methods. Let’s get started with the first up on the list: estimating . We all know from basic geometry that the value of  approximates to . There are obviously various ways to derive this value. Archimedes famously used hexagons to estimate that the value of  lies between . With later advances in math, mathematicians began to approach this problem from the angle of infinite series or products, the result of which were the Leibniz formula, Wallis product, Euler product, and the likes. And of course, modern computing now allows us to borrow the prowess of machinery to calculate this quantity with extreme accuracy. While the maths behind these derivations are fascinating, our approach will not take these routes; instead, we will use a crude Monte Carlo method. First, we draw a two-by-two square, inside of which we inscribe a circle of radius 1.",0,0,1,0,0,0,0,0
Dissecting LSTMs,"In that case, calculating the Hadamard product will also result in a value of an entry very close to 0. Given the interpretation that , also known as the cell state, is an artificial way of simulating long-term memory, we can see how having zeros is similar to forgetfulness: a zero entry effectively means that the network deemed a particular piece of information as obsolete and decided to forget it in favor of accepting new information. In short, the sigmoid activation and the Hadamard product form the basis of LSTM’s forget gate. By now, it should be apparent why we use sigmoid activations: instead of causing divergence with something like ReLU, we want to deliberately saturate and cause the network to produce some “vanishing” values. But if our LSTM network only keeps forgetting, obviously this is going to be problematic. Instead, we also want to update the cell state using the new input values. Let’s take a look at the cell state equation again: Previously when discussing the forget gate, we focused only on the first term.",0,1,0,0,0,0,0,0
Principal Component Analysis,"We also use the fact that the covariance matrix is symmetric. If we left multiply (18) by , But since , the first two terms go to zero. Also, the last term reduces to  since . This necessarily means that . If we plug this result back into (18), we end up with the definition of the eigenvector again, but this time for . Essentially, we iterate this process to find a specified number of principal components, which amounts to finding   number of eigenvectors of the sample covariance matrix. A while back, we discussed both eigendecomposition as well as singular value decomposition, both of which are useful ways of decomposing matrices into discrete factors. In this section, we will see how PCA is essentially a way of performing and applying these decomposition techniques under the hood. Recall that eigendecomposition is a method of decomposing matrices as follows: where  is a diagonal matrix of eigenvalues and  is a matrix of eigenvectors. PCA is closely related to eigendecomposition, and this should come as no surprise. Essentially, by finding the eigenvalues and eigenvectors of , we are performing an eigendecomposition on the covariance matrix: Notice that  is a matrix of principal components.",0,0,1,1,0,0,0,0
A PyTorch Primer,"Nonetheless, in this section, we will take a look at both ways of building models. Let’s start with the function-based method. The function-based method reminds me a lot of Keras’s sequential method. Let’s remind ourselves of Kera’s basic sequential model API: Now let’s compare this method with PyTorch’s way of declaring sequential models: This model declaration is in fact exactly identical to the simple model we have declared above. You can easily see how similar this code snippet is to the Keras example. The only difference is that the activation function is declared independently of the layer itself in PyTorch, whereas Keras combines them into one via  argument. Of course, you don’t have to specify this argument, and we can import the ReLU function from TensorFlow to make it explicit like the PyTorch example. The point, however, is that the sequential model API for both libraries are pretty similar. Another way to build models is by subclassing . The  submodule in PyTorch is the one that deals with neural networks; hence the . This subclassing might look as follows: This model is no different from the  we defined earlier.",0,1,0,0,0,0,0,1
A Brief Introduction to Recurrent Neural Networks,"we should not shuffle the datasets because making weather predictions requires us to understand temporal patterns. In predicting the weather 24 hours from today, data points pertaining to the last two days will be obviously much more important that those recorded a month ago. In such instances of time series analysis, recurrent neural networks perform better than other models we have looked at thus far. To better understand how RNNs work, let’s try to build a very simple recurrent neural network from scratch with . We will only implement forward propagation for the sake of simplicity,  but with some matrix calculus, one can imagine how deriving the formula for back propagation will be possible. Let’s cut to the chase: RNNs emulate memory by using the output from the previous sequence as an input to the next. Perhaps writing this down in matrix notation might give you a better idea of what the statement above means. Here is one way we might implement a very simple recurrent neural network. If the word “recursion” pops up into your mind, then you are on the right track.",0,1,0,0,0,0,0,0
Better seq2seq,"In the previous post, we took a look at how to implement a basic sequence-to-sequence model in PyTorch. Today, we will be implementing a small improvement to the previous model. These improvements were suggested in Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, by Cho, et. al. To cut to the chase, the image below, taken from Ben Trevett’s tutorial, encapsulates the enhancement we will be implementing today.  In the previous model, the hidden state of the model was a bottleneck: all the information from the encoder was supposed to be compressed into the hidden state, and even that encoder hidden state would have undergone changes as the decoder was unrolled with subsequent target sequences. To reduce this bottleneck and lessen the compression burden on the encoder hidden state, the improved architecture will allow the decoder to gain access to the encoder hidden state at each time step. Moreover, the final classifier output layer in decoder will have access to the original embedding of the target language token as well as the last hidden state of the encoder, represented as  in the diagram above.",0,1,0,0,0,0,0,1
VGG PyTorch Implementation,"In today’s post, we will be taking a quick look at the VGG model and how to implement one using PyTorch. This is going to be a short post since the VGG architecture itself isn’t too complicated: it’s just a heavily stacked CNN. Nonetheless, I thought it would be an interesting challenge. Full disclosure that I wrote the code after having gone through Aladdin Persson’s wonderful tutorial video. He also has a host of other PyTorch-related vidoes that I found really helpful and informative. Having said that, let’s jump right in. We first import the necessary  modules. Let’s first take a look at what the VGG architecture looks like. Shown below is a table from the VGG paper.  We see that there are a number of different configurations. These configurations typically go by the name of VGG 11, VGG 13, VGG 16, and VGG 19, where the suffix numbers come from the number of layers. Each value of the dictionary below encodes the architecture information for each model. The integer elements represents the out channel of each layer.  represents a max pool layer. You will quickly see that the dictionary is just a simple representation of the tabular information above.",0,1,0,0,0,0,0,1
Recommendation Algorithm with SVD,"This function tells us that our movie application should recommend to our user Movies 3 and 4, in that order. This result is not surprising given the fact that we have already observed the closeness between Movies 2 and 3—if a user likes Movie 2, we should definitely recommend Movie 3 to them. Our algorithm also tells us that the distance between Movie 2 and 4 is also pretty close, although not as close as the distance between Movies 2 and 3. What is happening behind the scene here? Our function simply calculates the distance between the vector representation of each movies as a dot product. If we were to print the local variable  array defined within the  function, for instance, we would see the following result. This tells us how close Movies 0, 1, 3, and 4 are with Movie 2. The larger the dot product, the closer the movie; hence, the more compelling that recommendation. The  function then sorts the  array and outputs the first  movies as a recommendation.",1,0,0,1,0,0,0,0
Introduction to tf-idf,"Let’s test in on our dummy dataset to see if we get the count of each tokenized word. This is precisely what we need to calculate idf. Recall the formula for calculating idf As noted earlier, the intuition behind idf was that important keywords probably appear only in specific relevant documents, whereas generic words of comparatively lesser importance appear throughout all documents. We transcribe (4) into code as follows: Now, we have the idf vectors for the nine terms in the dummy dataset. At this point, all there is left to do is to multiply the term frequencies with their corresponding idf scores. This is extremely easy, since we are essentially performing a dot product of the tf and idf vectors for each document. As a final step, we normalize the result to ensure that longer documents do not overshadow shorter ones. Normalizing is pretty simple, so we’ll assume that we have a function  that does the job for now. Before we test the code, we obviously need to implement . This can simply done by obtaining the sum of the L2 norm of each vector, then dividing each element by that constant.",1,0,0,0,0,0,0,0
"Newton-Raphson, Secant, and More","Recently, I ran into an interesting video on YouTube on numerical methods (at this pont, I can’t help but wonder if YouTube can read my mind, but now I digress). It was a channel called numericalmethodsguy, run by a professor of mechanical engineering at the University of Florida. While the videos themselves were recorded a while back in 2009 at just 240p, I found the contents of the video to be very intriguing and easily digestable. His videos did not seem to assume much mathematical knowledge beyond basic high school calculus. After watching a few of his videos, I decided to implement some numerical methods algorithms in Python. Specifically, this post will deal with mainly two methods of solving non-linear equations: the Newton-Raphson method and the secant method. Let’s dive right into it. Before we move on, it’s first necessary to come up with a way of representing equations in Python. For the sake of simplicity, let’s first just consider polynomials. The most obvious, simplest way of representing polynomials in Python is to simply use functions.",0,0,0,0,0,1,0,0
Wonders of Monte Carlo,"I have been putting off with blog postsings lately, largely because I was preoccupied with learning new languages I decided to pick up out of whim. Although I’m still learning the basics of these languages, namely HTML, CSS, and Javascript, I’m enjoying the process. I still have no idea where this spontaneous journey will take me, but hopefully I can make use of it in one way or another. The topic for today’s post is Monte Carlo methods, something that I have been very interested in for a long time, admittedly because of its eye-catching name. Contrary to my original expectation, Monte Carlo is not named after an eponymous mathematician, but a gambling hot spot in Monaco. The logic behind this nomenclature is that the simulation of random outcomes, such as in the context of an unpredictable gambling game, is what Monte Carlo methods are best suited for. To present a more formal definition, Monte Carlo methods refer to a broad category of  algorithms that use repeated random sampling to make estimations of unknown parameters. Basically, MC methods work by cleverly sampling from a distribution to estimate a variable of interest.",0,0,1,0,0,0,0,0
My First GAN,"By coercing a true label on the GAN, we are effectively forcing the generator to produce more compelling images, and penalizing it when it fails to do so. Personally, I find this part to be the genius and beauty of training GANs. Now that we have an idea of what the function accomplishes, let’s use it to start training. The  seems to fluctuate a bit, which is not necessarily a good sign but also quite a common phenomenon in GAN training. GANs are notoriously difficult to train, since it requires balancing the performance of the generator and the discriminator in such a way that one does not overpower the other. This is referred to as a min-max game in game theory terms, and finding an equilibrium in such structures are known to be difficult. Let’s take a look at the results now that the iterations are over.  The created images are admittedly fuzzy, pixelated, and some even somewhat alien-looking. This point notwithstanding, I find it incredibly fascinating to see that at least some generated images actually resemble ships in the sea. Of particular interest to me are the red ships that appear in  and .",0,1,0,0,0,0,0,0
Recommendation Algorithm with SVD,"Alternatively, we can also see this by taking a look at the full, unreduced version of the matrix  or . For example, the code snippet below displays the full version of the factor . It is not difficult to see that the last few columns of  contain values so small that their contribution to data is going to be minimal at best. This is not the most mathematical way of presenting the concept of data—doing so would require us to take a look at other metrics such as covariance—but this basic analysis will suffice for our purposes for now. The takeaway is that dimensionality reduction is a meaningful way to extract important information from our data. Now that we have performed SVD on the ratings matrix, let’s move onto the last step: crafting a model for our recommendation algorithm. My personal pet theory is that using any word in conjunction with “algorithm” makes the concept sound more complex than it actually is. This is exactly what we are doing here, because in reality, our so-called algoithm for movie recommendations is going to be very simple. The intuition behind the recommendation system is distance calculation.",1,0,0,1,0,0,0,0
k-Nearest Neighbors Algorithm from Scratch,"One way to go about this is to use Euclidean distance, which is defined as follows: It is not difficult to build an implementation of in Python. We can easily achieve this using . Let’s test the functionality of the  function using some dummy dataset. This data set was borrowed from Jason Brownlee. Great! As expected, the distance between a point and itself is 0, and other calculated distances also seem reasonable. The next step is to write a function that returns the  nearest neighbors of a point given a data set and parameter . There are many ways to implement this, but an example is shown below. First, we enumerate thorugh the data set to calculate all the distances between the given test instance and the data points in the data set. Next, we sort the list. The returned result is a list that contains the indices of the  nearest neighbors the algorithm found in the data set. Note that we use the  function we wrote above. Let’s see if the code works by testing it on our toy data. The task is to find 5 data points that are closest to the first row of .",1,0,0,0,1,0,0,0
Recommendation Algorithm with SVD,"Now that we have a mathematical understanding of how singular value decomposition, let’s see how we can apply SVD to build a simple recommendation algorithm. This section will continue as follows. First, we examine SVD as a technique of data compression and dimensionality reduction. Next, we generate some toy data of movie reviews and apply SVD to see how we can build a simple function that gives movie recommendations to users given their movie ratings history. Let’s jump right in. Why is singular value decomposition so important? Sure, it should now be fairly clear that SVD is a decomposition technique that can be applied to any matrix, whether square or not, which in and of itself makes it a very powerful tool in the statistician’s arsenal. But the true beauty of singular value decomposition comes from the fact that we can perform data compression by extracting meaningful information from the given data. This process is otherwise known as dimensionality reduction, and it is one of the most common applications of singular value decomposition. Let’s see what this means with an example. Here is , a target matrix for singuluar value decomposition.",1,0,0,1,0,0,0,0
Gaussian Process Regression,"This can easily be shown by comparing the LDU decomposition of  and  respectively: Therefore, we can rewrite the LDU decomposition of A as A nice property of diagonal matrices is that we can easily identify its square, namely, where  is a matrix whose diagonal entries are each square root of the corresponding originals in . The tranpose is not necessary since  is a diagonal matrix, but we do so for convenience purposes later on in the derivation. Note the trivial case of the identity matrix, whose square root is equal to itself since all diagonal elements take the value of 1 (and ). Given this piece of information, what we can now do is to rewrite the factorization of  as where . This is the Cholesky decomposition of symmetric matrices—to be more exact, positive semi-definite matrices. The reason why the Cholesky decomposition can only be performed on positive semi-definite matrices becomes apparent when we think about the definition of positive semi-definiteness. Given any non-zero vector , The key takeaway is that, given some positive semi-definite matrix, we can easily factor it into what we might consider to be its square root in the form of .",1,0,0,0,1,0,0,0
Dissecting LSTMs,"Thus, it is no surprise that the following two equations are structured the way they are: First, we see the familiar forward pass, a familiar structure we have seen earlier. Borrowing the analogy we established in the previous post,  is a filter that decides which information to use and drop. The raw material that we pass into this filter is in fact the cell state, processed by a  activation function. This is also a familiar structure we saw earlier in the information update sequence of the network. Only this time, we use the cell state to generate output. This makes sense somewhat intuitively, since the cell state is essentially the memory of the network, and hence to generate output would require the use of this memory. Of course, this should not be construed so literally since what ultimately happens during backpropagation is entirely up to the network, and at that point we simply lay back and hope for the network to learn the best. This point notwithstanding, I find this admittedly coarse heuristic to be nonetheless useful in intuiting the clockwork behind LSTMs.",0,1,0,0,0,0,0,0
Recommendation Algorithm with SVD,"Of course, we could think of an alternate implementation of this algorithm that makes use of the  matrix instead of , but that would be a slightly different recommendation system that uses past user’s movie ratings as information to predict whether or not the particular individual would like a given movie. As we can see, SVD can be used in countless ways in the domain of recommendation algorithms, which goes to show how powerful it is as a tool for data analysis. In today’s post, we dealt primarily with singular value decomposition and its application in the context of recommendation systems. Although the system we built in this post is extremely simple, especially in comparison to the complex models that companies use in real-life situations, nonetheless our exploration of SVD is valuable in that we started from the bare basics to build our own model. What is even more fascinating is that many recommendation systems involve singular value decomposition in one way or another, meaning that our exploration is not detached from the context of reality. Hopefully this post has given you some intuition behind how recommendation systems work and what math is involved in those algorithms.",1,0,0,1,0,0,0,0
Bayesian Linear Regression,"Why do we bother to pull out the exponent? This is because the integral of a probability density function evaluates to 1, leaving us only with the exponential term outside the integral. To proceed further from here, let’s take some time to zoom in on  for a second. Substituting , we get We can now plug this term back into (15) as shown below. Although it may seem as if we made zero progress by unpacking , this process is in fact necessary to complete the square of the exponent according to the Gaussian form after making the substitutions By now, you should be comfortable with this operation of backtracking a quadratic and rearranging it to complete the square, as it is a standard operation we have used in multiple parts of this process.",0,0,0,1,0,0,0,0
Better seq2seq,"” The way we allow the decoder to use its own hidden state as well as the context for computation is that we concatenate the context with its input embeddings. Effectively, we could think of this as creating a new embedding vectors, where the first half comes from actual embeddings of English tokens and the later half comes from the context vector. Another implementation detail not mentioned earlier is the dimension of the last fully connected classifier layer. Since we now concatenate the embedding vector with the hidden state from the GRU, context vector from the encoding, as well as the original embedding vectors, the classifier’s input dimensions are much larger than they were in the previous decoder model. Now it’s time to implement the sequence-to-sequence model. Most of the enhancements were already baked into the decoder, and the fundamental logic through which predictions are generated remain unchanged. Thus, only minimal changes have to be made to the seq2seq model: namely, we need to handle the context vector and pass it to the decoder at every time step. And from here on, the details are exactly identical; the same  and  functions can be used in the previous post.",0,1,0,0,0,0,0,1
Naive Bayes Model From Scratch,"To achieve this objective, we can create a function that returns a dictionary, where the key represents the class and the values contain the entries of the data set. One way to implement this process is represented below in the  function. Let’s see if the function works properly by passing  as into its argument. Great! As expected,  is a dictionary whose keys represent the class and values contain entries corresponding to that class. Now that we have successfully separated out the data by class, its’ time to write a function that will find the mean and standard deviation of each class data. This process is legitimate only because we assumed the data to be normally distributed—hence the name “Gaussian naive Bayes.” Let’s quickly see this in code. The  function receives a data set as input and returns a nested list that contains the mean and standard deviation of each column of the data set. For example, if we pass the toy data set  into , the returned list will contain two lists: the first list element corresponding to the mean and standard deviation of , and the second list element, .",1,0,0,0,1,0,0,0
"0.5!: Gamma Function, Distribution, and More","From there, we were able to derive and develop an intuition for the Gamma distribution, which models the waiting time required until the occurrence of the th event in a Poisson process. This may all sound very abstract because of the theoretical nature of our discussion. So in the posts to follow, we will explore how these distributions can be applied in different contexts. Specifically, we will take a look at Bayesian statistics and inference to demonstrate how distributions can be employed to express prior or posterior probabilities. At the same time, we will also continue our exploration of the distribution world by diving deeper into other probability distributions, such as but not limited to exponential, chi-square, normal, and beta distributions, in no specific order. At the end of the journey, we will see how these distributions are all beautifully interrelated. Catch you up in the next one!.",0,0,1,0,0,0,1,0
Maximum A Posteriori Estimation,"In the example in the previous post on likelihoods, we showed that MLE for a normal distribution is equivalent to setting  as the sample mean; , sample variance. But this convenient case was specific only to the Gaussian distribution. More generally, maximum likelihood estimation can be expressed as: It is not difficult to see why trying to compute this quantity may not be as easy as it seems: because we are dealing with probabilities, which are by definition smaller than 1, their product will quickly diverge to 0, which might cause arithmetic underflow. Therefore, we typically use log likelihoods instead. Maximizing the log likelihood amounts to maximizing the likelihood function since log is a monotonically increasing function. Finding the maximum could be achieved multiple ways, such as through derivation or gradient descent. As the name suggests, maximum a posteriori is an optimization method that seeks to maximize the posterior distribution in a Bayesian context, which we dealt with in this post. Recall the Bayesian analysis commences from a number of components, namely the prior, likelihood, evidence, and posterior.",0,0,1,0,0,0,0,0
"Linear Regression, in Two Ways","Answering this question requires a bit more math beyond what we have covered here, but to provide a short preview, it turns out that our error function, defined as  is a positive definite matrix, which guarantees that the critical point we find by calculating the gradient gives us a minimum instead of a maximum. This statement might sometimes be phrased differently along the lines of convexity, but this topic is better tabled for a separate future post. The key point here is that setting the gradient to zero would tell us when the error is minimized. This is equivalent to Therefore, Now we are done! Just like in the previous section,  gives us the parameters for our line of best fit, which is the solution to the linear regression problem. In fact, the keen reader might have already noted that (7) is letter-by-letter identical to formula (2) we derived in the previous section using plain old linear algebra! One the one hand, it just seems surprising and fascinating to see how we end up in the same place despite having taken two disparate approaches to the linear regression problem.",0,0,0,1,0,0,0,0
(Attempt at) Knowledge Distillation,"As you will see below, somehow the distilled student model ended up outperforming the teacher, which is something that normally does not happen. In a later section, I will try to provide some hypotheses as to why this happened. Let’s import necessary modules for this tutorial. We define a transformation we will be applying to each image. We can then create a training set and a testing set. Note that I didn’t have to download anything because I already had all my files on my local machine. Next, we create data loaders with a batch size of 64. Modeling is very dull and uninteresting here; we simply define a relatively large teacher model and a smaller student model. As per the experiment outlined in the original paper, we define a multi-layer perceptron with two hidden layers, each with 1200 units. The teacher network also uses dropout for regularization. Next up is the student model. The student model is a smaller version of the teacher network. It has two hidden layers, but 800 units as opposed to 1200. It also does not employ any dropout.",0,1,0,0,0,0,0,1
