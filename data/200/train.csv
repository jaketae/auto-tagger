title,body,analysis,probability_distribution,machine_learning,from_scratch,tensorflow,statistics,deep_learning,linear_algebra
Maximum A Posteriori Estimation,"In the example in the previous post on likelihoods, we showed that MLE for a normal distribution is equivalent to setting  as the sample mean; , sample variance. But this convenient case was specific only to the Gaussian distribution. More generally, maximum likelihood estimation can be expressed as: It is not difficult to see why trying to compute this quantity may not be as easy as it seems: because we are dealing with probabilities, which are by definition smaller than 1, their product will quickly diverge to 0, which might cause arithmetic underflow. Therefore, we typically use log likelihoods instead. Maximizing the log likelihood amounts to maximizing the likelihood function since log is a monotonically increasing function. Finding the maximum could be achieved multiple ways, such as through derivation or gradient descent. As the name suggests, maximum a posteriori is an optimization method that seeks to maximize the posterior distribution in a Bayesian context, which we dealt with in this post. Recall the Bayesian analysis commences from a number of components, namely the prior, likelihood, evidence, and posterior.",0,0,0,0,0,1,0,0
InceptionNet in PyTorch,"In today‚Äôs post, we‚Äôll take a look at the Inception model, otherwise known as GoogLeNet. I‚Äôve actually written the code for this notebook in October üò± but was only able to upload it today due to other PyTorch projects I‚Äôve been working on these past few weeks (if you‚Äôre curious, you can check out my projects here and here). I decided to take a brief break and come back to this blog, so here goes another PyTorch model implementation blog post. Let‚Äôs jump right into it! First, we import PyTorch and other submodules we will need for this tutorial. Because Inception is a rather big model, we need to create sub blocks that will allow us to take a more modular approach to writing code. This way, we can easily reduce duplicate code and take a bottom-up approach to model design. The  module is a simple convolutional layer followed by batch normalization. We also apply a ReLU activation after the batchnorm. Next, we define the inception block. This is where all the fun stuff happens. The motivating idea behind InceptionNet is that we create multiple convolutional branches, each with different kernel (also referred to as filter) sizes.",0,0,0,0,0,0,1,0
A Simple Autocomplete Model,"Instead, we want to introduce some noise so that the model faces subtle obstructions, thereby making it get more ‚Äúcreative‚Äù with its output instead of getting trapped in an infinite loop of some likely sequence. Below is a sample implementation of adding noise to the output using log and exponential transformations to the output vector of our model. The transformation might be expressed as follows: where   denotes a transformation,  denotes a prediction as a vector,  denotes temperature as a measure of randomness, and  is a normalizing constant. Although this might appear complicated, all it‚Äôs doing is that it is adding some perturbation or disturbance to the output data so that it is possible for less likely characters to be chosen as the final prediction. Below is a sample implementation of this process in code. Note that due to the algebraic quality of the vector transformation above, randomness is increased for large values of . Now it‚Äôs finally time to put our Nietzsche model to the test. How we will do this is pretty simple. First, we will feed a 60-character excerpt from the text to our model.",0,0,0,0,1,0,1,0
The Gibbs Sampler,"Now, we do the same to sample . Only this time, we can use the result from earlier, namely . We can see how this might help us yield a slightly more convincing result than simply using the random data. We still have to use random values for  through  since we haven‚Äôt sampled from their relevant conditional distributions just yet. However, as we go through all the random variables in order, it becomes obvious that we will no longer be using randomly initialized values at one point. Specifically, on the th iteration, we would expect something like this to happen:  can be any number between 1 and , since it is used to represent the th random variable. As we repeat more iterations of sampling, we will eventually end up with a plausible representation of -dimensional vectors, which is what we sought to sample from the intractable distribution! In this section, we will take a look at a very simple example, namely sampling from a bivariate Gaussian distribution. Although we have dealt with -dimensional examples in the algorithm analysis above, for the sake of demonstration, let‚Äôs work on a simple example that we can also easily visualize and intuit.",0,0,0,0,0,1,0,1
"Newton-Raphson, Secant, and More","If we move the  term to the LHS, then divide both sides by , we end up with Here, we used big-O notation to denote the order of magnitude of the trailing terms. The trailing terms are significant since they are directly related to the accuracy of our approximation. An error term of  means that, if we halve the step size, we will also halve the error. This is best understood as a linear relationship between error and the step size. We can conduct a similar mode of analysis with backward divided difference. By symmetry, we can express  as If we rearrange (6), we end up with (2). Again, we see that backward divided difference yields linear error, or a trailing term of . Here‚Äôs where things get more interesting: in the case of center divided difference, the magnitude of the error term is , meaning that halving the step size decreases the error by four-folds. This is why center divided difference yields much more accurate approximations than forward or backward divided difference. To see this, we subtract (5) from (4), then move some terms, and divide both sides by .",1,0,0,0,0,0,0,0
Riemann Zeta and Prime Numbers,"The first function we will build is one that randomly samples  natural numbers from 1 to , and checks if all number pairs within this sample is coprime. For this, we use , which reduces the result of applying  to two pairs of numbers in the randomly sampled number list.  is a quite common operation in functional programming, and we saw an example of this operation in the context of Spark in a previous post as well. Let‚Äôs see this function in action. I‚Äôve added a  flag for convenience of testing and demonstration. Let‚Äôs toggle this option on for now and see what we get. The GCD of 6, 7, and 1 are 1, so the returned result is  as we expect. We also notice that three numbers were returned since . Next, we define a testing function that will simulate multiple runs of the  test for us. Because this is a Monte Carlo simulation, to have confidence in our estimate, we need to iterate the sampling process multiple times until we have sufficient amount of data. The  parameter determines the number of simulations we will perform.",1,0,0,0,0,0,0,0
An Introduction to Markov Chain Monte Carlo,"In fact, let‚Äôs compare our Metropolis-Hastings sampler with the built-in function in the  library.  Pretty similar, wouldn‚Äôt you say? Markov Chain Monte Carlo is a powerful method with which we can estimate intractable posterior distributions. It is undoubtedly one of the most important tools that a Bayesian statistician should have under their belt. And even if you are frequentist, I still think MCMC models are worth looking at because it‚Äôs cool to see just how easily we can estimate a distribution with little to no knowledge about the mathematics involved in calculating the posterior. It‚Äôs also fascinating to see how the marriage of two seemingly unrelated concepts that arose out of different contexts‚ÄìMonte Carlo methods and Markov chains‚Äîcan produce such a powerful algorithm. In the next post, we will continue our journey down the Bayesian rabbit hole. Perhaps we will start with another application of Bayesian thinking in machine learning. If naive Bayes is your jam, make sure to tune in some other time.",0,0,0,1,0,0,0,0
"Linear Regression, in Two Ways","In this section, we will attempt to frame regression in linear algebra terms and use basic matrix operations to derive an equation for the line of best fit. In this section, we will use linear algebra to understand regression. An important theme in linear algebra is orthogonality. How do we determine if two vectors‚Äîor more generally, two subspaces‚Äîare orthogonal to each other? How do we make two non-orthogonal vectors orthogonal? (Hence Gram-Schmidt.) In our case, we love orthogonality because they are key to deriving the equation for the line of best fit through projection. To see what this means, let‚Äôs quickly assume a toy example to work with: assume we have three points,  and , as shown below.  As we can see, the three points do not form a single line. Therefore, it‚Äôs time for some regression. Let‚Äôs assume that this line is defined by . The system of equations which we will attempt to solve looks as follows: Or if you prefer the vector-matrix representation as I do, This system, remind ourselves, does not have a solution because we have geometrically observed that no straight line can pass through all three points.",0,0,0,0,0,0,0,1
Gaussian Process Regression,"In this post, we will explore the Gaussian Process in the context of regression. This is a topic I meant to study for a long time, yet was never able to due to the seemingly intimidating mathematics involved. However, after consulting some extremely well-curated resources on this topic, such as Kilian‚Äôs lecture notes and UBC lecture videos by Nando de Freitas, I think I‚Äôm finally starting to understand what GP is. I highly recommend that you check out these resources, as they are both very beginner friendly and build up each concept from the basics. With that out of the way, let‚Äôs get started. Let‚Äôs begin by considering the classic setup of a regression problem. The goal of regression is to predict some values given a set of observations, otherwise referred to as a training set. There are of course many variants of the regression problem. For instance, in a previous post we took a look at Bayesian linear regression, where instead of a single point estimate, we tried to derive a distribution of the predicted data at a given test point.",0,0,1,1,0,0,0,0
The Magic of Euler‚Äôs Identity,"You might recall from high school physics that the velocity vector is a derivative of the position vector with respect to time. In other words, Where  is a vector that denotes the position of an object at time . Now, let‚Äôs assume that  is such a position vector. Then, it follows from the principles of physics that its derivative will be a velocity vector. Therefore, we have What is so special about this velocity vector? For one, we can see that it is a scalar multiple of the original position vector, . Upon closer examination, we might also convince ourselves that this vector is in fact orthogonal to the position vector. This is because multiplying a point or vector by  in the complex plane effectively flips the object‚Äôs  and  components, which is precisely what a 90 degree rotation entails. What does it mean to have a trajectory whose instantaneous velocity is perpendicular to that of the position vector? Hint: think of planetary orbits. Yes, that‚Äôs right: this relationship is characteristic of circular motions, a type of movement in which an object rotates around a center of axis.",1,0,0,0,0,0,0,0
Wonders of Monte Carlo,"Now that we have established the basics of this game, let‚Äôs start coding away. Our little random walk simulator is now ready to go! Let‚Äôs perform a quick sanity check to see if the code works as expected. We see that the returned tuple contains the flag boolean value as well as a list containing the coordinates of the first ten steps the drunkard took in this experiment, which is exactly what we expected. Now it is time to put this Monte Carlo application to the test by simulating the walk many times and counting the instances of successes verses failures. The  function calls the  function as many times specified by . The function compiles all the results to return two pieces of information: the percentage of success, represented in decimals, and the average number of steps it took for the drunkard to reach the restroom. Notice that there is a 100-step cap, meaning if the drunkard was not able to find the restroom after a hundred steps, the trial was assumed a failure. We can verify the functionality of our design by calling the function.",0,0,0,0,0,1,0,0
Scikit-learn Pipelines with Titanic,"Sometimes, weakly correlated features can be combined together to form a new feature, which might exhibit higher correlation with respect to the target. We can combine  and  into a new feature, called . Strictly speaking, we would have to add 1, but adding all values by one corresponds to shifting everything by a constant value, which will not affect modeling since such constant adjustments will be taken care of by the preprocessing step anyway. Note that feature engineering is also applied to both the training and test set simultaneously. We have created two new features, namely  and . Let‚Äôs go ahead and perform feature engineering on the  column as well to squeeze out more information. Now we have some data that seems a lot more workable. However, we still have a problem with the  column: It seems like there are many titles, so we should probably perform some binning or grouping. For men, the most common title is ; for women,  and . Let‚Äôs see if there is a difference in the survival rate between the two most common title for females It seems like the the difference is insignificant, so we will simply group them together in one.",0,0,1,0,0,0,0,0
Demystifying Entropy (And More),"The entropy of the random variable in this case would be exactly 0 bits, since we don‚Äôt need any information to express an event that is certain. Let‚Äôs try to figure out this dynamic between success probability of  in a coin toss and entropy by creating a plot.  The first observation to make is that the graph is symmetrical. This is no surprise, since we would expect the entropy of a random variable involving a coin that lands tails with probability  to be equal to that which lands on heads with equal probability , i.e. whether the bias concerns heads or tails should not affect the calculation of entropy. Moreover, we see that the graph peaks when , meaning that a fair coin toss involves the most randomness. In other words, this translates to saying that a skewed distribution is less random and thus more predictable than a symmetric one. This makes sense, since the result of a biased coin is more predictable and less surprising than that of a fair one, with the extreme case being the coin that always lands on one side.",0,0,0,0,0,1,0,0
Maximum A Posteriori Estimation,"And indeed, this similarity can also be seen through math. And we see that (5) is almost identical to (3), the formula for MLE! The only part where (5) differs is the inclusion of an additional term in the end, the log prior. What does this difference intuitively mean? Simply put, if we specify a prior distribution for the model parameter, the likelihood is no longer just determined by the likelihood of each data point, but also weighted by the specified prior. Consider the prior as an additional ‚Äúconstraint‚Äù, construed in a loose sense. The optimal parameter not only has to conform to the given data, but also not deviate too much from the established prior. To get a more intuitive hold of the role that a Bayesian prior plays in MAP, let‚Äôs assume the simplest, most uninformative prior we can consider: the uniform distribution. A uniform prior conveys zero beliefs about the distribution of the parameter, i.e. all values of  are equally probable. The implication of this decision is that the prior collapses to a constant.",0,0,0,0,0,1,0,0
Convex Combinations and MAP,"In PDF terms, this translates to The log prior can simply be derived by casting the logarithmic function to the probability distribution function. Now we are ready to enter the maximization step of the sequence. To calculate the maximum of the posterior distribution, we need to derive the posterior and set the gradient equal to zero. For a more robust analysis, it would be required to show that the second derivative is smaller than 0, which is indeed true in this case. However, for the sake of simplicity of demonstration, we skip that process and move directly to calculating the gradient. Let‚Äôs rearrange the final equality in (7). From (8), we can finally derive an expression for . This value of the parameter is one that which maximizes the posterior distribution. And we have derived the MAP estimate for the mean of the univariate Gaussian! Maximum a posteriori analysis is great and all, but what does the final result exactly tell us? While there might be many ways to interpret understand the result as derived in (9), one particular useful intuition to have relates to the concept of convex combinations.",0,0,0,0,0,1,0,0
Traveling Salesman Problem with Genetic Algorithms,"Nonetheless, a lot of the adjacent cities are connected (hence the use of the aforementioned term, optimal segments). Considering the fact that there are a total of  possibilities, the fact that the algorithm was able to narrow it down to a plausible route that beats the baseline is still very interesting. Genetic algorithms belong to a larger group of algorithms known as randomized algorithms. Prior to learning about genetic algorithms, the word ‚Äúrandomized algorithms‚Äù seemed more like a mysterious black box. After all, how can an algorithm find an answer to a problem using pseudo-random number generators, for instance? This post was a great opportunity to think more about this naive question through a concrete example. Moreover, it was also interesting to think about the traveling salesman problem, which is a problem that appears so simple and easy, belying the true level of difficulty under the surface. There are many other ways to approach TSP, and genetic algorithms are just one of the many approaches we can take. It is also not the most effective way, as iterating over generations and generations can often take a lot of time.",0,0,0,1,0,0,0,0
PyTorch RNN from Scratch,"The last one is interesting, because it is the name of a close Turkish friend of mine. The model obviously isn‚Äôt able to tell us that the name is Turkish since it didn‚Äôt see any data points that were labeled as Turkish, but it tells us what nationality the name might fall under among the 18 labels it has been trained on. It‚Äôs obviously wrong, but perhaps not too far off in some regards; at least it didn‚Äôt say Japanese, for instance. It‚Äôs also not entirely fair game for the model since there are many names that might be described as multi-national: perhaps there is a Russian person with the name of Demirkan. I learned quite a bit about RNNs by implementing this RNN. It is admittedly simple, and it is somewhat different from the PyTorch layer-based approach in that it requires us to loop through each character manually, but the low-level nature of it forced me to think more about tensor dimensions and the purpose of having a division between the hidden state and output. It was also a healthy reminder of how RNNs can be difficult to train.",0,0,0,1,0,0,1,0
So What are Autoencoders?,"Next, we will add  noise to the data. Note that the MNIST dataset does not contain noise by default: we will have to artificially and intentionally tarnish the dataset to produce a noisy training set for the autoencoder model. The  function precisely performs this function. Using the  function, we can create a noisy sample. Note that  was set to 0.5, although I‚Äôd imagine other values within reasonable range would work equally well as well. Training the model is very simple: the training data is , the noisy dataset, and the predicted label is . Through this configuration, we essentially expect the autoencoder to be able to see noisy images, after which encoding and decoding is performed via a transformation to a latent dimension to ultimately reproduce a pristine image devoid of any noise. For experimental puposes, I tried using the  callback on Google Colab.  is a platform that gives developers full view of what happens during and after the training process. It makes observing metrics like loss and accuracy a breeze. I highly recommend that you check out this tutorial on how to use and configure this functionality on your notebook.",0,0,0,0,1,0,1,0
A PyTorch Primer,"During the backward pass, we compute the gradient. Here, we need to retrieve the  variable which was stored in the context. This is because the ReLU function takes the following form: Thus, its derivative is During backpropagation, this means that gradients will flow down to the next layer only for those indices whose input elements to te ReLU function were greater than 0. Thus, we need the input vector for reference purposes, and this is done via stashing it in the  variable. We will see how we can incorporate  into the model in the next section. In this example, we‚Äôll take a look at an extremely simple model to gain a better understanding of how everything comes into play in a more practical example. This is the method that I‚Äôve mostly been using when implementing simple dense fully-connected models in NumPy. The idea is that we would mathematically derive the formula for the gradients ourselves, then backpropagate these values during the optimization process. Of course, this can be done with PyTorch. To build our simple model, let‚Äôs first write out some variables to use, starting with the configuration of our model and its dimensions.",0,0,0,0,0,0,1,0
Convex Combinations and MAP,"And as we obtain larger quantities of data, the relative importance of the prior distribution starts to diminish. Imagine that we have an infinite number of data points. Then,  will soley be determined by the likelihood function, as the weight ascribed to the prior will decrease to zero. In other words, Conversely, we can imagine how having no data points at all would cause the weight values to shift in favor of the prior such that no importance is ascribed to the MLE estimate of the parameter. In this short article, we reviewed the concept of maximum a posteriori and developed a useful intuition about its result from the perspective of convex combinations. Maximum a posteriori, alongside its close relative maximum likelihood estimation,  is an interesting topic that deserives our attention. Hopefully through this post, you gained a better understanding of what the result of an MAP estimation actually means from a Bayesian point of view: a weighted average between the prior mean and the MLE estimate, where the weight is determined by the number of data points at our disposal. Also, I just thought that the name ‚Äúconvex conbinations‚Äù is pretty cool.",0,0,0,0,0,1,0,0
PyTorch RNN from Scratch,"We can now build our model and start training it. I realized that training this model is very unstable, and as you can see the loss jumps up and down quite a bit. Nonetheless, I didn‚Äôt want to cook my 13-inch MacBook Pro so I decided to stop at two epochs. Now we can test our model. We could look at other metrics, but accuracy is by far the simplest, so let‚Äôs go with that. The model records a 72 percent accuracy rate. This is very bad, but given how simple the models is and the fact that we only trained the model for two epochs, we can lay back and indulge in momentary happiness knowing that the simple RNN model was at least able to learn something. Let‚Äôs see how well our model does with some concrete examples. Below is a function that accepts a string as input and outputs a decoded prediction. I don‚Äôt know if any of these names were actually in the training or testing set; these are just some random names I came up with that I thought would be pretty reasonable. And voila, the results are promising.",0,0,0,1,0,0,1,0
"Linear Regression, in Two Ways","If there is one thing I recall most succinctly from my high school chemistry class, it is how to use Excel to draw basic plots. In the eyes of a naive freshman, visualizations seemed to add an air of professionalism. So I would always include a graph of some sort in my lab report, even when I knew they were superfluous. The final icing on the cake? A fancy regression with some r-squared. In today‚Äôs post, I want to revisit what used to be my favorite tinkering toy in Excel: regression. More specifically, we‚Äôll take a look at linear regression, which deals with straight lines and planes instead of curved surfaces. Although it sounds simple, the linear regression model is still widely used because it not only provides a clearer picture of obtained data, but can also be used to make predictions based on previous observations. Linear regression is also incredibly simple to implement using existing libraries in programming languages such as Python, as we will later see in today‚Äôs post. That was a long prologue‚Äî‚Äìlet‚Äôs jump right in.",0,0,0,0,0,0,0,1
Building Neural Network From Scratch,"Welcome back to another episode of ‚ÄúFrom Scratch‚Äù series on this blog, where we explore various machine learning algorithms by hand-coding them from scratch. So far , we have looked at various machine learning models, such as kNN, logistic regression, and naive Bayes. Now is time for an exciting addition to this mix: neural networks. Around last year December, I bought my first book on deep learning, titled Deep Learning from Scratch,  by Saito Goki. It was a Korean translation of a book originally published in Japanese by O‚ÄôReilly Japan. Many bloggers recommended the book as the go-to introductory textbook on deep learning, some even going as far as to say that it is a must-have. After reading a few pages in, I could see why: as the title claimed, the author used only  to essentially recreate deep learning models, ranging from simple vanilla neural networks to convolutional neural networks. As someone who had just started to learn Python, following the book was a lot harder than expected, but it was a worthwhile read indeed.",0,0,0,1,0,0,1,1
A Simple Autocomplete Model,"In the context of this tutorial, our neural network  should be able to somewhat immitate the speech of the famous German philosopher based on the training it went through with text data, although we would not expect the content generated by our neural network to have the same level of depth and profoundity as those of his original writings. As mentioned above, the objective of a generative model is to model the distribution of the latent space from which observed data points came from. At this point, our trained model should be able to model this distribution, and thus generate predictions given some input vector. However, we want to add some element of randomness of noise in the prediction. Why might we want to do this? Well, an intuitive pitfall we might expect is that the model might end up generating a repetition of some likely sequence of characters. For example, let‚Äôs say the model‚Äôs estimated distribution deems the sequence ‚ÄúGod is dead‚Äù to be likely. Then, the output of our model might end up being something like this: ‚Ä¶(some input text) God is dead God is dead God is dead‚Ä¶ (repetition elided) We don‚Äôt want this to happen.",0,0,0,0,1,0,1,0
Demystifying Entropy (And More),"It is not difficult to see why cross entropy is a useful cost function frequently used in the field of machine learning. Entropy is an interesting concept with which we can quantify randomness in data. This process is no rocket science, but simply a process that involves calculations with probabilities. Although the link may not be immediately apparent, randomness is just another way of expressing probabilities and uncertainty, and it is from this premise that information and entropy take off. Beyond that, however, entropy is now used extensively in the field of machine learning, specifically as a loss function. Although it was not noted explicitly above, cross entropy calculates the same quantity as the logarithmic loss function. Essentially, cross entropy is useful in that it provides us with some intuitive information of how far two distributions are apart. This distance is a metric with which we can evaluate the effectiveness of our model, which also means that the effectiveness of a model will be increased as cross entropy is increasingly minimized.",0,0,0,0,0,1,0,0
Principal Component Analysis,"Let‚Äôs define this transformation as , and the matrix corresponding to the decoding . In other words, PCA makes a number of assumptions to simplify this problem. The most important assumption is that each column of  is orthogonal to each other. As we will see later in an alternate derivation with statistics, this has to do with the notion of covariance. Another restriction is that the columns of  must have a Euclidean norm of one. This constraint is necessary for us to find a unique matrix  that achieves compression‚Äîotherwise, we could have any multiples, leading to an infinite number of such matrices. We make one more convenient assumption about the given data points, . That is,  is assumed to have a mean of zero, i.e. . If this is not the case, we can easily perform standardization by subtracting the mean from the data. With this setup in mind, let‚Äôs finally start the derivation. As said earlier, the goal of PCA is to compress data (and be able to uncompress it) with as little loss of information as possible.",0,0,0,0,0,1,0,1
Wonders of Monte Carlo,"This is a pattern that we saw with all three tasks we dealt with in today‚Äôs post. This sums up our post on Monte Carlo methods. In a future post, we will take a look at Markov Chain Monte Carlo, particularly Metropolis-Hastings, which uses the best of both worlds to analyze complicated probability distributions. I‚Äôm already excited for that post, because MCMC methods will bring together so many concepts that we have dealt with on this blog so far‚Äîranging from Bayesian inference, probability distributions, Markov chains, and so many more. Catch you up on the next one!.",0,0,0,0,0,1,0,0
A sneak peek at Bayesian Inference,"Conjugate priors streamline the Bayesian process of updating our priors with posteriors by ensuring that this condition is satisfied. In simple language, mathematicians have found that certain priors go well with certain likelihoods. For instance, a normal prior goes along with a normal likelihood; Gamma prior, Poisson likelihood; Gamma prior, normal likelihood, and so on. Our current combination, Beta prior and binomial likelihood, is also up on this list. To develop some intuition, here is a graphical representation of the Beta function for different values of  and . This code block produces the following diagram. Graphically speaking, the larger the value of  and , the more bell-shaped it becomes. Also notice that a larger  corresponds to a rightward shift, i.e. a head-biased coin; a larger , a tail-oriented one. When  and  take the same value, the local extrema of the Beta distribution is established at , when the coin is perfectly fair. Now that we have established the usability of the Beta function as a conjugate prior to the binomial likelihood function, let‚Äôs finally see Bayesian inference at work.",0,1,0,0,0,1,0,0
A Brief Introduction to Recurrent Neural Networks,"We use the sigmoid function since we want to conduct a sentiment analysis of determining whether a given movie review is positive or negative. That wasn‚Äôt so difficult. Let‚Äôs initialize our model by defining the model parameters , , and , then plot the model to see the structure of the network alongside the input and output dimensions of each layer. Note that we defined  to be 16, which means that each word is transformed into dense vectors living in 16 dimensions. By plotting the model, we can get a better idea of the layers that compose the model.  The next model we will build is a simple recurrent neural network. This neural network is going to have an embedding layer, just like the previous model. However, instead of a dense layer, it will have two consecutive  layers stacked on top of each other. The  layer is essentially the  implementation of the  model we built earlier. Let‚Äôs take a look. We instantiate the model and take plot the network, just as we have done above.  The  model we built is, as the name shamelessly puts out, pretty simple.",0,0,0,0,1,0,1,0
"Basel, Zeta, and some more Euler","Indeed, we know that this series converges to a real value. We also know that integration would give us a rough approximation. However, how can evaluate this series with exactitude? Euler‚Äôs solution, simple and elegant, demonstrates his genius and acute heuristics. Euler begins his exposition by analyzing the Taylor expansion of the sine function, which goes as follows: Dividing both sides by , we obtain the following: Now it‚Äôs time to follow Euler‚Äôs amazing intuition. If we take a close look at the equation , we can convince ourselves that its solutions will adhere to the form , where  is a non-zero integer between . This is expected given the periodic behavior of the sine function and its intercepts with the -axis. Given these zeros, we can then reconstruct the original function  as an infinite product using the fundamental theorem of algebra, or more specifically, Weierstrass factorization. Let‚Äôs try to factor out the coefficient of the  term through some elementary induction.",1,0,0,0,0,0,0,0
Logistic Regression Model from Scratch,"For instance, given the dimensions of a patient‚Äôs tumor, determine whether the tumor is malignant or benign. Another problem might involve classifying emails as either spam or not spam. We can label spam emails as 1 and non-spam emails as 0, feed the data into a predefined machine learning algorithm, and generate predictions using that model. If the output of an algorithm given some data point is larger than 0.5, it is likely that the given input is a spam; if it is smaller than the 0.5 threshold, chances are the email is not spam. Let‚Äôs take a look at the shape of the sigmoid function, which is a special case of the logistic function that we will use throughout this post. To plot the sigmoid function, we need to import some libraries. The sigmoid function is defined as follows: We can express this as a Python function, as demonstrated in the code snippet below. Let‚Äôs quickly plot the graph to see what the sigmoid function looks like.  As we can see, the sigmoid is a smooth, differentiable function that is bounded between 0 and 1.",0,0,1,1,0,0,0,0
Word2vec from Scratch,"For simplicity purposes, say we have a total of 5 words in the corpus, and that we want to embed these words as three-dimensional vectors. More specifically, here is the first weight layer of the model: A crucial observation to make is that, because the input is a sparse vector containing one-hot encoded vectors, the weight matrix effectively acts as a lookup table that moves one-hot encoded vectors to dense vectors in a different dimension‚Äîmore precisely, the row space of the weight matrix. In this particular example, the weight matrix was a transformation of . This is exactly what we want to achieve with embedding: representing words as dense vectors, a step-up from simple one-hot encoding. This process is exactly what embedding is: as we start training this model with the training data generated above, we would expect the row space of this weight matrix to encode meaningful semantic information from the training data. Continuing onwards, here is the second layer that receives as input the embeddings, then uses them to generate a set of outputs. We are almost done. All we now need in the last layer is a softmax layer.",0,0,0,1,0,0,1,0
A Brief Introduction to Recurrent Neural Networks,"An interesting point to note about the learning curve of this model is that the test data accuracy and loss seem to stay roughly stagnant despite the progression of epochs. To better understand this phenomena, we probably have to run more trials with more data over longer iterations than we have done in this tutorial. This point notwithstanding, it is interesting to see how a single layer LSTM network can outperform a stacked RNN network.  The last up on this list is the one-dimensional convolutional neural network. The convent produced very remarkable results in this experiment, especially given its extremely short training time. Recurrent neural networks typically take a lot of time to train‚Äîeven when they are not stacked‚Äîbecause each neuron is defined by a rather complicated operation involving many parameters, such as states, carriage, and so on. Convents, on the other hand, are relatively simpler, and thus take noticeably shorter to train and deploy. This tutorial demonstrates that convents can perform as well as simple recurrent networks to establish a baseline performance metric.  In this post, we briefly introduced and explored the concept of recurrent neural networks, how they work, and how to build them using the  functional API.",0,0,0,0,1,0,1,0
Likelihood and Probability,"For the purposes of this post, we look at the simplest way that involves just a bit of calculus. The best way to demonstrate how MLE works is through examples. In this post, we look at simple examples of maximum likelihood estimation in the context of normal distributions. We have never formally discussed normal distributions on this blog yet, but it is such a widely used, commonly referenced distribution that I decided to jump into MLE with this example. But don‚Äôt worry‚Äîwe will derive the normal distribution in a future post, so if any of this seems overwhelming, you can always come back to this post for reference. The probability density function for the normal distribution, with parameters  and , can be written as follows: Assume we have a list of observations that correspond to the random variable of interest, . For each  in the sample data, we can calculate the likelihood of a distribution with parameters  by calculating the probability densities at each point of the PDF where .",0,0,0,0,0,1,0,0
Building Neural Network From Scratch,"The most commonly used loss function in the context of classification problems is cross entropy, which we explored in this post previously on this blog. For a brief recap, presented below is the formula for calculating cross entropy given a true distribution  and a predicted distribution : Our goal is to train our neural network so that is output distribution  is as close to  as possible. In the case of binary classification, we might alter equation (9) to the following form: The reformulation as shown in equation (10) is the formula for what is known as binary cross entropy. This is the equation that we will be using in the context of our problem, since the dataset we have only contains two class labels of 0 and 1. Now that we have an idea of what the loss function looks like, it‚Äôs time to calculate the gradient. Since we are going to be back propagating the gradient, it makes sense to start from the very back of the neural network. Recall that our neural network is structured as follows: The last layer is a softmax unit that receives input  to produce output .",0,0,0,1,0,0,1,1
How lucky was I on my shift?,"This is what the binomial PMF is implying: calculating the probability that we get  successes in  trials requires that we multiply the probability of success  by  times and the probability of failure  by  times, because those are the numbers of successful and unsuccessful trials respectively. Now that we have reviewed the concept of binomial distribution, it is time to apply it to our example of phone calls at the Yongsan PMO. Although I do not have an  data sheet on me, let‚Äôs assume for the sake of convenience that, on average, 12 calls come to the PMO per shift, which is eight hours. Given this information, how can we simulate the situation as a binomial distribution? First, we have to define what constitutes a success in this situation. While there might be other ways to go about this agenda, the most straightforward approach would be to define a phone call as a success.",0,1,0,0,0,1,0,0
PyTorch Tensor Basics,"There are in-place versions of both  and  though, and that is simply adding a  to the end of the function. For example, Equivalently, calling  will remove the th dimension of the tensor. By default,  is 0. Squeezing and unsqueezing can get handy when dealing with single images, or just single inputs in general. Concatenation and stacking are very commonly used in deep learning. Yet they are also operations that I often had trouble imagining in my head, largely because concatenation can happen along many axes or dimensions. In this section, let‚Äôs solidify our understanding of what concatenation really achieves with some dummy examples. With a basic example, we can quickly verify that each tensor is a three-dimensional tensor whose individual elements are two-dimensional tensors of shape . Now, let‚Äôs perform the first concatenation along the 0-th dimension, or the batch dimension. We can verify that the concatenation occurred along the 0-th dimension by checking the shape of the resulting tensor. Since we concatenated two tensors each of shape , we would expect the resulting tensor to have the shape of , which is indeed what we got.",0,0,0,0,0,0,1,0
An Introduction to Markov Chain Monte Carlo,"To recap, the MCMC sampler works by assuming some value sampled from the proposal distribution, calculating the likelihood and posterior, and seeing if the new proposed value is worth accepting, i.e. if it is worth making a jump in the random walk. All of this sounds pretty abstract when written in words, but it is a simple idea encapsulated by (7). Let‚Äôs build the Metropolis-Hastings sampler by implementing the algorithm described above in code as shown below. I looked at Thomas Wiecki‚Äôs implementation for reference and modified it to suit the purposes of this post. Although Markov Chain Monte Carlo sounds complicated, really it is achieved by this single block of code. Of course, this code is limited in that is only applicable to a very specific situation, namely the task of deriving the posterior given a normal prior and a normal likelihood with known variance. Nevertheless, we can glean so much insight from this fascinating function. Let‚Äôs quickly test the  function by making it sample five estimations of the mean parameter. As expected, the sampler starts from the 0, which is the default argument  and took a jump at the second sample.",0,0,0,1,0,0,0,0
Fisher Score and Information,"And the variance-covariance matrix is simply a matrix that contains information on the covariance of multiple random variables in a neat, compact matrix form. A closed-form expression for the covariance matrix  given a random vector , which follows immediately from aforementioned definitions and some linear algebra, looks as follows: Enough of the prologue and review, now we‚Äôre ready to start talking about Fisher. The information matrix is defined as the covariance matrix of the score function as a random vector. Concretely, Note that the 0‚Äôs follow straight from the earlier observation that . Intuitively, Fisher‚Äôs information gives us an estimate of how certain we are about the estimate of the parameter . This can be seen by recognizing the apparent similarity between the definition of the covariance matrix we have defined above and the definition of Fisher‚Äôs information. In fact, the variance of the parameter  is explained by the inverse of Fisher‚Äôs information matrix, and this concept is known as the Cramer-Rao Lower Bound. For the purposes of this post, I won‚Äôt get deep into what CRLB is, but there are interesting connections we can make between Fisher‚Äôs information, CRLB, and the likelihood, which we will get into later.",0,0,0,0,0,1,0,0
Logistic Regression Model from Scratch,"Still, it provides a solid basis for the construction of the gradient descent algorithm in code, as shown below. To avoid compensating code readability, I made a stylistic choice of using  and  to denote the vector of coefficients instead of using  for notational consistency. Other than adding some switch optional parameters such as  or , the code simply follows the gradient descent algorithm outlined above. Note that equation (6) is expressed via ; equation (14) is expressed by the line . Let‚Äôs quickly check that the  function works as expected using the dummy data we created earlier. Great! We see that the average cross entropy decreases with more iterations. The returned  array contains the coefficients of the logistic regression model, which we can use to now make predictions. We can stop here, but just like we did in the post on k-nearest neighbors, let‚Äôs wrap all the functions we have created so far into a single function that represents the logistic regression model. Our model is ready. Time for testing with some real-world data. Let‚Äôs import some data from the web.",0,0,1,1,0,0,0,0
VGG PyTorch Implementation,"In today‚Äôs post, we will be taking a quick look at the VGG model and how to implement one using PyTorch. This is going to be a short post since the VGG architecture itself isn‚Äôt too complicated: it‚Äôs just a heavily stacked CNN. Nonetheless, I thought it would be an interesting challenge. Full disclosure that I wrote the code after having gone through Aladdin Persson‚Äôs wonderful tutorial video. He also has a host of other PyTorch-related vidoes that I found really helpful and informative. Having said that, let‚Äôs jump right in. We first import the necessary  modules. Let‚Äôs first take a look at what the VGG architecture looks like. Shown below is a table from the VGG paper.  We see that there are a number of different configurations. These configurations typically go by the name of VGG 11, VGG 13, VGG 16, and VGG 19, where the suffix numbers come from the number of layers. Each value of the dictionary below encodes the architecture information for each model. The integer elements represents the out channel of each layer.  represents a max pool layer. You will quickly see that the dictionary is just a simple representation of the tabular information above.",0,0,0,0,0,0,1,0
The Exponential Family,"This was the hard part: now, all that is left is to configure the rest of the functions to complete the factorization. One possible answer is presented below: By now, it should be sufficienty clear that the definition of the exponential family is robust enough to encompass at least the two probability distributions: the exponential and the Bernoulli. Although we do not go over other examples in this article, the exponential family is a well-defined set of probability distributions that, at thei core, are defined by a common structure. And as we will see in the next section, this underlying similarity makes certain calculations surprisingly convenient. In a previous post, we explorerd the notion of maximum likelihood estimation, and contrasted it with maximum a posteriori estimation. The fundamental question that maximum likelihood estimation seems to answer is: given some data, what parameter of a distribution best explains that observation? This is an interesting question that merits exploration in and of itself, but the discussion becomes a lot more interesting and pertinent in the context of the exponential family. Before diving into MLE, let‚Äôs define what is known as the canonical form of the exponential family.",0,1,0,0,0,1,0,0
"Linear Regression, in Two Ways","As we always like to do, let‚Äôs throw out the equation first to see what we‚Äôre getting into before anything else. We can represent the gradient of function  with respect to matrix  is a matrix of partial derivatives, defined as While this formula might seem complicated, in reality, it is just a convenient way of packaging partial derivatives of the function into a compact matrix. Let‚Äôs try to understand what this operation entails through a simple dummy example. As you can see, instead of a m-by-n matrix, we have a column vector  as an ingredient for a function. But don‚Äôt worry: the formula in (3) works for vectors as well, since vectors can be considered as matrices with only a single column. With that in mind, let‚Äôs define our function  as follows: Great! We see that the  is a scalar function that returns some value constructed using the entries of . Equation (3) tells us that the gradient of , then, is simply a matrix of partial derivatives whose dimension equals that of . Concretely, In other words, Notice that this is the single variable calculus equivalent of saying that .",0,0,0,0,0,0,0,1
The Exponential Family,"Recall from the definition of the exponential family that  is a normalizing constant that exists to ensure that the probability function integrates to one. In other words, This necessarily implies that Now that we have an expression for  to work with, let‚Äôs try to compute the derivative term we left unsolved in (19). The first and second equalities stand due to the chain rule, and the third equality is a simple algebraic manipulation that recreates the probability function within the integral, allowing us to ultimately express the partial derivative as an expected value of  for the random variable . This is a surprising result, and a convenient one indeed, because we can now use this observation to conclude that the gradient of the log likelihood function is simply the expected value of the sufficient statistic. Therefore, starting again from (19), we can continue our calculation of the gradient and set the quantity equal to zero to calculate the MLE estimate of the parameter.",0,1,0,0,0,1,0,0
Wonders of Monte Carlo,"This versatility is why MC method is such a powerful tool in the statistician‚Äôs arsenal. In today‚Äôs post, we will attempt to solve various bite-sized tasks using MC methods. These tasks will be of varying difficulty, but taken together, they will collectively demonstrate the useful applications of MC methods. Let‚Äôs get started with the first up on the list: estimating . We all know from basic geometry that the value of  approximates to . There are obviously various ways to derive this value. Archimedes famously used hexagons to estimate that the value of  lies between . With later advances in math, mathematicians began to approach this problem from the angle of infinite series or products, the result of which were the Leibniz formula, Wallis product, Euler product, and the likes. And of course, modern computing now allows us to borrow the prowess of machinery to calculate this quantity with extreme accuracy. While the maths behind these derivations are fascinating, our approach will not take these routes; instead, we will use a crude Monte Carlo method. First, we draw a two-by-two square, inside of which we inscribe a circle of radius 1.",0,0,0,0,0,1,0,0
Building Neural Network From Scratch,"Instead of cluttering the diagram by attempting to visualize all 64 neurons, I decided to simplify the picture by assuming that we have 16 neurons in each of the affine layers. But with the power of imagination, I‚Äôm sure it‚Äôs not so much difficult to see how the picture would change with 64 neurons.  Hopefully the visualization gave you a better understanding of what our model looks like. Now that we have a function that creates our model, we are ready to run the model! At this point, our neural network model is only a dictionary that contains matrices of specified sizes, each containing randomly genereated numbers. You might be wondering how a dictionary can be considered a model‚Äîafter all, a dictionary is merely a data structure, and so is incapable of performing any operations. To make our model to work, therefore, we need a function that performs matrix multiplications and applies activation functions based on the dictionary. The  function is precisely such a function that uses the weights stored in our model to return both the intermediary and final outputs, denoted as  and  respectively.  Note that we apply activation functions, such as  and  when appropriate.",0,0,0,1,0,0,1,1
"0.5!: Gamma Function, Distribution, and More","We can divide both sides by  to obtain the following expression: We can then apply the substitution  to obtain Notice that we can consider the integrand to be a probability distribution function since the result of integration over the prescribed domain yields 1, the total probability. Seen this way, we can finally delineate a definition of the Gamma distribution as follows, with a trivial substitution of parameters. If this derivation involving the Gamma function does not click with you, we can take the alternative route of starting from the functional definition of the Gamma distribution: the Gamma distribution models the waiting time until the occurrence of the th event in a Poisson process. In other words, given some rate  which denotes the rate at which an event occurs, what is the distribution of the waiting time going to look like? The Gamma distribution holds an answer to this question. Let‚Äôs jump right into derivation. The first ingredient we need is the equation for the Poisson probability mass function, which we might recall goes as follows:  denotes the number of events that occur in unit time, while  denotes the rate of success.",0,1,0,0,0,1,0,0
Dissecting the Gaussian Distribution,"Using the formula for the multivariate Gaussian we derived in (11), we can construct the probability distribution function given , , and . Note that computing , the inverse of the covariance matrix, can be accomplished simply by taking the reciprocal of its diagonal entries since  was assumed to be a diagonal matrix. Continuing, In other words, the probability distribution of seeing a random vector  given  and  is equal to the product of the two univariate Gaussians. This result is what we would expect given that . For instance, if  and  are independent, i.e. observing a value of  does not inform us of anything about  and vice versa, it would make sense that the possibility of observing a random vector  with entries  and  is merely the product of the independent probabilities of each observing  and . This example illustrates the intuitive link between the multivariate and univariate Gaussian distributions. In this post, we took a look at the normal distribution from the perspective of probability distributions. By working from the definition of what constitutes a normal data set, we were able to completely build the probability density function from scratch.",0,1,0,0,0,1,0,0
A Brief Introduction to Recurrent Neural Networks,"Convolutional neural networks are great at identifying spatial patterns in data, which is why they also perform reasonably well in natural language processing. Another huge advantage of convents over recurrent networks is that they took a lot lesser time and resources to train. This is why it is often a good idea to build a convent to establish a baseline performance metric. Let‚Äôs initialize the model with identical parameters and take a look at its internal structure.  Let‚Äôs train all four models using the training data. For control our experiment, we will train all four models over the same , , and . There isn‚Äôt much exciting here to look at it terms of code; it‚Äôs just a matter of patience, waiting for the models to hopefully converge to a global minimum. For future reference, all training history is dumped in the  object where  corresponds to the model number. After a long time of waiting, the training is finally complete! If you are following this tutorial on your local workstation, please note that the time required for training may vary depending on your hardware configurations or the specification of our instance if you are using a cloud-based platform like AWS.",0,0,0,0,1,0,1,0
Logistic Regression Model from Scratch,"Using the coefficients in the  list, we can generate predictions for each observation in the . The actual class information is stored in the  list. The dummy coefficients are poorly optimized, which is why the predicted class labels do not align well with the actual class labels. This tells us that more tuning is required to update the coefficients and build a robust logistic regression model. But how exactly can we tune our model? Simply eyeballing the actual and predicted labels of our data is probably not going to help us much. To optimize the coefficients to best fit our data, we need to construct some loss function‚Äîthat is, a function that describes how badly our model is performing. Then, we can optimize the weights of our model by minimizing that loss function, which would mean that our model gradually makes better predictions with each round of optimization. If you recall the previous post on entropy, you will remember that we discussed a concept called cross entropy. In that post, we derived the formula for cross entropy and intuitively understood it as a way of measuring the ‚Äúdistance‚Äù between two distributions.",0,0,1,1,0,0,0,0
Recommendation Algorithm with SVD,"This decomposition structure is similar to that of eigendecomposition, and this is no coincidence: in fact, formula (1) can simply be shown by performing an eigendecomposition on  and . Let‚Äôs begin by calculating the first case, , assuming formiula (1). This process looks as follows: The last equality stands since the inverse of an orthogonal matrix is equal to its transpose. Substituting  for , equation (2) simplifies to And we finally have what we have seen with eigendecomposition: a matrix of independent vectors equal to the rank of the original matrix, a diagonal matrix, and an inverse. Indeed, what we have in (3) is an eigendecomposition of the matrix . Intuitively speaking, because matrix  is not necessarily square, we calculate  to make it square, then perform the familiar eigendecomposition. Note that we have orthogonal eigenvectors in this case because  is a symmetric matrix‚Äîmore specifically, positive semi-definite. We won‚Äôt get into this subtopic too much, but we will explore a very simple proof for this property, so don‚Äôt worry. For now, let‚Äôs continue with our exploration of the SVD formula by turning our attention from matrix ‚Äîa factor of eigendecomposition on ‚Äîto the matrix .",0,0,0,1,0,0,0,1
Wonders of Monte Carlo,"Although we do get some points in the circle, it‚Äôs really hard to tell if the proportion of points in and out of the circle is going to be representative of the actual proportion of area between the circle and the square. Let‚Äôs push our MC algorithm to do a bit more by sampling more data points, this time with 100000 randomly generated points. As expected, with more data points, we get a better estimation of pi. Although the randomness created by the  call in our function means that this estimation will fluctuate with each execution, the value is reliably close to the actual value of , differing only by about 0.01 or less. If we draw the plot for our experiment, it is clear that our data points accurately capture the proportionality between the area of the square and the circle.  We can systematically verify that more samples tend to yield better results by graphing the magnitude of the error of our estimation plotted against the number of random samples generated.  The plot shows that, with larger sample sizes, the error quickly converges to around 0.",0,0,0,0,0,1,0,0
Gaussian Process Regression,"Assuming that the data is normally distributed, given a number of training points and their corresponding  values, how can we make predictions at test points? In other words,  are the test points; , the training points. Then, we can now establish the following: where  denotes the observed values in the training set and the s are each components of the kernel matrix for the entire dataset, including both the training and test sets: This partition also means that  is the kernel for the training set; , the kernel for the test set. You might be wondering how the generic formula for the conditional distribution morphed into (18). While the notation might obscure their similarity, (18) immediately follows from (1). First, because we assumed zero mean, the term  simply collapses into . The same line of reasoning applies to ‚Äô hence, the first term disappears from the mean. As for the covariance, a simple comparison is enough to show that the two equations are identical.",0,0,1,1,0,0,0,0
Revisiting Basel with Fourier,"Therefore, putting everything together, we end up with If we consider the case when , we have Do you smell the basel problem in the air? The summation on the right hand side is a great sign that we are almost done in our derivation. Moving the fractional term to the left hand side, we get: Diding both sides by 4, And there you have it, the answer to the Basel problem, solved using Fourier series! We can also derive a convergence value of the Dirichelt Eta function from this Fourier series as well. Recall that the Eta function looks as follows: Now how can we get a Dirichelt Eta function out of the fourier series of ? Well, let‚Äôs get back to (8) and think our way through. One noteworthy observation is that we already have  in the summation, which looks awfully similar to the Dirichlet Eta function. Since we want to get rid of the cosine term, we can simply set ‚Äîthis will make all cosine terms evaluate to 1, effectively eliminating them from the expression.",1,0,0,0,0,0,0,0
Building Neural Network From Scratch,"So presented in the next section is a nicer, cleaner implementation of a neural network model based off of the functions we designed above. A simple neural network model in just 56 lines of code, ready to be initialized, trained, deployed, and tested! You will see that much of the code is literally just copy and pasted from the original functions we designed above. But just to make sure that everything works fine, let‚Äôs try creating a neural network object and use the  function to see how well our model performs. I chose 99 as the number of neurons in the affine layers for no reason. In this instance, the accuracy of this model is 95 percent, similar to what we had above. At this point, one question that popped up in my mind was the relationship between the number of neurons and the performance of the neural network model. Intuitively, the more neurons there are, the higher the memory capacity of that model, and thus better the performance. Of course, the larger the number of neurons, the larger the risk of overfitting our model, which can also negatively impact the performance of the neural network.",0,0,0,1,0,0,1,1
BLEU from scratch,"The fact that unigrams are a poor way of evaluating translations becomes immediately clear once we plot the  in n-grams against modified precision.  As you can see, precision score decreases as  gets higher. This makes sense: a larger  simply means that the window of comparison is larger. Unless whole phrases co-occur in the translation and reference sentences‚Äîwhich is highly unlikely‚Äîprecision will be low. People have generally found that a suitable  value lies somewhere around 1 and 4. As we will see later, packages like  use what is known as cumulative 4-gram BLEU score, or BLEU-4. The good news is that our current implementation is already able to account for different  values. This is because we wrote a handy little function, . By passing in different values to , we can deal with different n-grams. Now we‚Äôre almost done. The last example to consider is the following translation: This is obviously a bad translation. However, due to the way modified precision is currently being calculated, this sentence will likely earn a high score. To prevent this from happening, we need to apply what is known as brevity penalty.",0,0,0,1,0,0,1,0
A sneak peek at Bayesian Inference,"To be fair, most coins are approximately fair (no pun intended) given the physics of metallurgy and center of mass, but for now let‚Äôs assume that we are ignorant of coin‚Äôs fairness, or the lack thereof. By employing Bayesian inference, we can update our beliefs on the fairness of the coin as we accumulate more data through repeated coin flips. For the purposes of this post, we will assume that each coin flip is independent of others, i.e. the coin flips are independent and identically distributed. Let‚Äôs start by coming up with a model representation of the likelihood function, which we might recall is the probability of having a parameter value of  given some data . It is not difficult to see that the best distribution for the likelihood function given the setup of the problem is the binary distribution since each coin flip is a Bernoulli trial. Let  denote a random variable that represents the number of tails in  coin flips. For convenience purposes, we define 1 to be heads and 0 to be tails.",0,1,0,0,0,1,0,0
Dissecting the Gaussian Distribution,"We‚Äôll take a look at the multivariate normal distribution in a later section. For now, let‚Äôs derive the univariate case. One of the defining properties of data that are said to be normally distributed when the rate at which the frequencies decrement is proportional to its distance from the mean and the frequencies themselves. Concretely, this statement might be translated as We can separate the variables to achieve the following expression: Integrating both sides yields Let‚Äôs get rid of the logarithm by exponentiating both sides. That‚Äôs an ugly exponent. But we can make things look better by observing that the constant term  can be brought down as a coefficient, since where we make the substitution . Now, the task is to figure out what the constants  and  are. There is one constraint equation that we have not used yet: the integral of a probability distribution function must converge to 1. In other words, Now we run into a problem. Obviously we cannot calculate this integral as it is. Instead, we need to make a clever substitution.",0,1,0,0,0,1,0,0
"PyTorch, From Data to Modeling","One of the many nice things about PyTorch is the clean, intuitive API. PyTorch comes with good GPU support, and one of the main ways through which this can be done is by creating a  object. Because I am running this notebook on my MacBook Pro, which obviously does not come with Nvidia cuda-enabled graphics cards, the device is set as the CPU. Now, I can ‚Äúmove‚Äù tensors and models up to the GPU by doing something like and these statements would allow inference and training to occur within the GPU. And below are some constants I will be using in this notebook. Namely, we will run training for a total of 4 epochs, with a batch size of 32 and a learning rate of 0.001. Now that we have all the things we need, let‚Äôs jump into some data preparation and modeling. Another thing I love about PyTorch is the sheer ease with which you can preprocess data. PyTorch makes it incredibly easy to combine and stack multiple transforms to create custom transformations to be applied to the dataset.",0,0,0,0,0,0,1,0
Scikit-learn Pipelines with Titanic,"In today‚Äôs post, we will explore ways to build machine learning pipelines with Scikit-learn. A pipeline might sound like a big word, but it‚Äôs just a way of chaining different operations together in a convenient object, almost like a wrapper. This abstracts out a lot of individual operations that may otherwise appear fragmented across the script. I also personally think that Scikit-learn‚Äôs ML pipeline is very well-designed. So here is a brief introduction to ML pipelines is Scikit-learn. For the purposes of this tutorial, we will be using the classic Titanic dataset, otherwise known as the course material for Kaggle 101. I‚Äôm still trying to get my feet into Kaggle, so it is my hope that this tutorial will also help those trying to break into data science competitions. First, let‚Äôs import the modules and datasets needed for this tutorial. Scikit-learn is the go-to library for machine learning in Python. It contains not only data loading utilities, but also imputers, encoders, pipelines, transformers, and search tools we will need to find the optimum model for the task. Let‚Äôs load the dataset using . Let‚Äôs observe the data by calling .",0,0,1,0,0,0,0,0
Markov Chain and Chutes and Ladders,"Now that we have reviewed some underlying concepts, perhaps it is time to apply our knowledge to a concrete example. Before we move on, I recommend that you check out this post I have written on the Markov process, just so that you are comfortable with the material to be presented in this section. In this post, we turn our attention to the game of Chutes and Ladders, which is an example of a Markov process which demonstrates the property of ‚Äúmemorylessness.‚Äù This simply means that the progress of the game depends only on the players‚Äô current positions, not where they were or how they got there. A player might have ended up where they are by taking a ladder or by performing a series of regular dice rolls. In the end, however, all that matters is that the players eventually hit the hundredth cell. To perform a Markov chain analysis on the Chutes and Ladders game, it is first necessary to convert the information presented on the board as a stochastic matrix. How would we go about this process? Let‚Äôs assume that we start the game at the th cell by rolling a dice.",0,0,0,0,0,0,0,1
BLEU from scratch,"As the name implies, this penalizes short candidate translations, thus ensuring that only sufficiently long machine translations are ascribed a high score. Although this might seem confusing, the underlying mechanism is quite simple. The goal is to find the length of the reference sentence whose length is closest to that of the candidate translation in question. If the length of that reference sentence is larger than the candidate sentence, we apply some penalty; if the candidate sentence is longer, than we do not apply any penalization. The specific formula for penalization looks as follows: The brevity penalty term is multiplied to the n-gram modified precision. Therefore, a value of 1 means that no penalization is applied. Let‚Äôs perform a quick sanity check to see whether the brevity penalty function works as expected. Finally, it‚Äôs time to put all the pieces together. The formula for BLEU can be written as follows: First, some notation clarifications.  specifies the size of the bag of word, or the n-gram.  denotes the weight we will ascribe to the modified precision‚Äî‚Äîproduced under that -gram configuration. In other words, we calculate the weighted average of log precision, exponentiate that sum, and apply some brevity penalty.",0,0,0,1,0,0,1,0
Bayesian Linear Regression,"First, let‚Äôs begin by importing all necessary modules. Let‚Äôs randomly generate two hundred data points to serve as our toy data set for linear regression. Below is a simple visualization of the generated data points alongside the true line which we will seek to approximate through regression.  Now is the time to use the  library. In reality, all of the complicated math we combed through reduces to an extremely simple, single-line command shown below. Under the hood, the  using variations of random sampling to produce an approximate estimate for the predictive distribution. Now that the trace plot is ready, let‚Äôs see what the estimated vallues are like. We drop the first hundred sampled values may have been affected by a phenomena known as . Intuitively, the sampler needs some time to stabilize around the mean value, which is why the first few samples may contain more noise and provide information of lesser value compared to the rest.  We see two lines for each plot because the sampler ran over two chains by default.",0,0,0,0,0,0,0,1
Fisher Score and Information,"Things start to get a little more interesting (and more complicated) as we move onto the discussion of Fisher‚Äôs Information Matrix.  There are two sides of the coin that we will consider in this discussion: Fisher‚Äôs information as understood as the covariance matrix of the score function, and Fisher‚Äôs information as understood as a Hessian of the negative log likelihood. The gist of it is that there are two different ways of understanding the same concept, and that they provide intriguing complementary views on the information matrix. Before jumping into anything else, perhaps it‚Äôs instructive to review variance, covariance, and the covariance matrix. Here is a little cheat sheet to help you out (and my future self, who will most likely be reviewing this later as well). An intuitive way to think about variance is to consider it as a measure of how far samples are from the mean. We square that quantity to prevent negative values from canceling out positive ones. Covariance is just an extension of this concept applied to a comparison of two random variables instead of one. Here, we consider how two variables move in tandem.",0,0,0,0,0,1,0,0
Demystifying Entropy (And More),"In a typical scenario, we might have a true probability distribution  that we are trying to model, and our deep learning algorithm might produce some approximate probability distribution . We might evaluate the effectiveness of our model by calculating the distance between  and . Seen in this light, cross entropy can be interpreted as a target cost function to be minimized. Here is the equation that explains the relationship between entropy, cross entropy, and KL divergence. where  denotes cross entropy; ,entropy, and the last term, KL divergence. Now, let‚Äôs try to understand what each of them means. KL divergence has many interpretations. One possible definition of KL divergence is that it measures the average number of extra information content required to represent a message with distribution  instead of . In Machine Learning: A Probabilistic Perspective, Kevin P. Murphy describes KL divergence as follows: ‚Ä¶ the KL divergence is the average number of extra bits needed to encode the data, due to the fact that we used distribution  to encode the data instead of the true distribution . Put differently, KL divergence is the amount of information that is lost when  is used to approximate .",0,0,0,0,0,1,0,0
A Step Up with  Variational Autoencoders,"It was a long journey, but definitely worth it because it exposed us to many core concepts in deep learning and statistics. At the same time, I found it fascinating to see how a model could learn from a representation to generate numbers, as we saw in the very last figure. In a future post, we will look at generative adversarial networks, or GANs, which might be considered as the pinnacle of generative models and a successor to autoencoders. GANs resemble autoencoders in that it is also composed of two models. One core difference, however, is that in GANs, the two models are in a competing relationship, whereas in autoencoders, the encoder and the decoder play distinct, complementary roles. If any of this sounds exciting, make sure to check out the next post. I hope you enjoyed reading. Catch you up in the next one!.",0,0,0,0,1,0,1,0
Convolutional Neural Network with Keras,"This function is essentially wraps all the functions we have created previously, first by loading the data, training using that data, building a model, training a model, and calling on the  function to provide a visualization of the model‚Äôs learning curve. One minor tweak I used to spice up this function is the , which basically creates more images for the neural network to train on by slightly modifying existing images in the training set. These modifications involve shifting, zooming, and flipping. When we don‚Äôt have enough data to train our model on, using the  can be useful. Finally, let‚Äôs see how well our model performs!  The result shows that the model has learned decently well, with testing accuracy of approximately 80 percent. This is not the best result, but it is certainly not bad, especially given the fact that the images in the dataset, as we have seen above, are very pixelated and sometimes difficult for even humans to decipher and categorize. That‚Äôs all for today. It‚Äôs fascinating to see how CNNs are capable of perceiving images and categorizing them after appropriate training. But as we all know, the potential of neural networks far extends beyond image classificaiton.",0,0,0,0,1,0,1,0
Principal Component Analysis,"Of course, in this case,  is a square matrix of full rank; to apply dimension compression, we need to slice the first  entries of . At any rate, it is clear that PCA involves eigendecomposition of the covariance matrix. Eigendecomposition can only be applied to matrices of full rank. However, there is a more generalized method for non-square matrices, which is singular value decomposition. Here is a blueprint of SVD: Where  is a matrix containing the roots of the eigenvalues, with appropriate dimensional configurations to accommodate the shape of the original matrix. We cannot perform eigendecomposition on , which has no guarantee that it is square; however, SVD is definitely an option. Assume that  can be decomposed into , , and . Then the covariance matrix becomes And we end up in the same place as we did in (25). This is no surprise given that the derivation of SVD involves eigendecomposition. In this post, we took a deep dive into the mathematics behind principal component analysis. PCA is a very useful technique used in many areas of machine learning. One of the most common applications is to apply PCA to a high-dimensional dataset before applying a clustering algorithm.",0,0,0,0,0,1,0,1
Building Neural Network From Scratch,"Indeed, the main reason why I love the Keras functional API so much is that it is so easy to code and deploy a neural network model. However, when we write such models by depending on preexisting libraries, we sometimes grow oblivious to the intricacies the take place under the hood. It is my hope that reading and following along this post gave you a renewed sense of respect for the writers of such libraries, as well as the beauty of neural network models themselves. I hope you enjoyed reading this post. Catch you up in the next one!.",0,0,0,1,0,0,1,1
Building Neural Network From Scratch,"First, it is susceptible to arithematic overflow. Because computing the softmax function require exponentiation, it is likely for the computer to end up with very large numerical quantities, making calculations unstable. One way to solve this problem is by subtracting values from the exponent. As the calculation shows, adding or subtracting the same value from the exponent of both the numerator and the denominator creates no difference for the output of the softmax function. Therefore, we can prevent numbers from getting too large by subtracting some value from the exponent, thus yielding accurate results from stable computation. We can further improve the softmax function for the purposes of this tutorial by supporting batch computation. By batch, I simply mean multiple inputs in the form of arrays. The function shown above is only able to account for a single vector, presumably given as a list or a one-dimensional numpy array. The implementation below uses a loop to calculate the softmax output of each instance in a matrix input, then returns the result. Note that it also prevents arithematic overflow by subtracting the  value of the input array. Let‚Äôs test the improved softmax function with a two-dimensional array containing two instances.",0,0,0,1,0,0,1,1
Demystifying Entropy (And More),"In fact, it is no coincidence that the notion of randomness in information theory, a subfield of math that we are going to be dipping our toes in, borrowed the term ‚Äúentropy‚Äù to express randomness exhibited in data. Just like entropy was used to quantify randomness in the scientific phenomena, the notion of entropy is used in information theory to denote randomness in data. The origin of information entropy can be traced back to Claude Shannon‚Äôs paper published in 1948, titled ‚ÄúA Mathematical Theory of Communication.‚Äù While working at Bell Labs, Shannon was experimenting with methods to most efficiently encode and transmit data without loss of information. It is in this context that Shannon proposed the notion of entropy, which he roughly defined as the smallest possible size of lossless encoding of a message that can be achieved for transmission. Of course, there is a corresponding mathematical definition for entropy. But before we jump straight into entropy, let‚Äôs try to  develop some preliminary intuition on the concept of information, which is the building block of entropy.",0,0,0,0,0,1,0,0
An Introduction to Markov Chain Monte Carlo,"This goes to show just how useful and powerful Markov Chain Monte Carlo can be: even if a complicated likelihood function in high dimensional space, we would be able to use a similar sampling sequence to estimate the posterior. What‚Äôs even more fascinating about Markov Chain Monte Carlo is that, regardless of the value we start off with in the proposal distribution, we will eventually be able to approximate the posterior. This is due to the Markov chain part of MCMC: one of the most interesting properties of Markov chains is that, no matter where we start, we end up in the same . Together, these properties makes MCMC models like Metropolis-Hastings incredible useful for solving intractable problems. is a library made specifically for Bayesian analysis. Of course, it includes functions that implement Markov Chain Monte Carlo models. Although building the Metropolis-Hastings algorithm from scratch was a worthy challenge, we can‚Äôt build models from scratch every time we want to conduct from Bayesian analysis involving an intractable posterior, which is why packages like  always come in handy. With just a few lines of code, we can perform the exact same operation we performed above.",0,0,0,1,0,0,0,0
How lucky was I on my shift?,"This brings us to the next question: how many trials do we have? Here is where things get a bit more complicated‚Äîwe don‚Äôt really have trials! Notice that this situation is somewhat distinct from coin tosses, as we do not have a clearly defined ‚Äútrial‚Äù or an experiment. Nonetheless, we can approximate the distribution of this random variable by considering each ten-minute blocks as a unit for a single trial, i.e. if a call is received by the PMO between 22:00 and 22:10, then the trial is a success; if not, a failure. Blocking eight hours by ten minutes gives us a total of 48 trials. Because we assumed the average number of phone calls on a single shift to be 12, the probability of success . Let‚Äôs simulate this experiment  times.",0,1,0,0,0,1,0,0
BLEU from scratch,"Although this can sound like a lot, really it‚Äôs just putting all the pieces we have discussed so far together. Let‚Äôs take a look at the code implementation. The weighting happens in the  part within the generator expression within the  statement. In this case, we apply weighting across  that goes from  to . Now we‚Äôre done! Let‚Äôs test out our final implementation with  for  from 1 to 4, all weighted equally. The  package offers functions for BLEU calculation by default. For convenience purposes, let‚Äôs create a wrapper functions. This wrapping isn‚Äôt really necessary, but it abstracts out many of the preprocessing steps, such as applying . This is because the  BLEU calculation function expects tokenized input, whereas  and  are untokenized sentences. And we see that the result matches that derived from our own implementation! In this post, we took a look at BLEU, a very common way of evaluating the fluency of machine translations. Studying the implementation of this metric was a meaningful and interesting process, not only because BLEU itself is widely used, but also because the motivation and intuition behind its construction was easily understandable and came very naturally to me.",0,0,0,1,0,0,1,0
Logistic Regression Model from Scratch,"99 percent is not a bad estimate at all. One interesting question to consider is how much boost in accuracy we see with each epoch, i.e. what is the bang-per-buck of each iteration cycle? This is an important question to consider because gradient descent is computationally expensive; if we can train our model in just 10 epochs instead of 1000, why not choose the former? To answer this question, let‚Äôs plot accuracy against epoch. For fun, I added the learning parameter  as an argument to the  function as well. Let‚Äôs create a plot to see how accuracy changes over 200 epochs, given a learning rate of 0.1.  We see that accuracy spikes up on the first 20- epochs or so and quite quickly converges to about 90 percent. Past a certain threshold, the model seems to hover consistently at around the high 90s range, but accuracy still continues to increase ever so slightly with each epoch, though not as quickly as before.",0,0,1,1,0,0,0,0
Convex Combinations and MAP,"The second equality is due to proportionality, whereby  is independent of  and thus can be removed from the argmax operation. The fourth equality is due to the monotonically increasing nature of the logarithmic function. We always love using logarithms to convert products to sums, because sums are almost always easier to work with than products, especially when it comes to integration or differentiation. If any of these points sounds confusing or unfamiliar, I highly recommend that you check out my articles on MAP and MLE. To proceed, we have to derive concrete mathematical expressions for the log likelihood and the log prior. Recall the formula for the univariate Gaussian that describes our data: Then, from (1), we know that the likelihood function is simply going to be a product of the univariate Gaussian distribution. More specifically, the log likelihood is going to be the sum of the logs of the Gaussian probability distribution function. There is the log likelihood function! All we need now is the log prior. Recall that the prior is a normal distribution centered around mean  with standard deviation of 1.",0,0,0,0,0,1,0,0
"Beta, Bayes, and Multi-armed Bandits","More specifically, we‚Äôre interested in how we are going to use posterior probabilities to make decisions on which slot machine to pull on. This is where Thompson sampling comes in. In the simple, greedy frequentist approach, we would determine which bandit to pull on given our historical rate of success. If the first slot machine approximately yielded success 60 percent of the time, whereas the second one gave us 40, we would choose the first. Of course, this approach is limited by the fact that, perhaps we only pulled on the second machine 5 times and got only 2 success out of them, whereas we pulled on the first bandit a hundred times and got 60 successes. Maybe it turns out that the second bandit actually has a higher success rate, and that we were simply unlucky those five turns. Thompson sampling remedies this problem by suggesting a different approach: now that we have Bayesian posteriors, we can now directly sample from those posteriors to get an approximation of the parameter values.",0,1,0,0,0,1,0,0
A sneak peek at Bayesian Inference,"Conditional probability provides us with an interesting way to analyze given information. For instance, let  be the event that it rains tomorrow, and  be the event that it is cloudy at the present moment. Although we are no experts in climatology and weather forecast, common sense tells us that since with the additional piece of information that current weather conditions are cloudy, we are inclined to believe that it will likely rain tomorrow, or in the near future. Like this, conditional probability allows us to update our beliefs on uncertainty given new information, and we will see in the later sections that this is the core idea behind Bayesian inference. Let‚Äôs return back to the example of the potential patient with a flu. Shortly afterwards at the hospital, the the man was convinced by the doctor and decided to take the clinical test, the result of which was positive. We cannot assume that the man is sick, however, since the test has a rather high rate of false positives as we saw earlier.",0,1,0,0,0,1,0,0
PyTorch RNN from Scratch,"For example, Once we have a decoded string, we then need to convert it to a tensor so that the model can process it. This can first be done by constructing a  mapping, as shown below. We see that there are a total of 59 tokens in our character vocabulary. This includes spaces and punctuations, such as ` .,:;-‚Äò(num_char, 59)(59,)`. We can now build a function that accomplishes this task, as shown below: If you read the code carefully, you‚Äôll realize that the output tensor is of size , which is different from the explanation above. Well, the reason for that extra dimension is that we are using a batch size of 1 in this case. In PyTorch, RNN layers expect the input tensor to be of size . Since every name is going to have a different length, we don‚Äôt batch the inputs for simplicity purposes and simply use each input as a single batch. For a more detailed discussion, check out this forum discussion. Let‚Äôs quickly verify the output of the  function with a dummy input. Now we need to build a our dataset with all the preprocessing steps.",0,0,0,1,0,0,1,0
Naive Bayes Model From Scratch,"To achieve this objective, we can create a function that returns a dictionary, where the key represents the class and the values contain the entries of the data set. One way to implement this process is represented below in the  function. Let‚Äôs see if the function works properly by passing  as into its argument. Great! As expected,  is a dictionary whose keys represent the class and values contain entries corresponding to that class. Now that we have successfully separated out the data by class, its‚Äô time to write a function that will find the mean and standard deviation of each class data. This process is legitimate only because we assumed the data to be normally distributed‚Äîhence the name ‚ÄúGaussian naive Bayes.‚Äù Let‚Äôs quickly see this in code. The  function receives a data set as input and returns a nested list that contains the mean and standard deviation of each column of the data set. For example, if we pass the toy data set  into , the returned list will contain two lists: the first list element corresponding to the mean and standard deviation of , and the second list element, .",0,0,1,1,0,0,0,0
A sneak peek at Bayesian Inference,"Notice that we have expressed  in terms of  and . From a statistics point of view, all we have to do now is conduct a random survey of the population to see the percentage of the demographic infected with the flu. Let‚Äôs say that 15 percent of the population has been affected with this flu. Plugging in the relevant value yields Using Bayes‚Äô theorem, we are able to conclude that there is roughly a 24 percent chance that the man who tests positive on this examination is affected by the flu. That seems pretty low given the 90 percent accuracy of the test, doesn‚Äôt it? This ostensible discrepancy originates from the fact that the test has a substantial false positive of 50 percent, and also that the vast majority of the population is unaffected by the disease. This means that, if the entire population were to conduct this test, there would be more false positives than there would be true positives; hence the distortion in the value of the conditional probability.",0,1,0,0,0,1,0,0
PyTorch Tensor Basics,"In the case of , it suffices to consider the size as a list, meaning that square bracket syntax can be used. In the case of , indices can directly be passed into as an argument to index individual elements in the size tensor. These past few days, I‚Äôve spent a fair amount of time using PyTorch for basic modeling. One of the main takeaways from that experience is that an intuition on dimensionality and tensor operations in general is a huge plus. This gets especially important for things like batching. One very basic thing I learned‚Äìadmittedly perhaps too belatedly‚Äìis the difference between  and  as dimensions. Here is a concrete example. This creates a one-dimensional tensor, which is effectively a list. We can check the dimensions of this tensor by calling , which is very similar to how NumPy works. On the other hand, specifying the size as  results in a two-dimensional tensor. The simple, barely passing answer to the question of why  is two-dimension would be that it has double layered brackets.",0,0,0,0,0,0,1,0
Gaussian Mixture Models,"Before we get into the details of what the EM algorithm is, it‚Äôs perhaps best to provide a very brief overview of how the EM algorithm works. A very simple way to understand EM is to think of the Gibbs sampler. Simply put, Gibbs sampling is a way of approximating some joint distribution given conditional distributions. The underlying idea was to sample from one distribution and use that sampled result to in turn generate another sample from the next conditional distribution. One might visualize this as a chain of circular dependence: if we obtain a sample from past samples, the new sample can then be used to generate the next sample. Repeat this process until convergence, and we are done. Turns out that the Gibbs sampler can be considered a specific flavor of the EM method. Although I am still far away from fully understanding the inner-workings of the EM algorithm, the underlying idea is clear: given some sort of dependence relationship, much like we saw in the case of Gibbs sampling above, we can generate some sample in one iteration and use that sample in the next.",0,0,1,0,0,1,0,0
Traveling Salesman Problem with Genetic Algorithms,"The children of one parent will not all have identical genes: due to mutation, which occurs by chance, some will acquire even more superior features that puts them far ahead of their peers. Needless to say, such beneficiaries of positive mutation will survive and leave offspring, carrying onto the next generation. Those who experience adversarial mutation, on the other hand, will not be able to survive. In genetic algorithm engineering, we want to be able to simulate this process over an extended period of time without hard-coding our solution, such that the end result after hundred or thousands of generations will contain the optimal solution. Of course, we can‚Äôt let the computer do everything: we still have to implement mutational procedures that define an evolutionary process. But more on that later. First, let‚Äôs begin with the simple task of building a way of modeling a population. First, let‚Äôs define a class to represent the population. I decided to go with a class-based implementation to attach pieces of information about a specific generation of population to that class object.",0,0,0,1,0,0,0,0
Logistic Regression Model from Scratch,"It is also symmetrical around the point , which is why we can use 0.5 as a threshold for determining the class of a given data point. The logistic regression model uses the sigmoid function to generate predictions, but how exactly does it work? Recall that, in the case of linear regression, our goal was to determine the coefficients of some linear function, specifically Logistic regression is not so different from linear regression. In fact, we can borrow the same notation we used for linear regression to frame logistic regression as follows: In other words, logistic regression can be understood as a process in which our goal is to find the weight coefficients in the equation above the best describe the given data set. Unlike in linear regression, where the predicted value is computed simply by passing the data as arguments into a linear function, logistic regression outputs numbers between 0 and 1, making binary classification possible. However, there is certainly an element of linearity involved, which is part of the reason why both linear and logistic regression models fall under a larger family of models called generalized linear models.",0,0,1,1,0,0,0,0
Riemann Zeta and Prime Numbers,"However, as the range expands all the way up to 200, we see that the probability of two numbers being coprime drops precipitously, roughly converging to the value of  as we expect. Indeed, the dots seem to cluster around the gold line, which represents the value of the Zeta function evaluated at 2. But does this result generalize to cases where we sample more than just two numbers? In other words, if we sample  numbers, would the probability that the numbers be coprime approach ? Let‚Äôs find out. Given a range of  values, the  function returns a plot for each , which is effectively  in our mathematical notation. We see that, for the three values of  that were tested‚Äî2, 3, and 4‚Äîthe Zeta function seems to approximate the probability of relative primeness pretty well. Based on our earlier mathematical analysis, we would expect this convergence to get even better as we expand out the  range, with an upper bound that is greater than the current 200.  While jumping around in Wikipedia, I came across the Dirichlet Eta function, which is a slight variant of the Riemann Zeta function.",1,0,0,0,0,0,0,0
Recommendation Algorithm with SVD,"The plot is shown below:  With some alteration of the viewing angle, now we see through visualization that Movies 2 and 3 are close, as we had expected from the original ratings matrix . This is an interesting result, and it shows just how powerful singular value decomposition is at extracting important patterns from given data sets. Now that we understand what SVD does for us, it‚Äôs time to code our recommender function that uses distance calculation to output movie recommendations. In this post, we will be using the dot product as a means of determining distance, although other metrics such as Euclidean distance would suit our purposes as well. An advantage of using the dot product is that it is computationally less expensive and easy to achieve with code, as shown below. The  function recommends an  number of movies given that the user rated  highly. For example, let‚Äôs say some user really liked Movie 2 and is looking for two more movies that are similar to Movie 2. Then, we can simply call the function above by passing in appropriate arguments as follows.",0,0,0,1,0,0,0,1
"Newton-Raphson, Secant, and More","is simply some small value we use to decide when to stop the update; if the change in the value of the root is so small that it is not worth the extra compute, we should stop.  determines how many iterations we want to continue. If the algorithm is unable to find the root within  iterations, it likely means that the function provided does not have a root, or at the very least, the root is not discoverable via the algorithm. Lastly,  is a flag that determines whether we return the full update history or simply the last value in the iteration as a single value. One peculiarity that deserves attention is the  exception, which occurs in this case if the number of arguments passed into the function does not match. I added this   block to take into account the fact that the  method and other approximate derivative calculation methods such as  have differing numbers of parameters. Let‚Äôs see if this actually works by using the example we‚Äôve been reusing thus far, , or  and , both of which we have already defined and initialized above. The root seems to be around 2.7.",1,0,0,0,0,0,0,0
Dissecting the Gaussian Distribution,"At this point in time, one might point out that covariance is not really a multivariate concept since it is defined for only two variables, not three or more. Indeed, the expression  is mathematically incoherent. However, covariance can be  a multivariate metric since we can express the covariance of any pairs of random variables by constructing what is called the covariance matrix. Simply put, the covariance matrix is a matrix whose elements are the pairwise covariance of two random variables in a random vector. Before we get into the explanation, let‚Äôs take a look at the equation for the covariance matrix: where  and . This is the matrix analogue of the expression which is an alternate definition of variance. It is natural to wonder why we replaced the squared expression with  instead of  as we did earlier with the term in the exponent. The simplest answer that covariance is expressed as a matrix, not a scalar value. By dimensionality,  produces a single scalar value, whereas  creates a matrix of rank one.",0,1,0,0,0,1,0,0
A sneak peek at Bayesian Inference,"Recall the simplified version of Bayes‚Äô theorem for inference, given as follows: For the prior and the likelihood, we can now plug in the equations corresponding to each distribution to generate a new posterior. Notice that , which stands for data, is now given in the form  where  denotes the number of heads; , the total number of coin flips. Notice also that constants, such as the combinatorial expression or the reciprocal of the Beta function, can be dropped since we are only establishing a proportional relationship between the left and right hand sides. Further simplifications can be applied: But notice that this expression for the posterior can be encapsulated as a Beta distribution since Therefore, we started from a prior of  to end up with a posterior of . This is an incredibly powerful mechanism of updating our beliefs based on presented data. This process also proves that, as purported earlier, the Beta distribution is indeed a conjugate prior of a binomial likelihood function. Now, it‚Äôs time to put our theory to the test with concrete numbers. Suppose we start our experiment with completely no expectation as to the fairness of the coin.",0,1,0,0,0,1,0,0
A sneak peek at Bayesian Inference,"In concrete form, we can rewrite this as Additionally, we can rewrite the conditional probability  in terms of  and  according to the definition of conditional probability we observed earlier. Applying these alterations to (4) to rewrite  produces equation (5): This is the equation of Bayes‚Äô theorem. In simple language, Bayes‚Äô theorem tells us that the conditional probability of some subset  given  is equal to its relevant fraction within a weighted summation of the conditional probabilities  given . Although this equation may seem complicated at a glance, we can develop an intuition for this formula by reminding ourselves of the definition of conditional probabilities, as well as the fact that independent events can be expressed as a union of intersections. At the end of the day, Bayes‚Äô theorem provides a powerful tool through which we can calculate a conditional probability in terms of its reverse, i.e. calculate  by utilizing . Why is this important at all? Let‚Äôs return back to our example of the potential patient. Recall that the conditional probability of our interest was while the pieces of information we were provided were This is where Bayes‚Äô theorem comes in handy.",0,1,0,0,0,1,0,0
A PyTorch Primer,"Note that differentiation is at the core of backpropagation, which is why demonstrating what might seem like a relatively low-level portion of the API is valuable. Let‚Äôs begin our discussion by first importing the PyTorch module. It isn‚Äôt difficult to see that  is a scientific computing library, much like . For instance, we can easily create a matrice of ones as follows: The  is a parameter we pass into the function to tell PyTorch that this is something we want to keep track of later for something like backpropagation using gradient computation. In other words, it ‚Äútags‚Äù the object for PyTorch. Let‚Äôs make up some dummy operations to see how this tagging and gradient calculation works. Note that  performs element-wise multiplication, otherwise known as the dot product for vectors and the hadamard product for matrics and tensors. Let‚Äôs look at how autograd works. To initiate gradient computation, we need to first call  on the final result, in which case . Then, we can simply call  to tell PyTorch to calculate the gradient. Note that this works only because we ‚Äútagged‚Äù  with the  parameter.",0,0,0,0,0,0,1,0
Revisiting Basel with Fourier,"So there we have it, the integral representation of the Basel problem! Let‚Äôs look at another example, this time using a double integral representation. The motivation behind this approach is simple. This is a useful result, since it means that we can express the Basel problem as an integral of two different variables. Now, all we need is a summation expression before the integration. And now we are basically back to the Basel problem. Note that we can also use the interchange of integral and summation technique again to reexpress (19) as shown below. Notice that now we have a geometric series, which means that now we can also express this integral as Like this, there are countless ways of using integrals to express the Basel problem. This representation, in particular, could be understood as an integral over a unit square in the cartesian coordinate over a bivariate function, . In this post, we took a look at a new way of approaching the Basel problem using Fourier expansion. We also looked at some interesting integral representations of the Basel problem.",1,0,0,0,0,0,0,0
BLEU from scratch,"The BLEU score is based on a familar concept in machine learning: precision. Formally, precision is defined as where  and  stand for true and false positives, respectively. In the context of machine translations, we can consider positives as roughly corresponding to the notion of hits or matches. In other words, the positives are the bag of word n-grams we can construct from a given candidate translation. True positives are n-grams that appear in both the candidate and some reference translation; false positives are those that only appear in the candidate translation. Let‚Äôs use this intuition to build a simple precision-based metric. First, we need to create some n-grams from the candidate translation. Then, we iterate through the n-grams to see if they exist in any of the n-grams generated from reference translations. We count the total number of such hits, or true positives, and divide that quantity by the total number of n-grams produced from the candidate translation. Below are some candidate sentences and reference translations that we will be using as an example throughout this tutorial. Comparing  with , it is pretty clear that the former is the better translation.",0,0,0,1,0,0,1,0
Dissecting the Gaussian Distribution,"Although this relationship may not be immediately apparent, that the two expressions are identical can be seen by setting the random vector as and performing basic matrix vector multiplication operations. For the sake of brevity, this is left as an exercise for the reader. We now have all the pieces we need to complete the puzzle. Recall that we were trying to derive the probability density function of the multivariate Gaussian by building on top of the formula for the univariate Gaussian distribution. We finished at then moved onto a discussion of variance and covariance. Now that we understand that the covariance matrix is the analogue of variance, we can substitute  with , the covariate matrix. Instead of leaving  at the denominator, let‚Äôs use the fact that to rearrange the expression. This is another example of when the matrix-scalar parallel intuition can come in handy: the scalar multiplicative identity is 1, whereas the equivalent in matrix world is the identity matrix . Therefore, the reciprocal of a matrix can be interpreted as its inverse. From this observation, we can conclude that We are almost done, but not quite.",0,1,0,0,0,1,0,0
First Neural Network with Keras,"Let‚Äôs quickly check if the necessary adjustments were made by looking up the dimensions of  and , respectively. Looks like the data has been reshaped successfully. Now, it‚Äôs finally time to get into the nuts and bolts of a neural network. The simplest neural network is the  model, which means that every neuron in one layer is connected to all other neurons in the previous layer. Building a simple neural network is extremely easy in a high level API like Keras. The model below has 784 input nodes. The input layer is then connected to a hidden layer with 512 neurons, which is then connected to a second hidden layer with also 512 neurons. Note that the hidden layer uses the  function as its activation function. The dropout layers ensure that our model does not overfit to the data. The last output layer has 10 neurons, each corresponding to digits from 0 to 9. The activation fuction of this last layer is the softmax function, which allows us to interpret the final results as a categorical distribution. Let‚Äôs double check that the layers have been formed correctly as per our intended design.",0,0,1,0,1,0,1,0
Principal Component Analysis,"We don‚Äôt want to compress data in a haphazard fashion; instead, we want the compression scheme to be able to preserve the structure of the data as much as possible in its lower dimensional representation. From this, we can come up with the following equation: In other words, the goal is to find  that which minimizes the difference between the original data and the reconstructed data. Note that finding this optimal  amounts to finding  that most effectively compresses given data. Instead of the L2 norm, let‚Äôs consider the squared L2 norm for convenience purposes. Note that minimizing the L2 norm is equal to minimizing the squared L2 norm, so there is no semantic difference. By definition of vector transpose, we can now express the squared L2 norm versions of (3) as follows: where the second to last equality is due to the fact that  and  are both constants that denote the same value. Also,  the argument of the minimum is with respect to , we can omit the first term, which is purely in terms of . It‚Äôs time to take derivatives.",0,0,0,0,0,1,0,1
Dissecting the Gaussian Distribution,"Recall the the constant coefficient of the probability distribution originates from the fact that We have to make some adjustments to the constant coefficient since, in the context of the multivariate Gaussian, the integral translates into While it may not be apparent immediately, it is not hard to accept that the correcting coefficient in this case has to be as there are  layers of iterated integrals to evaluate for each  through . Instead of the matrix , we use its determinant  since we need the coefficient to be a constant, not a matrix term. We don‚Äôt go into much detail about the derivation of the constant term; the bottom line is that we want the integral of the probability distribution function over the relevant domain to converge to 1. If we put the pieces of the puzzle back together, we finally have the probability distribution of the multivariate Gaussian distribution: To develop a better intuition for the multivariate Gaussian, let‚Äôs take a look at a case of a simple 2-dimensional Gaussian random vector with a diagonal covariance matrix. This example was borrowed from this source.",0,1,0,0,0,1,0,0
Riemann Zeta and Prime Numbers,"We performed this factorization in the previous post while introducing Euler‚Äôs infinite product representation of the Zeta function. Let‚Äôs go through this step again, as it is pivotal for our segue into the topic of prime numbers and probability. The idea is that, much like we multiply the ratio to a geometric sequence to calculate its sum, we can multiply terms to the Zeta function to factor out their multiples. For instance, let‚Äôs consider the case when we multiply  to the Zeta function. Since the Zeta function itself is an infinite series, we can subtract this result from the original Zeta function. This effectively filters or sieves out all terms whose denominator is a multiple of 2, effectively leaving only the odd terms. Concretely, If we repeat this process for all prime numbers, we will eventually be left with the following: where  denotes the set of all prime numbers.",1,0,0,0,0,0,0,0
Naive Bayes Model From Scratch,"Here,  refers to a single instance, represented as a vector with  entries; , the corresponding label or class for that instance. Note that  is not a feature matrix, but a single instance. More concretely, Of course,  is just a scalar value. This characterization is very apt in the context of machine learning. The underlying idea is that, given some data with  feature columns, we can derive a probability distribution for the label for that intance. The naive assumption that the naive Bayes classifier makes‚Äînow you can guess where that name comes from‚Äîis that each of the  variables in the instance vector are independent of one another. In other words, knowing a value for one of the features does not provide us with any information about the values for the other  feature columns. Combining this assumption of independence with Bayes‚Äô theorem, we can now restate (1) as follows: Pretty straight forward. We know that the demominator, which often goes by the name ‚Äúevidence‚Äù in Bayesian inference, is merely a normalizing factor to ensure that the posterior distribution integrates to 1.",0,0,1,1,0,0,0,0
Gaussian Process Regression,"However, we would expect them to look even smoother had we augmented the dimensions of the test data to 100 dimensions or more. Next, we need a function from which we generate dummy train data. For the purposes of demonstration, let‚Äôs choose a simple sine function. Let‚Äôs generate 15 training data points from this function. Note that we are performing a noiseless GP regression, since we did not add any Gaussian noise to . However, we already know how to perform GP with noise, as we discussed earlier how noise only affects the diagonal entries of the kernel. Now it‚Äôs time to model the posterior. Recall that the posterior distribution can be expressed as If we use  or  functions to calculate the inverse of the kernel matrix components, out life would admittedly be easy. However, using inversion is not only typically costly, but also prone to inaccuracy. Therefore, we instead opt for a safer method, namely using . In doing so, we will also be introducing some intermediate variables for clarity. Let‚Äôs begin with the expression for the posterior mean , which is .",0,0,1,1,0,0,0,0
Dissecting LSTMs,"Notice that we use a  activation instead of a sigmoid, since the aim of (3) is not to produce a filter with sparse entries, but rather to generate substance, or potential information to be stored in memory. This is more in line with the classic vanilla neural network architecture we are familiar with. Now, we can finally glue the pieces together to understand (4): we enforce forgetfulness, then supply the cell state with new information. This is now the updated cell state, which gets passed onto the next sequence as new inputs are fed into the LSTM. So far, we have only looked at the recurrent features of LSTM; in other words, how it uses information from the past to update its knowledge in the present at time . However, we haven‚Äôt yet discussed the most important part of any neural network: generating output. Obviously, all that hassle of forgetting and updating the cell state would be utterly meaningless if the cell state is not used to generate output. The whole purpose of maintaining a cell state, therefore, is to imitate long and short-term memory of the brain to generate some output.",0,0,0,0,0,0,1,0
Natural Gradient and Fisher,"In this case, the Lagrangian would be This immediately follows from using the constraint condition. To make progress, let‚Äôs use Taylor approximation again, both on the term for the loss function and the KL divergence. The good news is that we have already derived the expression for the latter. Noting the fact that there are several constants in this expression, we can simplify this into To minimize this expression, we set its gradient equal to zero. Note that we are deriving with respect to . Therefore, We are finally done with our derivation. This equation tells us that the direction of steepest descent is defined by the inverse of the Fisher matrix multiplied by the gradient of the loss function, up to some constant scaling factor. This is different from the vanilla batch gradient descent we are familiar with, which was simply defined as Although the difference seems very minor‚Äîafter all, all that was changed was the addition of Fisher‚Äôs matrix‚Äîyet the underlying concept, as we have seen in the derivation, is entirely different. This was definitely a math-heavy post.",0,0,1,0,0,1,0,0
Logistic Regression Model from Scratch,"This is exactly what we need: a way of quantifying how different the actual and predicted class labels are! Recall the formula for cross entropy: We can consider class labels as a Bernoulli distribution where data that belongs to class 1 has probability 1 of belonging to that class 1 and probability 0 of belonging to class 0, and vice versa for observations in class 0. The logistic regression model will output a Bernoulli distribution, such as , which means that the given input has a 60 percent chance of belonging to class 1; 40 percent to class 0. Applying this to (3), we get: And that is the loss function we will use for logistic regression! The reason why we have two terms, one involving just  and another involving  is due to the structure of the Bernoulli distribution, which by definition can be written as Now that we have a loss function to work with, let‚Äôs build a function that computes cross entropy loss given  and  using (4). The  function returns the average cross entropy over all input data.",0,0,1,1,0,0,0,0
Gamma and Zeta,"The Gamma function is written as And we all know that the Gamma function can be seen as an interpolation of the factorial function, since for non-negative integers, the following relationship stands: Note that there is also a variant of the Gamma function, known as the Pi function, which has somewhat of a nicer form: To the mathematically uninformed self, the Pi function seems a lot more tractable and intuitive. Nonetheless, the prevailing function is Euler‚Äôs Gamma function instead of Gauss‚Äôs Pi function. The reasons for Gamma‚Äôs dominance over Pi is discussed extensively in this math overflow thread. At any rate, it‚Äôs both interesting and yet also unsurprising to see that these two functions are brainchildren of Gauss and Euler, two names that arguably appear the most in the world of math. The Riemann zeta function is perhaps one of the most famous functions in the world of analysis. It is also sometimes referred to as the Euler-Riemann zeta function, but at this point, prefixing something with ‚ÄúEuler‚Äù loses significance since just about everything in mathematics seems to have some Euler prefix in front of it.",1,0,0,0,0,0,0,0
Naive Bayes Model From Scratch,"Welcome to part three of the ‚Äúfrom scratch‚Äù series where we implement machine learning models from the ground up. The model we will implement today, called the naive Bayes classifier, is an interesting model that nicely builds on top of the Bayesian mindset we developed in the previous post on Markov Chain Monte Carlo. Much like the logistic regression model, naive Bayes can be used to solve classification tasks, as opposed to regression in which case the goal is to predict a continuous variable. The main difference between logistic regression and naive Bayes is that naive Bayes is built on a probabilistic model instead of an optimization model such as graident descent. Hence, implementing naive Bayes is somewhat easier from a programming point of view. Enough of the prologue, let‚Äôs cut to the chase. To understand naive Bayes, we need not look further than Bayes‚Äôs theorem, which is probably the single most referenced theorem on this blog so far. I won‚Äôt explain this much since we have already seen it so many times, but presented below is the familar formula for reference and readability‚Äôs sake. Very standard, perhaps with the exception of some minor notation.",0,0,1,1,0,0,0,0
Fisher Score and Information,"The gist of it is simple: if we consider the Hessian or the second derivative to be indicative of the curvature of the likelihood function, the variance of our estimate of the optimal parameter  would be larger if the curvature was smaller, and vice versa. In a sense, the larger the value of the information matrix, the more certain we are about the estimate, and thus the more information we know about the parameter. I hope you enjoyed reading this post. Catch you up on another post, most likely on the Leibniz rule, then natural gradient descent!.",0,0,0,0,0,1,0,0
A sneak peek at Bayesian Inference,"Recall that some of the other functions we have looked at, namely binomial, Poisson, Gamma, or exponential are all defined within the unclosed interval , making it unsuitable for our purposes. The Beta distribution nicely satisfies this criterion. The Beta distribution is somewhat similar to the Gamma distribution we analyzed earlier in that it is defined by two shape parameters,  and . Concretely, the probability density function of the Beta distribution goes as follows: The coefficient, expressed in terms of a fraction of Gamma functions, provides a definition for the Beta function. The derivation of the Beta distribution and its apparent relationship with the Gamma function deserves an entirely separate post devoted specifically to the said topic. For the purpose of this post, an intuitive understanding of this distribution and function will suffice. A salient feature of the Beta distribution that is domain is contained within . This means that, application-wise, the Beta distribution is most often used to model a distribution of probabilities, say the batting average of a baseball player as shown in this post.",0,1,0,0,0,1,0,0
Traveling Salesman Problem with Genetic Algorithms,"The gist of it is that we run a simulation of population selection and mutation over  generations. The key part is  and . Basically, we obtain the children from the mutation and pass it over as the population bag of the next generation in the  constructor. Now let‚Äôs test it on our TSP example over 20 generations. As generations pass, the fitness score seems to improve, but not by a lot. Let‚Äôs try running this over an extended period of time, namely 100 generations. For clarity, let‚Äôs also plot the progress of our genetic algorithm by setting  to .  After something like 30 iterations, it seems like algorithm has converged to the minimum, sitting at around 86.25. Apparently, the best way to travel the cities is to go in the order of . But this was more of a contrived example. We want to see if this algorithm can scale. So let‚Äôs write some functions to generate city coordinates and corresponding adjacency matrices. generates  number of random city coordinates in the form of a numpy array. Now, we need some functions that will create an adjacency matrix based on the city coordinates.",0,0,0,1,0,0,0,0
Natural Gradient and Fisher,") Continuing our discussion of KL divergence, let‚Äôs try to expand the divergence term using Taylor approximation. Here,  is small distance in the distribution space defined by KL divergence as the distance metric. This can be a bit obfuscating notation-wise because of the use of  as our variable, assuming  as a fixed constant, and evaluating the gradient and the Hessian at the point where   since we want to approximate the value of KL divergence at the point where where . But really, all that is happening here is that in order to approximate KL divergence, we‚Äôre starting at the point where , and using the slope and curvature obtained at that point to approximate the value of KL divergence at distance  away. Picturing the simpler univariate situation in the Cartesian plane might help. The bottom line is that the KL divergence is effectively defined by the Fisher matrix. The implication of this is that now, the gradient descent algorithm is subject to the constraint where  is some constant. Now, the update rule would be To solve for the argument minima operation, we will resort to the classic method for optimization: Lagrangians.",0,0,1,0,0,1,0,0
Bayesian Linear Regression,"The reason why the normal equation method is unable to capture this uncertainty is that‚Äîas you might recall from the derivation of the formula for vanilla linear regression‚Äîthe tools we used did not involve any probabilistic modeling. Recall that we used only linear algebra and matrix calculus to derive the model for vanilla linear regression. Bayesian linear regression is more complicated in that it involves computations with probability density functions, but the end result is of course more rewarding. That‚Äôs it for today! I hope you enjoyed reading this post. Catch you up in the next one.",0,0,0,0,0,0,0,1
A Step Up with  Variational Autoencoders,"The goal here is to sample a random vector in the latent space from the distribution specified by the two parameters, mean and log variance. The sampling process can be expressed as follows: where  denotes the mean, corresponding to ,  denotes a tensor of random numbers sampled from the standard normal distribution, and  denotes the standard deviation (we will see how this is related to  in just a moment). Essentially, the goal here is to use a resampling technique such that we can sample from a standard normal distribution centered around mean 0 and a standard deviation of 1, but consequentially sample from a distribution of  living in the latent space. If you are wondering how (1) translates to the return statement, then the following equation might resolve your curiosity. This is the promised elaboration on the relationship between log variance and standard deviation: Therefore, multiplying 0.5 is just a simple algebraic manipulation to morph log variance to standard deviation. The reason why we use log variance instead of just variance or standard deviation is to ensure numerical stability in computation.",0,0,0,0,1,0,1,0
"0.5!: Gamma Function, Distribution, and More","After some thinking, we can convince ourselves that unraveling the sigma results in a chain reaction wherein adjacent terms nicely cancel one another, ultimately collapsing into a single term, which also happens to be the first term of the expression: Recalling that , we can rewrite the expression as follows: Notice the structural identity between the form we have derived and the equation of the Gamma distribution function introduced above, with parameters , , and . In the language of Poisson, these variables translate to , , and , respectively. To develop and intuition of the Gamma distribution, let‚Äôs quickly plot the function. If we determine the values for the parameters  and , the term  reduces to some constant, say , allowing us to reduce the PDF into the following form: This simplified expression reveals the underlying structure behind the Gamma distribution: a fight for dominance between two terms, one that grows polynomially and the other that decays exponentially. Plotting the Gamma distribution for different values of  and  gives us a better idea of what this relationship entails. Executing this code block produces the following figure.",0,1,0,0,0,1,0,0
Logistic Regression Model from Scratch,"For notational convenience, let‚Äôs denote the gradient as a derivative: The gradient in (7) can be broken down into distinct components via the chain rule: where So the task of calculating the gradient now boils down to finding the derivative for each of the terms shown in (8). Let‚Äôs start with the easiet one, the last term, which is the derivative of  with respect to . From (9), we can conclude that The next in line is the derivative of the sigmoid function, which goes as follows: Now we are almost there. The last piece of the puzzle is computing the first term in (8), the derivative of the cross entropy function. Putting this all together into (8), we get: Voila! We have derived an expression for the gradient of the cross entropy loss function. There is one more tiny little step we have to make to concretize this equation, and that is to consider the average of the total gradient, since (13) as it stands applies to only one data observation. Granted, this derivation is not meant to be a rigorous demonstration of mathematical proof, because we glossed over some details concerning matrix transpose, dot products, and dimensionality.",0,0,1,1,0,0,0,0
Natural Gradient and Fisher,"This conclusion tells us that the curvature of KL divergence is defined by Fisher‚Äôs matrix. In hindsight, this is not such a surprising result given that the KL divergence literally had a term for expected log likelihood. Applying the Leibniz rule twice to move the derivative into the integral, we quickly end up with Fisher‚Äôs matrix. At this point, you might be wondering about the implications of this conclusion. It‚Äôs great that KL divergence and the Fisher matrix are closely related via the Hessian, but what implication does it have for the gradient descent algorithm in distribution space? To answer this question, we first need to perform a quick multivariate second order Taylor expansion on KL divergence. Recall that the simple, generic case of multivariate Taylor expansion looks as follows: This is simply a generalization of the familiar univariate Taylor series approximation we saw earlier. (In most cases, we stop at the second order because computing the third order in the multivariate case requires us to obtain a three-dimensional symmetric tensor. I might write a post on this topic in the future, as I only recently figured this out and found it very amusing.",0,0,1,0,0,1,0,0
Riemann Zeta and Prime Numbers,"To cut to the chase, we get And there we have it, the relationship between the Dirichlet Eta function and the Riemann Zeta function! There are many more interesting things about the Eta function, such as its convergence property, but that is a topic for another post. In this post, we developed an intuition for the implications of the Riemann Zeta function from the perspective of relative primeness and probability. The Zeta function is one of those things in mathematics that appear so simple on the surface, yet is so wonderfully complex and convoluted in the inside. Although we haven‚Äôt discussed these other intricacies of the Riemann Zeta function‚Äîin particular, its relationship to the Riemann hypothesis, which states that the Zeta function has roots at negative even integers and complex numbers whose real part is ‚Äîbut the approach we took here with prime numbers are probabilities is fascinating in its own right, providing ample food for thought. Many thanks to Chris Henson again for the code and the post. It‚Äôs always a lot of fun to mesh mathematics with programming, and I think this is why I enjoyed writing this post.",1,0,0,0,0,0,0,0
The Magic of Euler‚Äôs Identity,"The position vector of a circular motion points outward from the center of rotation, and the velocity vector is tangential to the circular trajectory. The implication of this observation is that the trajectory expressed by the vector  is essentially that of a circle, with respect to time . More specifically, we see that at , , or , which means that the circle necessarily passes through the point  on the complex plane expressed as an Argand graph. From this analysis, we can learn that the trajectory is not just any circle, but a unit circle centered around the origin. But there‚Äôs even more! Recall that the velocity vector of the trajectory is a 90-degree rotation of the position vector, i.e. , . Earlier, we concluded that the trajectory expressed by the vector  is a unit circle, which necessarily means that  for all values of . Then, syllogism tells us that  is also one, i.e. the particle on the trajectory moves at unit speed along the unit circle! Now we finally have a full visualization of the position vector. The blue arrow represents the position vector at ; green, the velocity vector also at .",1,0,0,0,0,0,0,0
Scikit-learn Pipelines with Titanic,"Let‚Äôs top this discussion off with a look at the confusion matrix, which is another way of compactly encoding various pieces of information for model evaluation, namely true positives, true negatives, false positives, and false negatives. Note that precision and recall are all metrics that are computed using TP, TN, FP and FN as parameters. The confusion matrix shows that our model performs well at determining the death and survival of those passengers who actually died, but performs rather poorly on those who lived. Analyses like these cannot be obtained simply by looking at accuracy, which is why plotting the confusion matrix is always a good idea to get a sense of the model‚Äôs performance.  Although the titanic dataset is considered trite, much like MNIST is in the context of DL, I still think there is a lot to be learned. Even simple ML projects like these have infinite spaces and options for exploration and experimentation. I hope to go through these classic datasets and competitions to glean insight from excellent public kernels, just like this Kaggle kernel which I referenced extensively to write this tutorial.",0,0,1,0,0,0,0,0
On Expectations and Integrals,"There is an even more general interpretation of integrals called the Lebesgue integral, but we won‚Äôt get into that here. First, let‚Äôs take a look at the definition. The definition of the integral is actually a lot simpler than what one might imagine. Here,  is a value that falls within the interval . In short, we divide the interval of integration  into  infinitesimal pieces. Imagine this process as being similar to what we learn in Calculus 101, where integrals are visualized as an infinite sum of skinny rectangles as the limit approaches zero. Essentially, we are doing the same thing, except that now, the base of each rectangle is defined as the difference between  and  instead of  and  as is the case with the Riemann integral. Another way to look at this is to consider the integral as calculating the area beneath the curve represented by the parameterization . This connection becomes a bit more apparent if we consider the fact that the Riemann integral is calculating the area beneath the curve represented by . In other words, the Riemann-Stieltjes integral can be seen as dealing with a change of variables.",0,0,0,0,0,1,0,0
Traveling Salesman Problem with Genetic Algorithms,"The traveling salesman problem (TSP) is a famous problem in computer science. The problem might be summarized as follows: imagine you are a salesperson who needs to visit some number of cities. Because you want to minimize costs spent on traveling (or maybe you‚Äôre just lazy like I am), you want to find out the most efficient route, one that will require the least amount of traveling. You are given a coordinate of the cities to visit on a map. How can you find the optimal route? The most obvious solution would be the brute force method, where you consider all the different possibilities, calculate the estimated distance for each, and choose the one that is the shortest path. While this is a definite way to solve TSP, the issue with this approach is that it requires a lot of compute‚Äîthe runtime of this brute force algorithm would be , which is just utterly terrible. In this post, we will consider a more interesting way to approach TSP: genetic algorithms.",0,0,0,1,0,0,0,0
Scikit-learn Pipelines with Titanic,"It‚Äôs worth noting that the algorithm decided that the  is superior to , which in my opinion is no surprise. However, it is interesting to see our intuition being vindicated in this fashion nonetheless. Now it‚Äôs time for us to evaluate the model. While there are many different metrics we can use, in binary classification, we can look at things like accuracy, precision, recall, and the F1 score. Let‚Äôs take a look. The pipeline seems to be working correctly as expected, preprocessing and imputing the data as it was fit on the training data, then generating predictions using the model with optimized parameters. Let‚Äôs see how well our model is doing. One useful function in  is the  function, which, as the name implies, gives us a comprehensive report of many widely-used metrics, such as precision, recall, and the F1 score. The report suggests that the accuracy of our model on the test dataset is about 84 percent. We can manually verify this claim by calculating the accuracy ourselves using boolean indexing.",0,0,1,0,0,0,0,0
An Introduction to Markov Chain Monte Carlo,"The precise jump condition for the sampler goes as follows: where This simply means that we accept the prorposed pararmeter if the quantity calculated in (7) is larger than a random number between 0 and 1 sampled from a uniform distribution. This is why MCMC models involve a form of random walk‚Äîwhile leaving room for somewhat unlikely parameters to be selected, the model samples relatively more from regions of high posterior probability. Now that we have some understanding of how Markov Chain Monte Carlo and the Metropolis-Hastings algorithm, let‚Äôs implement the MCMC sampler in Python. As per convention, listed below are the dependencies required for this demonstration. Let‚Äôs start by generating some toy data for our analysis. It‚Äôs always a good idea to plot the data to get a sense of its shape.  The data looks roughly normal. This is because we created the toy data using the  function, which generates random numbers from a normal distribution centered around 0. The task for this tutorial, given this data, is going to be estimating the mean of the posterior distribution, assuming we know its standard deviation to be 1.",0,0,0,1,0,0,0,0
Recommendation Algorithm with SVD,"The rule of thumb is that the smaller the eigenvalue, the lesser contribution it has on expressing data on . In other words, we can obtain an approximation of  by extracting the first few columns and rows of each factor. For example, This may seem like a very clumsy way of approximating . However, this is because the toy matrix we dealt with was a mere two-by-three matrix with only two non-zero entries in the diagonal of . Imagine performing the same analysis on a much larger matrix, from which we extract  number of non-trivial entries of . On scale, singular value decomposition becomes more powerful, as it allows large amounts of data to be processed in managable bites. This is more than enough theory on SVD. Now is finally the time to jump into building our recommendation model with singular value decomposition. In this section, we will generate some random data, namely the ratings matrix. The row of the ratings matrix can be interpreted as users; the columns, movies. In other words,  denotes the ratings the th user gave for the th movie. The example we will use was borrowed from this post by Zacharia Miller.",0,0,0,1,0,0,0,1
PyTorch Tensor Basics,"To change  itself, we could do Or even better, we can use , which is an in-place operation by design. Notice that, unlike when we called ,  changes the tensor itself, in-place. In older versions of PyTorch,  existed as a non in-place operator. However, in newer versions of PyTorch, this is no longer the case, and PyTorch will complain with an informative deprecation error message. Note that  is not an in-place operator, meaning its behavior will largely be identical to that of . PyTorch keeps an internal convention when it comes to differentiating between in-place and copy operations. Namely, functions that end with a  are in-place operators. For example, one can add a number to a tensor in-place via , as opposed to the normal , which does not happen in-place. Observe that the addition is not reflected in , indicating that no operations happened in-place. , however, achieves the result without copying and creating a new tensor into memory. is another common function that is used to resize tensors. It has been part of the PyTorch API for quite a long time before  was introduced.",0,0,0,0,0,0,1,0
How lucky was I on my shift?,"So to answer the title of this post: about 2 in every 100 days, I will have a chill shift where I get lesser than five calls in eight hours. But all of this aside, I should make it abundantly clear in this concluding section that I like my job, and that I love answering calls on the phone. I can assure you that no sarcasm is involved. If you insist on calculating this integral by hand, I leave that for a mental exercise for the keen reader. Or even better, you can tune back into this blog a few days later to check out my post on the gamma function, where we explore the world of distributions beyond the binomial and the Poisson.",0,1,0,0,0,1,0,0
GAN in PyTorch,"In this blog post, we will be revisiting GANs, or general adversarial networks. This isn‚Äôt the first time we‚Äôve seen GANs on this blog: we‚Äôve implemented GANs in Keras, and we have also looked at the mathematics behind GANs. Well, this is somewhat of a repeat of what we‚Äôve done, since all we‚Äôre doing here is reimplementing GANs using PyTorch, but I still think it‚Äôs worth a revisit. As always, we start by importing the necessary modules in PyTorch, as well as other libraries. Although we‚Äôll be using CPU instead of GPUs, it‚Äôs always good to have a  object set up so that we can utilize GPUs should we run this script or notebook in a different environment. We‚Äôre not going to do anything too fancy (and I learned from experience that vanilla GANs are incredibly hard to train and can produce artifact-ridden results). Let‚Äôs keep in simple this time and try to implement a GAN that generates MNIST images. Below are the configurations we will use for our GAN. We can define a simple transformation that converts images to tensors, then applies a standard normalization procedure for easier training.",0,0,0,0,0,0,1,0
Word2vec from Scratch,"Note that actual models tend to use much higher dimensions, most commonly 300, but for our purposes this is not necessary. Let‚Äôs begin with forward propagation. Coding the forward propagation process simply amounts to transcribing the three matrix multiplication equations in (6) into NumPy code. For backpropagation, we will need all the intermediate variables, so we hold them in a dictionary called . However, if we simply want the final prediction vectors only, not the cache, we set  to . This is just a little auxiliary feature to make things slightly easier later. We also have to implement the  function we used above. Note that this function receives a matrix as input, not a vector, so we will need to slightly tune things up a bit using a simple loop. At this point, we are done with implementing the forward pass. However, before we move on, it‚Äôs always a good idea to check the dimensionality of the matrices, as this will provide us with some useful intuition while coding backward propagation later on.",0,0,0,1,0,0,1,0
Fourier Series,"This follows from the definition of a dot product, which has to do with cosines. With a stretch of imagination, we can extend this definition of orthogonality to the context of functions, not just vectors. For vectors, a dot product entails summing the element-wise products of each component. Functions don‚Äôt quite have a clearly defined, discrete component. Therefore, instead of simply adding, we integrate over a given domain. For example, The same applies to cosines and sines: where  and  can be any integer. In other words, cosine functions of different frequencies are orthogonal to each other, as are cosines are with sines! Now, why is orthogonality relevant at all for understanding the Fourier series? It‚Äôs time to sit back and let the magic unfold when we multiply  to (1) and integrate the entire expression. If we divide both sides of (5) by , you will realize that we have derived an expression for the constant corresponding to the  expansion term: The key takeaway here is this: by exploiting orthogonality, we can knock out every term but one, the very term that we multiplied to the expansion.",1,0,0,0,0,1,0,0
Building Neural Network From Scratch,"If you have read my previous post on the Keras functional API, you might recall that we used softmax and ReLU for certain dense layers. Back then, we considered them to be a blackbox without necessarily taking a look at what they do. Let‚Äôs explore the details and get our hands dirty today. Mathematically speaking, the softmax function is a function that takes a vector as input and outputs a vector of equal length. Concretely, where Although the formula may appear complex, the softmax function is a lot simpler than it seems. First, note that all  entries of the returned vector  add up to 1. From here, it is possible to see that the softmax function is useful for ascribing the probability that a sample belongs to one of  classes: the -th element of  would indicate the probability of the sample belonging to the -th class. Put another way, the index of the largest entry in  is the class label number that is most probable. Implementing the softmax function is extremely easy thanks to the vectorized computation made possible through . Presented below is one possible implementation of the softmax function in Python. This particular implementation, however, poses two problems.",0,0,0,1,0,0,1,1
Dissecting LSTMs,"In that case, calculating the Hadamard product will also result in a value of an entry very close to 0. Given the interpretation that , also known as the cell state, is an artificial way of simulating long-term memory, we can see how having zeros is similar to forgetfulness: a zero entry effectively means that the network deemed a particular piece of information as obsolete and decided to forget it in favor of accepting new information. In short, the sigmoid activation and the Hadamard product form the basis of LSTM‚Äôs forget gate. By now, it should be apparent why we use sigmoid activations: instead of causing divergence with something like ReLU, we want to deliberately saturate and cause the network to produce some ‚Äúvanishing‚Äù values. But if our LSTM network only keeps forgetting, obviously this is going to be problematic. Instead, we also want to update the cell state using the new input values. Let‚Äôs take a look at the cell state equation again: Previously when discussing the forget gate, we focused only on the first term.",0,0,0,0,0,0,1,0
"0.5!: Gamma Function, Distribution, and More","Let‚Äôs jump right into it by analyzing the Gamma function, specifically Euler‚Äôs integral of the second kind: At a glance, it is not immediately clear as to why this integral is an interpolation of the factorial function. However, if we try to evaluate this expression through integration by parts, the picture becomes clearer: Notice that the first term evaluates to 0. Moreover, the integral term can be expressed in terms of the Gamma function since Applying all the simplifications leave us with Notice that this is a recursive representation of the factorial, since we can further unravel  using the same definition. In other words, So it is now clear that the Gamma function is indeed an interpolation of the factorial function. But the Gamma function deserves a bit more attention and analysis than the simple evaluation we have performed above. Specifically, I want to introduce a few more alternative forms of expressing and deriving the Gamma function. There are many ways to approach this subject, and it would be impossible to exhaust through the entire list of possible representations. For the purposes of this post, we look at two forms of the Gamma function I find intriguing.",0,1,0,0,0,1,0,0
"Newton-Raphson, Secant, and More","Moreover, the list index representation is unable to represent polynomials that include terms whose powers are not positive integers. For these reasons, we will need some other methods of calculating derivatives as well. Hence the motivation for approximation methods, outlined in the section below. If you probe the deepest depths of your memory, somewhere you will recall the following equation, which I‚Äôm sure all of us saw in some high school calculus class: This equation, commonly known as the definition of a derivative, is also known as the forward divided difference formula of calculating derivatives. There is another variant, known as the backward divided difference formula: (1) and (2) are almost nearly identical, but the difference lies in which term is subtracted from who. In (1), we go an infinitesimal step forward‚Äîhence the ‚Äîand subtract the value at the point of approximation, . In (2), we go backwards, which is why we get . As  approaches 0, (1) and (2) asymptotically gives us identical results. Below is a Python variant of the backward divided difference formula.",1,0,0,0,0,0,0,0
Demystifying Entropy (And More),"This yields Recall that the definition of entropy goes as Plugging in this definition to (11) yields the simplified definition of cross entropy: If KL divergence represents the average amount of additional information needed to represent an event with  instead of , cross entropy tells us the average amount of total information needed to represent a stochastic event with  instead of . This is why cross entropy is a sum of the entropy of the distribution  plus the KL divergence between  and . Instead of dwelling in the theoretical realm regurgitating different definitions and interpretations of cross entropy and KL divergence, let‚Äôs take a look at a realistic example to gain a better grasp of these concepts. Say we have constructed a neural network to solve a task, such as MNIST hand-written digit classification. Let‚Äôs say we have fed our neural network an image corresponding to the number 2. In that case, the true distribution that we are trying to model, represented in vector form, will be  as shown below. The  statement is there to make sure that the probabilities sum up to 1. Let‚Äôs assume that our neural network made the following prediction about image.",0,0,0,0,0,1,0,0
A Step Up with  Variational Autoencoders,"In other words, This is no rocket science, and one can easily verify (11) by simply plotting the two functions on a Cartesian plane. Using (11), we can proceed in a different direction from the definition of KL divergence. Once again, we have shown that KL divergence is positive! Proving this isn‚Äôt really necessary in the grand scheme of exploring the mathematics behind VAEs, yet I thought it would help to have this adjunctive section to better understand KL divergence and familiarize ourselves with some standard algebraic manipulations that are frequently invoked in many derivations. Let‚Äôs jump back into variational inference and defining the cost function with ELBO. Recall from the setup of our Variational Autoencoder model that we have defined the latent vector as living in two-dimensional space following a multivariate Gaussian distribution. It‚Äôs time to apply the ELBO equation to this specific context and derive a closed-form expression of our loss function. Let‚Äôs recall the formula for ELBO: After some rearranging, we can decompose ELBO into two terms, one of which is a KL divergence: Now, it‚Äôs finally time for us to dive deep into math: let‚Äôs unpack the closed form expression in (13).",0,0,0,0,1,0,1,0
Complex Fibonacci,"In fact, it can be any number: rational, irrational, real, or even complex. The fact that the fibonacci numbers can extend to real number indexing becomes more apparent once we code out the formula. Nothing special at all, this is just a literal transcription of the formula presented above. But now, watch what happens when we try to get the 1.1th fibonacci number, for instance: Lo and behold, we get a complex fibonacci number! I thought this was so fascinating, almost like seeing a magic of some sort. Although I had known about the fibonacci sequence for as long as I can remember, I had never thought about it in continuous terms: in my mind, the fibonacci sequence was, after all, a sequence‚Äîa discrete set of numbers adhering to the simple rule that the next number in line is the sum of the previous two. The intriguing part is that, even in this complex fibonacci madness, the simple rule still holds. For instance, You might be wondering why we don‚Äôt compare things exactly by means of This is because this equality doesn‚Äôt hold due to floating point arithmetic.",1,0,0,0,0,0,0,0
Complex Fibonacci,"Intuitively, Binet‚Äôs formula has to do with the well-known fact that the ratio between two consecutive fibonacci numbers approaches the Golden ratio as  goes to infinity. In this light, we might understand the fibonacci sequence as a geometric sequence with a constant ratio. The goal, then, is to show that the ratio is in fact the Golden ratio. Then, we have the following recurrence relation between consecutive terms. Dividing both sides by , we get This is a simple quadratic equation that we can easily solve. With some precalculus algebra, we get And indeed we start to see the Golden ratio and its negative inverse as solutions to the quadratic. This means that we can express the fibonacci sequence as a linear combinations of these two solutions: Much like solving any difference equations, we have two initial conditions, namely that , . We also trivially know that , but only two conditions suffice to ascertain the value of the coefficients,  and . With some algebra, one can verify that Putting these together, we finally get Binet‚Äôs formula: An interesting point to note about Binet‚Äôs formula is that  doesn‚Äôt necessarily have to be a non-negative integer as we had previously assumed.",1,0,0,0,0,0,0,0
Complex Fibonacci,"A few days ago, a video popped up in my YouTube suggestions. We all know how disturbingly powerful the YouTube recommendation algorithm is: more than 90 percent of the times, I thoroughly enjoy all suggestions put forth by the mastermind algorithm. This time was no exception: in fact, I enjoyed it so much that I decided to write a short blog post about it. Also a quick plug: if you haven‚Äôt checked out Matt Parker‚Äôs channel, I highly recommend that you do. Let‚Äôs dive right into today‚Äôs topic: extending the fibonacci sequence to complex numbers. We all know what the fibonacci sequence looks like, but for formality and notational clarity‚Äôs sake, here is what the fibonacci sequence looks like: There are some different conventions as to where the sequence starts. I personally prefer the one that starts from zero, with zero indexing. Here is what I mean: Implementing fibonacci numbers in code is one of the most common exercises that are used to teach concepts such as recursion, memoization, and dynamic programming. This is certainly not the point of today‚Äôs post, but here is an obligatory code snippet nonetheless.",1,0,0,0,0,0,0,0
A PyTorch Primer,"One detail to note is that, unlike in the case above where we had to explicitly call  in order to obtain the loss value‚Äîwhich would be of type ‚Äîwe leave the computed loss to remain as a tensor in order to call . We also make sure to reset the gradients per epoch by calling . We can also improve our implementation by making use of the  class that we implemented earlier. This is simple as doing This might be a better way to implement the function for reasons of simplicity and readability. Although ing works, it‚Äôs more arguably cleaner to write a ReLU this way. Also, this is a dummy example, and we can imagine a lot of situations where we might want to write custom functions to carry out specific tasks. Much like TensorFlow, PyTorch offers to ways of declaring models: function-based and class-based methods. Although I have just started getting into PyTorch, my impression is that the later is more preferred by PyTorch developers, whereas this is not necessarily the case with Keras or . Of course, this is a matter of preference and development setting, so perhaps such first impression generalizations do not carry much weight.",0,0,0,0,0,0,1,0
Wonders of Monte Carlo,"As we might recall, variance measures, quite simply, the degree of variability in our data. The well-known formula for variation goes as follows. Using this formula, let‚Äôs plot variance against the number of samples to see what effect increasing the sample size has on variance. The  function accepts a list as an argument and returns the variance seen in the given data set. Now that we have this function ready, let‚Äôs use it to plot variance against the number of samples used in crude Monte Carlo integration  Notice that variance quickly converges to near zero as the number of samples gets larger! This means that, even if we do not know the true value of the integral expression, we can now be confident that the output of the crude Monte Carlo will have converged to an approximation of the true value with sampling size as big as 1000, or even something like 400. This gives us more confidence in saying that the integral expression in (1) is approximates to 0.247. The crude Monte Carlo algorithm we employed here used simple random sampling to generate a series of random numbers to be used for our estimation.",0,0,0,0,0,1,0,0
Word2vec from Scratch,"The dimensionality of the matrix after passing the first layer, or the embedding layer, is as follows: This is expected, since we want all the 330 tokens in the text to be converted into ten-dimensional vectors. Next, let‚Äôs check the dimensionality after passing through the second layer. This time, it is a 330-by-60 matrix. This also makes sense, since we want the output to be sixty dimensional, back to the original dimensions following one-hot encoding. This result can then be passed onto the softmax layer, the result of which will be a bunch probability vectors. Implementing backward propagation is slightly more difficult than forward propagation. However, the good news is that we have already derived the equation for backpropagation given a softmax layer with cross entropy loss in this post, where we built a neural network from scratch. The conclusion of the lengthy derivation was ultimately that given our model Since we know the error, we can now backpropagate it throughout the entire network, recalling basic principles of matrix calculus. If backprop is still confusing to you due to all the tranposes going on, one pro-tip is to think in terms of dimensions.",0,0,0,1,0,0,1,0
"Beta, Bayes, and Multi-armed Bandits","Recently, I fortuitously came across an interesting blog post on the multi-armed bandit problem, or MAB for short. I say fortuitous because the contents of this blog nicely coincided with a post I had meant to write for a very long time: revisiting the Beta distribution, conjugate priors, and all that good stuff. I decided that the MAB would be a refreshing way to discuss this topic. ‚ÄúBayesian‚Äù is a buzz word that statisticians and ML people love anyway, me shamelessly included. In this post, we will start off with a brief introduction into what the MAB problem is, why it is relevant, and how we can use some basic Bayesian analysis with Beta and Bernoulli distributions to derive a nice sampling algorithm, known as Thompson sampling. Let‚Äôs dive right into it. The multi-armed bandit problem is a classical gambling setup in which a gambler has the choice of pulling the lever of any one of  slot machines, or bandits. The probability of winning for each slot machine is fixed, but of course the gambler has no idea what these probabilities are. To ascertain out the values of these parameters, the gambler must learn through some trial and error.",0,1,0,0,0,1,0,0
Dissecting the Gaussian Distribution,"The result: From (4), we can express  in terms of : After applying the substitution, now our probability density function looks as follows: To figure out what  is, let‚Äôs try to find the variance of , since we already know that the variance should be equal to . In other words, from the definition of variance, we know that Using (5), we get We can use integration by parts to evaluate this integral. This integral seems complicated, but if we take a closer look, we can see that there is a lot of room for simplification. First, because the rate of decay of an exponential function is faster than the rate of increase of a first-order polynomial, the first term converges to zero. Therefore, we have But since Therefore, Great! Now we know what the constant  is: Plugging this expression back into (5), we finally have the equation for the probability distribution function of the univariate Gaussian. And now we‚Äôre done! Let‚Äôs perform a quick sanity check on (7) by identifying its critical points. Based on prior knowledge, we would expect to find the local maximum at , as this is where the bell curve peaks.",0,1,0,0,0,1,0,0
A Brief Introduction to Recurrent Neural Networks,"There are a lot more advanced recurrent neural networks that have complicated internal cell structures to better emulate human memory, in a sense. The biggest difference between a simple recurrent neural network and an LSTM is that LSTMs have a unique parameter known as the carrier that encodes an additional layer of information about the state of the cell. I might write a separate post devoted to the intricacies of the LSTM, but if you‚Äôre an avid reader who‚Äôs itching to know more about it right away, I highly recommend this excellent post by Christiopher Olah. For now, let‚Äôs just say that LSTMs represent a huge improvement over conventional RNNs, and that we can implement them in  by simply calling the  layer as shown below: Because LSTM layers take a lot longer to train than others, and because the representational capacity of a single LSTM layer is higher than that of others, I decided to use only one LSTM layer instead of two. Let‚Äôs initialize this model to take a better look.  The last model we will create is a convnet, which we explored on this previous post on image classification.",0,0,0,0,1,0,1,0
Fourier Series,"Taylor series is used in countless areas of mathematics and sciences. It is a handy little tool in the mathematicians arsenal that allows us to decompose any function into a series of polynomials, which are fairly easy to work with. Today, we are going to take a brief look at another type of series expansion, known as Fourier series. Note that these concepts are my annotations of Professor Gilbert Strang‚Äôs amazing lecture, available on YouTube. The biggest difference between Taylor series and Fourier series is that, unlike Taylor series, whose basic fundamental unit is a polynomial term, the building block of a Fourier series is a trigonometric function, namely one of either sine or cosine. Concretely, a generic formula of a Fourier expansion looks as follows: Personally, I found this formula to be more difficult to intuit than the Taylor series. However, once you understand the underlying mechanics, it‚Äôs fascinating to see how periodic wave functions can be decomposed as such. First, let‚Äôs begin with an analysis of orthogonality. Commonly, we define to vectors  and  as being orthogonal if That is, if their dot product yields zero.",1,0,0,0,0,1,0,0
Gamma and Zeta,"Maintaining momentum in writing and self-learning has admittedly been difficult these past few weeks since I‚Äôve started my internship. Normally, I would write one post approximately every four days, but this routine is no longer the norm. To my defense, I‚Äôve been learning a ton about Django and backend operations like querying and routing, and I might write a post about these in the future. But for today, I decided to revisit a topic we‚Äôve previously explored on this blog, partially in the hopes of using nostalgia as positive energy in restarting my internal momentum. I must also note that I meant to write this post for a very long time after watching this video by blackpenredpen whose videos have been a source of mathematical learning and inspiration for me. Let‚Äôs talk about the Gamma and Zeta functions. Before we begin the derivation, perhaps it‚Äôs a good idea to review what the two greek letter functions are.",1,0,0,0,0,0,0,0
Traveling Salesman Problem with Genetic Algorithms,"For the sake of simplicity, however, we don‚Äôt enforce this returning requirement in our modified version of TSP. Below are the modules we will be using for this post. We will be using , more specifically a lot of functions from  for things like sampling, choosing, or permuting.  arrays are also generally faster than using normal Python lists since they support vectorization, which will certainly be beneficial when building our model. For reproducibility, let‚Äôs set the random seed to 42. Now we need to consider the question of how we might represent TSP in code. Obviously, we will need some cities and some information on the distance between these cities. One solution is to consider adjacency matrices, somewhat similar to the adjacency list we took a look at on the post on Breadth First and Depth First Search algorithms. The simple idea is that we can construct some matrix that represent distances between cities  and  such that  represents the distance between those two cities. When , therefore, it is obvious that  will be zero, since the distance from city  to itself is trivially zero. Here is an example of some adjacency matrix.",0,0,0,1,0,0,0,0
PyTorch Tensor Basics,"More generally speaking, we can think that concatenation effectively brought the two elements of each tensor together to form a larger tensor of four elements. I found concatenation along the first and second dimensions to be more difficult to imagine right away. The trick is to mentally draw a connection between the dimension of concatenation and the location of the opening and closing brackets that we should focus on. In the case of the example above, the opening and closing brackets were the outer most ones. In the example below in which we concatenate along the first dimension, the brackets are those that form the boundary of the inner two-dimensional 3-by-4 tensor. Let‚Äôs take a look. Notice that the rows of  were essentially appended to those of , thus resulting in a tensor whose shape is . For the sake of completeness, let‚Äôs also take a look at the very last case, where we concatenate along the last dimension. Here, the brackets of focus are the innermost ones that form the individual one-dimensional rows of each tensor. Therefore, we end up with a ‚Äúlong‚Äù tensor whose one-dimensional rows have a total of 8 elements as opposed to the original 4.",0,0,0,0,0,0,1,0
Gaussian Mixture Models,"Then, since we are essentially summing up this quantity across the entire  data points in the dataset , we can interpret  to effectively be the number of points in the dataset that are assinged to the th cluster. Then, we can now simplify the MLE estimate of the mean as But we can now observe something interesting. Notice that a depend on . In turn,  is defined in terms of . This is the very circular dependency that we discussed earlier as we were introducing the EM algorithm and comparing it with the Gibbs sampler. Now it becomes increasingly apparent why the EM algorithm is needed to find a converging solution for the MLE estimates. We can take a similar approach to calculate the MLE of the other two remaining paramters, namely  and . The derivation is more complicated since  is a matrix;  is subject to constraints that apply to any categorical distribution: all elements must be positive and must sum up to one. For my own personal reference and the curious-minded, here is a link to a resource that contains the full derivation.",0,0,1,0,0,1,0,0
k-Nearest Neighbors Algorithm from Scratch,"One way to go about this is to use Euclidean distance, which is defined as follows: It is not difficult to build an implementation of in Python. We can easily achieve this using . Let‚Äôs test the functionality of the  function using some dummy dataset. This data set was borrowed from Jason Brownlee. Great! As expected, the distance between a point and itself is 0, and other calculated distances also seem reasonable. The next step is to write a function that returns the  nearest neighbors of a point given a data set and parameter . There are many ways to implement this, but an example is shown below. First, we enumerate thorugh the data set to calculate all the distances between the given test instance and the data points in the data set. Next, we sort the list. The returned result is a list that contains the indices of the  nearest neighbors the algorithm found in the data set. Note that we use the  function we wrote above. Let‚Äôs see if the code works by testing it on our toy data. The task is to find 5 data points that are closest to the first row of .",0,0,1,1,0,0,0,0
A Step Up with  Variational Autoencoders,"Let‚Äôs specify this setup, along with some other miscellaneous configurations, before we proceed with constructing the model architecture. It‚Äôs time to build our model‚Ä¶ or not quite now. Before we start stacking layers for the encoder and the decoder, we need to define a sampling function that will perform the meat of the variational inference involved in VAE. Let‚Äôs start out by taking a look at the sampling function we will use to define one of the layers of the variational Autoencoder network. Simply put, the  above below takes as arguments  and  in the form of a bundled list. As you can guess from the name of the variables, these two  parameters refer to the mean and log variance of the random vector living in our predefined latent space. Note that we are assuming a diagonal Gaussian here: in other words, the covariance matrix of the multi-dimensional Gaussian is assumed to be diagonal, meaning that each elements of the vector are independent. If any of this sounds foreign to you, I recommend that you read this post on the Gaussian distribution. Let‚Äôs continue our discussion with the sampling function.",0,0,0,0,1,0,1,0
Traveling Salesman Problem with Genetic Algorithms,"Simply put, evaluation amounts to calculating the fitness of each chromosome in the total population, determining who is best, storing the score information, and returning some probability vector whose each element represents the probability that the th element in the population bag is chosen as a parent. We apply some basic preprocessing to ensure that the worst performing chromosome has absolutely no chance of being selected. When we call , we get a probability vector as expected. From the result, it appears that the last element is the best chromosome; the second chromosome in the population bag is the worst. When we call , notice that we get the last element in the population, as previously anticipated. We can also access the score of the best chromosome. In this case, the distance is said to be 86.25. Note that the lower the score, the better, since these scores represent the total distance a salesman has to travel to visit all the cities. Now, we will select  number of parents to be the basis of the next generation.",0,0,0,1,0,0,0,0
Likelihood and Probability,"Combining these two results, we would expect the maximum likelihood distribution to follow  where  =  and  =  in our code. And that concludes today‚Äôs article on (maximum) likelihood. This post was motivated from a rather simple thought that came to my mind while overhearing a conversation that happened at the PMO office. Despite the conceptual difference between probability and likelihood, people will continue to use employ these terms interchangeably in daily conversations. From a mathematician‚Äôs point of view, this might be unwelcome, but the vernacular rarely strictly aligns with academic lingua. In fact, it‚Äôs most often the reverse; when jargon or scholarly terms get diffused with everyday language, they often transform in meaning and usage. I presume words such as ‚Äúlikelihood‚Äù or ‚Äúlikely‚Äù fall into this criteria. All of this notwithstanding, I hope this post provided you with a better understanding of what likelihood is, and how it relates to other useful statistical concepts such as maximum likelihood estimation. The topic for our next post is going to be Monte Carlo simulations and methods. If ‚ÄúMonte Carlo‚Äù just sounds cool to you, as it did to me when I first came across it, tune in again next week.",0,0,0,0,0,1,0,0
Wonders of Monte Carlo,"Our goal here is to simulate this motion many times to estimate the probability that the man would successfully end up in the restroom to go about his business. This example was borrowed from this post by Zacharia Miller. Before we start typing up some code, let‚Äôs first lay down the ground rules of this simulation. First, we assume that the pub is modeled as a ten-by-ten grid, the bottom-left point defined as  and the top-right . The drunkard will start his walk at his table, represented by the coordinate . For each walk, function will generate a random number to determine the direrction of his movement. The magnitude of each walk is 1 by default. Beforer a walk is performed, we will invoke another function to check if his movements are legal, i.e. whether he stepped out of the boundary of the pub. If his moves are legal, we continue with the movement; if not, we stop and assume that the trial has yielded a failure. The goal of this random walk is to end up in the top-right portion of the pub, a square defined by coordinates , and .",0,0,0,0,0,1,0,0
Building Neural Network From Scratch,"Our goal, then, is to compute the gradient where  and  each represent the values taken by the th and th neuron in layers  and , respectively. One point of caution is that it is important to consider whether  and  are equal, as this produces differences in the calculation of the gradient. First consider the case when : When : We see that the gradient is different in the two cases! This is certainly going to important for us when calculating the gradient of , the cross entropy loss function, with respect to . Specifically, we have to consider the two cases separately by dividing up the summation expression into two parts, as shown below: That was a long ride, but in the end, we end up with a very nice expression! This tells us that the gradient of the cross entropy loss function with respect to the second affine layer is simply the size of the error term. In other words, if we expand the result in (13) to apply to the entire matrix of layers, we get This provides a great place for us to start.",0,0,0,1,0,0,1,1
MLE and KL Divergence,"Indeed, it sort of makes intuitive sense to think that minimizing the distance between the true and approximated distribution is best done through maximum likelihood estimation, which is a technique used to find the parameter of the distribution that best describes given data. I personally find little derivations and proofs like these to be quite interesting, which is why I plan on doing more posts on the mathematics of deep learning and its related concepts in the future. Thanks for reading, and catch you up in the next one.",0,0,0,0,0,1,0,0
The Exponential Family,"It then follows that How do we interpret the final result in equation (25)? It looks nice, simple, and concise, but what does it mean to say that the expected value of the sufficient statistic is the average of the sufficient statistic for each observed individual data points? To remove  abstractness, let‚Äôs employ a simple example, the exponential distribution, and attempt to derive a clearer understanding of the final picture. Recall that the probability density function of the exponential distribution takes the following form according to the factorizations outlined below: Computing the derivative of the log of the normalizing term  as we did in (22), Because we know that the resulting quantity is the expected value of the sufficient statistic, we know that And indeed, this is true: the expected value of the random variable characterized by an exponential distribution is simply the inverse of the parameter defining that distribution. Note that the parameter for the exponential distribution is most often denoted as , in which case the expected value of the distribution would simply be written as .",0,1,0,0,0,1,0,0
Markov Chain and Chutes and Ladders,"For example,  does not reflect the fact that getting a 1 on a roll of the dice will move the player up to the thirty eighth cell; it supposes that the player would stay on the first cell. The new permutation matrix  would adjust for this error by reordering . For an informative read on the mechanics of permutation, refer to this explanation from Wolfram Alpha. Let‚Äôs perform a quick sanity check to verify that  contains the right information on the first ladder, namely the entry  in the  dictionary. Notice the  in the th entry hidden among a haystack of 100 s! This result tells us that  is indeed a permutation matrix whose multiplication with  will produce the final stochastic vector that correctly enumerates the probabilities encoded into the Chutes and Ladders game board. Here is our final product: We can visualize the stochastic matrix  using the  package. This produces a visualization of our stochastic matrix. So there is our stochastic matrix! Now that we have a concrete matrix to work with, let‚Äôs start by identifying its eigenvectors.",0,0,0,0,0,0,0,1
The Math Behind GANs,"On the other hand, for a generated sample , we expect the optimal discriminator to assign a label of zero, since  should be close to zero. To train the generator, we assume the discriminator to be fixed and proceed with the analysis of the value function. Let‚Äôs first plug in the result we found above, namely (12), into the value function to see what turns out. To proceed from here, we need a little bit of inspiration. Little clever tricks like these are always a joy to look at. If you are confused, don‚Äôt worry, you aren‚Äôt the only one. Basically, what is happening is that we are exploiting the properties of logarithms to pull out a  that previously did not exist. In pulling out this number, we inevitably apply changes to the terms in the expectation, specifically by dividing the denominator by two. Why was this necessary? The magic here is that we can now interpret the expectations as Kullback-Leibler divergence: And it is here that we reencounter the Jensen-Shannon divergence, which is defined as where .",0,0,0,0,0,1,1,0
Natural Gradient and Fisher,"Even after having written this entire post, I‚Äôm still not certain if I have understood the details and subtleties involved in the derivation. And even the details that I understand now will become confusing and ambiguous later when I return back to it. Hopefully I can retain most of what I have learned from this post. Before I close this post, I must give credit to Agustinus Kristiadi, whose blog post was basically the basis of this entire writing. I did look at a few Stack Overflow threads, but the vast majority of what I have written are either distillations or adaptations from their blog. It‚Äôs a great resource for understanding the mathematics behind deep learning. I hope you enjoyed reading this blog. See you in the next one!.",0,0,1,0,0,1,0,0
Recommendation Algorithm with SVD,"An impotant observation to make is that, as we have noted earlier, User 6 and User 8 have rows that are identical. While this should not be a surprise given that the two users had what seemed to be an identical taste in movies, it is still interesting to see how SVD is able to extract this information and display it onto a new axis. Next, let‚Äôs see what  looks like. Shown above is the transpose of , which means that  is just really . What‚Äôs important here is that the five movies have also been reduced to three dimensions. We don‚Äôt really know what the columns of this matrix means; all we know is that it is some distillation and amalgamation of information about the ten users on some unknown axis. At any rate, the previous ten dimensional vectors have now been reduced to three dimensions, which is great news for us‚Äîas three dimensional beings, it‚Äôs always easier to visualize and deal with three dimensions or less than 10D. Movie 2 and Movie 3 do not look as similar as they did before on the ratings matrix.",0,0,0,1,0,0,0,1
Convex Combinations and MAP,"In a previous post, we briefly explored the notion of maximum a posteriori and how it relates to maximum likelihood estimation. Specifically, we derived a generic formula for MAP and explored how it compares to that for MLE. Today‚Äôs post is going to be an interesting sequel to that story: by performing MAP on the univariate Gaussian, we will show how MAP can be interpreted as a convex combination, thus motivating a more intuitive understanding of what MAP actually entails under the hood. Let‚Äôs jump right into it. The univariate Gaussian is a good example to work with because it is simple and intuitive yet also complex enough for meaningful analysis. After all, it is one of the most widely used probability distributions and also one that models many natural phenomena. With that justification firmly in mind, let‚Äôs take a look at the setup of the MAP of the  mean for the univariate Gaussian. As always, we begin with some dataset of  independent observations. In this case, because we are dealing with the univariate Gaussian, each observations will simply be a scalar instead of a vector.",0,0,0,0,0,1,0,0
Principal Component Analysis,"I found this to be the more dominant interpretation of PCA, since indeed it is highly intuitive: the goal of PCA is to find the axes‚Äîor the principal components‚Äîthat which maximize the variance seen in the data. setosa.io has some excellent visualizations on the notion of covariance and how it relates to PCA, so I highly recommend that you go check it out. If were to derive PCA from the gecko with the covariance approach, we would be using an iterative approach to find a single principal component at a time. Specifically, our goal would be to find  that which maximizes Hence the problem is now framed as a constrained optimization problem. We use Lagrangians to solve constrained optimization. The intuition for the Lagrangian method is that the gradient of the constraint and the argument should be parallel to each other at the point of optimization. We go about this by taking the gradient of the argument with respect to : Since 2 is just a constant, we can absorb it into  to form a more concise expression.",0,0,0,0,0,1,0,1
A Step Up with  Variational Autoencoders,"The decoder network looks similar to the the encoder, except that much of the architecture is in reverse order. Most notably, we use  to undo the convolution done by the encoder. This allows us to effectively scale up the input back to its original dimension, which is what we want to do with a generative model like a VAE. One subtly worth mentioning is the fact that we use a sigmoid activation in the end. This is because we want the pixel values of the output to be between 0 and 1, just as the original input was normalized before it was fed into the encoder network via division by 255. The summary of the decoder network is presented below: Now that we have both the encoder and the decode network fully defined, it‚Äôs time to wrap them together into one autoencoder model. This can simply achieved by defining the input as the input of the encoder‚Äîthe normalized MNIST images‚Äîand defining the output as the output of the decoder when fed a latent vector. Concretely, this process might look as follows: Let‚Äôs look a the summary of the CVAE.",0,0,0,0,1,0,1,0
A PyTorch Primer,"If we try to call  on any of the other intermediate variables, such as  or , PyTorch will complain. Let‚Äôs try to understand the result of this computation. Let  denote the final  tensor. Since we called , and since  has a total of four elements, we can write out our dummy calculations mathematically in the following fashion: Using partial differentiation to obtain the gradients, Since , Since  is just an arbitrary, non-specific index out of a total of four, we can easily see that the same applies for all other indices, and hence we will end up with a matrix whose all four entries take the value of 4.5, as PyTorch has rightly computed. We can go even a step farther and declare custom operations. For example, here‚Äôs a dummy implementation of the ReLU function. Let‚Äôs talk about the  method first. Note that it takes in two argument parameters:  and . As you might have guessed,  is simply the value that the function will be provided with. The  can simply be thought of as a cache where we can store vectors or matrices to be used during backpropagation. In this case, we store the  by calling  method.",0,0,0,0,0,0,1,0
Word2vec from Scratch,"In a previous post, we discussed how we can use tf-idf vectorization to encode documents into vectors. While probing more into this topic and geting a taste of what NLP is like, I decided to take a jab at another closely related, classic topic in NLP: word2vec. word2vec is a technique introduced by Google engineers in 2013, popularized by statements such as ‚Äúking - man + woman = queen.‚Äù The gist of it, as you may know, is that we can express words as vectors that encode their semantics in a meaningful way. When I was just getting starting to learn TensorFlow, I came across the embedding layer, which performed exactly this operation: transforming words into vectors. While I thought this process was extremely interesting, I didn‚Äôt know about the internals of this structure until today, particularly after reading this wonderful tutorial by Chris McCornick. In this post, we will be implementing word2vec, a popular embedding technique, from scratch with NumPy. Let‚Äôs get started! Instead of going over the concepts and implementations separately, let‚Äôs jump straight into the whole implementation process and elaborate on what is necessary along the way.",0,0,0,1,0,0,1,0
A Step Up with  Variational Autoencoders,"This can be achieved through some clever algebraic manipulation: But since the the expected value of  is constant and that of  is zero, We can now plug this simplified expression back into the calculation of KL divergence, in (19): Since we will standardize our input such that  and , we can plug these quantities into (22) and show that We are almost done with deriving the expression for ELBO. I say almost, because we still have not dealt with the trailing term in (13): At this point, it is extremely useful to recall the definition of cross entropy, which is generically defined as follows: Therefore, we see that the trailing term in (13) is just a cross entropy between two distributions! This was a circumlocutions journey, but that is enough math we will need for this tutorial. It‚Äôs time to get back to coding. All that math was for this simple code snippet shown below: As you can see, this short code snippet shows, in essence, how we can define a compile a model with a custom loss function. In this case,  refers to the reconstruction loss, which is the cross entropy term we saw earlier.",0,0,0,0,1,0,1,0
So What are Autoencoders?,"The five numbers composing this vector somehow encodes the core information needed to then decode this vector back into the original 28-by-28 pixel RGB channel image. Of course, some information is inevitably going to be lost‚Äîafter all, how can five numbers describe the entirety of an image? However, what‚Äôs important and fascinating about autoencoders is that, with appropriate training and configuration, they manage to find ways to best compress input data into latent vectors that can be decoded to regenerate a close approximation of the input data. For the purposes of this demonstration, let‚Äôs configure the latent dimension of the encoder to be 128 dimensions‚Äîin other words, each 28-by-28, single-channel image will be encoded into vectors living in 128 dimensional space. It‚Äôs time to build the autoencoder model. In summary, an autoencoder is composed of two components: an encoder and a decoder. The encoder transfers input data into the latent dimension, and the decoder performs the exact reverse: it takes vectors in the latent space and rearranges it to bring it back into its original dimension, which is, in this case, a 28-by-28, single-channel image. The followign code snippet implements this logic using the  functional API.",0,0,0,0,1,0,1,0
How lucky was I on my shift?,"How can we obtain the probability of getting one head and nine tails? To begin with, here is the list of all possible arrangements: Notice that all we had to do was to choose one number  that specifies the index of the trial in which a coin toss produced a head. Because there are ten ways of choosing a number from integers  to , we got ten different arrangements of the situation satisfying the condition . You might recall that this combinatoric condition can be expressed as , which is the coefficient of the binomial distribution equation. Now that we know that there are ten different cases, we have to evaluate the probability that each of these cases occur, since the total probability , where . Calculating this probability is simple: take the first case,  as an example. Assuming independence on each coin toss, we can use multiplication to calculate this probability: Notice that  because we assumed the coin was fair. Had it not been fair, we would have different probabilities for  and , explained by the relationship that .",0,1,0,0,0,1,0,0
Principal Component Analysis,"But in order to do so, we need to unpack  , since we have no idea how to take its derivative. Using (2), we can reorganize (4) as follows: The last equality is due to the fact that we constrained the columns of  to be unit vectors that are orthogonal to each other. Now we can take a derivative of the argument with respect to  and set it equal to zero to find the minimum. This tells us that the optimal way of compressing  is simply by multiplying it by the transpose of the decoding matrix. In other words, we have found the transformation  in (2). For those of you who are confused about how gradients and matrix calculus work, here is a very short explanation. First, notice that  is just a scalar, since  is a column vector. Taking a gradient with respect to this quantity would mean that we get another column vector of equal dimensions with  with the following elements: And we know how to go from there. The same line of thinking can be applied to think about the second term, .",0,0,0,0,0,1,0,1
Gaussian Mixture Models,"Therefore, we end up with And there we have it, the density function of the Gaussian mixture model! We have a convex combination of  different Gaussian PDFs that compose the Gaussian mixture model. In the context of machine learning, the goal is to find the parameters of the model that best describe some given set of data. In the case of GMMs, this is no different. The parameters that we have to estimate are Of course,  and  are not single vectors or matrices, but  collection of such objects. But for notational simplicity, I opted to write them as such as shown above. Given a collection of  data points, denoted as , we can now come up with the following expression:  denotes the likelihood function, and  is a collection of all parameters that define the mixture model. In other words, All this is doing is that we are using the marginal distribution expression we derived earlier and applying that to a situation in which we have  data points instead of just one, which is the context we have been operating in so far. We multiply the probabilities given the assumption of independence.",0,0,1,0,0,1,0,0
Recommendation Algorithm with SVD,"However, perhaps this is due to the fact that all entries of this matrix have pretty small values, and it is difficult to see how the difference between Movie 2 and 3 compares to, say, the distance between Movies 1 and 4. Perhaps we should scale this in terms of relative distances or plot it on a three dimensional space, which is exactly what we are going to in a moment. Before we jump into visualizations, however, let‚Äôs deal with the elephant in the room first: is it okay to simply chop off a few dimensions to reduce a high dimensional image to fit into three-dimensional space? To answer this question, let‚Äôs check the  matrix for this particular instance of singular value decomposition. Note that we already have the first three values of  in our hands given that  in our instantiation of singular value decomposition. The information we lose pertains to the last two values, given by  and . These values are smaller in order of magnitude compared to, for instance, the largest value of , which is . This supports the idea that the information we lose amid dimensionality reduction is minimal.",0,0,0,1,0,0,0,1
The Gibbs Sampler,"The derivation was not the simplest, and granted we omitted a lot of algebra along the way, but it was a good mental exercise nonetheless. If you are interested in a simpler proof, I highly recommend that you check out the Stack Exchange post I linked above. I hope you enjoyed reading this post. Catch you up in the next one!.",0,0,0,0,0,1,0,1
"PyTorch, From Data to Modeling","Note that the loader returns a tuple of images and labels, which we can unpack directly within the  loop itself. We then move the two objects to , which would be necessary if we were running this one a Cuda-enabled computer. Then, we calculate the loss by calling , the loss function, and append the loss to the  list. Note that we have to call  since  itself is a one-by-one PyTorch tensor. Then comes the important part where we perform backprop. The idea is that we would The three steps correspond to each of the lines in the code above, starting from . As you might be able to guess from the name of the function, we zero the gradients to make sure that we aren‚Äôt accumulating gradient values from one iteration to the next. Calling  corresponds to calculating the new gradient values, and  performs the backprop. The last block of code is simply a convenient print function I‚Äôve written to see the progress of training at certain intervals. As you can see, the loss seems to be decreasing for the most part, although it is jumpy at times.",0,0,0,0,0,0,1,0
A sneak peek at Bayesian Inference,"Mathematically, we can define the conditional probability of event  given  as follows: This equation simple states that the conditional probability of  given  is the fraction of the marginal probability  and the area of intersection between those two events, . This is a highly intuitive restatement of the definition of conditional probability introduced above: given that event  has already occurred, conditional probability tells us the probability that event  occurs, which is then synonymous to that statement that  has occurred. By the same token, we can also define the reverse conditional probability of  given  through symmetry and substitution. Notice that the numerator stays unchanged since the operation of intersection is commutative. Now let‚Äôs develop an intuition for conditional probabilities by applying it to our example of clinical trials and the potentially affected patient. The purported accuracy of the clinical test is 90 percent, which we might express as follows, using the conditional probability notation: By the same token, we can also express the information on false positives as shown below. This conditional probability statement espouses that, given an individual who is not sick, the test returns a false positive 50 percent of the time.",0,1,0,0,0,1,0,0
BLEU from scratch,"Each component of BLEU addresses some problem with simpler metrics, such as precision or modified precision. It also takes into account things like abnormally short or repetitive translations. One area of interest for me these days is seq2seq models. Although RNN models have largely given way to transformers, I still think it‚Äôs a very interesting architecture worth diving into. I‚Äôve also recently ran into a combined LSTM-CNN approach for processing series data. I might write about these topics in a future post. I hope you‚Äôve enjoyed reading this post. Catch you up later!.",0,0,0,1,0,0,1,0
An Introduction to Markov Chain Monte Carlo,"Simply put, the summation experssion in the denominator would simply be replaced with that involving integration. The power of the proposition underlying Bayes‚Äôs theorem really comes into light when we consider it in the context of Bayesian analysis. The objective of Bayesian statistical analysis is to update our beliefs about some probability, known as the posterior, given a preestablished belief, called the prior, and a series of data observations, which might be decomposed into likelihood and evidence. Concretely, This statement is equivalent to where  denotes the likelihood function. In plain language, Bayesian statistics operates on the assumption that all probabilities are reflections of subjective beliefs about the distribution of some random variable. A prior expectation or belief we might have about this distribution is referred to as the prior. Then, we can update our prior belief based on sample observations, resulting in a posterior distribution. Roughly speaking, the posterior can be considered as the ‚Äúaverage‚Äù between the prior and the observed data. This process, which we went over in detail in this post, is at the heart of Bayesian inference, a powerful tool through which data and distributions can be understood.",0,0,0,1,0,0,0,0
Dissecting the Gaussian Distribution,"Because the first exponential term cannot equal zero, we can simplify the equation to Therefore, From this, we can see that the inflection point of the univariate Gaussian is exactly one standard deviation away from the mean. This is one of the many interesting properties of the normal distribution that we can see from the formula for the probability distribution. So far, we‚Äôve looked at the univariate Gaussian, which involved only one random variable . However, what if the random variable in question is a vector that contains multiple random variables? It is not difficult to see that answering this question requires us to think in terms of matrices, which is the go-to method of packaging multiple numbers into neat boxes, known as matrices. Instead of deriving the probability distribution for the multivariate Gaussian from scratch as we did for the univariate case, we‚Äôll build on top of the equation for the univariate Gaussian to provide an intuitive explanation for the multivariate case. In a previous post on linear regression, we took a look at matrix calculus to cover basic concepts such as the gradient.",0,1,0,0,0,1,0,0
The Exponential Family,"Normally, we omit the indicator function since it is self-apparent, but for the sake of robustness in our analysis, I have added it here. How can we coerce equation (6) to look more like (3), the archetypal form that defines the exponential family? Well, now it‚Äôs just a matter of drag and match: by paying close attention to the variables, parameters, and the output of each function, we can reconstruct (3) to take the form of (6). The easeist starting point is to observe the exponent to identify  and , after which the rest of the surrounding functions can be inferred. The end result is presented below: After substituting each function with their prescribed value in (8), it isn‚Äôt difficult to see that the exponential distribution can indeed by factorized according to the form outlined in (3). Although this is by no means a rigorous proof, we see not only the evident fact that the exponential distribution indeed belongs to the exponential family, but also that the factorization formula in (3) isn‚Äôt just a complete soup of equations and variables. We can do the same for the Bernoulli distribution, which also falls under the exponential family.",0,1,0,0,0,1,0,0
How lucky was I on my shift?,"From this definition, it flows that: But then the last term converges to 1 as  goes to : We can further simplify the rest of the terms in the limit expression as well. Specifically,  collapses to . These terms can be coupled with  in the denominator as follows: Putting this all together yields: And we have derived the PMF for the Poisson distribution! We can perform a crude sanity check on this function by graphing it and checking that its maximum occurs at . In this example, we use the numbers we assumed in the PMO phone call example, in which . The code produces the following graph. As expected, the graph peaks at . At a glance, this distribution resembles the binomial distribution we looked at earlier, and indeed that is no coincidence: the Poisson distribution is essentially a special case of binomial distributions whereby the number of trials is literally pushed to the limit. As stated earlier, the binomial distribution can be considered as a very rough approximation of the Poisson distribution, and the accuracy of approximation would be expected to increase as  increases.",0,1,0,0,0,1,0,0
Scikit-learn Pipelines with Titanic,"The idea is that one-hot encoding all categorical variables may very well lead to an unmanageable number of columns, thus causing one to flounder in the curse of dimensionality. A quick fix, then, is to apply PCA or some other dimensionality reduction technique onto the results of one-hot encoding. Back to the implementation, note that we can look inside the individual components of  by simply treating it as an iterable, much like a list or tuple. For example, Next, we need to do something similar for numerical variables. Only this time, we wouldn‚Äôt be one-hot encoding the data; instead, what we want to do is to apply some scaling, such as normalization or standardization. Recently in one of Andreas Mueller‚Äôs lectures on YouTube, I learned about the , which uses median and IQR instead of mean and standard deviation as does the . This makes the  a superior choice in the presence of outliers. Let‚Äôs try using it here. Now that we have the two pipelines for numeric and categorical columns, now it‚Äôs time to put them together into one nice package, then apply the process over the entire dataframe.",0,0,1,0,0,0,0,0
"Linear Regression, in Two Ways","We can further break this expression down by taking note of the fact that the norm of a vector can be expressed as a product of the vector and its transpose, and that  as established in the previous section of this post. Putting these together, Using distribution, we can simplify the above expression as follows: It‚Äôs time to take the gradient of the error function, the matrix calculus analogue of taking the derivative. Now is precisely the time when the propositions (4) and (5) we explored earlier will come in handy. In fact, observe that first term in (6) corresponds to case (5); the second term, case (4). The last term can be ignored because it is a scalar term composed of , which means that it will not impact the calculation of the gradient, much like how constants are eliminated during derivation in single-variable calculus. Now, all we have to do is to set the expression above to zero, just like we would do in single variable calculus with some optimization problem. There might be those of you wondering how we can be certain that setting this expression to zero would yield the minimum instead of the maximum.",0,0,0,0,0,0,0,1
The Math Behind GANs,"When training a GAN, we typically train one model at a time. In other words, when training the discriminator, the generator is assumed as fixed. We saw this in action in the previous post on how to build a basic GAN. Let‚Äôs return back to the min-max game. The quantity of interest can be defined as a function of  and . Let‚Äôs call this the value function: In reality, we are more interested in the distribution modeled by the generator than . Therefore, let‚Äôs create a new variable, , and use this substitution to rewrite the value function: The goal of the discriminator is to maximize this value function. Through a partial derivative of  with respect to , we see that the optimal discriminator, denoted as , occurs when Rearranging (12), we get And this is the condition for the optimal discriminator! Note that the formula makes intuitive sense: if some sample  is highly genuine, we would expect  to be close to one and  to be converge to zero, in which case the optimal discriminator would assign 1 to that sample.",0,0,0,0,0,1,1,0
A sneak peek at Bayesian Inference,"Although our example was just a simple coin toss, the sample principle and mechanism can be extended to countless other situations, which is why Baye‚Äôs theorem remains highly relevant to this day, especially in the field of machine learning and statistical analysis. Bayesian statistics presents us with an interesting way of understanding probability. The classical way of understanding probability is the frequentist approach, which purports that a probability for an event is the limit of its frequency in infinite trials. In other words, to say that a coin is fair is to say that, theoretically, performing an infinite number of coin flips would result in 50 percent heads and 50 percent tails. However, the Bayesian approach we explored today presents a drastically different picture. In Bayesian statistics, probability is an embodiment of our subjective beliefs about a parameter, such as the fairness of a coin. By performing trials, infinite or not, we gain more information about the parameter of our interest, which affects the posterior probability. Both interpretations of probability are valid, and they help complement each other to help us gain a broader understanding of what the notion of probability entails.",0,1,0,0,0,1,0,0
A PyTorch Primer,"Also, working with Django has somewhat helped me grasp the idea of classes more easily, which certainly helped me take in class-based concepts in PyTorch more easily. I distinctively remember people saying that PyTorch is more object-oriented compared to TensorFlow, and I might express agreement to that statement after having gone through the extreme basics of PyTorch. In the upcoming articles, I hope to use PyTorch to build more realistic models, preferrably in the domain of NLP, as that seems to be where PyTorch‚Äôs comparative advantage stands out the most compared to TensorFlow. Of course, this is not to say that I don‚Äôt like TensorFlow anymore, or that PyTorch is not an appropriate module to use in non-NLP contexts: I think each of them are powerful libraries of their own that provide a unique set of functionalities for the user. And being bilinguial‚Äîor even a polyglot, if you can use things like Caffe perhaps‚Äîin the DL module landscape will certainly not hurt at all. I hope you‚Äôve enjoyed this article. Catch you up in the next one!.",0,0,0,0,0,0,1,0
The Math Behind GANs,"This means that the expression in (15) can be expressed as a JS divergence: The conclusion of this analysis is simple: the goal of training the generator, which is to minimize the value function , we want the JS divergence between the distribution of the data and the distribution of generated examples to be as small as possible. This conclusion certainly aligns with our intuition: we want the generator to be able to learn the underlying distribution of the data from sampled training examples. In other words,  and  should be as close to each other as possible. The optimal generator  is thus one that which is able to mimic  to model a compelling model distribution . In this post, we took a brief tour of the math behind general adversarial networks. Since the publication of Goodfellow‚Äôs work, more GAN models have been introduced and studied by different scholars, such as the Wasserstein GAN or CycleGAN to name just a few. The underlying mathematics for these models are obviously going to be different from what we have seen today, but this is a good starting point nonetheless. I hope you enjoyed reading this post.",0,0,0,0,0,1,1,0
Fisher Score and Information,"To continue, we know that the maximum likelihood estimate of the distribution‚Äôs parameter is given by This is the standard drill we already know. The next step, as we all know, is to take the derivative of the term in the argument maxima, set it equal to zero, and voila! We have found the maximum likelihood estimate of the parameter. A quick aside that may become later is the fact that maximizing the likelihood amounts to minimizing the loss function. Now here comes the definition of Fisher‚Äôs score function, which really is nothing more than what we‚Äôve done above: it‚Äôs just the gradient of the log likelihood function. In other words, we have already been implicitly using Fisher‚Äôs score to find the maximum of the likelihood function all along, just without explicitly using the term. Fisher‚Äôs score is simply the gradient or the derivative of the log likelihood function, which means that setting the score equal to zero gives us the maximum likelihood estimate of the parameter. An important characteristic to note about Fisher‚Äôs score is the fact that the score evaluated the true value of the parameter equals zero.",0,0,0,0,0,1,0,0
Likelihood and Probability,"In other words, the likelihood function answers the question: provided some list of observed or sampled data , what is the likelihood that our parameter of interest takes on a certain value ? One measurement we can use to answer this question is simply the probability density of the observed value of the random variable at that distribution. In mathematical notation, this idea might be transcribed as: At a glance, likelihood seems to equal probability‚Äîafter all, that is what the equation (1) seems to suggest. But first, let‚Äôs clarify the fact that  is probability density, not probability. Moreover, the interpretation of probability density in the context of likelihood is different from that which arises when we discuss probability; likelihood attempts to explain the fit of observed data by altering the distribution parameter. Probability, in contrast, primarily deals with the question of how probable the observed data is given some parameter . Likelihood and probability, therefore, seem to ask similar questions, but in fact they approach the same phenomenon from opposite angles, one with a focus on the parameter and the other on data. Let‚Äôs develop more intuition by analyzing the difference between likelihood and probability from a graphical standpoint.",0,0,0,0,0,1,0,0
k-Nearest Neighbors Algorithm from Scratch,"There is a special function already in the  library that does all the shuffling and the splitting for us, but in light of the ‚Äúfrom scratch‚Äù spirit of this post, let‚Äôs try to write up the function ourselves. Great! We can now use this function to split the iris data set we have imported by using the following command. Let‚Äôs verify that the splitting has successfully been performed by checking the dimensions of the testing set. As we expect, the testing set is a 30-by-4 matrix. In other words, it contains 4 feature columns‚Äîthe width and length of sepals and petals, as mentioned earlier‚Äîand 30 observation of iris plants. We can now use the KNN model we have built to make predictions about these thirty samples. The choice of parameter  as 10 was arbitrary. That was very simple. The returned numpy array contains the class labels for each of the thirty observations in the  matrix. In other words, the first test data was predicted to belong to class 1; second data, class 0, third data, class 1, and so on. Let‚Äôs compare this predicted result with the actual labels.",0,0,1,1,0,0,0,0
Complex Fibonacci,"Well, turns out that this is, in fact, not a straight line. The only reason why it appears straight is that the snail pattern overshadows the little vibrations on this portion of the graph. Indeed, zooming in, we see that there is an interesting damping motion going on. This is what the fibonacci sequence would have looked like had we plotted only the positive domain of the real number line.  In this post, we took a look at the fibonacci sequence and its interpolation across the real number line. We could go even crazier, as did Matt Parker in his own video, by attempting to interpolate the sequence on the complex number plane, at which point we would now have a mapping from two dimensions to two dimensions, effectively forcing us to think in terms of four dimensions. There is no fast, handy way of drawing or visualizing four dimensions, as we are creatures that are naturally accustomed to three dimensions. There are interesting observations to be made with the full-fledged complex interpolation of the sequence, but I thought this is already interesting as it is nonetheless.",1,0,0,0,0,0,0,0
Bayesian Linear Regression,"What do those sampled values mean for us in the context of linear regression? Well, let‚Äôs plot some sampled lines using the  function conveniently made available through the  library.  We see that the gray lines, sampled by , all seem to be a good estimate of the true regression line, colored in gold. We might also notice that the sampled regression lines seem to stay below the true regression line for smaller values of . This is because we have more samples beneath the true regression line that we have above it. Bayesian linear regression is able to account for such variations in data and uncertainty, which is a huge advantage over the simple MLE linear regression method. The true power of Bayesian linear regression might be summarized as follows: instead of returning just a single line using the MLE weight estimate of data, Bayesian linear regression models the entire data set to create a distribution of linear functions so to speak, allowing us to sample from that distribution to obtain sample linear regression lines. This is an approach that makes much more sense, since it allows us to take into account the uncertainty in our linear regression estimate.",0,0,0,0,0,0,0,1
An Introduction to Markov Chain Monte Carlo,"defines Markov chains as follows: A Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. In other words, a Markov chain is a method of generating a sequence of random variables where the current value of that random variable probabilistically dpends on its prior value. By recursion, this means that the next value of that random variable only depends on its current state. To put this into context, we used Markovian analysis to assess the probability that a user on the internet would move to one site to another in the context of analyzing Google‚Äôs PageRank algorithm. Markov chains also popped up when we dealt with chutes and ladders, since the next position of the player in game only depends on their current position on the game board. These examples all demonstrate the Markov property, also known as memorylessness. Later on, we will see how Markov chains come in handy when we decide to ‚Äújump‚Äù from one number to the next when sampling from the posterior distribution to derive an approximation of the parameter of interest.",0,0,0,1,0,0,0,0
The Math Behind GANs,"The goal of the discriminator is to correctly label generated images as false and empirical data points as true. Therefore, we might consider the following to be the loss function of the discriminator: Here, we are using a very generic, unspecific notation for  to refer to some function that tells us the distance or the difference between the two functional parameters. (If this reminded you of something like cross entropy or Kullback-Leibler divergence, you are definitely on the right track.) We can go ahead and do the same for the generator. The goal of the generator is to confuse the discriminator as much as possible such that it mislabels generated images as being true. The key here is to remember that a loss function is something that we wish to minimize. In the case of the generator, it should strive to minimize the difference between 1, the label for true data, and the discriminator‚Äôs evaluation of the generated fake data. A common loss function that is used in binary classification problems is binary cross entropy. As a quick review, let‚Äôs remind ourselves of what the formula for cross entropy looks like: In classification tasks, the random variable is discrete.",0,0,0,0,0,1,1,0
Understanding the  Leibniz Rule,"Why is this the case? It turns out that the Leibniz rule can be proved by using the definition of derivatives and some Taylor expansion. Recall that the definition of a derivative can be written as This is something that we‚Äôd see straight out of a calculus textbook. As simple as it seems, we can in fact analyze Leibniz‚Äôs rule by applying this definition, as shown below: Thus we have shown that, if the limits of integration are constants, we can switch the order of integration and differentiation. But because our quench for knowledge is insatiable, let‚Äôs consider the more general case as well: when the limits are not bounded by constant, but rather functions. Specifically, the case we will consider looks as follows. In this case, we see that  and  are each functions of variable . With some thinking, it is not difficult to convince ourselves that this will indeed introduce some complications that require modifications to our original analysis. Now, not only are we slightly moving the graph of  in the  axis, we are also shifting the limits of integration such that there is a horizontal shift of the area box in the  axis.",1,0,0,0,0,0,0,0
Wonders of Monte Carlo,"Now, to simplify things a bit, we are going to take a look at an example that does not involve much probability distributions. Conside the following integal of sine, a classic in calculus 101: The reason why we chose this integral is that we know how to calculate it by hand. Therefore, we can match the accuracy of our crude Monte Carlo against an actual, known value. Let‚Äôs fist quickly compute this integral. Now time for Monte Carlo. Notice that there is no probability distribution explicitly defined over the domain of integration in our example. In other words,  simply follows a continuous uniform distribution, meaning that all values of  within  are equally likely. All we have to do, therefore, is to compute the expected value of the integrand by randomly generating a series of numbers within the specified domain, plug those values into the function , and take their average. This is a very elementary function that simply generates a specified number of samples given  within the domain . These numbers are then plugged into the function , after which an unweighted mean of these values are computed to approximate an integral.",0,0,0,0,0,1,0,0
Principal Component Analysis,"Also, since the covariance matrix is by definition symmetric, we can simplify things further to end up with And once again, we have shown that the principal components are the eigenvectors of the covariance matrix. But the procedure outlined above can be used to find only one principal component, that is the eigenvector with the largest eigenvalue. How do we go about searching for multiple eigenvectors? This can be done, once again, with Lagrangians, with the added caveat that we will have more trailing terms in the end. Let‚Äôs elaborate on this point further. Here, we assume that we have already obtained the first component, , and our goal is to find the next component, . With induction, we can easily see how this analysis would apply to finding . Simply put, the goal is to maximize  under the constraint that  is orthogonal to  while also satisfying the constraint that it is a unit vector. (In reality, the orthogonality constraint is automatically satisfied since the covariance matrix is symmetric, but we demonstrate this nonetheless.) Therefore, Using Lagrangians, In the last equality, we make a trivial substitution to simplify and get rid of the constant.",0,0,0,0,0,1,0,1
Demystifying Entropy (And More),"Therefore, if  and  are close, KL divergence would be low, whereas the converse would be true when the two distributions are different. We can also extend this notion a bit farther to apply it in the context of Bayesian inference. Recall that Bayesian inference is the process by which we start from some prior distribution and update our beliefs about the distribution with more data input to derive a posterior. In this context, KL divergence can be viewed as the amount of information gained as we move from the prior  to the posterior . Let‚Äôs derive the mathematical definition of KL divergence using likelihoods. The derivation process to be introduced is based on this source. We begin with the likelihood ratio: We can consider  as representing how probable the sample  came from distribution  than , given that  was sampled from some unknown distribution. If , the more likely it is that the data came from ; if , the more probable it is that the sample came from . Say we have multiple independent observations of data. Then, we can use (6) to compute the likelihood ratio for each sample.",0,0,0,0,0,1,0,0
Building Neural Network From Scratch,"The output of the first affine layer, , is modified by a ReLU unit. Then, the output is passed onto the second affine layer, , the output of which is passed onto a softmax unit. The output of the softmax function is the final output of our model. Note that ReLU and softmax are denoted as max and sigma, respectively. The code below is a function that intializes our network. Because our  data only has two classes, with each data point containing two entries corresponding to the  and  coordinates of that point, we set both  and  arguments to 2 by default. The number of neurons in the affine layers, denoted as , is arbitrarily set to 64. The  returns a dictionary that contains all the weights of the model. Note that we need to pay close attention to the dimensionality of our data to ensure that matrix multiplication is possible. We don‚Äôt have to worry about the dimensionality of the bias since  supports broadcasting by default. Presented below is a visualization of our neural network, created using NN-SVG.",0,0,0,1,0,0,1,1
An Introduction to Markov Chain Monte Carlo,"We also explored Monte Carlo in some detail here on this blog. For those of you who haven‚Äôt already, I highly recommend reading the post, as we developed a good intuition of when Monte Carlo simulations can come in handy to deal with tasks of varying difficulty. To cut to the chase, Monte Carlo methods are used to solve intractable problems, or problems that require expensive computing. Instead of systematically deriving a closed-forrm solution, we can alternatively opt for a scheme of random sampling and hope that, with a sufficient sample size, we would eventually be able to derive an approximation of the parameter. Although this seems stupid at first, it is an incredibly powerful approach to solving many problems, including the one presented here involving posterior calculation in Bayesian inference. The  is one of the first Markov Chain Monte Carlo model that was developed in the late 20th century to simulate particle movement. More advanced MCMC models have been introduced since; however, the Metrapolis-Hastings algorithm still deserves our attention as it demonstrates the basis of how many Markov Chain Monte Carlo models operate. Let‚Äôs get into the details of the model.",0,0,0,1,0,0,0,0
PyTorch RNN from Scratch,"Let‚Äôs collect all the decoded and converted tensors in a list, with accompanying labels. The labels can be obtained easily from the file name, for example . We could wrap this in a PyTorch  class, but for simplicity sake let‚Äôs just use a good old  loop to feed this data into our model. Since we are dealing with normal lists, we can easily use ‚Äôs  to separate the training data from the testing data. Let‚Äôs see how many training and testing data we have. Note that we used a  of 0.1. We will be building two models: a simple RNN, which is going to be built from scratch, and a GRU-based model using PyTorch‚Äôs layers. Now we can build our model. This is a very simple RNN that takes a single character tensor representation as input and produces some prediction and a hidden state, which can be used in the next iteration. Notice that it is just some fully connected layers with a sigmoid non-linearity applied during the hidden state computation. We call  at the start of every new batch. For easier training and learning, I decided to use  to initialize these hidden states.",0,0,0,1,0,0,1,0
Gaussian Process Regression,"The underlying idea is that we can apply Cholesky decomposition on , and use that as a way to circumvent the need for direct inversion. Let , then Let‚Äôs make another substitution, . Then, This means that we can calculate the mean  as Similarly, for the covariance , we can introduce an intermediate variable  from which we obtain Notice that the final expressions for mean and covariance do not require any form of inversion, which was our end goal for efficient and accurate computation. Let‚Äôs transcribe everything back to code. Let  refer to  (and by the same token  refers to ). Then, Just to be safe, let‚Äôs check that  is of the desired shape, namely a vector with 50 entries. Continuing with our computation of the posterior covariance, As expected,  is a 50-by-50 matrix. We are now almost done. Since we have computed the mean  and covariance , all there is left is to generate samples from this distribution. For that, we resort to Cholesky decomposition again, recalling the idea discussed earlier in (19). Let‚Äôs sample a total of 50 samples. now contains 50 samples generated from the posterior.",0,0,1,1,0,0,0,0
Traveling Salesman Problem with Genetic Algorithms,"The contrived semi-circle example, for instance, took somewhere around five to ten minutes to fully run on my 13-inch MacBook Pro. Nonetheless, I think it is an interesting way well worth the time and effort spent on implementation. I hope you‚Äôve enjoyed reading this post. Catch you up in the next one!.",0,0,0,1,0,0,0,0
Gaussian Process Regression,"Normally, an integral like the one above would be intractable without a solution in closed form. Hence, we would have to rely on random sampling methods such as MCMC. However, in this case, we do have a closed form solution, and we know what it looks like: a Gaussian! This means that we uniquely identify the final posterior distribution through GP regression‚Äîall we need is the mean and covariance. Let‚Äôs start with the easy one first: the mean. The mean is a trivial parameter because we can always normalize the mean to zero by subtracting the mean from the data. Therefore, for simplicity purposes, we assume a zero mean throughout this post. The interesting part lies in the covariance. Recall that the covariance matrix is defined as follows: Roughly speaking, covariance tells us how correlated two entries of the random vector are. This is where I think GPs get really interesting: the key point of GP is to realize that we want to model some smooth function that best fits our data.",0,0,1,1,0,0,0,0
My First GAN,"By coercing a true label on the GAN, we are effectively forcing the generator to produce more compelling images, and penalizing it when it fails to do so. Personally, I find this part to be the genius and beauty of training GANs. Now that we have an idea of what the function accomplishes, let‚Äôs use it to start training. The  seems to fluctuate a bit, which is not necessarily a good sign but also quite a common phenomenon in GAN training. GANs are notoriously difficult to train, since it requires balancing the performance of the generator and the discriminator in such a way that one does not overpower the other. This is referred to as a min-max game in game theory terms, and finding an equilibrium in such structures are known to be difficult. Let‚Äôs take a look at the results now that the iterations are over.  The created images are admittedly fuzzy, pixelated, and some even somewhat alien-looking. This point notwithstanding, I find it incredibly fascinating to see that at least some generated images actually resemble ships in the sea. Of particular interest to me are the red ships that appear in  and .",0,0,0,0,1,0,1,0
Introduction to tf-idf,"Next, we need to tokenize the strings by splitting them into words. In this process, we will also convert all documents to lower case as well. Note that  works on each documents, not the entire collection. Let‚Äôs try calling the function with the first document in our dummy example. Finally, as part of the preprocessing step, let‚Äôs build the corpus. The corpus simply refers to the entire set of words in the dataset. Specifically for our purposes, the corpus will be a dictionary whose keys are the words and values are an ordinal index. Another way to think about the corpus in this context is to consider it as a word-to-index mapping. We will be using the indices to represent each word in the tf, idf, and tf-idf vectors later on in the tutorial. Because we have a very simple example, our corpus only contains 9 words. This also means that our tf-idf vectors for each document will also be a list of length 9. Thus it isn‚Äôt difficult to see how tf-idf vectorization can result in extremely high-dimensional matrices, which is why we often apply techniques such as lemmatization or PCA on the final result.",0,0,0,1,0,0,0,0
"Beta, Bayes, and Multi-armed Bandits","Since we are sampling from a distribution instead of relying on a point estimate as we do for the greedy approach, this allows for both exploration and exploitation to happen at reasonable frequencies. If a posterior distribution has large variance, this means that we will explore that particular bandit slightly more than others. If a posterior has a large mean‚Äîa high success parameter‚Äîthen we will exploit that machine a bit more to earn more profit, or, in this context, to minimize regret. Before we move on any farther, perhaps‚Äô it‚Äôs worth discussing what the term ‚Äúregret‚Äù means in this context. Simply put, regret refers to the amount that we have comparatively lost by making a sub-optimal choice from the get go. Here is a visual diagram I came across on Analytics Vidhya.  The maximum reward would obviously be achieved if we pull on the slot machine with the highest success parameter from trial 1. However, this does not happen since the gambler dives into the game without this prior knowledge. Hence, they have to learn what the optimal choice is through exploration and exploitation.",0,1,0,0,0,1,0,0
Recommendation Algorithm with SVD,"Alternatively, we can also see this by taking a look at the full, unreduced version of the matrix  or . For example, the code snippet below displays the full version of the factor . It is not difficult to see that the last few columns of  contain values so small that their contribution to data is going to be minimal at best. This is not the most mathematical way of presenting the concept of data‚Äîdoing so would require us to take a look at other metrics such as covariance‚Äîbut this basic analysis will suffice for our purposes for now. The takeaway is that dimensionality reduction is a meaningful way to extract important information from our data. Now that we have performed SVD on the ratings matrix, let‚Äôs move onto the last step: crafting a model for our recommendation algorithm. My personal pet theory is that using any word in conjunction with ‚Äúalgorithm‚Äù makes the concept sound more complex than it actually is. This is exactly what we are doing here, because in reality, our so-called algoithm for movie recommendations is going to be very simple. The intuition behind the recommendation system is distance calculation.",0,0,0,1,0,0,0,1
MLE and KL Divergence,"The familiar equation for KL divergence goes as follows: In Bayesian terms, KL divergence might be used to compare the prior and the posterior distribution, where  represents the posterior and , the prior. In machine learning,  is often the true distribution which we seek to model, and  is the approximation of that true distribution, which is also the prediction generated by the model. Note that KL divergence is not a true measure of distance, since it is asymmetric. In other words, The focus of this post is obviously not on distance metrics, and I plan on writing a separate post devoted to this topic. But as a preview of what is to come, here is an appetizer to get you interested. An alternative to KL divergence that satisfies the condition of symmetry is the Jensen-Shannon Divergence, which is defined as follows: where One can intuit JSD as being a measurement that somewhat averages the two asymmetric quantities of KL divergence. We will revisit JSD in the future when we discuss the mathematics behind GANs. But for now, it suffices to know what KL divergence is and what it measures.",0,0,0,0,0,1,0,0
A Brief Introduction to Recurrent Neural Networks,"Instead of using sparse vectors to represent each word, we can simply use a denser vector of smaller dimensions to encode our data. Another advantage of this approach is that dense vectors can be used to encode semantic information. You might have heard of the famous example that ‚Äúking minus male equals queen minus female.‚Äù If we were to represent the words king, queen, male, and female as vectors, we can add and subtract vectors to represent and distill meaningful information. This vector-based computation is the key to natural language processing with deep neural networks: by back propagating and adjusting the weights of our embedding layer, our model can eventually be trained to ‚Äúunderstand‚Äù the meaning of words and their relationship with other words in the form of dense vectors. Enough talking, let‚Äôs use the embedding layer to build our neural networks, starting with the simple feed-forward model. The feed-forward neural network model will first have an embedding layer that processes input. Then, the output of this embedding layer will be flattened to be passed onto a dense layer with one output transformed by the sigmoid activation function.",0,0,0,0,1,0,1,0
Demystifying Entropy (And More),"Calculating the product of these ratios will tell us which distribution is more likely given all available data points. In other words, A technique we saw when we were exploring the topic of likelihood maximization was log likelihood. Log likelihoods are useful because we can reexpress products as sums, using the property of logs. (7) makes sense, but it weighs all likelihood ratios equally. In reality, most samples are not equiprobable; some values are more likely than others, unless in the context of uniform distributions. To account for this, let‚Äôs reframe (7) as an expected values calculation, i.e. give different weight to each likelihood ratio depending on the probability of observing that data point. Let‚Äôs make (8) look better by unpacking the fraction sitting in the log function as a subtraction of two terms. The final key is to realize the secret that In other words, we have derived the mathematical definition of KL divergence! The mathematical definition of cross entropy can simply be derived by plugging in (10) into (5).",0,0,0,0,0,1,0,0
Recommendation Algorithm with SVD,"Calculating , we get which is symmetric as we expect. We can calculate the eigenvalues of this matrix by finding the roots of the following characteristic polynomial: Since  in SVD is the diagonal matrix that contains the square roots of the eigenvalues of , we can conclude that where  denotes the value of the th diagonal entry in . Therefore, given the dimensionality of , we can conclude that Next, we find the eigenvalues of . This process can be performed by identifying the null space of the matrix . For instance, given , Given the orientation of this matrix, we see that By doing the same for , we can construct the matrix : Repeating the procedure for  to obtain the factor , we can complete the singular value decomposition on A: The key to dimensionality reduction is that the first few columns of , its corresponding eigenvalues in , and the corresponding first few rows of  contain the most amount of information on matrix . As we  go down the diagonal entries of , we see that the eigenvalues get smaller.",0,0,0,1,0,0,0,1
Likelihood and Probability,"We can then make the following statement about these probabilities: In other words, to maximize the likelihood simply means to find the value of a parameter that which maximizes the product of probabilities of observing each data point. The assumption of independence allows us to use multiplication to calculate the likelihood in this manner. Applied in the context of normal distributions with  observations, the likelihood function can therefore be calculated as follows: But finding the maximum of this function can quickly turn into a nightmare. Recall that we are dealing with distributions here, whose PDFs are not always the simplest and the most elegant-looking. If we multiply  terms of the normal PDF, for instance, we would end up with a giant exponential term. To prevent this fiasco, we can introduce a simple transformation: logarithms. Log is a monotonically increasing function, which is why maximizing some function  is equivalent to maximizing the log of that function, . Moreover, the log transformation expedites calculation since logarithms restructure multiplication as sums. With that in mind, we can construct a log equation for MLE from (3) as shown below. Because we are dealing with Euler‚Äôs number, , the natural log is our preferred base.",0,0,0,0,0,1,0,0
A Brief Introduction to Recurrent Neural Networks,"The dimensionality of the output is a little more tricky because of the  option. What concatenate does is that it basically flattens all  number of outputs into a single list. In this case, because we set the option to , we get a flattened list containing , or 6400 elements. The main takeaway is that recurrent neural networks can be used to implement some sort of memory functionality, which is useful when dealing with datasets where there exists some sort of sequential structure. One way to implement memory is by using the output of the previous sequence to define a  variable, which is used to compute the next output as we have done above. Now let‚Äôs get down to business with the  API. Implementing a recurrent neural network is not so much different from building a simple feed forward or convolutional neural network: we simply import a RNN-specific layer and arrange these layers to construct a working model. Before we proceed any further, let‚Äôs first import all necessary dependencies for this tutorial. As with any tutorial, we need to start by loading and preprocessing data.",0,0,0,0,1,0,1,0
Likelihood and Probability,"‚ÄúI think that‚Äôs very unlikely.‚Äù ‚ÄúNo, you‚Äôre probably right.‚Äù These are just some of the many remarks we use in every day conversations to express our beliefs. Linguistically, words such as ‚Äúprobably‚Äù or ‚Äúlikely‚Äù serve to qualify the strength of our professed belief, that is, we express a degree of uncertainty involved with a given statement. In today‚Äôs post, I suggest that we scrutinize the concept of likelihood‚Äîwhat it is, how we calculate it, and most importantly, how different it is from probability. Although the vast majority of us tend to conflate likelihood and probability in daily conversations, mathematically speaking, these two are distinct concepts, though closely related. After concretizing this difference, we then move onto a discussion of maximum likelihood, which is a useful tool frequently employed in Bayesian statistics. Without further ado, let‚Äôs jump right in. As we have seen in an earlier post on Bayesian analysis, likelihood tells us‚Äîand pardon the circular definition here‚Äîhow likely a certain parameter is given some data.",0,0,0,0,0,1,0,0
Dissecting the Gaussian Distribution,"The derivation of the multivariate Gaussian was complicated by the fact that we were dealing with matrices and vectors instead of single scalar values, but the matrix-scalar parallel intuition helped us a lot on the way. Note that the derivation of the multivariate Gaussian distribution introduced in this post is not a rigorous mathematical proof, but rather intended as a gentle introduction to the multivariate Gaussian distribution. I hope you enjoyed reading this post on normal distributions. Catch you up in the next one.",0,1,0,0,0,1,0,0
Naive Bayes Model From Scratch,"We can combine both  and  functions to create a new wrapper function  that returns the mean and standard deviation of each column for each class. This is a crucial step that will allow us to perform a MAP approximation for the distribution of variables for each class. Testing out the function on  created earlier yields the desired result. Notice that the returned dictionary contains information for each class, where the key corresponds to the label and the value contains the parameters calculated from . A good way to understand this data is through visualization. Let‚Äôs try to visualize what the distribution of  and  looks like for data labeled class . We can use the  library to create a joint plot of the two random variables.  We see that both variables are normally distributed. Therefore, we can imagine data points for class  to be distributed across a three-dimensional Gaussian distribution whose center lies at the point where the the plot has the darkest color, i.e. . I find this way of understanding data to be highly intuitive in this context. Now, it‚Äôs time to bake Bayesian philosophy into code.",0,0,1,1,0,0,0,0
Revisiting Basel with Fourier,"While a lot of this is just simple calculus and algebra, I nonetheless find it fascinating how the Basel problem can be approached from so many different angles‚Äîhence my renewed respect for Euler and other mathematicians who wrestled with this problem hundreds of years ago. As simple as it appears, there are so many different techniques and modes of analysis we can use to approach the problem. It was nice exercise and review of some calculus techniques. I‚Äôve been digging more into the curious interchange of integral and summation recently, and when this operation is allowed, if at all. Turns out that this problem is slightly more complicated than it appears and requires some understanding of measure theory, which I had tried getting into a few months ago without much fruition. Hopefully this time, I‚Äôll be able to figure something out, or at the very least gain some intuition on this operation. I hope you‚Äôve enjoyed reading this post. Catch you up in the next one.",1,0,0,0,0,0,0,0
Naive Bayes Model From Scratch,"So we can discard this piece of information and distill (2) down even farther: Equation (3) tells us that it is possible to calculate the probability of instance  belonging to class  systematically. Why is this important? The simple answer is that we can use (3) to train the naive Bayes classifier. Say we know that for a particular instance , the label is . Then, we have to find the distribution for each feature such that we can maximize . Does this ring any bells? Yes‚Äîit is maximum a posteriori estimation! In other words, our goal would be to maximize the posterior distribution for each training instance so that we can eventually build a model that would output the most likely label that the testing instance belongs to. In other words, our training scheme can be summarized as: But this is all to abstract. Let‚Äôs get into the details by implementing the naive Bayes classifer from scratch. Before we proceed, however, I must tell you that there are many variations of the naive Bayes classifer.",0,0,1,1,0,0,0,0
Dissecting the Gaussian Distribution,"We can also see why (9) is coherent by unpacking the expected values expression as shown below: Using the linearity of expectation, we can rewrite the equation as Therefore, we end up with which almost exactly parallels the definition of variance, which we might recall is where . The key takeaway is that the covariance matrix constructed from the random vector  is the multivariable analogue of variance, which is a function of the random variable . To gain a better idea of what the covariance matrix actually looks like, however, it is necessary to review its structure element-by-element. Here is the brief sketch of the -by- covariance matrix. This might seem complicated, but using the definition of covariance in (8), we can simplify the expression as: Note that the covariance matrix is a symmetric matrix since . More specifically, the covariance matrix is a positive semi-definite matrix. This flows from the definition of positive semi-definiteness. Let  be some arbitrary non-zero vector. Then, You might be wondering how (9) ends up as (10).",0,1,0,0,0,1,0,0
"Linear Regression, in Two Ways","But on the other hand, this is what we should have expected all along: no matter what method we use, the underlying thought process behind both modes of approach remain the same. Whether it be through projection or through derivation, we sought to find some parameters, closest to the values we are approximating as much as possible, that would turn an otherwise degenerate system into one that is solvable. Linear regression is a simple model, but I hope this post have done it justice by demonstrating the wealth of mathematical insight that can be gleaned from its derivation.",0,0,0,0,0,0,0,1
Introduction to tf-idf,"‚Äù I know that I use this word a lot before writing down equations or formulas, just for the sake of notational clarity. However, the word itself carries little information on what the post is about. The same goes for other words, such as ‚Äúexample,‚Äù ‚Äúhowever,‚Äù and so on. So term frequency only doesn‚Äôt really tell us much; instead, we want to pay attention to words that occur frequently in a given document, but doesn‚Äôt appear a lot in others‚Äîsuch words are most likely to be unique keywords that potentially capture the gist of that document. Given this analysis, it isn‚Äôt difficult to see why tf-idf is designed the way it is. Although we give priority weight to words with high term frequency, we discount words that appear frequently across all documents by dividing tf by idf, or inverse document frequency. In short, document frequency tells us how frequently a given word appears throughout all documents; the inverse is the reciprocal of that quantity. In practice, we often apply a logarithm to prevent the idf score from exploding. Also, we add some smoothing to prevent division by zero.",0,0,0,1,0,0,0,0
PyTorch Tensor Basics,"More exactly speaking, having an additional layer means that it is capable of storing another tensor within it; hence,  is living in a dimension that is one above that of . As mentioned earlier, batch dimension is something that becomes very important later on. Some PyTorch layers, most notably RNNs, even have an argument , which accepts a boolean value. If , PyTorch expects the first dimension of the input to be the batch dimension. If , which is the case by default, PyTorch assumes that the first dimension would be the sequence length dimension. A common operation that is used when dealing with inputs is , or its inverse, . Before explaining what these operations perform, let‚Äôs just take a look at an example. Let‚Äôs start with , the random tensor of size  initialized above. If we apply  to , we essentially add a new dimension to the 0-th position of ‚Äôs shape. As you can see, now there is an additional batch dimension, thus resulting in a tensor whose shape is  as opposed to the original . However, of course this operation is not performed in-place, meaning that  will still remain unchanged.",0,0,0,0,0,0,1,0
"Newton-Raphson, Secant, and More","We confirm that this is indeed the root of the equation. Now that we have looked at both methods, it‚Äôs time to make a quick comparison. We will be comparing three different methods: By setting  to be , we can obtain a full list of root updates as the three methods begin their quest for the root of the function. We can then see which method converges the quickest. Let‚Äôs see how this little experiment turns out. We first begin by importing some dependencies to plot the history of values. Then, we obtain the history for each of the three approaches and plot them as a scatter plot. The result is shown below.  You might have to squint your eye to see that  (Netwon-Raphson with direct derivatives) and  (Newton-Raphson with center divided difference) almost coincide exactly at the same points. I was honestly somewhat surprised by the result: although we had verified the advantage of using center divided difference over forward or backward divided difference via some simple big-O analysis with trailing error terms, I did not expect the two to coincide with such exactitude.",1,0,0,0,0,0,0,0
"PyTorch, From Data to Modeling","The easiest way to go about this is to use the  method, which looks like this: Here, we are applying to transformations: the first changes the dataset and casts it into PyTorch tensors,, and the second one normalizes the dataset to have a mean of 0.5 and a standard deviation of also 0.5 across all three channels of RGB. How can we apply this transform? Well, we can pass it to initialize the datasets as shown below: Because I already have the CIFAR10 downloaded in the  directory of my local, PyTorch does not download the dataset again. We could go with the dataset as-is, but we can use the  class to further batch and shuffle the dataset, which we normally want 99 percent of the time. This is as simple as calling  and passing in the dataset we want to load. If we loop through the , for instance, we can see that it is giving us a nice batch of 32 photos. Note that the dimensions are in the form of . As for the labels, we get 32 values where each number corresponds to an image.",0,0,0,0,0,0,1,0
Gaussian Process Regression,"This is expected given that the components of , namely  in (19), will never be square unless the number of test and training points are equal. Now let‚Äôs generate some dummy data. In theory, the final function sampled through GP is considered an infinite dimensional vector, but for practical reasons of implementation, the vector in this case will be at most 60 dimensions: ten training points and 50 test points, appended together as one vector. Next, let‚Äôs build the kernel with the test points and draw random samples to see what our prior looks like. Recall that sampling can be easily achieved by performing the Cholesky decomposition on the kernel. Let‚Äôs plot the ten random samples drawn from the prior. Note that at this point, we have not seen any training data at all. The only stricture imposed on GP vis a vis the kernel is the fact that the function must be smooth, i.e. points that are close to each other in must be highly correlated.  Indeed, the sampled data seems to present somewhat smooth curves, although the smoothness is somewhat mitigated by the fact that the model are only vectors of 50 dimensions.",0,0,1,1,0,0,0,0
Fisher Score and Information,"The derivation is not the easiest, but I‚Äôll try to provide a concise version based on my admittedly limited understanding of this topic. Let‚Äôs start from some trivially obvious statements. First, from the definition of a PDF and the derivative operation, we know that Therefore, both the first and second derivative of this function are going to be zero. In multivariate speak, both the gradient and the Hessian are zero vectors and matrices, respectively. Using the Leibniz rule we saw earlier, we can interchange the derivative and come up with the following expressions. Granted, these expressions somewhat muffle the shape of the quantity we are dealing with, namely vectors and matrices, but it is concise and intuitive enough for our purposes. With these statements in mind, let‚Äôs now begin the derivation by first taking a look at the Hessian of the score function. From the chain rule, we know that This does not look good at all. However, let‚Äôs not fall into despair, since our goal is not to calculate the second derivative or the Hessian itself, but rather its negative expected value.",0,0,0,0,0,1,0,0
Natural Gradient and Fisher,"Due to the definition of entropy, KL divergence ends up having a log likelihood term, while Fisher‚Äôs matrix is the negative expected Hessian of the log likelihood function, or the covariance matrix of Fisher‚Äôs score, which is the gradient of the log likelihood. Either way, we know that likelihood is the fundamental bridge connecting the two. Let‚Äôs try to compute the KL divergence between  and . Conceptually, we can think of  as the previous point of the parameter and  as the newly updated parameter. In this context, the KL divergence would tell us the effect of one iteration of natural gradient descent. This time, instead of using integral, let‚Äôs try to simplify a bit by expressing quantities as expectations. We see the familiar log likelihood term. Given the fact that the Fisher matrix is the negative expected Hessian of the log likelihood, we should be itching to derive this expression twice to get a Hessian out of it. Let‚Äôs first obtain the gradient, then get its Jacobian to derive a Hessian. This derivation process was heavily referenced from Agustinus Kristiadi‚Äôs blog. Let‚Äôs do this one more time to get the Hessian.",0,0,1,0,0,1,0,0
k-Nearest Neighbors Algorithm from Scratch,"As expected, the returned list contains the indices of the data points in  that are closest to . We can confirm this by looking at the results of the distance calculation we obtained when testing the  function. Note that the indices are in order; that is, indice 0 corresponds to the closet neighbor‚Äîit is in fact that data point itself‚Äîand index 2 refers to the farthest neighbor among the  selections. Say we have successfully obtained the list of  nearest neighbors. Now what? Well, it‚Äôs time to look up the labels of these neighboring data points to see which class is the most prevalent. The KNN model will then conclude that the most prevalent class label is the one that which the data point belongs to. Because this function has to perform more tasks than the functions we wrote earlier, the example code is slightly longer, but here it goes: Basically, the  function counts the number of labels of each class and stores the results in a dictionary. Then, it normalizes the values of the dictionary by dividing its values by the total number of data points seen.",0,0,1,1,0,0,0,0
A sneak peek at Bayesian Inference,"In other words, the prior would appear to be a uniform distribution, which is really a specific instance of a Beta distribution with . Presented below is a code snippet that simulates 500 coin flips, throughout which we perform five calculations to update our posterior. Executing this code block produces the following figure. This plot shows us the change in our posterior distribution that occurs due to Bayesian update with the processing of each data chunk. Specifically, we perform this Bayesian update after  trials. When no coin flips are performed, as shown in the first subplot, the prior follows a uniform distribution as detailed above. As more coin tosses are performed, however, we start to develop an understanding of the fairness of the coin. When we only have a few data points, the more probability there is that we obtain skewed data, which is why the mean estimate of our posterior seems skewed as well. However, with a larger number of trials, the law of large numbers guarantees that we will eventually be able to identify the value of our parameter , which is indeed the case. The key takeaway from this code block is the line .",0,1,0,0,0,1,0,0
BLEU from scratch,"This value, however, is clipped by , which is the maximum number of occurrence of that n-gram in any one of the reference sentences. In other words, for each reference, we count the number of occurrence of that n-gram and take the maximum value among them. This can seem very confusing, but hopefully it‚Äôs clearer once you read the code. Here is my implementation using . Notice that we use a  in order to remove redundancies.  corresponds to ;  corresponds to . Using this modified metric, we can see that the  is now penalized quite a lot through the clipping mechanism. But there are still problems that modified precision doesn‚Äôt take into account. Consider the following example translation. To us, it‚Äôs pretty obvious that  is a bad translation. Although some of the key words might be there, the order in which they are arranged violates English syntax. This is the limitation of using unigrams for precision analysis. To make sure that sentences are coherent and read fluently, we now have to introduce the notion of n-grams, where  is larger than 1. This way, we can preserve some of the sequential encoding in reference sentences and make better comparison.",0,0,0,1,0,0,1,0
How lucky was I on my shift?,"At the Yongsan Provost Marshall Office, I receive a wide variety of calls during my shift. Some of them are part of routine communications, such as gate checks or facility operation checks. Others are more spontaneous; fire alarm reports come in from time to time, along with calls from the Korean National Police about intoxicated soldiers who get involved in mutual assault or misdemeanors of the likes. Once, I got a call from the American Red Cross about a suicidal attempt of a soldier off post. All combined, I typically find myself answering about ten to fifteen calls per shift. But yesterday was a special day, a good one indeed, because I received only five calls in total. This not only meant that USAG-Yongsan was safe and sound, but also that I had a relatively light workload. On other days when lawlessness prevails over order, the PMO quickly descends into chaos‚Äîpatrols get dispatched, the desk sergeant files mountains of paperwork, and I find myself responding to countless phone calls while relaying relevant information to senior officials, first sergeants, and the Korean National Police.",0,1,0,0,0,1,0,0
Natural Gradient and Fisher,"In case of batch gradient descent, we used Euclidean distance. This made sense since we were simply measuring the distance between two parameters, which are effectively scalars or vector quantities. If we want to search the distribution space, on the other hand, we would have to measure the distance between two probability distributions, one that is defined by the previous parameter and the other defined by the newly found parameter after natural gradient descent. Well, we know one great candidate for this task right off the bat, and that is KL divergence. Recall that KL divergence is a way of quantifying the pseudo-distance between two probability distributions. The formula for KL divergence is shown below. And while we‚Äôre at it, let‚Äôs throw cross entropy and entropy into the picture as well, both for review and clarity‚Äôs sake: For a short, simple review of these concepts, refer to this previous article, or Aurelien Geron‚Äôs video on YouTube. In most cases,  is the true distribution which we seek to model, while  is some more tractable distribution at our disposal. In the classic context of ML, we want to minimize the KL divergence.",0,0,1,0,0,1,0,0
How lucky was I on my shift?,"So let me ask the question again: how lucky was I yesterday? The probability distribution function of the Poisson distribution tells us that  can be calculated through the following equation: The result given by the Poisson distribution is somewhat larger than that derived from the binomial distribution, which was . This discrepancy notwithstanding, the fact that I had a very lucky day yesterday does not change: I would have days like these once every 100 days, and those days surely don‚Äôt come often. But to really calculate how lucky I get for the next 18 months of my life in the military, we need to do a bit more: we need to also take into account the fact that receiving lesser than 5 calls on a shift also constitutes a lucky day. In other words, we need to calculate , as shown below: This calculation can be done rather straightforwardly by plugging in numbers into the Poisson distribution function as demonstrated above. Of course, this is not the most elegant way to solve the problem. We could, for instance, tweak the Poisson distribution function and perform integration.",0,1,0,0,0,1,0,0
Scikit-learn Pipelines with Titanic,"By default, this shows us the first five rows and as many columns as it can fit within the notebook. Let‚Äôs take a look at the data in more depth to build a foundation for our analysis. This step typically involves the following steps: Let‚Äôs proceed in order. Before proceeding with any data analysis, it‚Äôs always a good idea to pay attention to missing values‚Äîhow many of them there are, where they occur, et cetera. Let‚Äôs take a look. The  is useful, but is doesn‚Äôt really show us how many values are missing for each column. To probe into this issue in more detail, we need to use  instead. I recently realized that there is also a very cool data visualization library called  for observing missing data.  This visualization gives us a more intuitive sense of where the values are missing. In this case, the missing values seem to be distributed somewhat evenly or randomly. However, we can also imagine cases were missing values might have something to do with an inherent attribute in the dataset (e.g. only male participants of a survey might reply ‚ÄúN/A‚Äù to some health questionaire involving inquiries on pregnancy).",0,0,1,0,0,0,0,0
A sneak peek at Bayesian Inference,"I hope this post gave you a better understanding as to why distributions are important‚Äîspecifically in the context of conjugate priors. In a future post, we will continue our exploration of the Beta distribution introduced today, and connect the dots between Beta, Gamma, and many more distributions in the context of Bayesian statistics. See you in the next one.",0,1,0,0,0,1,0,0
Gamma and Zeta,"After some cosmetic simplifications, we end up with Putting everything together, now we have derived a nice expression that places both the Riemann zeta and the Gamma functions together: Or, alternatively, a definition of the Riemann zeta in terms of the Gamma: And indeed, with (13), we can evaluate the Riemann zeta function at non-integer points as well. This is also the definition of the Riemann zeta function introduced in Wikipedia. The article also notes, however, that this definition only applies in a limited number of cases. This is because we‚Äôve assumed, in using the summation of the geometric series formula, the fact that . Today‚Äôs post was a short yet very interesting piece on the relationship between the Gamma and the Riemann zeta. One thing I think could have been executed better is the depth of the article‚Äîfor instance, what is the Bose integral and when is it used? I‚Äôve read a few comments on the original YouTube video by blackpenredpen, where people were saying that the Bose integral is used in statistical mechanics and the study of black matter, but discussing that would require so much domain knowledge to cover.",1,0,0,0,0,0,0,0
Gaussian Process Regression,"Gaussian Processes (GPs) are similar to Bayesian linear regression in that the final result is a distribution which we can sample from. The biggest point of difference between GP and Bayesian regression, however, is that GP is a fundamentally non-parametric approach, whereas the latter is a parametric one. I think this is the most fascinating part about GPs‚Äîas we will see later on, GPs do not require us to specify any function or model to fit the data. Instead, all we need to do is to identify the mean and covariance of a multivariate Gaussian that defines the posterior of the GP. All of this sounds too good be true‚Äîhow can a single multivariate Gaussian distribution be enough for what could potentially be a high-dimensional, complicated regression problem? Let‚Äôs discuss some mathematical ideas that enable GP to be so powerful. Gaussians are essentially a black hole of distributions: once a Gaussian, always a Gaussian. For example, we know from a previous post on Gibbs sampling that the conditional of a multivariate Gaussian is also Gaussian. given the setup that and And of course, the marginal of a multivariate Gaussian also produces another Gaussian.",0,0,1,1,0,0,0,0
Scikit-learn Pipelines with Titanic,"If certain features in the raw data are deemed useless, we need to either drop it or engage in some sort of feature engineering to produce a new set of more correlated features.  From this preliminary analysis, it seems like there are some very weekly correlated features, namely  and . The week correlation suggests that perhaps we need to engage in some feature engineering to extract more meaningful information out of the dataset. Let‚Äôs use the findings from the previous section to engineer some more informative features. One popular approach is to make use of names to derive a  feature. Intuitively, this makes sense: Mr. and Mrs, Cpt. and Dr. might be of interest for our model. Another popular approach is to combine the less important features‚Äî and ‚Äîinto something like . Implementing these should fairly be simple, so let‚Äôs try it here. Note that in an actual setting, there will be no answer to reference; we will have to rely on our own domain knowledge and more extensive EDA to figure out which features matter, and what new features we will need.",0,0,1,0,0,0,0,0
A Step Up with  Variational Autoencoders,"In a previous post, we took a look at autoencoders, a type of neural network that receives some data as input, encodes them into a latent representation, and decodes this information to restore the original input. Autoencoders are exciting in and of themselves, but things can get a lot more interesting if we apply a bit of twist. In this post, we will take a look at one of the many flavors of the autoencoder model, known as variational autoencoders, or VAE for short. Specifically, the model that we will build in this tutorial is a convolutional variational Autoencoder, since we will be using convolutional layers for better image processing. The model architecture introduced in this tutorial was heavily inspired by the one outlined in Fran√ßois Chollet‚Äôs Deep Learning with Python, as well as that from a separate article on the Keras blog. Let‚Äôs start by importing the modules necessary for this demonstration. The objective of today‚Äôs task is to build an autoencoder model that produces MNIST hand-written digits. The hidden dimension, or the latent space of the model, is going to a random vector living in two-dimensional space.",0,0,0,0,1,0,1,0
The Magic of Euler‚Äôs Identity,"Therefore, the Taylor series will provide the most accurate estimation of the original function around that point, and the farther we get away from , the worse the approximation will be. For the purpose of our analysis, let‚Äôs examine the Taylor polynomials for the following three functions: , and . Recall that the derivative of  is , which is precisely what the Taylor series suggests. It is also interesting to see that the Taylor series for  is an odd function, while that for  is even, which is coherent with the features of their respective original functions. Last but not least, notice that the derivative of Taylor polynomial of  gives itself, as it should. Now that we have the Taylor polynomials, proving Euler‚Äôs identity becomes a straightforward process of plug and play.",1,0,0,0,0,0,0,0
Recommendation Algorithm with SVD,"Singular value decomposition can intuitively be thought of as a square root version of eigendecomposition, since essentially  and  are all derivatives that come from the ‚Äúsquare‚Äù of a matrix, the two transpose multiples. This intuition also aligns with the fact that  is a diagonal matrix containing the square roots of eigenvalues of the transpose products. With these in mind, let‚Äôs get ready to build the recommendation model. In this optional section, we take a look at two mathematical propositions we referenced while motivating the SVD formula: first, that symmetric matrices have orthogonal eigenvectors; second, that  and  have identical non-zero eigenvalues. The proof for both of these statements are simple, but feel free to gloss over this section if you just want to see SVD at work instead of the mathematical details behind singular value decomposition. Let  be some symmetric matrix, i.e. . Also assume that  has two distinct eigenvectors,  and  with corresponding eigenvalues  and .",0,0,0,1,0,0,0,1
Markov Chain and Chutes and Ladders,"In a previous post, we briefly explored the notion of Markov chains and their application to Google‚Äôs PageRank algorithm. Today, we will attempt to understand the Markov process from a more mathematical standpoint by meshing it together the concept of eigenvectors. This post was inspired and in part adapted from this source. In linear algebra, an eigenvector of a linear transformation is roughly defined as follows: a nonzero vector that is mapped by a given linear transformation onto a vector that is the scalar multiple of itself This definition, while seemingly abstract and cryptic, distills down into a simple equation when written in matrix form: Here,  denotes the matrix representing a linear transformation; , the eignevector; , the scalar value that is multiplied onto the eigenvector. Simply put, an eigenvector  of a linear transformation is one that is‚Äîallow me to use this term in the loosest sense to encompass positive, negative, and even imaginary scalar values‚Äî‚Äústretched‚Äù by some factor  when the transformation is applied, i.e. multiplied by the matrix  which maps the given linear transformation. The easiest example I like to employ to demonstrate this concept is the identity matrix .",0,0,0,0,0,0,0,1
Wonders of Monte Carlo,"Crude Monte Carlo is powerful, but in a way it is inefficient because we have to sample large amounts to ensure that the resulting sample is representative, which is a condition that must be satisfied to produce a reliable estimate. There are a plethora of mathematical techniques that build on top of crude Monte Carlo to ensure that sampling is done correctly and more efficiently, such as importance sampling, but for the purposes of this post, we will stop here and move onto the last task: simulating random walk. The last task we will deal with in this post is simulating what is known as the drunkard‚Äôs walk, a version of which is introduced here. The drunkard‚Äôs walk is a type of random walk with a specified termination condition. As the name suggests, the drunkard‚Äôs walk involves a little story of an intoxicated man trying to reach (or avoid) some destination, whether that be a cliff or, in our case, a restroom. Because he is drunk, he cannot walk to the restroom in a straight path as a normal person would do; instead, he stumbles this way and that, therefore producing a random walk.",0,0,0,0,0,1,0,0
A PyTorch Primer,"The only notable difference is that we didn‚Äôt define a separate  type function. For the most part, the overall idea boils down to Now let‚Äôs take a look at what the training code looks like. Although things might look a bit different, there‚Äôs not much going on in this process, other than the fact that some of the functions and logic we wrote before are now abstracted away by PyTorch. For example, we see , which is effectively the mean squared error loss, similar to how we defined  above. Another difference we see is , which, as the variable name makes apparent, is the optimizer that we use for backpropagation. In this specific instance, we use SGD. Each backpropagation step is then performed simply via . In this tutorial, we took a very brief look at the PyTorch model. This is by no means a comprehensive guide, and I could not even tell anyone that I ‚Äúknow‚Äù how to use PyTorch. Nonetheless, I‚Äôm glad that I was able to gain some exposure to the famed PyTorch module.",0,0,0,0,0,0,1,0
Recommendation Algorithm with SVD,"Now that we have a mathematical understanding of how singular value decomposition, let‚Äôs see how we can apply SVD to build a simple recommendation algorithm. This section will continue as follows. First, we examine SVD as a technique of data compression and dimensionality reduction. Next, we generate some toy data of movie reviews and apply SVD to see how we can build a simple function that gives movie recommendations to users given their movie ratings history. Let‚Äôs jump right in. Why is singular value decomposition so important? Sure, it should now be fairly clear that SVD is a decomposition technique that can be applied to any matrix, whether square or not, which in and of itself makes it a very powerful tool in the statistician‚Äôs arsenal. But the true beauty of singular value decomposition comes from the fact that we can perform data compression by extracting meaningful information from the given data. This process is otherwise known as dimensionality reduction, and it is one of the most common applications of singular value decomposition. Let‚Äôs see what this means with an example. Here is , a target matrix for singuluar value decomposition.",0,0,0,1,0,0,0,1
Bayesian Linear Regression,"where  denotes precision, the inverse of variance. Note that we have a diagonal covariance matrix in place of variance, the distribution for  will be a multivariate Gaussian. The next ingredient we need for our recipe is the likelihood function. Recall that likelihood can intuitively be understood as an estimation of how likely it is to observe the given data points provided some parameter for the true distribution of these samples. The likelihood can easily be computed by referencing back to equation (1) above. Note that the dot product of  with itself yields the sum of the exponents, which is precisely the quantity we need when computing the likelihood. where  is a design matrix given by and  is a column vector given by Before calculating the posterior, let‚Äôs recall what the big picture of Bayesian inference looks like. where  denotes the parameter of interest for inference. In plain terms, the proposition above can be written as In other words, the posterior distribution can be obtained by calculating the product of the prior distribution and the likelihood function.",0,0,0,0,0,0,0,1
Stirling Approximation,"For those of you who are feeling rusty on the Poisson distribution as I was, here is a simple explanation on the Poisson‚Äîspecifically, its mean and variance. By the virtue of the definition of the parameter, it should be fairly clear why :  is a rate parameter that indicates how many events occur within a window of unit time. The expected calculation can easily be shown using Taylor expansion: Next, we prove that the variance of a Poisson random variable defined by parameter  is equal to . Let  be a Poisson random variable. Then, Then, using the definition of variance, we know that From this, we are once again reminded of the defining property of the Poisson, which is that both the mean and variance of a Poisson random variable is defined by the parameter . Let‚Äôs tie this back to our original discussion of the Central Limit Theorem. CLT states that, even if a distribution of a random variable is not normal, the distribution of the sums of these random variables will approximate a normal distribution.",1,1,0,0,0,1,0,0
Dissecting LSTMs,"Note that  had been concatenated to the input in the form of  throughout the forward pass. Because this was a variable that was used during computation, we need to calculate its gradient as well. This might appear rather confusing since we are currently looking at time , and it seems as if the gradient for  variables should be happening in the next iteration of backpropagation. While this is certainly true for the most part, due to the recurrent nature of LSTMs, we need to compute these gradients for  in this step as well. This is precisely what we were talking about earlier when discussing the recurrent nature of backprop; the  we compute here will be used in the next iteration of backpropagation, just like we added  in the current backprop to calculate . Becaue  was used in many different places during the forward pass, we need to collect the gradients. Given an intermediate variable we can express the gradient in the following fashion: Then, we can obtain  by un-concatenation: where  denotes the number of neurons in the LSTM layer. We can do the same for .",0,0,0,0,0,0,1,0
Revisiting Basel with Fourier,"We can derive this by considering the following integral: since this integral evaluates to . One way to look at (10) would be to consider it as a sum of some geometric series whose first term begins with 1 and has a constant ratio of . In other words, Here is where a bit of complication comes in. Turns out that under certain conditions, we can exchange the summation and the integral (or, more strictly speaking, the limit and the integral), using things like the dominating convergence theorem of Fubini‚Äôs theorem. However, these are topics for another post. For now, we will assume that this trick is legal and continue on. Now we have Now that we have a summation representation of , let‚Äôs move onto (11). We use the same trick we used earlier to interchange the summation and the integral. This gives us Since we have to terms with negative ones with the same exponent, we can safely remove both of them: And notice that we now have the Basel problem! If you plug in  and increment  from there, it is immediately apparent that this is the case.",1,0,0,0,0,0,0,0
A Brief Introduction to Recurrent Neural Networks,"None of our models reached the threshold of ninety percent accuracy, but they all managed to converge to some reasonable number, hovering around the high seventies to low eighties. Let‚Äôs test the performance of our models by using the  and  data, both of which none of our models have seen before. Based on the results, it looks like the LSTM model performed best, beating other models by a small margin. At this point, we cannot conclude as to whether or not this marginal boost in performance is significant. Judging this would not only depend on the context, but also most likely require us to have a larger test dataset that captures the statistics of the population data. This point notwithstanding, it is certainly beneficial to know that LSTM networks are good at detecting sequential patterns in data. Last but not least, let‚Äôs visualize the training scheme of all four models to take a identify any possible signs of convergence and overfitting, if any. To do that, we will be using the  function shown below. The dense feed-forward network seems to have a very linear pattern.",0,0,0,0,1,0,1,0
An Introduction to Markov Chain Monte Carlo,"At the heart of Metrapolis-Hastings is the proposal distribution, which we use to simulate the Markov chain random walk part of the model. Setting the parameters for this proposal distribution can be done arbitrarily, i.e. we can set it to be any random numbers. Theoretically, regardless of the parameters of the proposal distribution, the MCMC model would give us the same result after infinite iterations of sampling. In the Metrapolis-Hastings model, the proposal distribution is assumed as normal. Next, we have to decide if the current value of  is a value to accept or not. Accepting a randomly sampled value means adding it to our list of historic observations‚Äîif we draw a histogram of the entries in this list, it is our hope that we would end up with a close approximation of the posterior distribution. Accepting this value is often referred to as a ‚Äújump,‚Äù because we can visualize this process as a random walk in the posterior sample space from point  to .",0,0,0,1,0,0,0,0
PyTorch RNN from Scratch,"The model seems to have classified all the names into correct categories! This is cool and all, and I could probably stop here, but I wanted to see how this custom model fares in comparison to, say, a model using PyTorch layers. GRU is probably not fair game for our simple RNN, but let‚Äôs see how well it does. Let‚Äôs declare the model and an optimizer to go with it. Notice that we are using a two-layer GRU, which is already one more than our current RNN implementation. The training appeared somewhat more stable at first, but we do see a weird jump near the end of the second epoch. This is partially because I didn‚Äôt use gradient clipping for this GRU model, and we might see better results with clipping applied. Let‚Äôs see the accuracy of this model. And we get an accuracy of around 80 percent for this model. This is better than our simple RNN model, which is somewhat expected given that it had one additional layer and was using a more complicated RNN cell model. Let‚Äôs see how this model predicts given some raw name string.",0,0,0,1,0,0,1,0
Scikit-learn Pipelines with Titanic,"When it comes to general fitting and testing, a useful tip I found on Kaggle is the following rule of thumb: If you think about it for a second, this configurations makes a lot of sense: if the pipeline contains a model, it means that it is the full package. All the steps prior to the model would involve wrangling the data; the last step would have the model use the data to make a prediction. Therefore, calling  should apply only to the last model after  is called on all the preprocessing steps. If the pipeline itself is just a bundle of preprocessors, on the other hand, we should only be able to call . Scikit-learn‚Äôs models are great, but in a sense they are too great. This is because there are a lot of hypterparameters to tune. Fortunately for us, we can somewhat resort to a quasi-brute force approach to deal with this: train models on a number of different combinations of hyperparameters and find the one that performs best! Well, this is what  does.",0,0,1,0,0,0,0,0
My First GAN,"Generative models are fascinating. It is no wonder that GANs, or General Adversarial Networks, are considered by many to be where future lies for deep learning and neural networks. In this post, we will attempt to create a very simple vanilla GAN using TensorFlow. Specifically, our goal will be to train a neural network that is capable of generating compelling images of ships. Although this is a pretty mundane task, it nonetheless sheds lights on the potential that GAN models hold. Let‚Äôs jump right into it. Below are the dependencies and settings we will be using throughout this tutorial. Before we start building the GAN model, it is probably a good idea to define some variables that we will be using to configure the parameters of convolutional layers, namely the dimensionality of the images we will be dealing with, as well as the number of color channels and the size of the latent dimension. Similar to variational autoencoders, GANs are composed of two parts: the generator and the discriminator.",0,0,0,0,1,0,1,0
A sneak peek at Bayesian Inference,"From a Bayesian standpoint, we would then replace the point probability in our example above with an equation for the distribution, from which we can proceed with the Bayesian analysis of updating our prior with the posterior through repeated testing and computation. Bayes‚Äô theorem, specifically in the context of statistical inference, can be expressed as where  stands for observed or measured data,  stands for parameters, and  stands for some probability distribution. In the language of Bayesian inference,  is the posterior distribution for the parameter ,  is the likelihood function that expresses the likelihood of having parameter  given some observed data ,  is the prior distribution for the parameter , and  is evidence, the marginal probability of seeing the data, which is determined by summing or integrating across all possible values of the parameter, weighted by how strongly we believe in those particular values of . Concretely, Notice that this is not so different from the expansion of the denominator we saw with Bayes‚Äô theorem, specifically equation (5). The only difference here is that the integral takes continuous probability density functions into account, as opposed to discrete point probabilities we dealt with earlier.",0,1,0,0,0,1,0,0
Logistic Regression Model from Scratch,"We use average cross entropy instead of total cross entropy, because it doesn‚Äôt make sense to penalize the model for high cross entropy when the input data set was large to begin with. Now what‚Äôs next? Since we have a loss function, we need to build an algorithm that will allow us to minimize this cost function. One of the most common methods used to achieve cost minimization is gradient descent. As you might be able to tell, this algorithm has a lot to do with gradients, which can loosely be understood as a fancy way of saying derivatives. Below is an illustration of the gradient descent algorithm in action, sourced from this blog.  Basically, what gradient descent does is that it takes the derivative of the loss function with respect to the weight vector every epoch, or iteration, and takes a small step in the opposite direction of that derivative. If you think of this in the context of two dimensions as shown in the illustration, the gradient descent algorithm ends up moving down the parabola, taking little steps each time, until it eventually reaches the global minimum.",0,0,1,1,0,0,0,0
Dissecting LSTMs,"In this post, we will revisit the topic of recurrent neural networks, or RNNs. Although we have used RNNs before in a previous post on character-based text prediction, we glossed over LSTM and assumed it as a black box that just worked. Today, we will take a detailed look at how LSTMs work by dissecting its components. Note that this post was inspired by this article by Kristiadi. I also heavily referenced this post by Christopher Olah. If you find any part of this article intriguing and intellectually captivating, you will surely enjoy reading their blogs as well. With this in mind, let‚Äôs jump right into it. Long Short-Term Memory networks, or LSTMs for short, are one of the most widely used building blocks of Recurrent Neural Networks, or RNNs. This is because LSTMs overcame many of the limitations of basic vanilla RNNs: while simple RNN gates are bad at retaining long-term information and only remember input information that were fed into it relatively recently, LSTMs do a great job of retaining important information, even if they were fed into the cell long time ago.",0,0,0,0,0,0,1,0
Bayesian Linear Regression,"For any regression problem, we first need a data set. Let  denote this pre-provided data set, containing  entries where each entry contains an -dimensional vector and a corresponding scalar. Concretely, where The goal of Bayesian linear regression is to find the predictive posterior distribution for . This is where the difference between Bayesian linear regression and the normal equation method becomes most apparent. Whereas vanilla linear regression only gives us a single point estimate given an input vector, Bayesian linear regression gives an entire distribution. For the purposes of our demonstration, we will define the predictive posterior to take the following form as shown below, with precision  pre-given. Precision is simply the reciprocal of variance and is commonly used as an alternative way of parametrizing Gaussian distributions. In other words, we assume the model Our goal will be to derive a posterior for this distribution by performing Bayesian inference on , which corresponds to the slope of the linear regression equation, where  denotes noise and randomness in the data, thus affecting our final prediction. To begin Bayesian inference on parameter , we need to specify a prior. Our uninformed prior will look as follows.",0,0,0,0,0,0,0,1
My First GAN,"Given the simplicity of the structure of our network, I would say that this is a successful result. Let‚Äôs take a look at the learning curve of the GAN.  As you might expect, the loss is very spiky and erratic. This is why it is hard to determine when to stop training a GAN. Of course, there are obvious signs of failure: when the loss of one component starts to get exponentially larger or smaller than its competitor, for instance. However, this did not happen here, so I let the training continue until the specified number of interactions were over. The results, as shown above, suggest that we haven‚Äôt failed in our task. In a future post, we will be taking a look at the mathematics behind GANs to really understand what‚Äôs happening behind the scenes when we pit the generator against its mortal enemy, the discriminator. See you in the next post!.",0,0,0,0,1,0,1,0
Scikit-learn Pipelines with Titanic,"Imputation refers to a technique used to replace missing values. There are many techniques we can use for imputation. From the analysis above, we know that the columns that require imputation are as follows: Let‚Äôs first take a look at the data types for each column. Checking data types is necessary both for imputation and general data preprocessing. Specifically, we need to pay attention as to whether a given column encodes categorical or numerical variables. For example, we can‚Äôt use the mean to impute categorical variables; instead, something like the mode would make much more sense. The best way to determine whether a variable is categorical or not is simply to use domain knowledge and actually observe the data. Of course, one might use hacky methods like the one below: Although you might think that this is a working hack, this approach is in fact highly dangerous, even in this toy example. For example, consider , which is supposedly a numerical variable of type . However, earlier with , we saw that  is in fact a ordinal variable taking discrete values, one of 1.0, 2.0, and 3.0.",0,0,1,0,0,0,0,0
"0.5!: Gamma Function, Distribution, and More","In a previous post, we looked at the Poisson distribution as a way of modeling the probability of some event‚Äôs occurrence within a specified time frame. Specifically, we took the example of phone calls and calculated how lucky I was on the day I got only five calls during my shift, as opposed to the typical twelve. While we clearly established the fact that the Poisson distribution was a more accurate representation of the situation than the binomial distribution, we ran into a problem at the end of the post: how can we derive or integrate the Poisson probability distribution, which is discontinuous? To recap, let‚Äôs reexamine the Poisson distribution function: As you can see, this function is discontinuous because of that one factorial term shamelessly flaunting itself in the denominator. The factorial, we might recall, is as an operation is only defined for integers. Therefore, although we can calculate expression such as , we have no idea what the expression  evaluates to. Or do we? Here is where the Gamma function kicks in. This is going to be the crux of today‚Äôs post.",0,1,0,0,0,1,0,0
Markov Chain and Chutes and Ladders,"This block produces the following figure: I doubt that anyone would play Chutes and Ladders for this long, but after about 150 rolls of the dice, we can expect with a fair amount of certainty that the game will come to an end. The graph above presents information on cumulative fractions, but we can also look at the graph for marginal probabilities by examining its derivative: And the result: From the looks of it, the maximum of the graph seems to exist somewhere around . To be exact, . This result tells us that we will finish the game in 19 rolls of the dice more often than any other number of turns. We can also use this information to calculate the expected value of the game length. Recall that Or if the probability density function is continuous, In this case, we have a discrete random variable, so we adopt the first formula for our analysis. The formula can be achieved in Python as follows: This result tells us that the typical length of a Chutes and Ladders game is approximately 36 turns.",0,0,0,0,0,0,0,1
Demystifying Entropy (And More),"If bits sounds similar to bytes or gigabytes we use for storage, you‚Äôre exactly on the right path. In fact, the relationship between bit and byte is established directly by the fact that where  denotes bits and  denotes bytes. This is why we use bytes to represent the amount of disk storage in computers, for instance. It is also worth mentioning that the alternative name for bits is Shannons, named eponymously after the mathematician who pioneered the field of information theory, as mentioned above. Now that we have some idea of what information is and how we can quantify it using binary numbers in bits, it‚Äôs time to get into the math. Information can be calculated through the formula where  is the information need to express the random event , and  is the probability that event  occurs, i.e. . There are different versions of this formula, such as the one that uses Euler‚Äôs constant as the log base instead of 2. Whereas the unit of information as measured through (1) is in bits, that calculated through (2) as shown below is in the unit of nats.",0,0,0,0,0,1,0,0
"PyTorch, From Data to Modeling","The difference between the former and latter is that, while the former applies a softmax function to the output before calculating the actual loss, the latter does not. In our case, since we simply output the raw logits instead of applying a softmax calculation, we need to use the former. Let‚Äôs return where we were. Before we jump into training and defining the training loop, it‚Äôs always a good idea to see if the output of the model is what you‚Äôd expect. In this case, we can simply define some random dummy input and see if the output is correct. Now that we‚Äôve verified the input and output dimensions, we can move onto defining the training loop. Defining the training loop may seem difficult at first, especially if you‚Äôre coming from a Keras background, but actually a lot of it is boiler plate, and things are not as difficult as they may seem. First, we define a list to hold the loss values per iteration. We will be using this list for visualization later. The exciting part comes next. For each epoch, we load the images in the .",0,0,0,0,0,0,1,0
Building Neural Network From Scratch,"The advantage of using a minibatch is that it is computationally lighter and less expensive. Minibatch gradient descent can be considered a happy point of compromise between stochastic and batch gradient descent, which lie on the polar opposite ends of the spectrum. Let‚Äôs first take a look at the  function, which divides the  and  into  and  given a . Internally, the  function calls the  gradient descent algorithm to update the weights and finally returns the  which contains updated parameters based on the training data. As mentioned above, each  and  are minibatches that will be feeded into our  gradient descent function. Note that the  function is simply an implementation of equation (7). At the core of the  function is the  function, which is our implementation of back propagation. This provides a nice point of transition to the next section. Back propagation is a smart way of calculating gradients. There are obviously many ways one might go about gradient calculation. We can simply imagine there being a loss function that is a function of all the thousands of weights and biases making up our neural network, and calculate partial derivatives for each parameter.",0,0,0,1,0,0,1,1
"Basel, Zeta, and some more Euler","My exploration of the field of mathematics is somewhat like a random walk, moving from one point to another with no apparent pattern or purpose other than my interest and Google‚Äôs search suggestions, but my encounter with Euler will recur continuously throughout this journey for sure. But for now, I‚Äôm going to take a brief break from Euler and return back to the topic of good old statistical analysis, specifically Bayesian methods and Monte Carlo methods. Catch you up in the next one!.",1,0,0,0,0,0,0,0
Building Neural Network From Scratch,"This is conventional wisdom in the land of deep learning. Let‚Äôs create a function to plot the performance of a neural network and the number of its neurons. Below is a  function that achieves this task. The function receives , , and  as arguments. The first two arguments specify the range for the number of neurons that we are interested in. For example, if we set them to 3 and 40, respectively, that means we want to see the accuracy of models with number of neurons ranging from 3 to 40 in a single layer. The  argument specifies the number of experiments we want to conduct. This way, we can calculate the mean accuracy, just as we did previously. Let‚Äôs call the function to create a plot.  The result shows that the performance of the neural network generally increases as the number of neurons increase. We don‚Äôt see signs of overfitting, but we know it happens: recall that our neural network model with 99 and 64 hidden neurons hit an accuracy of about 95 percent, whereas the model with only 30 to 40 neurons seem to be outperforming this metric by an accuracy hovering around 98 percent.",0,0,0,1,0,0,1,1
A sneak peek at Bayesian Inference,"Then, the conditional probability of obtaining  heads given a fairness parameter  can be expressed as We can perform a quick sanity check on this formula by observing that, when , the probability of observing  heads diminishes to 0, unless , in which case the probability becomes 1. This behavior is expected since  represents a perfectly biased coin that always shows tails. By symmetry, the same logic applies to a hypothetical coin that always shows heads, and represents a fairness parameter of 1. Now that we have derived a likelihood function, we move onto the next component necessary for Bayesian analysis: the prior. Determining a probability distribution for the prior is a bit more challenging than coming up with the likelihood function, but we do have certain clues as to what characteristics our prior should look possess. First, the domain of the prior probability distribution should be contained within . This is because the range of the fairness parameter  is also defined within this range. This constraint immediately tells us that where  is represents the probability density function that represents the prior.",0,1,0,0,0,1,0,0
Introduction to tf-idf,"Also note that published modules use sparse representations to minimize computational load, as we will later see with scikit-learn. Now it‚Äôs time to implement the first step: calculating term frequency. In Python, this simply amounts to looping through each document, creating a tf vector per iteration, and making sure that they are normalized as frequencies at the very end. In creating tf vectors for each document, we will be referencing the word-to-index mapping in our corpus. Let‚Äôs see what we get for the four documents in our dummy example. Due to floating point arithmetic, the decimals don‚Äôt look the most pleasing to the eye, but it‚Äôs clear that normalization has been performed as expected. Also note that we get 4 vectors of length 9 each, as expected. Next, it‚Äôs time to implement the idf portion of the vectorization process. In order to calculate idf, we first need a total count of each number in the entire document collection. A module that is perfect for this job is , which accepts as input an iterable and outputs a dictionary-like object whose values represent the count of each key.",0,0,0,1,0,0,0,0
Stirling Approximation,"It‚Äôs about time that we go back to the old themes again. When I first started this blog, I briefly dabbled in real analysis via Euler, with a particular focus on factorials, interpolation, and the Beta function. I decided to go a bit retro and revisit these motifs in today‚Äôs post, by introducing Stirling‚Äôs approximation of the factorial. There are many variants of Stirling‚Äôs approximation, but here we introduce the general form as shown: Let‚Äôs begin the derivation by first recalling the Poisson distribution. The Poisson distribution is used to model the probability that a certain event occurs a specified number of times within a defined time interval given the rate at which these events occur. The formula looks as follows: One interesting fact about the Poisson distribution is that, when the parameter  is sufficiently large, the Poisson approximates the Gaussian distribution whose mean and variance are both . This happens when the random variable . We can easily simplify (2) since the power of the exponent is zero.",1,1,0,0,0,1,0,0
Recommendation Algorithm with SVD,"This function tells us that our movie application should recommend to our user Movies 3 and 4, in that order. This result is not surprising given the fact that we have already observed the closeness between Movies 2 and 3‚Äîif a user likes Movie 2, we should definitely recommend Movie 3 to them. Our algorithm also tells us that the distance between Movie 2 and 4 is also pretty close, although not as close as the distance between Movies 2 and 3. What is happening behind the scene here? Our function simply calculates the distance between the vector representation of each movies as a dot product. If we were to print the local variable  array defined within the  function, for instance, we would see the following result. This tells us how close Movies 0, 1, 3, and 4 are with Movie 2. The larger the dot product, the closer the movie; hence, the more compelling that recommendation. The  function then sorts the  array and outputs the first  movies as a recommendation.",0,0,0,1,0,0,0,1
On Expectations and Integrals,"Expectation is a core concept in statistics, and it is no surprise that any student interested in probability and statistics may have seen some expression like this: In the continuous case, the expression is most commonly presented in textbooks as follows: However, this variant might throw you off, which happened to me when I first came across it a few weeks ago: I mean, my calculus is rusty, but it kind of makes sense: the probably density function is, after all, a derivative of the cumulative density function, and so notationally there is some degree of coherency here. But still, this definition of the expected value threw me off quite a bit. What does it mean to integrate over a distribution function instead of a variable? After some research, however, the math gurus at Stack Exchange provided me with an answer. So here is a brief summary of my findings. The integral that we all know of is called the Riemann integral. The confusing integral is in fact a generalization of the Riemann integral, known as the Riemann-Stieltjes integral (don‚Äôt ask me how to pronounce the name of the Dutch mathematician).",0,0,0,0,0,1,0,0
The Exponential Family,"The formula for the Bernoulli distribution goes as follows: Again, I have added a very simple indicator function to ensure that the the probability mass function is well-defined across the entire real number line. Again, the indicator function is a simple boolean gate function that checks whether  is an element within a set of zero and one: Factorizing the Bernoulli is slightly more difficult than doing the same for the exponential distribution, largely because it is not apparent from (9) how factorization can be achieved. For example, we do not see any exponential term embedded in (9) as we did in the case of the exponential distributions. Therefore, a simple one-to-one correspondence cannot be identified. The trick to get around this problem is to introduce a log transformation, then reapplying an exponential. In other words, By applying this manipulation, we can artificially create an exponential term to more easily coerce (9) into the factorization mold. Specifically, observe that the power of the exponent can be expressed as a dot product between two vectors, each parameterized by  and  , respectively.",0,1,0,0,0,1,0,0
On Expectations and Integrals,"You might be wondering why the Riemann-Stieltjes integral is necessary in the first place. After all, the definition of expectation we already know by heart should be enough, shouldn‚Äôt it? To answer this question, consider the following  function: This cumulative mass function is obviously discontinuous since it is a step-wise function. This also means that it is not differentiable; hence, we cannot use the definition of expectation that we already know. However, this does not mean that the random variable  does not have an expected value. In fact, it is possible to calculate the expectation using the Riemann-Stieltjes integral quite easily, despite the discontinuity! The integral we wish to calculate is the following: Therefore, we should immediately start visualizing splitting up the domain of integration, the real number line, into infinitesimal pieces. Each box will be of height  and width . In the context of the contrived example, this definition makes the calculation extremely easy, since   equals zero in all locations but the jumps where the discontinuities occur. In other words, We can easily extend this idea to calculating things like variance or other higher moments. A more realistic example might be the Dirac delta function.",0,0,0,0,0,1,0,0
Dissecting LSTMs,"At this point, we‚Äôre not quite done yet;  is not a vector of probabilities indicating which letter is the most likely in a one-hot encoded representation. Therefore, we will need to pass it through another affine layer, than apply a softmax activation. Hence,  is the final output of an LSTM layer. Here comes the tricky part: backprop. Thankfully, backprop is somewhat simple in the case of LSTMs due to the use of Hadamard products. The routine is not so much different from a vanilla neural network, so let‚Äôs try to hash out the equations. As we already know, backpropagation in neural networks is merely an extended application of the chain rule, with some minor caveats that matrix calculus entails. First, let‚Äôs begin slow and easy by deriving the expressions for the derivative of the sigmoid and the  functions. First, below is the derivative of the sigmoid with respect to , the input. Recall that the sigmoid function is defined as . Let‚Äôs do the same for . One useful fact about  is the fact that it is in fact nothing more than just a rescaled sigmoid.",0,0,0,0,0,0,1,0
Word2vec from Scratch,"And of course, this vector is not a collection of some randomly initialized numbers, but a result of training with context data generated through the sliding window algorithm described above. In other words, these vectors encode meaningful semantic information that tells us which words tend to go along with each other. While this is a relatively simple, basic implementation of word2vec, the underlying principle remains the same nonetheless. The idea is that, we can train a neural network to generate word embeddings in the form of a weight matrix. This is why embedding layers can be trained to generate custom embeddings in popular neural network libraries like TensorFlow or PyTorch. If you end up training word embeddings on large datasets like Wikipedia, you end up with things like word2vec and GloVe, another extremely popular alternative to word2vec. In general, it‚Äôs fascinating to think that, with enough data, we can encode enough semantics into these embedding vectors to see relationships such as ‚Äúking - man + woman = queen.‚Äù I hope you‚Äôve enjoyed reading this post. See you in the next one.",0,0,0,1,0,0,1,0
"PyTorch, From Data to Modeling","In the coming notebooks, we will take a deeper dive into implementing models with PyTorch, starting from RNNs all the way up to classic SOTA vision models like InceptionNet, ResNet, and seq2seq models. I can definitely tell you that these are coming, because, as funny as this sounds, I already have all the notetbooks and code ready; I just have to annotate them. I hope you‚Äôve enjoyed reading this post. Catch you up in the next one!.",0,0,0,0,0,0,1,0
First Neural Network with Keras,"It‚Äôs time to train the neural network with the training data,  and , over a specified number of epochs. As promised, we will use the  to stop graident descent from making unnecessary computations down the road. We also specify that  and  are components of the validation set. Keras shows us how much our neural network improves over each epoch. This is convenient, but can we do better? The answer is a sure yes. Let‚Äôs quickly plot a graph to see how model accuracy improves over time, while cross entropy loss decreases with more epochs.  As the last step, we might want to save our trained model. This can be achieved with a single line of code. We can load pre-saved models as well. That‚Äôs it for today! Obviously there are a lot more we can do with , such as building deeper neural networks or non-sequential models such as CNN or GAN, but these are topics we might look at a later date when I grow more proficient with the Keras API and deep learning in general. For now, consider this to be a gentle introduction to neural networks with Keras.",0,0,1,0,1,0,1,0
A Brief Introduction to Recurrent Neural Networks,"we should not shuffle the datasets because making weather predictions requires us to understand temporal patterns. In predicting the weather 24 hours from today, data points pertaining to the last two days will be obviously much more important that those recorded a month ago. In such instances of time series analysis, recurrent neural networks perform better than other models we have looked at thus far. To better understand how RNNs work, let‚Äôs try to build a very simple recurrent neural network from scratch with . We will only implement forward propagation for the sake of simplicity,  but with some matrix calculus, one can imagine how deriving the formula for back propagation will be possible. Let‚Äôs cut to the chase: RNNs emulate memory by using the output from the previous sequence as an input to the next. Perhaps writing this down in matrix notation might give you a better idea of what the statement above means. Here is one way we might implement a very simple recurrent neural network. If the word ‚Äúrecursion‚Äù pops up into your mind, then you are on the right track.",0,0,0,0,1,0,1,0
"Newton-Raphson, Secant, and More","For example, we can express  as However, a downside of this approach is the fact that it‚Äôs difficult to directly derive this equation despite the fact that it is nothing more than just a simple polynomial. So instead, we will use a list index-based representation. Namely, the  th element of a list represents the coefficient of the th power in a polynomial equation. In other words,  would translate into . The  is a function that returns a Python function given a list that conforms to this list index representation. Let‚Äôs see if this works as expected. , so the function passes our quick sanity test. One useful helper function that I also implemented for the sake of convenience is a array-to-equation parser that translates a list representation into a mathematical expression in Python. This is best demonstrated than explained, so I‚Äôll defer myself to an example. Below is the full definition of the  function. At this point, I also thought that it would be useful and interesting to compose a function that translates the string output of  into a proper Python function we can use to calculate values.",1,0,0,0,0,0,0,0
Building Neural Network From Scratch,"However, this naive aproach is problematic because it is so computationally expensive. Moreover, if you think about it for a second, you might realize that doing so would result in duplicate computations due to the chain rule. Take the simple example below. If we were to calculate the gradient of the loss function with respect to  and , all we need to compute is the gradient of , since that of  will naturally be obtained along the way. In other words, computing the gradient simply requires that we start from the very end of the neural network and propagate the gradient values backwards to compute the partial derivatives according to the chain rule. This is what is at the heart of back propagation: in one huge swoop, we can obtain the gradient for all weights and parameters at once instead of having to calculate them individually. For a more detailed explanation of  this mechanism, I strongly recommend that you take a look at this excellent blog post written by Christopher Olah. How do we go about back propagation in the case of our model? First, it is necessary to define a loss function.",0,0,0,1,0,0,1,1
A Step Up with  Variational Autoencoders,"Note that the encoder and the decoder look like individual layers in the grand scheme of the VAE architecture. We have almost everything we need, but there is one crucial step that is missing: compiling the model with an optimizer and a loss function. Normally, defining a loss function is very easy: in most cases, we  use pre-made loss functions that are available through the TensorFlow API, such as cross entropy or mean squared error. In the case of variational autoencoders, however, this is not such an easy task: how do we judge the robustness or the effectiveness of the decoder, which is essentially a generative algorithm? Of course, we could stop training once the figures it generates becomes reasonable, i.e. the mock MNIST digits it creates looks compelling to the human eye. However, this is a subjective metric at best, and we can‚Äôt expect there to be a ML engineer peering at the screen, looking at the outputs of the decoder per each epoch. To tackle this challenge, we need to dive into some math. Let‚Äôs take a look. First, let‚Äôs carefully review what our goal is for this task.",0,0,0,0,1,0,1,0
Likelihood and Probability,"To get started, recall the that This is the good old definition of probability as defined for a continuous random varriable , given some probability density function  with parameter . Graphically speaking, we can consider probability as the area or volume under the probability density function, which may be a curve, plane, or a hyperplane depending on the dimensionality of our context. Unlike probability, likelihood is best understood as a point estimate on the PDF. Imagine having two disparate distributions with distinct parameters. Likelihood is an estimate we can use to see which of these two distributions better explain the data we have in our hands. Intuitively, the closer the mean of the distribution is to the observed data point, the more likely the parameters for the distribution would be. We can see this in action with a simple line of code. This code block creates two distributions of different parameters,  and . Then, we assume that a sample of value 1 is observed. Then, we can compare the likelihood of the two parameters given this data by comparing the probability density of the data for each of the two distributions. In this case,  seems more likely, i.e.",0,0,0,0,0,1,0,0
"PyTorch, From Data to Modeling","Then, we also see per-class accuracy; that is, whether our model is good at predicting any particular class. This ensures that the model‚Äôs performance is balanced throughout all labels. And here is the result! An overall accuracy of 70 percent is definitely not impressive, and we certainly could have done better by building a deeper model, or by using more complex architectures. However, this isn‚Äôt the worst performance considering the fact that we only had three convolutional layers. The more important takeaway from this tutorial is how to prepare data, build models, and train and evaluate them through a custom loop. From the tone and style of my writing, it is perhaps immediately clear to you that I am not officially a PyTorch fanboy. Yes, I will admit that I loved Keras for its simplicity, but after having spent more time learning python and DL, I now much prefer the freedom provided by PyTorch‚Äôs reasonably abstract API. I hope this notebook provided you with a nice, simple introduction to PyTorch.",0,0,0,0,0,0,1,0
"Linear Regression, in Two Ways","We started off by plotting three data points, which we observed did not form a straight line. Therefore, we set out to identify the line of best fit by expressing the system of equations in matrix form, , where . But because this system does not have a solution, we ended up modifying the problem to , since this is as close as we can get to solving an otherwise unsolvable system. So that‚Äôs where we are with equation (2): a formula for , which contains the parameters that define our line of best fit. Linear regression is now complete. It‚Äôs time to put our equation to the test by applying it to our toy data set. Let‚Äôs apply (2) in the context of our toy example with three data points to perform a quick sanity check. Calculating the inverse of  is going to be a slight challenge, but this process is going to be a simple plug-and-play for the most part. First, let‚Äôs remind ourselves of what  and  are: Let‚Äôs begin our calculation: Calculating the inverse, Now, we can put this all together.",0,0,0,0,0,0,0,1
Moments in Statistics,"The word ‚Äúmoment‚Äù has many meanings. Most commonly, it connotes a slice of time. In the realm of physics, moment refers to the rotational tendency of some object, similar to how torque measures the change in an object‚Äôs angular momentum. As statisticians, however, what we are interested in is what moment means in math and statistics. In this post, we will attempt to shed new light on the topic of probability distributions through moment generating functions, or MGF for short. The mathematical definition of moments is actually quite simple. And of course, we can imagine how the list would continue: the th moment of a random variable would be . It is worth noting that the first moment corresponds to the mean of the distribution, . The second moment is related to variance, as . The third moment relates to the symmetry of the distribution, or the lack thereof, a quality which goes by the name of skewness. The fourth moment relates to kurtosis, which is a measure of how heavy the tail of a distribution is. Higher kurtosis corresponds to many outliers, while the converse would signify that the distribution contains little deviations.",0,1,0,0,0,1,0,0
A Step Up with  Variational Autoencoders,"Note that the ELBO expression applies to just about any distribution, but since we chose a multivariate Gaussian to be the base distribution, we will see how it unfolds specifically in this context. Let‚Äôs begin by assuming the distribution of our models to be Gaussian. Namely, Because  is an approximation of , we naturally assume the same model for the approximate distribution: Now we can derive an expression for the negative KL divergence sitting in the ELBO expression: This may seem like a lot, but it‚Äôs really just plugging in the distributions into the definition of KL divergence as an expectation and using some convenient properties of logarithms to perform simple algebraic simplifications. To proceed further, observe that the first term is a constant that can escape out of the expectation: From the definition of variance and expectation, we know that Therefore, we can simplify (17) as follows: Let‚Äôs zoom in on the expected value term in (19). Our goal is to use (18) again so that we can flesh out another one half from that term.",0,0,0,0,1,0,1,0
"Beta, Bayes, and Multi-armed Bandits","However, after some trial and error, the gambler starts to figure out which bandit is the best and starts pulling more of those, ultimately ending up at the point that is quite close to the maximum reward, though not quite due to the earlier opportunities that may have been lost due to exploration and sampling. In this post, we took a look at the multi-armed bandit problem and how it relates to Bayesian analysis with the Beta and Binomial distributions. I personally enjoyed writing this post, not only because I hadn‚Äôt written in a long time, but also because it helped me revisit some statistics, which is something that I desperately needed to do‚ÄîI‚Äôm spending way too much time dealing with Django and Selenium these days. Time and again, I realize that there is an element of intuitiveness to Bayesian statistics that, though not obvious at first, starts to make more sense as I explore more into that realm. Of course, frequentist statistics offers a quick and easy way of interpreting certain metrics, but the notion of expectation is something that I had thought was complicated and unnecessary at first, but have gradually come to understand, embrace, and even support.",0,1,0,0,0,1,0,0
Principal Component Analysis,"We know that  is a row vector since its dot product with  should be possible dimensionally speaking. Then, we know that the gradient with respect to  should give each of the elements of , but in column vector format‚Äîhence the need for a transpose. In general, the rule of thumb is that the gradient of a scalar with respect to a vector or a matrix should return a vector or matrix of the same dimension. Recall from (1) that reconstruction can be achieved by applying compression followed by a decoding operation: Since we know that  is just  and  is  by definition, we can express (1) in a different way. In retrospect, this is somewhat intuitive since  can roughly be thought of as a pseudo-orthonormal matrix‚Äîpseudo since there is no guarantee that it is a square matrix. Now, all that is left is to find the matrix . The way to go about this is to reconsider (3), the notion of minimizing data loss, given our findings in (6). In other words, Instead of considering a single observation, here we consider the design matrix in its entirety. Note that  is a design matrix whose rows correspond to a single observation.",0,0,0,0,0,1,0,1
Naive Bayes Model From Scratch,"The data set contains three target classes, labeled as integers from 0 to 2, and thirteen feature columns, listed below: As I am not a wine afficionado, I have no idea what some of these columns represent, but that is irrelevant to the purpose of this tutorial. Let‚Äôs jump right in by loading the data. It always a good idea to get a sense of what the data looks like by verifying its dimension. Note that we used the  function we wrote in previous posts to shuffle and slice the data into training and validation sets. For convenience, the code for this function is presented below. Now it‚Äôs finally time to check our model by making predictions. This can simply be done by passing the training and testing data set into the  function that represented our Gaussian naive Bayes model. Let‚Äôs check if the predicted class labels match the answer key, i.e. the  array. Eyeballing the results, it seems like we did reasonably well! In fact, the line below tells us that our model mislabeled only one test instance! We can quantify the performance our model through the metric of accuracy. The  function does this for us.",0,0,1,1,0,0,0,0
Building Neural Network From Scratch,"After having realized this, I considered re-running the  function with a different range, but eventuially decided to stop the experiment  because running the  function took a lot more time than I had expected, even on Google Colab. Creating and training the model takes a long time, especially if we are repeating this process  times. For now, the simple observation that the performance seems to increase with more neurons, then fall at one point once overfitting starts to happen, will suffice to satisfy our curiosity. In this post, we built a neural network only using  and math. This was a lot more difficult than building other machine learning models from scratch particularly because of the heavy mathematics involved. However, it was definitely worth the challenge becasue completing and writing up this tutorial made me think a lot more about the clockwork of a neural network model. It is easy to think of neural networks as a black box, especially given the sheer ease of creating it. With just , one can build a simple neural network like this one in no time.",0,0,0,1,0,0,1,1
Maximum A Posteriori Estimation,"Concretely, The objective of Bayesian inference is to estimate the posterior distribution, whose probability distribution is often intractable, by computing the product of likelihood and the prior. This process could be repeated multiple times as more data flows in, which is how posterior update can be performed. We saw this mechanism in action with the example of a coin flip, given a binomial likelihood function and a beta prior, which are conjugate distribution pairs. Then what does maximizing the posterior mean in the context of MAP? With some thinking, we can convince ourselves that maximizing the posterior distribution amounts to finding the optimal parameters of a distribution that best describe the given data set. This can be seen by simply interpreting the posterior from a conditional probability point of view: the posterior denotes the probability of the value of the model parameter is  given data . Put differently, the value of  that maximizes the posterior is the optimal parameter value that best explains the sample observations. This is why at its heart, MAP is not so much different from MLE: although MLE is frequentist while MAP is Bayesian, the underlying objective of the two methods are fundamentally identical.",0,0,0,0,0,1,0,0
BLEU from scratch,"Let‚Äôs see if the simple precision metric is able to capture this intuition. And indeed that seems to be the case! However, the simple precision-based metric has some huge problems. As an extreme example, consider the following  candidate translation. Obviously,  is a horrible translation, but the simple precision metric fails to flag it. This is because precision simply involves checking whether a hit occurs or not: it does not check for repeated bag of words. Hence, the original authors of BLEU introduces modified precision as a solution, which uses clipped counts. The gist of it is that, if some n-gram is repeated many times, we clip its count through the following formula: Here,  refers to the number of hits we assign to a certain n-gram. We sum this value over all distinct n-grams in the candidate sentence. Note that the distinction requirement effectively weeds out repetitive translations such as  we looked at earlier.  refers to the number of occurrences of a n-gram in the candidate sentence. For example, in , the unigram  appears 13 times, and so .",0,0,0,1,0,0,1,0
Building Neural Network From Scratch,"Note that the indicator function, denoted as , is a simple gate function that calculates the gradient of the ReLU unit: It isn‚Äôt difficult to see that the indicator function is simply a derivative of the ReLU function as shown in equation (3). Now, it is time to translate our findings into Python. Because our neural network model is represented as a dictionary, I decided to adopt the same data structure for the gradient. Indeed, that is how we designed the  function above. The  function below is an implementation of back propagation that encapsulates equations (14) through (17). There is a subtlety that I did not discuss previously, which has to do with the bias terms. It may appear as if the gradient of the bias term does not match that of the bias term itself. Indeed, that is a valid observation according to equation (16). The way we go about this is that we add up the elements of the matrix according to columns. This is exactly what we do with the  command invoked when computing  and , which represent the gradient of the bias terms.",0,0,0,1,0,0,1,1
Dissecting the Gaussian Distribution,"If we were to dig a bit deeper into prior knowledge, we would expect the point of inflection to be one standard deviations away from the mean, left and right. Let‚Äôs verify if these are actually true. From good old calculus, we know that we can obtain the local extrema by setting the first derivative to zero. We can ignore the constants as they are non-zero. Then, we end up with Because the exponent is always positive, the only way for the expression to evaluate to zero is if This tells us that the local maximum of the univariate Gaussian occurs at the mean of the distribution, as we expect. The inflection point can be obtained by setting the second order derivative of the probability distribution function equal to zero. Luckily, we‚Äôre already halfway done with calculating the second order derivative since we‚Äôve already computed the first order derivative above. As we have done above, let‚Äôs ignore the constants since they don‚Äôt affect the calculation.",0,1,0,0,0,1,0,0
Principal Component Analysis,"This makes it easier for the ML model to cluster data, since the data is now aligned in such a way that it shows the most variance. Upon some more research, I also found an interesting paper that shows that there is a solid mathematical relationship between K-means clustering and PCA. I haven‚Äôt read the paper from top to bottom, but instead glossed over a summary of the paper on this thread on stack overflow. It‚Äôs certainly a lot of information to take in, and I have no intent of covering this topic in this already rather lengthy post on PCA. So perhaps this discussion will be tabled for a later time, as interesting as it seems. I hope you enjoyed reading this post. Amidst the chaos of the COVID19 pandemic, let‚Äôs try to stay strong and find peace ruminating over some matrices and formulas. Trust me, it works better than you might think.",0,0,0,0,0,1,0,1
Fisher Score and Information,"Fisher‚Äôs information is an interesting concept that connects many of the dots that we have explored so far: maximum likelihood estimation, gradient, Jacobian, and the Hessian, to name just a few. When I first came across Fisher‚Äôs matrix a few months ago, I lacked the mathematical foundation to fully comprehend what it was. I‚Äôm still far from reaching that level of knowledge, but I thought I‚Äôd take a jab at it nonetheless. After all, I realized that sitting down to write a blog post about some concept forces me to study more, so it is a positive, self-reinforcing cycle. Let‚Äôs begin. Fisher‚Äôs score function is deeply related to maximum likelihood estimation. In fact, it‚Äôs something that we already know‚Äìwe just haven‚Äôt defined it explicitly as Fisher‚Äôs score before. First, we begin with the definition of the likelihood function. Assume some dataset  where each observation is identically and independently distributed according to a true underlying distribution parametrized by . Given this probability density function , we can write the likelihood function as follows: While it is sometimes the convention that the likelihood function be denoted as , we opt for an alternative notation to reserve  for the loss function.",0,0,0,0,0,1,0,0
Recommendation Algorithm with SVD,"In this section, we take a look at the mathematical clockwork behind the SVD formula. In doing so, we might run into some concepts of linear algebra that requie us to understand some basic the properties of symmetric matrices. The first section is devoted to explaining the formula using these properties; the second section provides explanations and simple proofs for some of the properties that we reference duirng derivation. We might as well start by presenting the formula for singular value decomposition. Given some -by- matrix , singular value decomposition can be performed as follows: There are two important points to be made about formula (1). The first pertains to the dimensions of each factor: , , . In eigendecomposition, the factors were all square matrices whose dimension was identical to that of the matrix that we sought to decompose. In SVD, however, since the target matrix can be rectangular, the factors are always of the same shape. The second point to note is that  and  are orthogonal matrices; , a diagonal matrix.",0,0,0,1,0,0,0,1
VGG PyTorch Implementation,"Now it‚Äôs time to build the class that, given some architecture encoding as shown above, can produce a PyTorch model. The basic idea behind this is that we can make use of iteration to loop through each element of the model architecture in list encoding and stack convolutional layers to form a sub-unit of the network. Whenever we encounter , we would append a max pool layer to that stack. This is probably the longest code block I‚Äôve written on this blog, but as you can see, the meat of the code lies in two methods,  and . These methods are where all the fun stacking and appending described above takes place. I actually added a little bit of customization to make this model a little more broadly applicable. First, I added batch normalization, which wasn‚Äôt in the original paper. Batch normalization is known to stabilize training and improve performance; it wasn‚Äôt in the original VGG paper because the batch norm technique hadn‚Äôt been introduced back when the paper was published. Also, the model above can actually handle rectangular images, not just square ones.",0,0,0,0,0,0,1,0
Demystifying Entropy (And More),"Seen in this light, entropy is just the total amount of information content expressed by the distribution of a random variable. Because we were dealing with a very simple example of biased coins, entropy values we calculated did not go past 1 bits, but we can easily imagine situations were it might, such as a dice roll or more complicated real-life examples. In this light, entropy is one metric with which we can quantify randomness, which is the mission we set out to accomplish in this post. But entropy is a lot more than just an interesting concept on randomness. Entropy has wide ranging applications, especially in the field of machine learning and neural networks. Among these, we will be looking at cross entropy and Kullback-Leibler divergence, two derivative concepts of entropy that are arguably most commonly used. Let‚Äôs start with the big picture first‚Äîwhen these concepts are used, what is the relationship between the two, why they are important‚Äîand move onto the details later. First, we have to know that cross entropy, unlike just entropy, deals with two probability distributions instead of one. Specifically, cross entropy is a way of measuring the pseudo-distance between two probability distributions.",0,0,0,0,0,1,0,0
Recommendation Algorithm with SVD,"Much like we understood  as a factor of eigendecomposition,  can be seen as a factor of eigendecomposition, this time on the matrix . Concretly, Notice the parallel between (2) and (4). It‚Äôs not difficult to see that, by symmetry,  is also going to be an orthogonal matrix containing the eigenvectors of . The most important difference between  and  concerns dimensionality: while  is a -by- matrix, V is an -by-. This disparity originates from the fact that  itself is a rectangular matrix, meaning that the dimensions of  and  are also different. Another point that requires clarification pertains to . Earlier, we made a substitution of  for . This tells us that  contains the square roots of the eigenvalues of  and , which, it is important to note, has identical non-zero eigenvalues. If this point brings confusion, I recommend that you peruse over the next subsection on linear algebra. Let‚Äôs conclude this section with the formula for singular value decomposition: Hopefully, now it is clear what , , and  are.",0,0,0,1,0,0,0,1
So What are Autoencoders?,"But more importantly, as we have seen in this tutorial, autoencoders can be used to perform certain tasks, such as removing noise from data, and many more. In the next post, we will take a look at a variant of this vanilla autoencoder model, known as variational autoencoders. Variataional autoencoders are a lot more powerful and fascinating because they can actually be used to generate data instead of merely processing them. I hope you enjoyed reading this post. Stay tuned for more!.",0,0,0,0,1,0,1,0
An Introduction to Markov Chain Monte Carlo,"Theoretically, everything should work fine: given some prior and some sample observation data, we should be able to derive a posterior distribution for the random variable of interest. No big deal. Or so it seems. If we take a look again at equation (3), we will realize that there is an evidence term that we have to calculate sitting in the denominator. The formula for evidence can be expressed as follows: Computing this quantity is not as easy as it may appear. Indeed, this is one of the reasons why the Bayesian way of thinking was eschewed for so long by statisticians: prior to the advent of calculators and computers, mathematicians had trouble deriving the closed-form expression for the evidence term with just pen and paper. We might consider options other than direct calculation, such as Monte Carlo approximation or deriving a proportionality experssion by assuming evidence to be a constant. Indeed, the latter is the approach we took in this post on Bayesian inference. Using a beta prior and binomial likelihood, we used the property of conjugacy to derive a formula for the posterior.",0,0,0,1,0,0,0,0
Dissecting LSTMs,"This relationship becomes a bit more apparent when we graph the two functions side by side.  Let‚Äôs refer to the definition of  to derive an expression for its derivative (no pun intended). Now comes the more complicated part. Thankfully, we‚Äôve already done something very similar in the past when we were building a vanilla neural network from scratch. For instance, we know from this post that, given a cross entropy loss function, the gradient of the softmax layer can simply be calculated by subtracting the predicted probability from the true distribution. In other words, given an intermediate variable we know that Note that  is denoted as  in the code segment. Since we have this information, now it‚Äôs just a matter of back propagating the gradients to the lower segments of the acyclic graph that defines the neural network. Given (8), it only makes sense to continue with the next parameter, . The transpose or the order in which the terms are multiplied may be confusing, but with just some scratch work on paper, it isn‚Äôt difficult to verify these gradients by checking their dimensions. The equation for  is even simpler, since there is no matrix multiplication involved.",0,0,0,0,0,0,1,0
Bayesian Linear Regression,"Why do we bother to pull out the exponent? This is because the integral of a probability density function evaluates to 1, leaving us only with the exponential term outside the integral. To proceed further from here, let‚Äôs take some time to zoom in on  for a second. Substituting , we get We can now plug this term back into (15) as shown below. Although it may seem as if we made zero progress by unpacking , this process is in fact necessary to complete the square of the exponent according to the Gaussian form after making the substitutions By now, you should be comfortable with this operation of backtracking a quadratic and rearranging it to complete the square, as it is a standard operation we have used in multiple parts of this process.",0,0,0,0,0,0,0,1
Gaussian Process Regression,"It‚Äôs important to keep in mind that these samples are each 50-dimensional vectors‚Äîin a sense, they can be considered as ‚Äúfunctions‚Äù, which is why the Gaussian process is often referred to as sampling functions from a multivariate Gaussian. Let‚Äôs plot the final result, alongside the actual function . In red, I‚Äôve also plotted the average of all the 50 samples to see how accurate the result holds up.  The model behaves exactly as we would expect: where there is data, we are confident; where there is no data, we are uncertain. Therefore, we see little variation on test points near the data. In sparse regions where there is no training data, the model reflects our uncertainty, which is why we observe variation within the sampled functions. Comparing the region  where there is a lot of training data, and  where there is little data, this point becomes apparent. Overall, the average of the fifty samples seems to somewhat capture the overall sinusoidal trend present in the training data, notwithstanding the extraneous curvature observed in some regions.",0,0,1,1,0,0,0,0
"Linear Regression, in Two Ways","The final result tells us that the line of best fit, given our data, is Let‚Äôs plot this line alongside our toy data to see how the equation fits into the picture.  It‚Äôs not difficult to see that linear regression was performed pretty well as expected. However, ascertaining the accuracy of a mathematical model with just a quick glance of an eye should be avoided. This point then begs the question: how can we be sure that our calculated line is indeed the best line that minimizes error? To that question, matrix calculus holds the key. We all remember calculus from school. We‚Äôre not going to talk much about calculus in this post, but it is definitely worth mentioning that one of the main applications of calculus lies in optimization: how can we minimize or maximize some function, optionally with some constraint? This particular instance of application is particularly pertinent and important in our case, because, if we think about it, the linear regression problem can also be solved with calculus.",0,0,0,0,0,0,0,1
How lucky was I on my shift?,"We can model this binomial distribution as follows: This code block produces the following output: Under this assumption, we can also calculate how lucky I was yesterday when I only received five calls by plugging in the appropriate values into the binomial PMF function: From a frequentist‚Äôs point of view, I would have lazy days like these only 7 times for every thousand days, which is nearly three years! Given that my military service will last only 1.5 years from now, I won‚Äôt every have such a lucky day again, at least according to the binomial distribution. But a few glaring problem exists with this mode of analysis. For one, we operated under a rather shaky definition of a trial by arbitrarily segmenting eight hours into ten-minute blocks. If we modify this definition, say, by saying that a single minute comprises an experiment, hence a total of 480 trials, we get a different values for  and , which would clearly impact our calculation of .",0,1,0,0,0,1,0,0
Recommendation Algorithm with SVD,"SVD is an incredibly powerful way of processing data, and also ties in with other important techniques in applied statistics such as principal component analysis, which we might also take a look at in a future post. Enough with the preface, let‚Äôs dive right into developing our model. Before we start coding away, let‚Äôs first try to understand what singular value decomposition is. In a previous post on Markov chains, we examined the clockwork behind eigendecomposition, a technique used to decompose non-degenerate square matrices. Singular value decomposition is similar to eigendecomposition in that it is a technique that can be used to factor matrices into distinct components. In fact, in deriving the SVD formula, we will later inevitably run into eigenvalues and eigenvectors, which should remind us of eigendecomposition. However, SVD is distinct from eigendecomposition in that it can be used to factor not only square matrices, but any matrices, whether square or rectangular, degenerate or non-singular. This wide applicability is what makes singular decomposition such a useful method of processing matrices. Now that we have a general idea of what SVD entails, let‚Äôs get down into the details.",0,0,0,1,0,0,0,1
A Step Up with  Variational Autoencoders,"This is why VAEs are considered to be generative models: if we feed the VAE some two-dimensional vector living in the latent space, it will spit out a digit. Whether or not that digit appears convincing depends on the random vector the decoder was provided as input: if the vector is close to the learned mean, , then the result will be convincing; if not, we might see a confusing blob of black and white. Let‚Äôs see what exactly is going on in the fuzzy region of the image, because that is apparently where all the digits mingle together and seem indistinguishable from one another. Put differently, if we vary the random vector little by little across that region, we will be able to see how the digit slowly morphs into another number.  How cool is that? We were able to get a VAE to show us how one digit can shift across a certain domain of the latent space. This is one of the many cool things we can do with a generative model like a variational autoencoder. In this post, we took a deep dive into the math behind variational autoencoders.",0,0,0,0,1,0,1,0
Dissecting the Gaussian Distribution,"Here‚Äôs a suggestion: how about we get rid of the complicated exponential through the substitution Then, it follows that Therefore, the integral in (1) now collapses into Now that looks marginally better. But we have a very dirty constant coefficient at the front. Our natural instinct when we see such a square root expression is to square it. What‚Äôs nice about squaring in this case is that the value of the expression is going to stay unchanged at 1. Because the two integrals are independent, i.e. calculating one does not impact the other, we can use two different variables for each integral. For notational convenience, let‚Äôs use  and . We can combine the two integrals to form an iterated integral of the following form: The term  rings a bell, and that bell sounds like circles and therefore polar coordinates. Let‚Äôs implement a quick change of variables to move to polar coordinates. Now we have something that we can finally integrate. Using the chain rule in reverse, we get We can consider there to be 1 in the integrand and continue our calculation.",0,1,0,0,0,1,0,0
"Newton-Raphson, Secant, and More","And indeed, if we cube it, we end up with a value extremely close to 20. In other words, we have successfully found the root to . Instead of the direct derivative, , we can also use approximation methods. In the example below, we show that using  results in a very similar value (in fact, it is identical in this case, but we need to take into other factors such as numerical stability and overflow which might happen with such high-precision numbers). This result aligns with the earlier observation that center divided difference provides very accurate approximations of the first derivative. Note that the advantage of using  is that we can now apply Newton-Raphson to non-polynomial equations that cannot be formulated in list index representation format. For instance, let‚Äôs try something like . To verify that this is indeed correct, we can plug  back into . Also, given that , we can use some heuristics to realize that the answer must be only ever so slightly larger than 1. Notice that the result is extremely close to zero, suggesting that we have found the correct root.",1,0,0,0,0,0,0,0
Logistic Regression Model from Scratch,"If we set the learning rate  to a smaller number, we would expect the model to take a lot longer to tune, and indeed this seems to be true:  With a much smaller learning rate, the model seems to struggle to achieve high accuracy. However, although there are a lot of uneven spikes, the model still manages to reach a pretty high accuracy score by 200 epochs. This tells us that the success of model training depends a lot on how we set the learning rate; setting an excessively high value for the learning rate might result in overshooting, while a low learning rate might prevent the model from quickly learning from the data and making meaningful progress. Accuracy helps us intuitively understand how well our model is doing, but recall that the main objective of gradient descent is not to maximize accuracy, but to minimize the cross entropy loss function. Therefore, perhaps it makes more sense to evaluate the performance of our logistic regression model by plotting cross entropy. Presented below is a simple function that plots epoch versus cross entropy given a list of learning rates, .",0,0,1,1,0,0,0,0
Fisher Score and Information,"Because Fisher‚Äôs information requires computing the expectation given some probability distribution, it is often intractable. Therefore, given some dataset, often times we use the empirical Fisher as a drop-in substitute for Fisher‚Äôs information. The empirical Fisher is defined quite simply as follows: In other words, it is simply an unweighted average of the covariance of the score function for each observed data point. Although this is a subtlety, it helps to clarify nonetheless. Something that may not be immediately apparent yet nonetheless true and very important about Fisher‚Äôs information is the fact that it is the negative expected value of the second derivative of the log likelihood. In our multivariate context where  is a vector, the second derivative is effectively the Hessian. In other words, You might be wondering how the information matrix can be defined in two says, the covariance and the Hessian. Indeed, this threw me off quite a bit as well, and I struggled to find and understand a good resource that explained why this was the case. Thankfully, Mark Reid‚Äôs blog and an MIT lecture contained some very helpful pointers that got me a long way.",0,0,0,0,0,1,0,0
The Exponential Family,"Probability Density Functions Probability Mass Functions Of course, there are examples of common distributions that do not fall under this category, such as the uniform distribution or the student -distribution. This point notwithstanding, the sheer coverage of the exponential family makes it worthy of exploration and analysis. Also, notion of an exponential family itself is significant in that it allows us to frame problems in meaningful ways, such as through the notion of conjugate priors: if you haven‚Äôt noticed, the distributions outlined above all have conjugate priors that also belong to the exponential family. In this sense, the exponential family is particularly of paramount importance in the field of Bayesian inference, as we have seen many times in previous posts. Let‚Äôs concretize our understanding of the exponential family by applying factorization to actual probability distributions. The easiest example, as you might have guessed, is the exponential distribution. Recall that the formula for the exponential distribution is where the indicator function, denoted as , takes the following form: The indicator function is a simple  modification applied to ensure that the function is well-defined across the entire real number domain.",0,1,0,0,0,1,0,0
Markov Chain and Chutes and Ladders,"On the other hand, it is interesting to see that the eigenvector reveals information about the structure of the Markov chain in this example. Markov chains like these are referred to as absorbing Markov chains because the stationary equilibrium always involves a non-escapable state that ‚Äúabsorbs‚Äù all other states. One might visualize this system as having a loop on a network graph, where it is impossible to move onto a different state because of the circular nature of the edge on the node of the absorbing state. At this point, let‚Äôs remind ourselves of the end goal. Since we have successfully built a stochastic matrix, all we have to do is to set some initial starting vector  and perform iterative matrix calculations. In recursive form, this statement can be expressed as follows: The math-inclined thinkers in this room might consider the possibility of conducting an eigendecomposition on the stochastic matrix to simply the calculation of matrix powers. There is merit to considering this proposition, although later on we will see that this approach is inapplicable to the current case. Eigendecomposition refers to a specific method of factorizing a matrix in terms of its eigenvalues and eigenvectors.",0,0,0,0,0,0,0,1
The Exponential Family,"‚Äù This is obviously my poor attempt at an intellectual joke, to which many of you might cringe, but I personally think it somewhat captures the idea that many probability distributions that we see on the textbook are, in fact, surprisingly more related than we might think. At least to me, it wasn‚Äôt obvious from the beginning that the exponential and the Bernoulli distributions shared the same structure, not to mention the wealth of other distributions that belong to the exponential family.  Also, the convenient factorization is what allowed us to perform an MLE estimation, which is an important concept in statistics with wide ranging applications. This post in no way claims to give a full, detailed view of the exponential family, but hopefully it gave you some understanding of what it is and why it is useful. In the next post, we will take a look at maximum a posteriori estimation and how it relates to the concept of convex combinations. Stay tuned for more.",0,1,0,0,0,1,0,0
Naive Bayes Model From Scratch,"Recall that to perform Bayesian analysis, we first need to specify a prior. Although we could use an uninformed prior, in this case, we have data to work with. The way that makes the most sense would be to count the number of data points corresponding to each class to create a categorical distribution and use that as our prior, as shown below. If we use the  created from , we should get a very simple prior whereby  since there is an equal number of data points belonging to the two classes in the toy data set. Indeed, this seems to be true. Next, it‚Äôs time to model the likelihood function. I won‚Äôt get into the specifics of this function, but all it does is that it calculates the likelihood by using the parameters returned by the  function to indicate the likelihood that a particular  belongs to a certain class. As per convention of this tutorial, the returned dictionary has keys corresponding to each class and values indicating the likelihood that the  belongs to that class. We can see the  function in action by passing a dummy test instance.",0,0,1,1,0,0,0,0
A sneak peek at Bayesian Inference,"In this light, Bayesian statistics offers a new way to compute new information and update our beliefs about an event in probabilistic terms. Bayesian inference is nothing more than an extension of Bayes‚Äô theorem. The biggest difference between the two is that Bayesian inference mainly deals with probability distributions instead of point probabilities. The case of the potential patient we analyzed above was a simple yet illuminating example, but it was limiting in that we assumed all parameters to be simple constants, such as  for test accuracy and  for false positive frequency. In reality, most statistical estimates exist as probability distributions since there are limitations to our ability to measure and survey data from the population. For example, a simple random sampling of the population might reveal that 15 percent of the sample population is affected with the flu, but this would most likely produce a normal distribution with mean centered around 0.15 instead of a point probability.",0,1,0,0,0,1,0,0
A Brief Introduction to Recurrent Neural Networks,"Below is a sample code implementation of this process. Let‚Äôs load the data using the  function after specifying necessary parameters. Now that the data is here and ready to go, it‚Äôs time to  build our neural network. To spice things up a bit, let‚Äôs create four different models and compare their performance. Before we jump into that, however, we first need to understand what an embedding layer is, as it is key to natural language processing. Simply put, an embedding layer is a layer that transforms words or integers that encode words into dense vectors. This is a necessary transformation since neural networks are incapable of dealing with non-quantitative variables. Why dense vectors, then? Can‚Äôt we simply use one-hot encoding? That is a valid point, since one-hot encoding is how we mostly deal with categorical variables in a dataset. However, one downside of this approach is that we end up with many sparse vectors. In other words, a lot of resources are wasted because the model now has to process vectors of thousands or millions of dimensions, depending on the vocabulary size.",0,0,0,0,1,0,1,0
An Introduction to Markov Chain Monte Carlo,"The benefit of working with this dumb example is that we can analytically derive a closed-form exprerssion for the posterior distribution. This is because a normal prior is conjugate with a normal likelihood of known variance, meaning that the posterior distribution for the mean will also turn out to be normal. If you are wondering if this property of conjugacy is relevant to the notion of conjugacy discussed above with Bayesian inference, you are exactly correct: statisticians have a laundry list of distributions with conjugate relationships, accessible on this Wikipedia article. The bottom line is that we can calculate the posterior analytically, which essentially gives us an answer with which we can evaluate our implementation of the Metropolis-Hastings algorithm. The equation for the posterior is presented below. This assumes that the data is normally distributed with known variance and that the prior is normal, representable as For a complete mathematical derivation of (8), refer to this document. As we will see later on, we use (9) to calculate the likelihood and (10) to calculate the prior. For now, however, let‚Äôs focus on the analyticial derivation of the posterior.",0,0,0,1,0,0,0,0
A Step Up with  Variational Autoencoders,"The motivating idea behind variational autoencoders is that we want to model a specific distribution, namely the distribution of the latent space given some input. As you recall, this latent space is a two dimensional vector modeled as a multivariate diagonal Gaussian. Using Bayes‚Äô theorem, we can express this distribution as follows: By now, it is pretty clear what the problem its: the evidence sitting in the denominator is intractable. Therefore, we cannot directly calculate or derive  in its closed form; hence the need for variational inference. The best we can do is to find a distribution  that best approximates . How do we find this distribution? Well, we know one handy concept that measures the difference or the pseudo-distance between two distributions, and that is Kullback-Leibler divergence. As we discussed in this post on entropy, KL divergence tells us how different two distributions are. So the goal here would be find a distribution that minimizes the following expression: Using the definition of conditional probability, we can simplify (4) as follows: The trick is to notice that  is a constant that can break out of the expectation calculation. Let‚Äôs continue by deriving an expression for the evidence term.",0,0,0,0,1,0,1,0
Word2vec from Scratch,"We would expect this number to have been larger had we used a larger window. 60 is the size of our corpus, or the number of unique tokens we have in the original text. Since we have one-hot encoded both the input and output as 60-dimensional sparse vectors, this is expected. Now, we are finally ready to build and train our embedding network. At this point, you might be wondering how it is that training a neural network that predicts some nearby context word given an input token can be used to embed words into vectors. After all, the output of the network is going to be some probability vector that passed through a softmax layer, not an embedding vector. This is entirely correct, and this is a question that came to my mind as well. However, this is the part that gets the most interesting: the rows of the intermediate weight matrix is the embedding we are looking for! This becomes much more apparent once we consider the dimensions of the weight matrices that compose the model.",0,0,0,1,0,0,1,0
Demystifying Entropy (And More),"Machine learning is often referred to as a black box that we can simply use without much knowledge on how it works, but I personally find studying these underlying clockwork behind the hood to be much more interesting than blindly applying to to churn out numbers and predictions. I hope you enjoyed reading. Merry Christmas and Happy holidays!.",0,0,0,0,0,1,0,0
The Gibbs Sampler,"In this post, we will explore Gibbs sampling, a Markov chain Monte Carlo algorithm used for sampling from probability distributions, somewhat similar to the Metropolis-Hastings algorithm we discussed some time ago. MCMC has somewhat of a special meaning to me because Markov chains was one of the first topics that I wrote about here on my blog. It‚Äôs been a while since I have posted anything about math or statistics-related, and I‚Äôll admit that I‚Äôve been taking a brief break from these domains, instead working on some personal projects and uping my Python coding skills. This post is going to be a fun, exciting mesh of some Python and math. Without further ado, let‚Äôs get started. I remember struggling to understand Metropolis-Hastings a while back. Gibbs sampling, on the other hand, came somewhat very naturally and intuitively to me. This is not because I‚Äôve suddenly grown intelligent over the past couple of months, but because Gibbs sampling is conceptually simpler, at least in my humble opinion. All that is necessary to understand Gibbs sampling is the notion of conditional probability distributions.",0,0,0,0,0,1,0,1
"Basel, Zeta, and some more Euler","We can see this by reminding ourselves of the clockwork behind the sieve of Eratosthenes, which is basically how the elimination and factorization works in the derivation of Euler‚Äôs identity. Taking this into account, we can deduce that Euler‚Äôs identity will take the following form: This expression is Euler‚Äôs infinite product representation of the zeta function. These days, I cannot help but fall in love with Euler‚Äôs works. His proofs and discoveries are simple and elegant yet also fundamental and deeply profound, revealing hidden relationships between numbers and theories that were unthought of during his time. I tend to avoid questions like ‚Äúwho was the best  in history‚Äù because they most often lead to unproductive discussions that obscure individual achievements amidst meaningless comparisons, but I dare profess here my belief that only a handful of mathematicians can rival Euler in terms of his genius and prolific nature. That is enough Euler for today. I‚Äôm pretty sure that this is not going to be the last post on Euler given the sheer amount of work he produced during his lifetime.",1,0,0,0,0,0,0,0
Principal Component Analysis,"We also use the fact that the covariance matrix is symmetric. If we left multiply (18) by , But since , the first two terms go to zero. Also, the last term reduces to  since . This necessarily means that . If we plug this result back into (18), we end up with the definition of the eigenvector again, but this time for . Essentially, we iterate this process to find a specified number of principal components, which amounts to finding   number of eigenvectors of the sample covariance matrix. A while back, we discussed both eigendecomposition as well as singular value decomposition, both of which are useful ways of decomposing matrices into discrete factors. In this section, we will see how PCA is essentially a way of performing and applying these decomposition techniques under the hood. Recall that eigendecomposition is a method of decomposing matrices as follows: where  is a diagonal matrix of eigenvalues and  is a matrix of eigenvectors. PCA is closely related to eigendecomposition, and this should come as no surprise. Essentially, by finding the eigenvalues and eigenvectors of , we are performing an eigendecomposition on the covariance matrix: Notice that  is a matrix of principal components.",0,0,0,0,0,1,0,1
Likelihood and Probability,"Using the property in (3), we can simplify the equation above: To find the maximum of this function, we can use a bit of calculus. Specifically, our goal is to find a parameter that which makes the first derivative of the log likelihood function to equal 0. To find the optimal mean parameter , we derive the log likelihood function with respect to  while considering all other variables as constants. From this, it follows that Rearranging this equation, we are able to obtain the final expression for the optimal parameter  that which maximizes the likelihood function: As part 2 of the trilogy, we can also do the same for the other parameter of interest in the normal distribution, namely the standard deviation denoted by . We can simplify this equation by multiplying both sides by . After a little bit of rearranging, we end up with Finally, we have obtained the parameter values for the mean and variance of a normal distribution that maximizes the likelihood of our data.",0,0,0,0,0,1,0,0
A Simple Autocomplete Model,"You might remember back in the old days when autocomplete was just terrible. The suggestions provided by autocomplete would be useless if not downright stupid‚ÄîI remember that one day when I intended to type ‚ÄúGimme a sec,‚Äù only to see my message get edited into ‚ÄúGimme a sex‚Äù by the divine touches of autocomplete. On the same day, the feature was turned off on my phone for the betterment of the world. Now, times have changed. Recently, I decided to give autocorrect a chance on my iPhone. Surprisingly, I find myself liking autocomplete more than hating it, especially now that the weather is getting colder by each day: when my frost-numbed finger tips touch on the wrong places of the phone screen to produce words that aren‚Äôt really words, iPhone‚Äôs autocomplete somehow magically reads my mind to rearrange all that inscrutable alphabet soup into words that make actual, coherent sense. Sometimes, it‚Äôs so good at correcting my typos that I intentionnally make careless mistakes on the keyboard just to see how far it can go. One of the obvious reasons behind such drastic improvements in autocomplete functionality is the development of deep neural networks.",0,0,0,0,1,0,1,0
Revisiting Basel with Fourier,"In the last post, we revisited the Riemann Zeta function, which we had briefly introduced in another previous post on Euler‚Äôs take on the famous Basel problem. It seems like math is my jam nowadays, so I decided to write another post on this topic‚Äîbut this time, with some slightly different takes. In this post, we will explore an alternative way of solving the Basel problem using Fourier series expansion, and also discuss a alternative representations of the Basel problem in integral form. For the integral representations, I‚Äôm directly referencing Flammable Maths, a YouTube channel that I found both entertaining and informative. Let‚Äôs get started. First, let‚Äôs recall what the Basel problem is. The problem is quite simple: the goal is to obtain the value of an infinite series, namely This seems like an innocuous, straightforward problem. One can easily prove, for instance, the fact that this series converges using integral approximation. However, to obtain the value of this series is a lot more difficult than it appears‚Äîit is no coincidence that this problem remained unsolved for years until Euler came along.",1,0,0,0,0,0,0,0
"Newton-Raphson, Secant, and More","Now that we have seen the robustness of the Newton-Raphson method, let‚Äôs take a look at another similar numerical method that uses backward divided difference for derivative approximation. In this section, we will look at the secant method, which is another method for identifying the roots of non-linear equations. Before we get into a description of how this method works, here‚Äôs a quick graphic, again from Wikipedia, on how the secant method works.  As the name implies, the secant function works by drawing secant lines that cross the function at each iteration. Then, much like the Newton-Raphson method, we find the -intercept of that secant line, find a new point on the graph whose -coordinate corresponds to that intercept, and use the point from the previous iteration to find a new secant line. This process is very intuitively outlined in this video by numericalmethodsguy. The update rule for the secant method can be expressed as We can derive (7) simply by slightly modifying the update rule we saw for Newton-Raphson.",1,0,0,0,0,0,0,0
"Beta, Bayes, and Multi-armed Bandits","Last but not least, let‚Äôs revisit the cumulative regret graph introduced earlier. We can draw our own cumulative regret graph by first simulating what would have been the optimal result‚Äîin other words, we need to obtain the amount of reward the gambler would have earned had they simply pulled on the best bandit the entire time. Let‚Äôs quickly simulate that first. And it turns out that the maximum amount they would have earned, in this particular instance, is 74. I say in this particular instance, since the expected value of the maximum reward is simply 70, given that the highest success parameter is 0.7. This minor detail notwithstanding, we can now use the  quantity and the  list in order to recreate our own cumulative regret graph, as shown below.  It is obvious that the cumulative regret is highest when we start since the current reward is at 0.",0,1,0,0,0,1,0,0
PyTorch RNN from Scratch,"In this post, we‚Äôll take a look at RNNs, or recurrent neural networks, and attempt to implement parts of it in scratch through PyTorch. Yes, it‚Äôs not entirely from scratch in the sense that we‚Äôre still relying on PyTorch autograd to compute gradients and implement backprop, but I still think there are valuable insights we can glean from this implementation as well. For a brief introductory overview of RNNs, I recommend that you check out this previous post, where we explored not only what RNNs are and how they work, but also how one can go about implementing an RNN model using Keras. This time, we will be using PyTorch, but take a more hands-on approach to build a simple RNN from scratch. Full disclaimer that this post was largely adapted from this PyTorch tutorial this PyTorch tutorial. I modified and changed some of the steps involved in preprocessing and training. I still recommend that you check it out as a supplementary material. With that in mind, let‚Äôs get started. The task is to build a simple classification model that can correctly determine the nationality of a person given their name.",0,0,0,1,0,0,1,0
Demystifying Entropy (And More),"Although I‚Äôm no science major, one of the few things that I recall from high school chemistry class is the Gibbs free energy equation for calculating the spontaneity of reactions, which went as follows: where the term for entropy, denoted as  satisfies the condition that We won‚Äôt get into the details of these equations, but an intuition that we can glean from this equation is that the randomness of a particle is determined by the number of potential states that are possible for that given particle. In other words, a gas particle that freely moves across space at ATP is going to be more random than a near-static water particle composing an ice cub. We might take a step further and say that the gas particle carries a larger amount of information than the particle in a solid since more information is required to express its randomness. Entropy in science, denoted above as , provides us with a valuable intuition on the topic of randomness and information.",0,0,0,0,0,1,0,0
An Introduction to Markov Chain Monte Carlo,"This can be achieved by translating the equation in (8) into code as shown below. Let‚Äôs see what the posterior looks like given the prior . For sample observations, we use the toy data set  we generated earlier.  There we have it, the posterior distribution given sample observations and a normal prior. As expected, we see that the result is normal, a result due to the property of conjugacy. Another observation we might make is that the posterior mean is seems to be slightly larger than 0. This is an expected result given that the mean of the numbers in our toy data set was larger than 0. Because the posterior can be intuited as an ‚Äúaverage‚Äù between the observed data set and the prior, we would expect the posterior to be centered around a value greater than zero, which is indeed the case. Now that we have a full answer key to our problem, it‚Äôs time to build the Metropolis-Hastings sampler from scratch and compare the estimation generated by the sampler with the true analytical posterior we have just derived. Let‚Äôs go back to equation (7), which is the crux of the Metropolis-Hastings sampler.",0,0,0,1,0,0,0,0
Moments in Statistics,"Using the chain rule, At , So we have confirmed again that the mean of a Poisson distribution is equal to . Let‚Äôs take another distribution as an example, this time the exponential distribution. We have not looked specifically at the exponential distribution in depth previously, but it is a distribution closely related to the , which we derived in this post. Specifically, when parameter  in a Gamma distribution, it is in effect an exponential distribution. Perhaps we will explore these relationships, along with the Erlang distribution, in a future post. For now, all we have to know is the probability density function of the exponential distribution, which is This time, the task is to obtain the third moment of the distribution, i.e. . But the fundamental approach remains identical: we can either use the definition of expected values to calculate the third moment, or compute the MGF and derive it three times. At a glance, the latter seems a lot more complicated. However, it won‚Äôt take long for us to see that sometimes, calculating the MGF is sometimes as easy as, if not easier than, taking the expected values approach.",0,1,0,0,0,1,0,0
Introduction to tf-idf,"This is because the way we built ordinal indexing in corpus is probably different from how scikit-learn implements it internally. This point notwithstanding, it‚Äôs clear that the values of each vectors are identical, disregarding the fact that the result produced by our algorithm has more decimal points due to floating point arithmetic. This was a short introductory post on tf-idf vectors. When I first heard about tf-idf vectors from a friend studying computational linguistics, I was intimidated. However, now that I have a project I want to complete, namely an auto-tagging and classification NLP model, I‚Äôve mustered more courage and motivation to continue my study the basics of NLP. I hope you‚Äôve enjoyed reading this post. Catch you up in the next one! (Yes, this is a trite ending comment I use in almost every post, so the idf scores for the words in these two sentences are going to be very low.).",0,0,0,1,0,0,0,0
GAN in PyTorch,"This is expected, since we are going to be training two different networks in a different manner. Before we get into the details of training, here is a simple utility function in which we zero the gradients for both the generator and the discriminator. We want the discriminator to be able to distinguish real from fake images. Therefore, the discriminator will have a combined loss: the loss that comes from falsely identifying real images as fake, and the loss that comes from confusing fake, generated images as real ones. For the generator, the loss is actually dependent upon the classifier: the loss comes from the classifier correctly identifying generated images as fake. Let‚Äôs see what this means below.      And we see that the generator was able to produce somewhat confusing hand-written images of digits! Granted, this is far from perfect, and there are images that look somewhat funky. However, there are also somewhat realistic images here and there, and it is impressive that a simple densely connected network was able to achieve this performance. Had we used CNNs, the result might have been even better.",0,0,0,0,0,0,1,0
Moments in Statistics,"where the third equality stands due to the variant of the Taylor series for the exponential function we looked at earlier: Therefore, we have confirmed that the mean of a Poisson distribution is equal to , which aligns with what we know about the distribution. Another way we can calculate the first moment of the Poisson is by deriving its MGF. This might sound a lot more complicated than just computing the expected value the familiar way demonstrated above, but in fact, MGFs are surprisingly easy to calculate, sometimes even easier than using the definition expectation. Let‚Äôs begin by presenting a statement of the MGF. Let‚Äôs factor out terms that contain lambda, which is not affected by the summation. Again, we refer to equation (9) to realize that the sigma expression simplifies into an exponential. In other words, From this observation, we can simplify equation (10) as follows: And there is the MGF of the Poisson distribution! All we have to do to obtain the first moment of the Poisson distribution, then, is to derive the MGF once and set  to 0.",0,1,0,0,0,1,0,0
k-Nearest Neighbors Algorithm from Scratch,"These days, machine learning and deep neural networks are exploding in importance. These fields are so popular that, unless you‚Äôre a cave man, you have probably heard it at least once. The exaggeration not withstanding, there is perhaps no necessity to justify the topic for today‚Äôs blog post: exploring a machine learning algorithm by building it from scratch. Apparently, ‚Äúfrom scratch‚Äù is now a trendy pedagogical methodology employed in many websites and resources that claim to educate their readers about machine learning. To this, I agree: by constructing algorithms from the ground up, one can glean meaningful insights on how machine learning actually works, as opposed to viewing ML as some dark magic that suddenly makes computers intelligible beings. Enough of the prologue, let‚Äôs jump right in. As the name implies, the k-nearest neighbors algorithm works by findinng the nearest neighbors of some give data. Then, it looks at the labels of  neighboring points to produce a classification prediction. Here,  is a parameter that we can tweak to build the KNN model. For instance, let‚Äôs say we have a binary classification problem. If we set  to 10, the KNN modell will look for 10 nearest points to the data presented.",0,0,1,1,0,0,0,0
The Exponential Family,"Despite its grandiose nomenclature, the canonical form simply refers to a specific flavor of factorization scheme where in which case (3) simplifies to We will assume some arbitrary distribution in the exponential family following this canonical form to perform maxmimum likelihood estimation. Much like in the previous post on maximum likelihood estimation, we begin with some data set of  independent and identically distributed observations. This is going to be the setup of the MLE problem. Given this dataset, the objective of maximum likelihood estimation is to identify some parameter  that maximizes the likelihood, i.e. the probability of observing these data points under a probability distribution defined by . In other words, How do we identify this parameter? Well, the go-to equipment in a mathematician‚Äôs arsenal for an optimization problem like this one is calculus. Recall that our goal is to maximize the likelihood function, which can be calculated as follows: The first equality stands due to the assumption that all data are independent and identically distributed. Maximizing (16) is a complicated task, especially because we are dealing with a large product. Products aren‚Äôt bad, but we typically prefer sums because they are easier to work with.",0,1,0,0,0,1,0,0
The Gibbs Sampler,"We then take turns sampling from the conditional probability distributions using the sampled values, and append to a list to accumulate the result. The more difficult part here is deriving the equation for the conditional probabiilty distributions of the bivariate Gaussian. The full result is available on Wikipedia, but it‚Äôs always more interesting and rewarding to derive these results ourselves. But in this section, we will assume that we already know the final result of the derivation and use the formula for now. Note that the two functions are symmetrical, which is expected given that this is a bivariate distribution. These functions simulate a conditional distribution, where given a value of one random variable, we can sample the value of the other. This is the core mechanism by which we will be sampling from the joint probability distribution using the Gibbs sampling algorithm. Let‚Äôs initialize the parameters for the distribution and test the sampler. Great! This works as expected. For the purposes of demonstrating the implications of burn-in, let‚Äôs define discard the first 100 values that were sampled. Below is the plot of the final resulting distribution based on sampled values using the Gibbs sampler.",0,0,0,0,0,1,0,1
"PyTorch, From Data to Modeling","These past few weeks, I‚Äôve been powering through PyTorch notebooks and tutorials, mostly because I enjoyed the PyTorch API so much and found so many of it useful and intuitive. Well, the problem was that I ended up writing something like ten notebooks without ever publishing them on this blog. So really, I‚Äôm going over some old notebooks I‚Äôve coded out more than a month ago to finally make it live. That‚Äôs enough excuses, let‚Äôs get into the basics of PyTorch modeling in this notebook with the CIFAR10 dataset and some basic CNNs. The setup is pretty simple here. We import some modules and functions from PyTorch, as well as  to be able to show some basic training plots. One thing I have noticed is that a lot of people do something like which I personally don‚Äôt really get, because you can easily just do If you ask me, I think the latter is more elegant and less cluttered (after all, we don‚Äôt have to repeat  twice). I don‚Äôt think the two import statements are functionally different, but if I do figure out any differences, I will make sure to update future notebooks.",0,0,0,0,0,0,1,0
How lucky was I on my shift?,"So yesterday got me thinking: what is the probability that I get only five calls within a time frame of eight hours, given some estimate of the average number of calls received by the PMO, say 12? How lucky was I? One way we might represent this situation is through a binomial distribution. Simply put, a binomial distribution simulates multiple Bernoulli trials, which are experiments with only two discrete results, such as heads and tails, or more generally, successes and failures. A binomial random variable  can be defined as the number of success in  repeated trials with probability of success . For example, if we perform ten tosses of a fair coin, the random variable would be the number of heads;  would be , and  would be . Mathematically, the probability distribution function of a binomial distribution can be written as follows: We can derive this equation by running a simple thought experiment. Let‚Äôs say we are tossing a coin ten times.",0,1,0,0,0,1,0,0
"Beta, Bayes, and Multi-armed Bandits","The result is as we would expect: the bandit with the highest success parameter of 0.7 seems to have been pulled on the most, which explains why its variance is the smallest out of the bunch. Moreover, the mean of that particular posterior is also close to 0.7, its true value. Notice that the rest of the posteriors also somewhat have this trend, although more uncertainty is reflected into the shape of the distributions via the spread. It is interesting to see how we can go from a uniform prior, or , to almost normal-shaped distributions as we see above. To corroborate our intuition that the best bandit was indeed the most pulled, let‚Äôs quickly see the proportion of the pulls through a pie chart.  As you can see, there seems to be a positive correlation between the success parameter and the number of pulls. This is certainly good, since we want the gambler to pull on the best bandit the majority of times while avoiding the worse ones as much as possible. This certainly seems to be the case, given that the bandit with the worse parameter‚Äî0.1‚Äîwas pulled the least.",0,1,0,0,0,1,0,0
How lucky was I on my shift?,"The following is a code block that produces a visualization of what this integral would look like on a graph. Here is the figure produced by executing the code block above. You might notice from the code block that the integrand is not quite the Poisson distribution‚Äîinstead of a factorial, we have an unfamiliar face, the  function. Why was this modification necessary? Recall that integrations can only be performed over smooth and continuous functions, hence the classic example of the absolute value as a non-integrable function. Factorials, unfortunately, also fall into this category of non-integrable functions, because the factorial operation is only defined for integers, not all real numbers. To remedy this deficiency of the factorial, we resort to the gamma function, which is essentially a continuous version of the factorial. Mathematically speaking, the gamma function satisfies the recursive definition of the factorial: Using the gamma distribution function, we can then calculate the area of the shaded region on the figure above. Although I do not present the full calculation process here, the value is approximately equal to that we obtained above, .",0,1,0,0,0,1,0,0
Maximum A Posteriori Estimation,"In a previous post on likelihood, we explored the concept of maximum likelihood estimation, a technique used to optimize parameters of a distribution. In today‚Äôs post, we will take a look at another technique, known as maximum a posteriori estimation, or MAP for short. MLE and MAP are distinct methods, but they are more similar than different. We will explore the similar mathematical underpinnings behind the methods to gain a better understanding of how distributions can be tweaked to best fit some given data. Let‚Äôs begin! Before we jump right into comparing MAP and MLE, let‚Äôs refresh our memory on how maximum likelihood estimation worked. Recall that likelihood is defined as In other words, the likelihood of some model parameter  given data observations  is equal to the probability of seeing  given . Thus, likelihood and probability are inevitably related concepts that describe the same landscape, only from different angles. The objective of maximum likelihood estimation, then, is to determine the values for a distribution‚Äôs parameters such that the likelihood of observing some given data is maximized under that distribution.",0,0,0,0,0,1,0,0
Scikit-learn Pipelines with Titanic,"This packaging can nicely be abstracted via the , which is the magic glue to put all the pieces together. We simply have to tell which transformer applies to which column, along with the name for each process. is the complete package that we will use to transform our data. Note that  allows us to specify which pipeline will be applied to which column. This is useful, since by default, imputers or transformers apply to the entire dataset. More often or not, this is not what we want; instead, we want to be able to micro-manage categorical and numerical columns. The combination of  and  is thus a very powerful one. Now all that is left is to build a final pipeline that includes the classifier model. Let‚Äôs see how well our model performs on a stratified 5-fold cross validation. Note that this is without any hyperparameter tuning. And just like that, we can evaluate the performance of our model.",0,0,1,0,0,0,0,0
Demystifying Entropy (And More),"These two distributions, although similar, are different. But the question is, how different? Creating a visualization might give us some idea about the difference between the two distributions.  The two distributions are quite similar, meaning that our neural network did a good job of classifying given data. However, we can get a bit more scientific by calculating the cross entropy to see exactly how well our model performed with the given data. To achieve this, let‚Äôs quickly write some functions to calculate KL divergence and cross entropy. We will be reusing the  function we defined above. On a trivial note, we prevent Python from running into math domain errors, we add  to the provided distribution if the list contains 0. The result is unsurprising. If we recall that the definition of entropy is the amount of information content needed to encode information, we will quickly realize that  is a distribution with probability 1, which is why it makes sense that entropy converges to 0. Therefore, in this case, KL divergence equals cross entropy, which computes to approximately 0.415.",0,0,0,0,0,1,0,0
First Neural Network with Keras,"In doing so, we will be dealing with a classic in machine learning literature known as the the MNIST data set, which contains images of hand-written digits from 0 to 9, each hand-labeled by researchers. The  variable denotes the total number of class labels available in the classification task, which is 10.  specifies the number of iterations the gradient descent algorithm will run for. Let‚Äôs begin by loading the data from . Now we have to slightly modify the loaded data so that its dimensions and values are made suitable to be fed into a neural network. Changing the dimensionality of data can be achieved through the  function, which takes in the number of rows and columns as its argument. We convert the numbes into type , then normalize it so that its values are all between 0 and 1. Although we won‚Äôt get into too much detail as to why normalization is important, an elementary intuition we might develop is that normalization effectively squishes all values into the same bound, making data much more processable. We also implement one-hot encoding on  through the  function.",0,0,1,0,1,0,1,0
"Newton-Raphson, Secant, and More","Recall that the Newton-Raphson update rule was written as The only modification we need to make to this update rule is to replace  with an approximation using the backward divided difference formula. Here, we make a slight modification to (2), specifically by using values from previous iterations. If we plug (8) back into (4), with some algebraic simplifications, we land on (7), the update rule for the secant method. This is left as an exercise for the reader. Now let‚Äôs take a look at how we might be able to implement this numerical method in code. Presented below is the  method, which follows the same general structure as the  function we looked at earlier. The only part that differs is the part where we use backward divided difference, which requires that we look up not only the immediately previous root approximation, but the value prior to that as well. In other words, we need both  and  to calculate  via an iterative update. And here is an obligatory sanity check using our previous example. 2.7 is a familiar value, and indeed it is what was returned by the Newton-Raphson method as well.",1,0,0,0,0,0,0,0
Convolutional Neural Network with Keras,"To get a better idea of what the CIFAR10 data looks like, here is a basic function that will display the images in the data set for us using . We can see that, although the images are very pixelated, it is somewhat possible to make out what each image is showing. Of course, this task is going to be a lot more difficult for our neural network.  Now is finally the time to buid the neural network. This network pretty much follows the standard vanilla convolutional neural network model, which typically involves stacking convolution, batch normalization, and pooling layers on top of each other. The dropout layers were also added so as to minimize any potential overfitting. The  argument was configured as , named after Kaiming He who found the optimal weight initialization kernel for convolutional layers. The  argument ensures that the the feature maps are not downsized too quickly due to repeated applications of convolution and pooling. The  layer simply normalizes the tensor returned from the previous layer. There are ongoing research as to what effect batch normalization has on neural networks, but the general consensus is that it helps the model learn more quickly and efficiently.",0,0,0,0,1,0,1,0
Bayesian Linear Regression,"Finally, we have derived the predictive distribution in closed form: With more simplification using the , it can be shown that And there‚Äôs the grand formula for Bayesian linear regression! This result tells us that, if we were to simply get the best point estimate of the predicted value , we would simply have to calculate , which is the tranpose product of the MAP estimate of the weights and the input vector! In other words, the answer that Bayesian linear regression gives us is not so much different from vanilla linear regression, if we were to reduce the returned predictive probability distribution into a single point. But of course, doing so would defeat the purpose of performing Bayesian inference, so consider this merely an intriguing food for thought. As promised, we will attempt to visualize Bayesian linear regression using the  library. Doing so will not only be instructive from a perspective of honing probabilistic programming skills, but also help us better understand and visualize Bayesian inference invovled in linear regression as explored in the context of this article. Note that, being a novice in , I borrowed heavily from this resource available on the  official documentation.",0,0,0,0,0,0,0,1
Wonders of Monte Carlo,"Cool! The Monte Carlo algorithm thus tells us that the probability of success is about 10 percent, which is a lot smaller thant I had personally anticipated. Think about how complicated it would have been to calculate this probability by hand. By simulating this game multiple times and counting the instances of successes, we can derive an estimation of the success rate of our particular random walk model. Let‚Äôs see what happens when we simulate the drunkard‚Äôs walk thirty times. In the particular instance that I have below, we see that the drunkard successfully reached the rest room four out of thirty attempts, which roughly equals the success probability of ten percent we saw earlier.  By now, hopefully you have been convinced that Monte Carlo is a wonderful method of solving problems. Although the examples we looked at were mostly simple, these algorithms can easily be applied to solve much harder ones. Simply put, Monte Carlo uses a brute force approach to simulate a particular instance of a model multiple times. Through such repeated sampling, we are able to gain a better understanding of the parameters underlying the issue at hand, no matter how complex.",0,0,0,0,0,1,0,0
Word2vec from Scratch,"In order to create word embeddings, we need some sort of data. Here is a text on machine learning from Wikipedia. I‚Äôve removed some parentheses and citation brackets to make things slightly easier. Since we can‚Äôt feed raw string texts into our model, we will need to preprocess this text. The first step, as is the approach taken in many NLP tasks, is to tokenize the text, i.e. splitting the text up into smaller units like words, getting rid of punctuations, and so on. Here is a function that does this trick using regular expressions. Let‚Äôs create tokens using the Wikipedia excerpt shown above. The returned object will be a list containing all the tokens in . Another useful operation is to create a map between tokens and indices, and vice versa. In a sense, we are creating a lookup table that allows us to easily convert from words to indices, and indices to words. This will be particularly useful later on when we perform operations such as one-hot encoding. Let‚Äôs check if the word-to-index and index-to-word maps have successfully been created.",0,0,0,1,0,0,1,0
Complex Fibonacci,"This is sort of tricky if you think about it: the normal two-dimensional plane as we know it can only represent a mapping from the -axis to the -axis‚Äîin other words, a transformation from one-dimensional space to another one-dimensional space. A three-dimensional -coordinate system, on the other hand, represents a transformation from a two-dimensional space, represented by  and , to another one-dimensional space, namely . We aren‚Äôt used to going to other way around, where a one-dimensional space is mapped to a two-dimensional space, as is the case here. A simple hack that nonetheless makes a lot of sense in this case is to use the real-number line for two purposes: representing the input dimension, namely the real number line, and one component of the output dimension‚Äîthe real number portion of the output to Binet‚Äôs formula. This admittedly results in a loss of information, since finding the point where  won‚Äôt give us the th fibonacci number; instead, it will only tell us what the fibonacci number is whose real number component equals . Nonetheless, this is an approach that makes sense since the real number line is a common dimension in both the input and output data.",1,0,0,0,0,0,0,0
Gaussian Mixture Models,"As we will see in this section, such a relationship can clearly be established, which is why EM is so commonly used in the context of Gaussian mixture models. Let‚Äôs begin by defining some quantities, the first being the posterior distribution. In other words, given some data, what is the probability that it will belong to a certain class? Using the definition of conditional probability, we can arrive at the following conclusion: This represents the probability that, given some point , the point belongs in the th cluster. With this result, we can now rewrite the MLE calculation that we were performing earlier. Using the new  notation, we can thus simplify the result down to We can then simplify this expression to derive an expression for . An important trick is here to use the fact that the covariance matrix is positive semi-definite. Therefore, the covariance matrix plays no role in driving the value down to zero. With some algebraic manipulations, we arrive at Let‚Äôs introduce another notational device to simplify the expresison even further. Let Recall that  was defined to be the posterior probability that a given point  belongs to the th cluster.",0,0,1,0,0,1,0,0
Demystifying Entropy (And More),"The other day, my friend and I were talking about our mutual friend Jeremy. ‚ÄúHe‚Äôs an oddball,‚Äù my friend Sean remarked, to which I agreed. Out of nowhere, Jeremy had just told us that he would not be coming back to Korea for the next three years. ‚ÄúHe is just about the most random person I know.‚Äù And both of us, being aspiring statistics majors, began wondering: is there a systematic way of measuring randomness? It is from here that we went down the rabbit hole of Google and Wikipedia search. I ended up landing on entropy land, which is going to be the topic for today‚Äôs post. It‚Äôs a random post on the topic of randomness. To begin our discussion of randomness, let‚Äôs take a look at how scientists measure the randomness involved in natural phenomena, such as particle movement. If you are a chemist or physicist, you might be familiar with the concept of entropy, one of the core elements of thermodynamics and a topic that recurs throughout many subfields of natural sciences.",0,0,0,0,0,1,0,0
"Basel, Zeta, and some more Euler","The more I continue my journey down the rabbit hole of mathematics, the more often I stumble across one name: Leonhard Euler. Nearly every concept that I learn, in one way or another, seems to be built on top of some strand of his work, not to mention the unending list of constants, formulas, and series that bears his name. It is simply mind-blowing to imagine that a single person could be so creative, inventive, and productive to the extent that the field of mathematics would not be where it is today had it not been for his birth on April 15, 1707. Why such intensive fanboying, you might ask. Well, let‚Äôs remind ourselves of the fact that the interpolation of the factorial through the Gamma function was spearheaded by Euler himself. But this is just the start of the beginning. Consider, for example, the Basel problem, an infamous problem that mathematicians have been trying to solve for nearly a century with no avail, until the 28-year-old Euler came to the rescue. The Basel problem can be stated as follows: At a glance, this seems like a fairly simple problem.",1,0,0,0,0,0,0,0
Understanding the  Leibniz Rule,"Before I begin, I must say that this video by Brian Storey at Olin College is the most intuitive explanation of the Leibniz rule I have seen so far. Granted, my greedy search over the internet space was by no means exhaustive, so I‚Äôve probably missed some other hidden gems  here and there. Also, the video is intended as a visual explanation for beginners rather than a robust analytical proof of the Leibniz rule. This point notwithstanding, I highly recommend that you check out the video. This post is going to provide a short, condensed summary of the proof presented in the video, minus the fancy visualization that pen and paper can afford. The Leibniz rule, sometimes referred to as Feynman‚Äôs rule or differentiation-under-the-integral-sign-rule, is an interesting, highly useful way of computing complicated integrals. A simple version of the Leibniz rule might be stated as follows: As you can see, what this rule essentially tells us is that integrals and derivatives are interchangeable under mild conditions. We‚Äôve used this rule many times in a previous post on Fisher‚Äôs information matrix when computing expected values that involved derivatives.",1,0,0,0,0,0,0,0
PyTorch Tensor Basics,"In this post, we took a look at some useful tensor manipulation operations and techniques. Although I do have some experience using Keras and TensorFlow, I never felt confident in my ability to deal with tensors, as that felt more low-level. PyTorch, on the other hand, provides a nice combination of high-level and low-level features. Tensor operation is definitely more on the low-level side, but I like this part of PyTorch because it forces me to think more about things like input and the model architecture. I will be posting a series of PyTorch notebooks in the coming days. I hope you‚Äôve enjoyed this post, and stay tuned for more!.",0,0,0,0,0,0,1,0
PyTorch Tensor Basics,"Without getting into too much technical detail, we can roughly understand view as being similar to  in that it is not an in-place operation. However, there are some notable differences. For example, this Stack Overflow post introduces an interesting example: On the other hand,  does not run into this error. The difference between the two functions is that, whereas  can only be used on contiguous tensors. This SO thread gives a nice explanation of what it means for tensors to be contiguous; the bottom line is that, some operations, such , do not create a completely new tensor, but returns a tensor that shares the data with the original tensor while having different index locations for each element. These tensors do not exist contiguously in memory. This is why calling  after a transpose operation raises an error. , on the other hand, does not have this contiguity requirement. This felt somewhat overly technical, and I doubt I will personally ever use  over , but I thought it is an interesting detail to take note of nonetheless. Another point of confusion for me was the fact that there appeared to be two different ways of initializing tensors:  and .",0,0,0,0,0,0,1,0
Scikit-learn Pipelines with Titanic,"In such cases, using this library to visualize where missing values occur is a good idea, as this is an additional dimension of information that calling  wouldn‚Äôt be able to reveal. Now that we have a rough sense of where missing values occur, we need to decide from one of few choices: Indeed, in this case, we will go ahead and drop the  attribute. This choice becomes more obvious when we compute the percentage of null values. This shows us that 77 percent of the rows have missing  attribute values. Given this information, it‚Äôs probably a bad idea to try and impute these values. We opt to drop it instead. Correlation (or somewhat equivalently, covariance) is a metric that we always care about, since ultimately the goal of ML engineering is to use a set of input features to generate a prediction. Given this context, we don‚Äôt want to feed our model useless information that lacks value; instead, we only want to feed into the model highly correlated, relevant, and informative features.",0,0,1,0,0,0,0,0
Dissecting LSTMs,"Taking a look at the second term, we note that the term is adding some value to the cell state that has been updated to forget information. It only makes sense, then, for the second term to perform the information update sequence. But to understand the second term, we need to take a look at two other equations: (2) is another forward pass involving concatenation, much like we saw in (1) with . The only difference is that, instead of forgetting,  is meant to simulate an update of the cell state. In some LSTM variants,  is simply replaced with , in which case the cell state update would be rewritten as However, we will stick to the convention that uses  instead of the simpler variant as shown in (4-2). The best way to think of  or  is a filter:  is a filter that determines which information to be updated and passed onto the cell state. Now all we need are the raw materials to pass into that filter. The raw material is , defined in (3).",0,0,0,0,0,0,1,0
Word2vec from Scratch,"‚Äù If the model was trained properly, the most likely word should understandably be ‚Äúmachine.‚Äù And indeed, when that is the result we get: notice that ‚Äúmachine‚Äù is at the top of the list of tokens, sorted by degree of affinity with ‚Äúlearning.‚Äù Building and training was fun and all, but our end goal was not to build a neural network; we wanted to get word embeddings. As stated earlier in this post, the key behind word embeddings is that the rows of the first weight matrix is effectively a dense representation of one-hot encoded vectors each corresponding to various tokens in the text dataset. In our example, therefore, the embedding can simply be obtained by But of course, this is not a user-friendly way of displaying the embeddings. In particular, what we want is to be able to input a word through a function and receive as output the embedding vector for that given word. Below is a function that implements this feature. When we test out the word ‚Äúmachine,‚Äù we get a dense ten-dimensional vector as expected.",0,0,0,1,0,0,1,0
The Gibbs Sampler,"The gist of the Gibbs sampler is simple: sample from known conditional distributions, and use that resulting value to sample the next random variable from the following conditional probability distribution, ad infinitum. But this is just a lot of words and some needless Latin for fun and flair, so let‚Äôs hash out what the sentence really means. Continuing on from our generic example, let‚Äôs say we sampled a value from the first conditional probability distribution. We will use a superscript and subscript notation to each denote the iteration and the sequence of random variable. Assume that we start from some random -dimensional vector to start with. Following our notation, this vector would be The superscripts are all 0 since this is the first ‚Äúsample‚Äù we will start off with. Theoretically, it doesn‚Äôt matter what these random numbers are‚Äîasymptotically speaking, we should still be able to approximate the final distribution, especially if given the fact that we take burn-in into account. On the first iteration, we will begin by sampling from the first probability distribution. Note that we simply used the initial random values for  through  to sample the first value from a conditional probability distribution.",0,0,0,0,0,1,0,1
"Beta, Bayes, and Multi-armed Bandits","The notion of conjugate priors is something that recurs extremely often in Bayesian analysis. This is mainly because conjugate priors offer a nice closed-form solution to posterior distribution computation. Simply put, a conjugate prior to some likelihood function is a type of prior that, when multiplied with a given likelihood, produces a posterior distribution that is of the same form as itself. For example, a Beta prior, when multiplied with a binomial likelihood, produces a Beta posterior. This is what we want in Bayesian analysis, since we can now assume that posterior to be our new prior in the next iteration of experiments. In this section, we will build on top of this high level understanding of conjugacy to show that a Beta distribution is indeed a conjugate prior to the binomial likelihood function. On that end, let‚Äôs start with a quick definition review of the Beta distribution and the Beta function. The Beta distribution looks as follows: where Comparing (3) and the integral representation of the Beta function in (4), you will see that there is a clear resemblance, and this is no coincidence.",0,1,0,0,0,1,0,0
Recommendation Algorithm with SVD,"On a tangential note, recently, I have begun to realize that linear algebra as a university subject is very different from linear algebra as a field of applied math. Although I found interest in linear algebra last year when I took the course as a first-year, studying math on my own has endowed me with a more holistic understanding of how the concepts and formulas we learned in linear algebra class can be used in real-life contexts. While I am no expert in pedagogy or teaching methodology, this makes me believe that perhaps linear algebra could be taught better if students were exposed to applications with appropriate data to toy around with. Just a passing thought. Anyhow, that‚Äôs enough blogging for today. Catch you up in the next one.",0,0,0,1,0,0,0,1
Bayesian Linear Regression,"MAP versus MLE is a recurring theme that appears throughout the paradigmatic shift from frequentist to Bayesian, so it merits discussion. Now that we have a posterior distribution for  which we can work with, it‚Äôs time to derive the predictive distribution. We go about this by marginalizing  using the property of conditional probability, as illustrated below. This may seem like a lot, but most of it was simple calculation and distributing vectors over parentheses. It‚Äôs time to use the power of conjugacy again to extract a normal distribution out of the equation soup. Let‚Äôs complete the square of the exponent according to the Gaussian form after making the appropriate substitutions Again, observing this is not a straightforward process, especially if we had no idea what the final distribution is going to look like. However, given that the resulting predictive posterior will take a Gaussian form, we can backtrack using this knowledge to obtain the appropriate substitution parameters in (13). Continuing, where the last equality stands because we can pull out terms unrelated to  by considering them as constants.",0,0,0,0,0,0,0,1
Wonders of Monte Carlo,"Let‚Äôs test the accuracy of this crude Monte Carlo method by using our example of  computed earlier. The result is a very poor approximation that is way off, most likely because we only used ten randomly generated numbers. Much like earlier, however, we would expect Monte Carlo to perform better with larger samples. The example we have analyzed so far was a very simple one, so simple that we would probably have been better off calculating the integral by hand than writing code. That‚Äôs why it‚Äôs time to put our crude Monte Carlo to the test with integrals more difficult to compute. Consider the following expression: One might try calculating this integral through substitution or integration by parts, but let‚Äôs choose not to for the sake of our mental health. Instead, we can model the integrand in Python and ask Monte Carlo to do the work for us. Concretely, this process might look as follows. We can now plug this function into our crude Monte Carlo  and hope that the algorithm will provide us with an accurate estimation of this expression.",0,0,0,0,0,1,0,0
"0.5!: Gamma Function, Distribution, and More","To generalize this formula by removing the unit time constraint, we can perform a rescaling on  to produce the following: where  denotes the probability that  events occur within time . Notice that setting  gives us the unit time version of the formula presented above. The link between the Poisson and Gamma distribution, then, is conveniently established by the fact that the time of the th arrival is lesser than  if more than  events happen within the time interval . This proposition can be expressed as an identity in the following form. Notice that the left-hand side is a cumulative distribution function of the Gamma distribution expressed in terms of . Given the derivative relationship between the CDF and PDF, we can obtain the probability distribution function of Gamma by deriving the right-hand side sigma expression with respect to .",0,1,0,0,0,1,0,0
A Simple Autocomplete Model,"As we know, neural networks are great at learning hidden patterns as long as we feed it with enough data. In this post, we will implement a very simple version of a generative deep neural network that can easily form the backbone of some character-based autocomplete algorithm. Let‚Äôs begin! Let‚Äôs first go ahead and import all dependencies for this tutorial. As always, we will be using the  functional API to build our neural network. We will be training our neural network to speak like the great German philosopher Friedrich Nietzsche (or his English translations, to be more exact). First, let‚Äôs build a function that retrieves the necessary  text file document from the web to return a Python string. Let‚Äôs take a look at the text data by examining its length. Just to make sure that the data has been loaded successfully, let‚Äôs take a look at the first 100 characters of the string. ## Preprocessing It‚Äôs time to preprocess the text data to make it feedable to our neural network. As introduced in this previous post on recurrent neural networks, the smart way to deal with text preprocessing is typically to use an embedding layer that translates words into vectors.",0,0,0,0,1,0,1,0
k-Nearest Neighbors Algorithm from Scratch,"Although this process is not necessary, it helps us interpret the results in terms of percentages. For example, the example below tells us that approximately 71 percent of the neighbors of  are labeled 0; 28 percent are labeled 1. Now we have all the building blocks we need. We can stop here, but let‚Äôs nicely wrap all the functions we have build into a single function that we can use to train and test data. In retrospect, we could have built a class instead, but this implementation also works fine, so let‚Äôs stick to it for now. Let‚Äôs see what the  tells us about . Here, we pass on  onto the  argument because we want to prevent the algorithm from making a prediction based on a data set that contains the data itself; that would defeat the purpose of making a prediction. Let‚Äôs see how the model performs. The KNN model rightly predicts that  is labeled 0. Great! But we have only been testing our model on a rather dumb data set. Let‚Äôs see whether the model works with larger, closer-to-real-life data sets.",0,0,1,1,0,0,0,0
Markov Chain and Chutes and Ladders,"This step is key to understanding Markov processes since the eigenvector of the stochastic matrix whose eigenvalue is 1 is the stationary distribution vector, which describes the Markov chain in a state of equilibrium. For an intuitive explanation of this concept, refer to this previous post. Let‚Äôs begin by using the  package to identify the eigenvalues and eigenvectors of the stochastic matrix. This code block produces the following output: The first entry of this array, which is the value , deserves our attention, as it is the eigenvalue which corresponds to the stationary distribution eigenvector. Since the index of this value is , we can identify its eigenvector as follows: Notice that this eigenvector is a representation of a situation in which the player is in the th cell of the game board! In other words, it is telling us that once the user reaches the th cell, they will stay on that cell even after more dice rolls‚Äîhence the stationary distribution. On one hand, this information is impractical given that a player who reaches the end goal will not continue the game to go beyond the th cell.",0,0,0,0,0,0,0,1
Traveling Salesman Problem with Genetic Algorithms,"Let‚Äôs perform a quick sanity check to see if  works as expected. Here, give vertices of a unit square as input to the function. While we‚Äôre at it, let‚Äôs also make sure that  indeed does create city coordinates as expected. Now, we‚Äôre finally ready to use these functions to randomly generate city coordinates and use the genetic algorithm to find the optimal path using  with the appropriate parameters. Let‚Äôs run the algorithm for a few iterations and plot its history.  We can see that the genetic algorithm does seems to be optimizing the path as we expect, since the distance metric seems to be decreasing throughout the iteration. Now, let‚Äôs actually try plotting the path along with the corresponding city coordinates. Here‚Äôs a helper function to print the optimal path. And calling this function, we obtain the following:  At a glance, it‚Äôs really difficult to see if this is indeed the optimal path, especially because the city coordinates were generated at random. I therefore decided to create a much more contrived example, but with many coordinates, so that we can easily verify whether the path decided on by the algorithm is indeed the optimal path.",0,0,0,1,0,0,0,0
Gamma and Zeta,"This means that , using which we can establish the following: With some algebraic implications, we end up with Dividing both sides by , we get All the magic happens when we cast a summation on the entire expression: Notice that now we have the Riemann zeta function on the left hand side. All we have to do is to clean up what is on the right. As it stands, the integral is not particularly tractable; however, we can swap the integral and the summation expression to make progress. I still haven‚Äôt figured out the details of when this swapping is possible, which has to do with absolute divergence, but I will be blogging about it in the future once I have a solid grasp of it, as promised before. The expression in the parentheses is just a simple sum of geometric series, which we know how to calculate. Therefore, we obtain To make this integral look more nicer into a form known as the Bose integral, let‚Äôs multiply both the numerator and the denominator by .",1,0,0,0,0,0,0,0
Wonders of Monte Carlo,"So we have a number! But how do we know if this is an accurate estimation? Unlike in previous problems, where we already knew the true value of our estimate and measured the error of our simulation by comparing it with the known value, the true value of the integral expression is unknown in this problem because we have not evaluated the integral by hand. One way we can go about this dilemma is to calculate variance. Intuitively, if our estimate is indeed accurate, running the same Monte Carlo simulation would yield a value very similar to that of the previous. Conversely, if our estimate is inaccurate, the variance would be large, suggesting that our estimate has not converged to a value yet. Indeed, this is exactly what we attempted to visualize with the error plot above in our  estimation example. However, in most cases where Monte Carlo methods are used, we have no idea about the true value of the quantity we wish to estimate, like the complicated integral problem in this case, which is why we cannot simply calculate error by substracting our estimate from the true value.",0,0,0,0,0,1,0,0
Recommendation Algorithm with SVD,"Luckily for us, the  module contains some excellent functionality to help us with singular value decomposition. Using this library, singular value decomposition can very simply be achieved with just a few lines of code. The parameters of the  function are , the ratings matrix, and , the number of non-trivial entries of  to select for dimensionality reduction, as we have seen earlier. More technically speaking,  corresponds to the number of ‚Äúconcepts‚Äù or dimensions that we will extract from the matrix. Let‚Äôs see what this means by actually running this function. Great! This is what dimensionality reduction means in the loosest sense. Instead of having 5 entries each row, as we had with the original ratings matrix , we now have 3 entries per row. In other words, the information on users has been compressed into three dimensions. Unlike in , where each column corresponded to some movie, we don‚Äôt really know what the columns of  stand for. It might be some genre, actress, or any hidden patterns in the data set that we are not aware of. Regardless, what‚Äôs important here is that we can now understand data more easily in smaller dimensions.",0,0,0,1,0,0,0,1
Principal Component Analysis,"Principal component analysis is one of those techniques that I‚Äôve always heard about somewhere, but didn‚Äôt have a chance to really dive into. PCA would come up in papers on GANs, tutorials on unsupervised machine learning, and of course, math textbooks, whether it be on statistics or linear algebra. I decided that it‚Äôs about time that I devote a post to this topic, especially since I promised one after writing about  on this blog some time ago. So here it goes. What do we need principal component analysis for? Or more importantly, what is a principal component to begin with? Well, to cut to the chase, PCA is a way of implementing dimensionality reduction, often referred to as lossy compression. This simply means that we want to transform some data living in high dimensional space into lower dimensions. Imagine having a data with hundreds of thousands of feature columns. It would take a lot of computing power to apply a machine learning model to fit the data and generate predictions.",0,0,0,0,0,1,0,1
Principal Component Analysis,"If you had prior exposure to PCA, you might know that the standard way of obtaining principal components is by calculating the covariance matrix of the data and finding its eigenvectors. Here, I attempt to present an explanation of how and why the procedure outlined in the preceding section is essentially achieving the same tasks, albeit through a different frame of thought. The unbiased sample covariance matrix is given by Of course, this is operating under the assumption that  has already been standardized such that the mean of the data is zero. You might be thinking that the formulation in (14) looks different from the one introduced previously on this post on SVD. In that particular post, I stated that covariance could be calculated as (14) and (15) certainly look different. However, under the hood, they express the same quantity. So in a nutshell, the conclusion we arrived at in the preceding section with the minimization of residual sums ultimately amounts to finding the covariance matrix and its eigenvectors.",0,0,0,0,0,1,0,1
PyTorch RNN from Scratch,"In the coming posts, we will be looking at sequence-to-sequence models, or seq2seq for short. Ever since I heard about seq2seq, I was fascinated by tthe power of transforming one form of data to another. Although these models cannot be realistically trained on a CPU given the constraints of my local machine, I think implementing them themselves will be an exciting challenge. Catch you up in the next one!.",0,0,0,1,0,0,1,0
Word2vec from Scratch,"So let‚Äôs summarize the entire process a little bit. First, embeddings are simply the rows of the first weight matrix, denoted as . Through training and backpropgation, we adjust the weights of , along with the weight matrix in the second layer, denoted as , using cross entropy loss. Overall, our model takes on the following structure: where  is the matrix contains the prediction probability vectors. With this in mind, let‚Äôs actual start building and train our model. Let‚Äôs start implement this model in code. The implementation we took here is extremely similar to the approach we took in this post. For an in-depth review of backpropagation derivation with matrix calculus, I highly recommend that you check out the linked post. The representation we will use for the model is a Python dictionary, whose values are the weight matrices and keys, the name with which we will refer to the weight matrices. In accordance with the nomenclature established earlier, we stick with  and  to refer to these weights. Let‚Äôs specify our model to create ten-dimensional embeddings. In other words, each token will be represented as vectors living in ten-dimensional space.",0,0,0,1,0,0,1,0
Wonders of Monte Carlo,"Although the rate of convergence dramatically decreases after the first few iterations, the pattern of convergence is apparrent. So how does this work? The mechanism is extremely simple: if we were to randomly generate an infinite number of dots, the proportion of the number of dots that fall within the circle versus those that do not fall within it would converge to some constant, i.e. . Why is this the case? Intuitively, the larger the area, the larger the number of points that fall into that area. Given this proportional relationship, the number of randomly generated points in an area after a simulation would mirror the actual area of the circle and the rectangle, hence the proportional expression above. By following this line of reasoning, we can then resort to Monte Carlo to generate these random points, after which we can make a reasonable estimation of . But approximation is not the only domain in which Monte Carlo methods become useful‚Äìthey can also be used to calculate complicated integrals. We all know from calculus class that integration can be difficult.",0,0,0,0,0,1,0,0
Gaussian Process Regression,"This can easily be shown by comparing the LDU decomposition of  and  respectively: Therefore, we can rewrite the LDU decomposition of A as A nice property of diagonal matrices is that we can easily identify its square, namely, where  is a matrix whose diagonal entries are each square root of the corresponding originals in . The tranpose is not necessary since  is a diagonal matrix, but we do so for convenience purposes later on in the derivation. Note the trivial case of the identity matrix, whose square root is equal to itself since all diagonal elements take the value of 1 (and ). Given this piece of information, what we can now do is to rewrite the factorization of  as where . This is the Cholesky decomposition of symmetric matrices‚Äîto be more exact, positive semi-definite matrices. The reason why the Cholesky decomposition can only be performed on positive semi-definite matrices becomes apparent when we think about the definition of positive semi-definiteness. Given any non-zero vector , The key takeaway is that, given some positive semi-definite matrix, we can easily factor it into what we might consider to be its square root in the form of .",0,0,1,1,0,0,0,0
A Step Up with  Variational Autoencoders,"Due to its bow-like shape, the expected value of a convex function evaluated across a given interval will always be greater or equal to the function evaluated at the expected value of the random variable.  How is Jensen‚Äôs inequality related to the non-negativity of KL divergence? Let‚Äôs return back to the definition of KL divergence. For simplicity and to reduce notational burden, we briefly depart from conditional probabilities  and return back to generic distributions  and . Notice that the definition of KL divergence itself is an expected value expression. Also, note that  is a convex function‚Äî itself is concave, but the negative sign flips the concavity the other way. With these observations in mind, we can apply Jensen‚Äôs inequality to derive the following: Therefore, we have shown that KL divergence is always greater or equal to zero, which was our end goal. There is another version of a proof that I found a lot more intuitive and easier to follow than the previous approach. This derivation was borrowed from this post. We start from the simple observation that a logarithmic function is always smaller than  a linear one.",0,0,0,0,1,0,1,0
Markov Chain and Chutes and Ladders,"But an issue with using expected value as a metric of analysis is that long games with infinitesimal probabilities are weighted equally to short games of substantial probability of occurrence. This mistreatment can be corrected for by other ways of understanding the distribution, such as median: This function tries to find the point in the cumulative distribution where the value is closest to , i.e. the median of the distribution. The result tells us that about fifty percent of the games end after 29 turns. Notice that this number is smaller than  because it discredits more of the long games with small probabilities. The Markov chain represents an in interesting way to analyze systems that are memoryless, such as the one in today‚Äôs post, the Chutes and Ladders game. Although it is a simple game, it is fascinating to see just how much information and data can be derived from a simple image of the game board. In a future post, we present another way to approach similar systems, known as Monte Carlo simulations. But that‚Äôs for another time. Peace!.",0,0,0,0,0,0,0,1
Recommendation Algorithm with SVD,"Simply put, if users have similar movie preferences, the points representing the two users will appear to be close when plotted on a graph. Let‚Äôs see what this means by plotting  using . This can be achieved with the following code. We can pass  as an argument for the  function to see a three-dimensional plot of users‚Äô movie preferences, as shown below.  Note that the points corresponding to User 6 and User 8 exactly overlap, which is why the points look darker despite being positioned near the corner of the plot. This is also why we can only count seven points in total despite having plotted eight data points. In short, this visualization shows how we might be able to use distance calculation to give movie recommendations to a new user. Assume, for instance, that we get a new suscriber to our movie application. If we can plot  onto the space above, we will be able to see to whom User 10‚Äôs preference is most similar. This comparison is useful since User 10 will most likely like the movie that the other use also rated highly. We can also create a similar plot for movies instead of users.",0,0,0,1,0,0,0,1
"PyTorch, From Data to Modeling","In the initialization function, we also define a number of layers that will be used in forward propagation. You might be wondering why these have to initialized in the initialization function, as opposed to the forward function itself. While I don‚Äôt have a complete, technically cogent explanation to that question, intuitively, we can understand a model‚Äôs layers as being components of the model itself. After all, the weights of these layers are adjusted with each iteration or epoch. In that sense, we want the layers to be attached to the model instance itself; hence the OOP design of PyTorch‚Äôs model class. In this particular instance, we define a number of convolutional layers, a pooling layer, and two fully connected layers used for classification output. The declaration of the layers themselves are not too different from other frameworks, such as TensorFlow. Also, I‚Äôve written out all the named arguments so that it is immediately clear what each argument is configuring. Once we‚Äôve declared all the necessary components in the initialization function, the next steps to actually churn out forward propagation results given some input. In PyTorch, this is done by defining the  function.",0,0,0,0,0,0,1,0
Complex Fibonacci,"Nowadays, I‚Äôm reminded of just how many things that I thought I knew well‚Äîlike the fibonacci sequence‚Äîare rife with things to study and rejoice in wonder. More so than the value of understanding something brand new, perhaps the value of intellectual exploration lies in realizing just how ignorant one is, as ironic as it sounds. I didn‚Äôt want to end on such a philosophical note, but things have already precipitated contrary to my intentions. Anyhow, I hope you‚Äôve enjoyed reading this post. Catch you up in the next one.",1,0,0,0,0,0,0,0
"Beta, Bayes, and Multi-armed Bandits","In the Beta distribution, the Beta function which appears in the denominator is simply a normalizing constant that ensures that integrating the probability distribution function from 0 to 1 produces 1. The specifics of the Beta function and how it relates to the Gamma function might be a topic for a another post. For now, it suffices to show the general probability function as well as its building blocks, namely the Beta function itself. These will become relevant later when we derive the posterior with a binomial likelihood function to show conjugacy. To prove conjugacy, it suffices to show that multiplying a Beta prior with a binomial likelihood produces a Beta posterior. Roughly speaking, the big picture we will be using is essentially equation (1). For notation consistency, I decided to use  for conditional probability and  for function parametrization. For a rather pedantic discussion on this notational convention, I recommend that you take a quick look at this cross validated post. Let‚Äôs now plug in the Beta and binomial distributions into (5) to see what comes out of it.",0,1,0,0,0,1,0,0
"Newton-Raphson, Secant, and More","Some tweaks have been made to the formula for use in the section that follows, but at its core, it‚Äôs clear that the function uses the approximation logic we‚Äôve discussed so far. Another variant of the forward and backward divided difference formula is the center divided difference. By now, you might have some intuition as to what this formula is‚Äîas the name implies, we somehow use the center point, instead of going forward or backwards from the point of approximation. Here is the formula: Heuristically, this formula also makes sense. We can imagine going both a step forward and backward, then dividing the results by the total of two steps we‚Äôve taken, one in each direction. Shown below is the Python implementation of the center divided difference formula. According to conventional mathematical wisdom, the center divided method normally provides a more robust way of approximating first order derivatives. In this subsection, we discuss why this is the case. Using Taylor expansion, we can approximate the value of  as follows, given that  goes to 0 under the limit. Notice that we can manipulate (4) to derive the forward divided difference equation in (1).",1,0,0,0,0,0,0,0
Complex Fibonacci,"Therefore, we simply verify equivalence by comparing their magnitude with an arbitrarily small number, . The takeaway from the code snippet is that holds, regardless of whether or not  is a non-negative integer. Indeed, Binet‚Äôs formula gives us what we might refer to as the interpolation of the fibonacci sequence, in this case extended along the real number line. A corollary of the real number interpolation of the fibonacci sequence via Binet‚Äôs formula is that now we can effectively plot the complex fibonacci numbers on the Cartesian plane. Because  can be continuous, we would expect some graph to appear, where the -axis represents real numbers, and , the imaginary. This requires a bit of a hack though; note that the result of Binet‚Äôs formula is a complex number, or a two-dimensional data point. The input to the function is just a one-dimensional real number. Therefore, we need a way of representing a map from a one-dimensional real number line to a two-dimensional complex plane.",1,0,0,0,0,0,0,0
Naive Bayes Model From Scratch,"We are almost done! All that we have to do is to create a funcition that returns the predicted label of a testing instance given some labeled training data. Implemenitng this process is straightforward since we have all the Bayesian ingredients we need, namely the prior and the likelihood. The last step is to connect the dots with Bayes‚Äô theorem by calculating the product of the prior and likelihood for each class, then return the class label with the largest posterior, as illustrated below. Let‚Äôs see if the  works as expected by seeing if passing as argument , for which we know that its label is 1, actually returns 1. The function is only able to process a single testing instance. Let‚Äôs complete our model construction by writing the  function that takes labeled data and a testing set as its argument to return a  array containing the predicted class labels for each instance in the testing set. Done! Let‚Äôs import some data from the  library. The wine set data is a classic multi-class classfication data set.",0,0,1,1,0,0,0,0
Building Neural Network From Scratch,"This process of deriving an output from an input using a neural network is known as forward propagation. Forward propagation is great and all, but without appropriately trained weights, our model is obviously going to spit out meaningless predictions. The way to go about this is to use the gradient descent algorithm with back propagation. We will discuss more about back propagation in the next subsection, as it is a meaty topic that deserves space of its own. We deal primarily with the former in this section. This is not the first time that we have come across gradient descent on this blog. In fact, we used gradient descent to optimizse our logistic regression model in this post. Recall that the gradient descent algorithm can be summarized as where  represents the parameters, or weights,  represents the learning rate, and  represents the loss function. This is the vanilla gradient descent algorithm, which is also referred to as batch gradient descent. Minibatch gradient descent is similar to gradient descent. The only point of difference is that it calculates the gradient for each minibatch instead of doing so for the entire dataset as does batch gradient descent.",0,0,0,1,0,0,1,1
A Simple Autocomplete Model,"6: is a woman‚Äìwhat then? is there not ground 
for suspecting to and the world will had to a such that the basis of the incussions of the spirit as the does not because actian free spirits of intellect of the commstical purtious expression of men are so much he is not unnor experiences of self-conturity, and 
as anifegently religious in the man would not consciously, his action is not be actian at in accombs life for the such all procees of great and the heart of this conduct the spirity of the man can provate for in any 
once in any of the suriticular conduct that which own needs, when they are therefore, as 
such action and some difficulty that the strength, it, himself which has to its fine term of pricismans the exacte in its self-recuphing and every strength and man to wist the action something man as the worst, that the was of a longent that the whole not be all the very subjectical proves the stronger extent he is necessary to metaphysical figure of the faith in the bolity in the pure belief‚Äìas ‚Äúthe such a successes of the values‚Äìthat is he ‚Äã Generated text at temperature 0.9: is a woman‚Äìwhat then? is there not ground 
for suspecting that they grasutes, and so farmeduition of the does not only with this 
constrbicapity have honour‚Äìand who distical seclles are denie‚Äôn, is one samiles are no luttrainess, 
and ethic and matficulty, concudes of morality to 
rost were presence of lighters caseful has prescally here at last not and servicatity, leads falled for child real appreparetess of worths‚Äìthe 
resticians when one to persans as a what a mean of that is as to the same heart tending noble stimptically and particious, we pach yought for that mankind, that the same take frights a contrady has howevers of a surplurating or in fact a sort, without present superite fimatical matterm of our being interlunally men who cal 
scornce. the shrinking‚Äôs 
proglish, and traints he way to demitable pure explised and place can 
deterely by the compulse in whom is phypociative cinceous, and the higher and will bounthen‚Äìin itsiluariant upon find the ‚Äúfirst the whore we man will simple condection and some than us‚Äìa valuasly refiges who feel Generated text at temperature 1.2: is a woman‚Äìwhat then? is there not ground 
for suspecting that he therefore when shre, mun, a schopenhehtor abold gevert. 120 =as in 
find that is _know believinally bad,[
euser of view.‚Äìbithic 
iftel canly 
in any 
knowitumentially. the charm surpose again, in 
swret feathryst, form of kinne of the world bejud‚Äìage‚Äìimplaasoun ever? but that the is any 
appearance has clenge: the? a plexable gen preducl=s than condugebleines and aligh to advirenta-nasure; 
findiminal it as, not take. the ideved towards upavanizing, would be 
thenion, in all pespres: it is of 
a concidenary, which, well founly con-utbacte udwerlly upon mansing‚Äìfrauble of ‚Äúarrey been can the pritarnated from their 
christian often‚Äìthink prestation of mocives.‚Äù legt, lenge:‚Äìthis deps 
telows, plenhance of decessaticrances). hyrk an interlusally‚Äù tone‚Äìunder good haggy,‚Äù 
is have we leamness of conschous should it, of 
sicking ummenfeckinal zerturm erienweron of noble of 
himself-clonizing there is conctumendable prefersy 
exaitunia states,‚Äù whether 
they deve oves any of hispyssesss. int The results are fascinating. Granted, our model is still bad at immitating Nietzsche‚Äôs style of writing, but I think the performance is impressive given that this was a character-based text generation model. Think about it for a second: to write even a single word, say ‚Äúpresent,‚Äù the model has to correctly predict ‚Äúp‚Äù, ‚Äúr‚Äù, ‚Äúe‚Äù, ‚Äús‚Äù, ‚Äúe‚Äù, ‚Äún‚Äù,  and ‚Äút,‚Äù all in tandem. Imagine doing this for extended cycles, long enough to generate text that is comfortably a paragraph long. It‚Äôs amazing how the text it generates even makes some sense at all. Then, as temperature rises, we see more randomness and ‚Äúcreativity‚Äù at work. We start to see more words that aren‚Äôt really words (the one I personally like is ‚Äúfarmeduition‚Äù‚Äîit sounds like it could be either some hard, obscure word that no one knows, or a failed jumble of ‚Äúfarm,‚Äù ‚Äúeducation,‚Äù and ‚Äúintuition‚Äù). At temperature 1.2, the model is basically going crazy with randomness, adding white spaces where there shouldn‚Äôt be and sounding more and more like a speaker of Old English or German, something that one might expect to see in English scripts written in pre-Shakesperean times. At any rate, it is simply fascinating to see how a neural network can be trained to immitate some style of writing. Hopefully this tutorial gave you some intuition of how autocomplete works, although I presume business-grade autocomplete functions on our phones are based on much more complicated algorithms. Thanks for reading this post. In the next post, we might look at another example of a generative model known as generative adversarial networks, or GAN for short. This is a burgeoning field in deep learning with a lot of prospect and attention, so I‚Äôm already excited to put out that post once it‚Äôs done. See you in the next post. Peace!.",0,0,0,0,1,0,1,0
A Simple Autocomplete Model,"One of the objectives of this tutorial was to demonstrate the fun we can have with generative models, namely neural networks that can be used to generate data themselves, not just classify or predict data points. To put this into perspective, let‚Äôs compare the objectives of a generative model with that of a discriminative model. Simply put, the goal of a discriminative model is to model and calculate where  is a label and  is some input vector. As you can see, discriminative models arise most commonly from the context of supervised machine learning, such as regression or classification. In contrast, the goal of a generative model is to approximate the distribution which we might construe to be the probability of observing evidence or data. By modeling this distribution, the goal is that we might be able to generate samples that appear to have been sampled from this distribution. In other words, we want our model to generate likely data points based on an approximation of the true distribution from which these observations came from.",0,0,0,0,1,0,1,0
Riemann Zeta and Prime Numbers,"The other day, I came across an interesting article by Chris Henson on the relationship between the Riemann Zeta function and prime numbers. After encountering a similar post on the math Stack Exchange, I thought I‚Äôd write an article on the same topic as well, perhaps as sort of a prologue to a previous article posted on this blog, Basel, Zeta, and some more Euler. The code introduced in this blog are adaptations of those written by Chris Henson, so all credits go to the original author. With that said, let‚Äôs dive right into it. The Riemann Zeta function is perhaps one of the most deeply studied functions in all of modern mathematics. It is sometimes also referred to as the Euler-Riemann Zeta function, but I will not adhere to this convention not only because it is needlessly long and cumbersome, but also because Euler already has so many constants and functions and theorems bearing his name. Anyhow, the Riemann Zeta function looks as follows: While there are so many layers to explore with this function, one relatively simple and interesting route is to factorize this function.",1,0,0,0,0,0,0,0
Bayesian Linear Regression,"In today‚Äôs post, we will take a look at Bayesian linear regression. Both Bayes and linear regression should be familiar names, as we have dealt with these two topics on this blog before. The Bayesian linear regression method is a type of linear regression approach that borrows heavily from Bayesian principles. The biggest difference between what we might call the vanilla linear regression method and the Bayesian approach is that the latter provides a probability distribution instead of a point estimate. In other words, it allows us to reflect uncertainty in our estimate, which is an additional dimension of information that can be useful in many situations. By now, hopefully you are fully convinced that Bayesian linear regression is worthy of our intellectual exploration. Let‚Äôs take a deep dive into Bayesian linear regression, then see how it works out in code using the  library. In this section, we will derive the formula for Bayesian linear regression step-by-step. If you are feeling rusty on linear algebra or Bayesian analysis, I recommend that you go take a quick review of these concepts before proceeding. Note that I borrowed heavily from this video for reference.",0,0,0,0,0,0,0,1
MLE and KL Divergence,"Now that we have reviewed the essential concepts that we need, let‚Äôs get down to the proof. Let‚Äôs start with the statement of the parameter  that minimizes the KL divergence between the two distribution  and the approximate distribution : Not a lot has happened in this step, except for substituting the  expression with its definition as per (2). Observe that in the last derived expression in (4), the term  does not affect the argument of the minima, which is why it can safely be omitted to yield the following simplified expression: We can change the argument of the minima operator to the maxima given the negative sign in the expression for the expected value. To proceed further, it is necessary to resort to the Law of Large Numbers, or LLN for short. The law states that the average of samples obtained from a large number of repeated trials should be close to the expected value of that random variable. In other words, the average will approximate the expected value as more trials are performed. More formally, LLN might be stated in the following fashion. Suppose we perform an experiment involving the random variable  and repeat it  times.",0,0,0,0,0,1,0,0
Fourier Series,"First, it integrates to one if the domain includes . This is the point where the graph peaks in the diagram. Second, the delta function is even. This automatically tells us that when we perform a Fourier expansion, we will have no sine functions‚Äîsine functions are by nature odd. With this understanding in mind, let‚Äôs derive the Fourier series of the Dirac delta by starting with . The equality is due to the first property of the delta function outlined in the previous paragraph. The derivation of the rest of the constants can be done in a similar fashion. The trick is to use the fact that the delta function is zero in all domains but . Therefore, the oscillations of  will be nullified by the delta function in all but that one point, where  is just one. Therefore, (13) simply reduces to integrating the delta function itself, which is also one! To sum up, we have the following: I find it fascinating to see how a function so singular and unusual as the Dirac delta can be reduced to a summation of cosines, which are curvy, oscillating harmonics.",1,0,0,0,0,1,0,0
Introduction to tf-idf,"The definition of a tf score might be thus expressed as where the denominator denotes the count of all occurrences of the term  in document , and the numerator represents the total number of terms in the document. Roughly speaking, inverse document frequency is simply the reciprocal of document frequency. Therefore, it suffices to show what document frequency is, since idf would immediately follow from df. Before getting into the formula, I think it‚Äôs instructive to consider the motivation behind tf-idf, and in particular what role idf plays in the final score. The motivation behind tf-idf commences from a simple question: how do we determine the semantic importance of a word in a set of documents? On one hand, words the appear a lot are probably worth paying attention to. For example, in one of my posts on Gaussian distributions, the word ‚ÄúGaussian‚Äù probably appears many times throughout the post. A keyword probably appears frequently in the document; hence the need to calculate tf. On the other hand, there might be words that appear a lot, but aren‚Äôt really that important at all. For example, consider the word ‚Äúdenote.",0,0,0,1,0,0,0,0
Gaussian Mixture Models,"We‚Äôve discussed Gaussians a few times on this blog. In particular, recently we explored Gaussian process regression, which is personally a post I really enjoyed writing because I learned so much while studying and writing about it. Today, we will continue our exploration of the Gaussian world with yet another machine learning model that bears the name of Gauss: Gaussian mixture models. After watching yet another inspiring video by mathematicalmonk on YouTube, I meant to write about Gaussian mixture models for quite some time, and finally here it is. I would also like to thank ritvikmath for a great beginner-friendly explanation on GMMs and Expectation Maximization, as well as fiveMinuteStats for a wonderful exposition on the intuition behind the EM algorithm. Without further ado, let‚Äôs jump right into it. The motivating idea behind GMMs is that we can model seemingly complicated distributions as a convex combination of Gaussians each defined by different parameters. One visual analogy I found particularly useful is imagining Gaussians as some sort of hill or mountain on a contour map. If we have multiple hills adjacent to one another, we can essentially model the topography of the region as a combination of Gaussians.",0,0,1,0,0,1,0,0
"PyTorch, From Data to Modeling","Indeed, plotting  makes it clear that the loss has been decreasing, though not entirely in a steady, monotonous fashion.  In retrospect, we could have probably added a batch normalization layer to stabilize and expedite training. However, since this post is largely meant as an introduction to PyTorch modeling, not model optimization or design, the example suffices. Now we have finally reached the last step of the development cycle: testing and evaluating the model. This last step will also require us to write a custom loop as we receive batches of data from the  object we‚Äôve created above. The good news, however, is that the testing loop is not going to look too much different from the training loop; the only difference will be that we will not be backpropagating per each iteration. We will also be using  to make sure that the model is in the evaluation mode, not its default training mode. This ensures that things like batch normalization and dropout work correctly. Let‚Äôs talk briefly about the details of this loop. Here, the metric we‚Äôre collecting is accuracy. First, we generally see how many correct predictions the model generates.",0,0,0,0,0,0,1,0
Gaussian Process Regression,"What does this ‚Äúsmoothness‚Äù mean in terms of covariance? The answer is that  values that are close to each other must be highly correlated, whereas those that are far apart would have low covariance. In other words, knowing the value of  tells us a lot about the value of , whereas it tells us very little about the value of . In short, the closer the values, the higher the covariance. So at the end of the day, all there is to GP regression is to construct this covariance matrix using some distance function. In GPs, these covariance matrices are referred to as kernels. Kernels can be understood as some sort of prior that we impose upon the regression problem. The idea of smoothness noted earlier is one such example of a prior. But it is a general prior that makes a lot of sense, since we normally don‚Äôt want stepwise or non-differential functions as the result of regression. But there are hundreds and thousands of kernels out there that each suit different purposes.",0,0,1,1,0,0,0,0
"Linear Regression, in Two Ways","Answering this question requires a bit more math beyond what we have covered here, but to provide a short preview, it turns out that our error function, defined as  is a positive definite matrix, which guarantees that the critical point we find by calculating the gradient gives us a minimum instead of a maximum. This statement might sometimes be phrased differently along the lines of convexity, but this topic is better tabled for a separate future post. The key point here is that setting the gradient to zero would tell us when the error is minimized. This is equivalent to Therefore, Now we are done! Just like in the previous section,  gives us the parameters for our line of best fit, which is the solution to the linear regression problem. In fact, the keen reader might have already noted that (7) is letter-by-letter identical to formula (2) we derived in the previous section using plain old linear algebra! One the one hand, it just seems surprising and fascinating to see how we end up in the same place despite having taken two disparate approaches to the linear regression problem.",0,0,0,0,0,0,0,1
A Step Up with  Variational Autoencoders,", as you might expect, simply refers to KL divergence. Notice how there is a  multiplying factor in the  expression, just like we did when we derived it in the section above. With some keen observations and comparisons, you will easily see that the code is merely a transcription of (13), with some minor differences given dimensionality. One important fact to note is that the gradient descent algorithm, by default, seeks to minimize the loss function. However, we discussed above how the objective of VAE is to maximize ELBO. Therefore, we modify ELBO into a loss function that is to be minimized by defining the loss function as the negative of ELBO. In other words, the cost function  is defined as ; hence the difference in sign. It‚Äôs finally time to test the model. Let‚Äôs first begin with data preparation and preprocessing. Now, we should have the training and test set ready to be fed into our network. Next, let‚Äôs define a simple callback application using the  monitor so that training can be stopped when no substantial improvements are being made to our model.",0,0,0,0,1,0,1,0
A sneak peek at Bayesian Inference,"If we temporarily disregard the constants that show up in (6), we can conveniently trim down the equation for Bayesian inference as follows: This idea is not totally alien to us‚Äîindeed, this is precisely the insight we gleaned from the example of the potential patient. This statement is also highly intuitive as well. The posterior probability would be some mix of our initial belief, expressed as a prior, and the data newly presented, the likelihood. Bayesian inference, then, can be understood as a procedure for incorporating prior beliefs with evidence in order to derive an updated posterior. What makes Bayesian inference such a powerful technique is that the derived posterior can themselves be used as a prior for subsequent inference conducted with new data. To see Bayesian inference in action, let‚Äôs dive into the most classic, beaten-to-death yet nonetheless useful example in probability and statistics: the coin flip. This example was borrowed from the following post. Assume that we have a coin whose fairness is unknown.",0,1,0,0,0,1,0,0
GAN in PyTorch,"Next, we create a dataset object and a data loader that batches and shuffles post-transformation images for us. You might recall that GANs are composed of two different networks: a discriminator and a generator. The discriminator, also metaphorically referred to as the police, is a classier that tries to determine whether a given data is real or fake, i.e. produced by the generator network. The generator, on the other hand, tries to generate images that are as realistic as possible, and so is referred to as the counterfeiter. Below is our generator network. Although we could have created DCGANs, or deep convolutional adversarial networks, let‚Äôs go simple here and just use fully connected layers. Notice that I‚Äôve used the  PyTorch API instead of using class-based models. In this particular instance, we won‚Äôt have a complicated forward method, so the sequential API will suffice. Next, we create the generator. It is also a sequential model, inside of which are stacks of linear layers with ReLU activations. Before we jump into training, let‚Äôs move these networks to the  object we‚Äôve created earlier. The interesting part starts here. Notice that we have different optimizers for the discriminator and the generator.",0,0,0,0,0,0,1,0
The Gibbs Sampler,"For this reason, the bivariate Gaussian distribution is a sensible choice. For this post, I‚Äôll be using , which is a data visualization library built on top of . I‚Äôll simply be using  to display a bivariate Gaussian. For reproducibility‚Äôs sake, we will also set a random seed. The code for the Gibbs sampler is simple, partially because the distribution we are dealing with is a bivariate Gaussian, not some high-dimensional intractable distribution. This point notwithstanding, the  function shows the gist of how Gibbs sampling works. Here, we pass in parameters for the conditional distribution, and start sampling given an initial  value corresponding to . As stated earlier, this random value can be chosen arbitrarily. Of course, if we start from a value that is way off, it will take much longer for the algorithm to converge, i.e. we will have to discard a large portion of initially sampled values. This is known as burn-in. In this case, however, we will apply a quick hack and start from a plausible value to begin with, reducing the need for burn-in.",0,0,0,0,0,1,0,1
"0.5!: Gamma Function, Distribution, and More","From there, we were able to derive and develop an intuition for the Gamma distribution, which models the waiting time required until the occurrence of the th event in a Poisson process. This may all sound very abstract because of the theoretical nature of our discussion. So in the posts to follow, we will explore how these distributions can be applied in different contexts. Specifically, we will take a look at Bayesian statistics and inference to demonstrate how distributions can be employed to express prior or posterior probabilities. At the same time, we will also continue our exploration of the distribution world by diving deeper into other probability distributions, such as but not limited to exponential, chi-square, normal, and beta distributions, in no specific order. At the end of the journey, we will see how these distributions are all beautifully interrelated. Catch you up in the next one!.",0,1,0,0,0,1,0,0
Dissecting LSTMs,"In other words, they are able to somewhat mimic the function of the brain, which involves both long and short-term memory. The structure of an LSTM cell might be summarized as follows: Note that  represents the Hadamard Product, which is nothing more than just the element-wise multiplication of matrices. This long list of equations surely looks like a lot, but each of them has a specific purpose. Let‚Äôs take a look. The first component of LSTM is the forget gate. This corresponds to these two set of equations: (1) is nothing more than just the good old forward pass. We concatenate  and , then multiply it with some weights, add a bias, and apply a sigmoid activation. At this point, you might be wondering why we use a sigmoid activation instead of something like ReLU. The reason behind this choice of activation function becomes apparent once we look at (4), which is how LSTM imitates forgetting. For now, we will only focus on the first term in (4). Recall that the output of a sigmoid activation is between 0 and 1. Say the output of applying a sigmoid activation results in some value that is very close to 0.",0,0,0,0,0,0,1,0
Fisher Score and Information,"Concretely, this means that given a true parameter , This might seem deceptively obvious: after all, the whole point of Fisher‚Äôs score and maximum likelihood estimation is to find a parameter value that would set the gradient equal to zero. This is exactly what I had thought, but there are subtle intricacies taking place here that deserves our attention. So let‚Äôs hash out exactly why the expectation of the score with respect to the true underlying distribution is zero. To begin, let‚Äôs write out the full expression of the expectation in integral form. If we evaluate this integral at the true parameter, i.e. when , The key part of this derivation is the use of the Leibniz rule, or sometimes known as Feynman‚Äôs technique or differentiation under the integral sign. I am most definitely going to write a post detailing in intuitive explanation behind why this operation makes sense in the future, but to prevent unnecessary divergence, for now it suffices to use that rule to show that the expected value of Fisher‚Äôs score is zero at the true parameter.",0,0,0,0,0,1,0,0
An Introduction to Markov Chain Monte Carlo,"However, this raises yet another question: what if the prior and likelihood do not have a conjugate relationship? What if we have a very messy prior or complicated likelihood function, so convoluted to the point that calculating the posterior is near impossible? Simple Monte Carlo approximation might not do because of a problem called the curse of dimensionality: the volume of the sample space increases exponentially with the number of dimensions. In high dimensions, the brute force Monte Carlo approach may not be the most appropriate. Markov Chain Monte Carlo seeks to solve this conundrum of posterior derivation in high dimensions sample space. And indeed, it does a pretty good job of solving it. How does Markov Chain Monte Carlo get around the problem outlined above? To see this, we need to understand the two components that comprise Markov Chain Monte Carlo: Markov chains and Monte Carlo methods. We covered the topic of Markov chains on two posts, one on PageRank and the other on the game of chutes and ladders. Nonetheless, some recap would be of help.",0,0,0,1,0,0,0,0
The Gibbs Sampler,"To cut to the chase, we end up with Plugging these results back into (8), and with some elided simplification steps, we end up with Note that we can more conveniently express the result in the following fashion: We‚Äôre now almost done. Heuristically, we know that the addition in  will become a multiplication when plugged back into the original formula for the multivariate Gaussian as shown in (3), using (4). Therefore, if we divide the entire expression by , we will only end up with the term produced by . Using this heuristic, we conclude that Notice that this result is exactly what we have in the function which we used to sample from the conditional distribution. The Gibbs sampler is another very interesting algorithm we can use to sample from complicated, intractable distributions. Although the use case of the Gibbs sampler is somewhat limited due to the fact that we need to be able to access the conditional distribution, it is a powerful algorithm nonetheless. We also discussed the notion of conditional distributions of the multivariate Gaussian in this post.",0,0,0,0,0,1,0,1
Principal Component Analysis,"And because we are dealing with matrices, the Euclidean norm was replaced with its matrix equivalent, the Frobenius norm. Observe that the first term  can safely be removed from the argument since it is a constant with respect to ; let‚Äôs also change the argument of the minimum to the maximum given the negative sign. The Frobenius norm of a real matrix can be calculated as Therefore, The last equality is due to a useful property of trace, which is that we can cycle the order of matrices without changing its value. Let‚Äôs consider a single column in , denoted as . You might also imagine this as a situation where  is one-dimensional, meaning we want to compress data into a single scalar value. It is not difficult to see that the trace of , which is a scalar in the one-dimensional case, is maximized when  is an eigenvector of  with the largest eigenvalue. Generalizing this result back to , we see that  is a matrix whose columns correspond to the eigenvectors of  in descending order.",0,0,0,0,0,1,0,1
Logistic Regression Model from Scratch,"This tutorial is a continuation of the ‚Äúfrom scratch‚Äù series we started last time with the blog post demonstrating the implementation of a simple k-nearest neighbors algorithm. The machine learning model we will be looking at today is logistic regression. If the ‚Äúregression‚Äù part sounds familiar, yes, that is because logistic regression is a close cousin of linear regression‚Äîboth models are employed in the context of regression problems. Linear regression is used when the estimation parameter is a continuous variable; logistic regression is best suited to tackle binary classification problems. Implementing the logistic regression model is slightly more challenging due to the mathematics involved in gradient descent, but we will make every step explicit throughout the way. Without further ado, let‚Äôs get into it. To understand the clockwork behind logistic regression, it is necessary to understand the logistic function. Simply put, the logistic function is a s-shaped curve the squishes real values between positive and negative infinity into the range . This property is convenient from a machine learning perspective because it allows us to perform binary classification. Binary classification is a type of classification problem where we are assigned the task of categorizing data into two groups.",0,0,1,1,0,0,0,0
Riemann Zeta and Prime Numbers,"From this, we can come up with the following alternative representation of the Riemann Zeta function: which can also be expressed as Other than the fact that the factorization of the Riemann Zeta function is satisfying in and of itself, the result in (5) also provides us with an interesting probabilistic interpretation on coprimeness. The intuition is pretty simple: given some random natural number , the probability that a prime  will divide  is simply . For example, if we come up with some random number, the probability that 2 will divide that number (that the number is even) is 0.5; the probability that the number will be a multiple of 3 is one-thirds. A corollary of this simple analysis is that we can now express the probability that a given random number will be a prime number as follows, using (6): In other words, the reciprocal of the Zeta function tells us the probability that a randomly chosen number will be a prime! The more interesting part of the story is that, we can now extend this single-number example to the case of multiple numbers.",1,0,0,0,0,0,0,0
Gaussian Process Regression,"My first attempt at understanding Gaussian processes probably dates back to earlier this year, when I obtained an electronic copy of Rasmussen‚Äôs Gaussian Process for Machine Learning. I gave up on chapter 1. The book is still far beyond my current level of mathematics, but nonetheless I am glad that I was able to gain at least a cursory understanding of GP regression. I hope you‚Äôve enjoyed reading this post. In a future post, I hope to dive into another topic I‚Äôve not been able to understand back then: Gaussian mixture models. See you in the next one!.",0,0,1,1,0,0,0,0
InceptionNet in PyTorch,"I‚Äôm not going to train this model on my GPU-less MacBook, and if you want to use InceptionNet, there are plenty of places to find pretrained models ready to be used right out of the box. However, I still think implementing this model helped me gain a finer grasp of PyTorch. I can say this with full confidence because a full month has passed since I coded out this Jupyter notebook, and I feel a lot more confident in PyTorch than I used to before. I hope you‚Äôve enjoyed reading this blog post. Catch you up in the next one (where I‚Äôll probably post another old notebook that‚Äôs been sitting on my computer for a month).",0,0,0,0,0,0,1,0
Revisiting Basel with Fourier,"To continue, we can derive a very similar expression for , given the specified interval from . Now that we have reviewed what Fourier series is and how we can construct it, let‚Äôs jump into the Basel problem. Just like the Taylor series, we can use Fourier expansion to represent any function continuous function. For our purposes, let‚Äôs try to expand a simple polynomial function, , using Fourier. We can begin with . Let‚Äôs continue with finding the even coefficients corresponding to the cosines. With some integration by parts, we can all agree that where the  terms appear because we end up plugging  into , a periodic function. And we can do the same for sine. Or, even better, with the key insight that  is an even function, we might intelligently deduce that there will be no sine terms at all, since sine functions are by nature odd. In other words, all . This can of course be shown through derivation as we have done above for the cosine coefficients.",1,0,0,0,0,0,0,0
Likelihood and Probability,"Notice that, in the context of normal distributions, the ML parameters are simply the mean and standard deviation of the given data point, which closely aligns with our intuition: the normal distribution that best explains given data would have the sample mean and variance as its parameters, which is exactly what our result suggests. Beyond the specific context of normal distributions, however, MLE is generally very useful when trying to reconstruct or approximate the population distribution using observed data. Let‚Äôs wrap this up by performing a quick verification of our formula for maximum likelihood estimation for normal distributions. First, we need to prepare some random numbers that will serve as our supposed observed data. We then calculate the optimum parameters  and  by using the formulas we have derived in (5) and (6). We then generate two subplots of the log likelihood function as expressed in (4), where we vary  while keeping  at  in one and flip this in the other. This can be achieved in the following manner. Executing this code block produces the figure below. From the graph, we can see that the maximum occurs at the mean and standard deviation of the distribution as we expect.",0,0,0,0,0,1,0,0
Dissecting LSTMs,"Since the forward pass is recurrent, so is the backward pass. Since we have , now it‚Äôs time to move further. Let‚Äôs derive the expression for the gradient for . Let‚Äôs do the same for the other term, . To make things easier, let‚Äôs make a quick substitution with an intermediate variable, i.e. let . Then, But  was just an intermediate variable. How can we get the gradient for  itself? Well, since the only transformation was just a , chain rule tells us that all we need is to multiply the antiderivative of , which we already derived above. Also keep in mind that since  is a recurrent variable, we have to apply the gradient from the next call as well, just like . Note that all we had to do is to multiply the  function we derived above, then add the backpropgation from the next iteration to account for the recurrent nature of the network. We still have a decent amount of work to do, but the fortunate news is that once we derive an expression for one parameter, the rest can also be obtained in an identical fashion.",0,0,0,0,0,0,1,0
"Basel, Zeta, and some more Euler","First, we observe that calculating the product of the first two terms produces the following expression: Then, we can express the target coefficient, denoted by , as follows: Where  denotes the coefficient of  obtained by expanding the rest of the terms following  in the infinite product above. If we repeat this process once more, a clear pattern emerges: Iterating over this process will eventually allow us to express our target coefficient as a sum of inverse squares multiplied by some constant, in this case : But then we already know the value of  from the modification of the Taylor polynomial for sine we saw earlier, which is ! Therefore, the Basel problem reduces to the following: Therefore, And there we have the solution to the Basel problem. In hindsight, solving the Basel problem is not rocket science; it is a mere application of the Taylor polynomial, coupled with some modifications and operations to mold the problem into a specific form. However, it takes extreme clairvoyance to see the link between the Basel problem and the Taylor polynomial of the sine function.",1,0,0,0,0,0,0,0
Stirling Approximation,"Thus, we have By simply rearranging (3), we arrive at Stirling‚Äôs approximation of the factorial: This is cool, but we still haven‚Äôt really shown why a Poisson can be used to approximate a Gaussian‚Äîafter all, this premise was the bulk of this demonstration. To see the intuition behind this approximation, it is constructive to consider what happens when we add independent Poisson random variables. Say we have  and , both of which are independent Poisson random variables with mean  and . Then,  will be a new Poisson random variable with mean . If we extend this idea to apply to  independent random variables instead of just two, we can conclude that  collection of independent random variables from  to  sampled from a population of mean  will have mean . And by the nature of the Poisson distribution, the same goes for variance (We will elaborate on this part more below). The Central Limit Theorem then tells us that the distribution of the sum of these random variables will approximate a normal distribution. This concludes a rough proof of the Stirling approximation.",1,1,0,0,0,1,0,0
"PyTorch, From Data to Modeling","As the last step, let‚Äôs just make sure that we know what each of these labels correspond to. The  is a tuple of strings that translate label indices to actual strings we can interpret. For example, if we see the label , we know that it denotes , which is . Modeling in PyTorch is the probably the part I love the most. TensorFlow‚Äôs sequential API is a great way to start, and PyTorch also provides the same sort of way of building sequential models. However, once you try to build anything that‚Äôs more complicated than that, I think PyTorch‚Äôs class-based way of approaching modeling makes a lot more intuitive sense and provides more room for experimentation and customization. Before getting into too much detail, below is a very simple CNN we will refer to as an example throughout this post. The first thing you will realize is that the model itself is a class that inherits from . This is a pattern you will see all the time with PyTorch models.  is a super class from which we can inherit to build anything from full-fledged models to custom blocks or layers to be used in some other larger model.",0,0,0,0,0,0,1,0
Gamma and Zeta,"The Riemann zeta function takes the following form: But this definition, as simple and intuitive as it is, seems to erroneously suggest that the Riemann zeta function is only defined over non-negative integers. This is certainly not the case. In fact, the reason why the Riemann zeta function is so widely studied in mathematics is that its domain ranges over the complex number plane. While we won‚Äôt be discussing the complexity of the Riemann zeta function in this regard (no pun intended), it is nonetheless important to consider about how we might calculate, say, . This is where the Gamma function comes in. As hinted earlier, an alternative definition of the Riemann zeta function can be constructed using the Gamma function that takes the Riemann zeta beyond the obvious realm of integers and into the real domain. We first start with a simple change of variables. Specifically, we can substitute  for .",1,0,0,0,0,0,0,0
An Introduction to Markov Chain Monte Carlo,"The trace plot below shows that, although the MCMC model does manage to sample many values, it likes to stay too much in its current state, thus making taking much longer for the sampler to properly estimate the posterior by sampling a wide range of values. The bottom line is that setting the right proposal distribution is important, and that trace plots are a good place to start to check if the proposal distribution is set up properly.  Now, it‚Äôs time to look at the answer key and see if our sampler has done well. Let‚Äôs plot  sampled by our Metropolis-Hastings sampler with the analytic posterior to see if they roughly match.  Fantastic! Although the estimated posterior is not exactly equal to the analytic posterior, the two are quite similar to each other. We could quantify how similar or different they are by using metrics such as , but for simplicity‚Äôs sake, let‚Äôs contain the post within the realm of Bayes as we have done so far.",0,0,0,1,0,0,0,0
The Gibbs Sampler,"Nonetheless, deriving this is a good mental exercise that merits some discussion. Just for the sake of quick review, let‚Äôs briefly revisit the familiar definition of a conditional probability: In the context of random vectors, we can rewrite this as Of course, if  and  are scalars, we go back to the familiar bivariate context of our example. In short, deriving the expression for the conditional distribution simply amounts to simplifying the fraction whose denominator is the marginal distribution and the numerator is the joint distribution. Let‚Äôs clarify the setup and notation first. We define a -dimensional random vector that follows a multivariate Gaussian distribution, namely . This vector, denoted as , can be split into a -dimensional vector  and -dimensional vector  in the following fashion: It is apparent that . Similarly, we can split up the covariance matrix  in the following fashion where , , . Also, given the symmetric property of the covariance matrix, . The goal is to derive is the conditional distribution, . This derivation was heavily adapted from this source and this thread on Stack Exchange.",0,0,0,0,0,1,0,1
Wonders of Monte Carlo,"Everyone has encountered integrals of varying monstrosity at one point in their lives, scrambling to solve it with integration by parts or some obscure, creative substitution, only to realize that everything wounds up in the middle of nowhere. Well, good news for all of us‚ÄîMonte Carlo methods can be used to estimate the value of mind-pulverizing, complicated definite integrals. Let‚Äôs say we want to estimate the value of an integral of a function  over some domain . Now assume that there is some probability density function  defined over . Then, we can alter this integral as  shown below. Notice that this integral can now be understood as an expected value for some continuous random variable. In other words,  collapses into the following expression. What does this tell us? This means that we can simply calculate an integral by randomly sampling the values of  such that  follows some probability distribution . The probability distribution part simply ensures that values of  that are more probable are sampled more often than others. Intuitively, we are effectively taking a weighted mean of the values of , which is the loose definition of expected values.",0,0,0,0,0,1,0,0
Dissecting LSTMs,"Thus, the gradient flows backwards without any modification. Moving down a layer, we come across (6): Let‚Äôs begin by trying to find the gradient for . You might be wondering what the  term is doing in that equation. After all, isn‚Äôt that quantity precisely what we are trying to calculate? This is the one tricky yet also interesting part about RNN backpropagation. Recall that the whole point of a recurrent neural network is its use of variables from the previous forward pass. For example, we know that in the next forward pass,  will be concatenated with the input . In the backpropagation step corresponding to that forward pass, we would have computed ; thus, this gradient flows into the current backpropagation as well. Although this diagram applies to a standard RNN instead of an LSTM, the recurrent nature of backprop still stands. I present it here because I find this diagram to be very intuitive.  If you look at the right, the star represents the gradient from the last pass. If you look to the left, you will see that there is going to be a gradient for  that will eventually be passed over to the next backpropgation scheme.",0,0,0,0,0,0,1,0
Word2vec from Scratch,"When the output is passed into this layer, it is converted into probability vectors whose elements sum up to one. This final output can be considered as context predictions, i.e. which words are likely to be in the window vicinity of the input word. In training‚Äîspecifically error calculation and backpropagation‚Äîwe would be comparing this prediction of probability vectors with its true one-hot encoded targets. The error function that we use with softmax is cross entropy, defined as I like to think of this as a dot product of the target vector and the log of the prediction, because that is essentially what the summation is doing. In this alternate formulation, the cross entropy formula can be rewritten as Because  a one-hot encoded vector in this case, all the elements in  whose entry is zero will have no effect on the final outcome. Indeed, we simply end up taking the negative log of the prediction. Notice that the closer the value of the prediction is to 1, the smaller the cross entropy, and vice versa. This aligns with the behavior we want, since we want the predicted probability to be as close to 1 as possible.",0,0,0,1,0,0,1,0
"Linear Regression, in Two Ways","The intuition behind this approach was that we can construct an expression for the total error given by the regression line, then derive that expression to find the values of the parameters that minimize the error function. Simply put, we will attempt to frame linear regression as a simple optimization problem. Let‚Äôs recall the problem setup from the linear algebra section above. The problem, as we framed it in linear algebra terms, went as follows: given some unsolvable system of equations , find the closest approximations of  and , each denoted as  and  respectively, such that the system is now solvable. We will start from this identical setup with the same notation, but approach it slightly differently by using matrix calculus. The first agenda on the table is constructing an error function. The most common metric for error analysis is mean squared error, or MSE for short. MSE computes the magnitude of error as the squared distance between the actual value of data and that predicted by the regression line. We square the error simply to prevent positive and negative errors from canceling each other out. In the context of our regression problem, where  denotes the error function.",0,0,0,0,0,0,0,1
Natural Gradient and Fisher,"This is where the notion of natural gradients come into play: if our goal is to minimize the cost function, which is effectively equivalent to maximizing the likelihood, why not search within the distribution space of the likelihood function instead? After all, this makes more sense since gradient descent in parameter space is likely to be easily perturbed by the mode of parametrization, such as using precision instead of variance in a normal distribution, whereas searching in the distribution space would not be subject to this limitation. So the alternative to this approach would be to search the distribution space and find the distribution that which makes value of the cost function the smallest. This is the motivation behind the notion of a natural gradient. Now you might be wondering how all this has anything to do with the Fisher matrix, which we looked at in the previous post. Well, it turns out there are some deep, interesting questions to be posed and connections to be uncovered. If we‚Äôre going to search around the distribution space, one natural question to consider is what distance metric we will use for our search.",0,0,1,0,0,1,0,0
"Beta, Bayes, and Multi-armed Bandits","‚Äù I highly recommend that you read this post on cross validated for many intuitive explanations of what the Beta distribution is and how it can be useful, such as in this Bayesian context. Now we know that there is a closed-form solution for Bayesian problems involving conjugate priors and likelihood functions, such as a Beta prior coupled with the binomial likelihood. But we want to be able to interpret the posterior distribution. We might start from rather simple metrics like the mean to get a better idea of what the posterior tells us. Note that we can always generalize such quantities into moments. Given , the expected value of the Beta random variable can be expressed as Proving this is pretty straightforward if we simply use the law of the unconscious statistician. Using the definition of expectation, we can derive the following: Here, we use the Gamma representation of the Beta function. This conclusion gives an even nicer, more intuitive interpretation of the Bayesian update we saw earlier with the Beta prior and binomial likelihood.",0,1,0,0,0,1,0,0
Moments in Statistics,"Let‚Äôs twist up the order and try the MGF method first. We can pull out the lambda and combine the exponential terms to get This is an easy integral. Let‚Äôs proceed with the integration and evaluation sequence: Now, all we have to do is to derive the result in (12) three time and plug in . Although calculating the third order derivative may sound intimidating, it may seem easier in comparison to evaluating the integral which would require us to use integration by parts. In the end, both (12) and (13) are pointing at the same quantity, namely the third moment of the exponential distribution. Perhaps the complexity of calculating either quantity is similar, and the question might just boil down to a matter of preference. However, this example shows that the MGF is a robust method of calculating moments of a distribution, and even more, potentially less computationally expensive than using the brute force method to directly calculate expected values. This was a short post on moments and moment generating functions. Moments was one of these terms that I had come across on Wikipedia or math stackexchange posts, but never had a chance to figure out.",0,1,0,0,0,1,0,0
Introduction to tf-idf,"There seems to be many variations of how smoothing is implemented in practice, but here I present one way that seems to be adopted by scikit-learn. For other schemes, refer to this table on Wikipedia. However, this is a mere technically; the intuition we motivated earlier still applies regardless. With these ideas in mind, let‚Äôs go implement tf-idf vectorization in Python! In this section, we will develop a simple set of methods to convert a set of raw documents to tf-idf vectors, using a dummy dataset. Below are four documents (again, I know they‚Äôre short) that we will be using throughout this tutorial. The first step is to preprocess and tokenize the data. Although the specifics of preprocessing would probably differ from task to task, in this simple example, we simply remove all punctuations, change documents to lower case letters, and tokenize them by breaking down documents into a bag of words. Other possible techniques not discussed here include stemming and lemmatization. The  function accepts as input a set of documents and removes all the punctuation in each document. Here is the result of applying our function to the dummy data.",0,0,0,1,0,0,0,0
Recommendation Algorithm with SVD,"Let‚Äôs quickly build this ratings matrix using  and  as shown below. Let‚Äôs first see what this matrix looks like. We can do this simply by calling the  function and saving it to some variable. For notational consistency, let‚Äôs name this variable . Great! Now we have a matrix of binary numbers, where  denotes the fact that the user liked the movie and  the fact that they disliked it. We can make some cursory qualitative observations of this toy data. Note, for instance, that users who like Movie 2 also tend to like Movie 3. Also, User 6 and User 8 have identical prefernece for movies‚Äîperhaps they both like a particular genre, or tend to like the movie starred by some actor or actress. We would expect singular value decomposition to capture these observations in some way, albeit approximately. Now, let‚Äôs actually perform singular value decomposition on the ratings matrix. We could try to do this manually by hand, but let‚Äôs utilize the power of modern computing to save ourselves of the time and mental effort involved in calculating the eigenvalues and eigenvectors of a ten-by-ten matrix.",0,0,0,1,0,0,0,1
First Neural Network with Keras,"Lately, I have been on a DataCamp spree after unlocking a two-month free unlimited trial through Microsoft‚Äôs Visual Studio Dev Essentials program. If you haven‚Äôt already, make sure to check it out, as it offers a plethora of tools, journal subscriptions, and software packages for developers. Anyhow, one of the courses I decided to check out on DataCamp was titled ‚ÄúIntroduction to Deep Learning with Python,‚Äù which covered basic concepts in deep learning such as forward and backward propagation. The latter half of the tutorial was devoted to the introduction of the Keras API and the implementation of neural networks. I created this notebook immediately after finishing the tutorial for memory retention and self-review purposes. First, we begin by importing the  library as well as other affiliated functions in the module. Note that Keras uses TensorFlow as backend by default. The warning in the code block below appears because this notebook was written on Google Colab, which informs users that the platform will be switching over to TensorFlow 2 in the future. As you might be able to guess from one of the imported modules, the objective of the neural network will be to classify hand-written digits.",0,0,1,0,1,0,1,0
"Linear Regression, in Two Ways","What we can do, however, is find a projection of the vector  onto matrix  so that we can identify a solution that is closest to , which we shall denote as . As you can see, this is where all the linear algebra kicks in. Let‚Äôs start by thinking about , the projection of  onto . After some thinking, we can convince ourselves that  is the component of  that lives within the column space of , and that  is the error component of  that lives outside the column space of . From this, it follows that  is orthogonal to , since any non-orthogonal component would have been factored into . Concretely, since the transpose is an alternate representation of the dot product. We can further specify this equation by using the fact that  can be expressed as a linear combination of the columns of . In other words, where  is the solution to the system of equations represented by . Let‚Äôs further unpackage (1) using matrix multiplication. Therefore, We finally have a formula for : Let‚Äôs remind ourselves of what  is and where we were trying to get at with projection in the context of regression.",0,0,0,0,0,0,0,1
"Beta, Bayes, and Multi-armed Bandits","Namely, given the new posterior we know that sampling from that new posterior will give us a mean value that somewhat resembles the crude approach we would have taken without the prior expectation. Here, the crude approach is referring to the raw frequentist estimate we would have made had we not taken the Bayesian approach to the problem. It is obvious that the two hyperparameters of the prior are acting as an initial weight of sorts, making sure that when little data is available, the prior overshadows observations, but when ample amount of data is collected and available, eventually yields to those observations to estimate the parameter. Now we can turn our attention back to the multi-armed bandit problem. Now we can build on top of our knowledge of the Beta-Binomial update and refine what the frequentist greedy approach. We will also write out some simple functions to simulate the bandit problem and thus demonstrate the effectiveness of the Bayesian approach. Note that a lot of the code in this post was largely inspired by Peter‚Äôs blog. Before we get into any modeling, let‚Äôs first import the modules we‚Äôll need and set things up.",0,1,0,0,0,1,0,0
Fourier Series,"By the same token, therefore, we can  deduce that we can do the same for the sine terms: The only small caveat is that the case is a bit more specific for . When ,  reduces to a constant of one, which is why we end up with  instead of . In other words, Hence, we end up with This exceptional term has a very intuitive interpretation: it is the average of the function  over the domain of integration. Indeed, if we were to perform some expansion, it makes intuitive sense that we start from an average. One observation to make about Fourier expansion is the fact that it is a combination of sines and cosines‚Äîand we have seen those before with, lo and behold, Euler‚Äôs formula. Recall that Euler‚Äôs formula is a piece of magic that connects all the dots of mathematics. Here is the familiar equation: Using Euler‚Äôs formula, we can formulate an alternative representation of Fourier series: Let‚Äôs unequivocally drive this concept home with a simple example involving the Dirac delta function. The delta function is interesting function that looks like this:  The delta function has two nice properties that make it great to work with.",1,0,0,0,0,1,0,0
Convolutional Neural Network with Keras,"Note that this Jupyter Notebook was written on Google Colaboratory. The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x. We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the  magic: more info. The  function loads the CIFAR10 data set from  module, then applies some basic preprocessing to make it more usable for deep learning purposes. The CIFAR10 data set contains 50000 training images, each labeled with 1 of 10 classes, ranging from cats, horses, and trucks to airplanes. This is a classic classification task. The preprocessing occurs on two levels: first, the images are normalized so that its pixels takes values between 0 and 1. The training labels are transformed from an integer class label to a one-hot encoded vector. Let‚Äôs load the data to proceed with our analysis. Let‚Äôs see what the dimensions of the data are. Because the CIFAR10 data set include color images for its training set, we would expect three channels, each corresponding to red, green, and blue (hence RGB). As expected, it appears that the training data is a tensor of four dimensions, while the target labels are 10 dimensional vectors.",0,0,0,0,1,0,1,0
Moments in Statistics,"To convince ourselves of this statement, we need to start by looking at the Taylor polynomial for the exponential. It‚Äôs not difficult to see the coherency of this expression by taking its derivative‚Äîthe derivative of the polynomial is equal to itself, as we would expect for . From here, we can sufficiently deduce that The coherency of (3) can simply be seen by making the substitution . To continue, now that we have an expression for , we can now calculate , which we might recall is the definition of a moment generating function. where the second equality stands due to linearity of expectation. All the magic happens when we derive this function with respect to . At , all terms in (5) except for the very first one go to zero, leaving us with In other words, deriving the MGF once and plugging in 0 to  leaves us with the first moment, as expected. If we derive the function again and do the same, And by induction, we can see how the th derivative of the MGF at  would give us the th moment of the distribution, .",0,1,0,0,0,1,0,0
The Exponential Family,"This is all great, but there is still an unanswered question lingering in the air: what is the MLE estimate of the parameter  ? This moment is precisely when equation (25) comes in handy. Recall that Therefore, Finally, we have arrived at our destination: We finally know how to calculate the parameter under which the likelihood of observing given data is maximized. The beauty of this approach is that it applies to all probability distributions that belong to the exponential family because our analysis does not depend on which distribution is in question; we started from the canonical form of the exponential family to derive a set of generic equations. This is the convenience of dealing with the exponential family: because they are all defined by the same underlying structure, the MLE equations hold general applicability. In this post, we explored the exponential family of distributions, which I flippantly ascribed the title ‚ÄúThe Medici of Probability Distributions.",0,1,0,0,0,1,0,0
Traveling Salesman Problem with Genetic Algorithms,"For convenience purposes, we will represent cities by their indices. Now it‚Äôs time for us to understand how genetic algorithms work. Don‚Äôt worry, you don‚Äôt have to be a biology major to understand this; simple intuition will do. The idea is that, we can use some sort of randomized approach to generate an initial population, and motivate an evolutionary cycle such that only superior genes will survive successive iterations. You might be wondering what genes are in this context. Most typically, genes can be thought of as some representation of the solution we are trying to find. In this case, an encoding of the optimal path would be the gene we are looking for. Evolution is a process that finds an optimal solution for survival through competition and mutation. Basically, the genes that have superior traits will survive, leaving offspring into the next generation. Those that are inferior will be unable to find a mate and perish, as sad as it sounds. Then how do these superior or inferior traits occur in the first place? The answer lies in random mutations.",0,0,0,1,0,0,0,0
Logistic Regression Model from Scratch,"Let‚Äôs plot cross entropy loss for three different values of : 0.05, 0.1, and 0.5. Just like before, we cap the number of iterations to 200 epochs.  The graph shows that the larger the learning rate, the quicker the decrease in cross entropy loss. This result is coherent with what the previous visualizations on accuracy suggested: the higher the learning rate, the quicker the model learns from the training data. In this post, we built the logistic regression model from scratch by deriving an equation for gradient descent on cross entropy given a sigmoid function. In the process, we brought together many useful concepts we explored on this blog previously, such as matrix calculus, cross entropy, and more. It‚Äôs always exciting to see when seemingly unrelated concepts come together to form beautiful pictures in unexpected ways, and that is what motivates me to continue my journey down this road. The logistic regression model is simple yet incredibly powerful in the context of binary classification.",0,0,1,1,0,0,0,0
Recommendation Algorithm with SVD,"I‚Äôve been using a music streaming service for the past few weeks, and it‚Äôs been a great experience so far. I usually listen to some smoothing new age piano or jazz while I‚Äôm working, while I prefer K-pop on my daily commutes and bass-heavy house music during my workouts. Having processed these information through repeated user input on my part, the streaming application now regularly generates playlists each reflective of the three different genres of music that I enjoy most. This got me wondering: what is the underlying algorithm beind content selection and recommendation? How do prominent streaming services such as Netflix and Spotify provide recommendations to their users that seem to reflect their personal preferences and tastes? From a business perspective, these questions carry extreme significance since the accuracy of a recommendation algorithm may directly impact sales revenue. In this post, we will dive into this question by developing an elementary recommendation engine. The mechanism we will use to achieve this objective is a technique in linear algebra known as singular value decomposition or SVD for short.",0,0,0,1,0,0,0,1
Demystifying Entropy (And More),"How might we be able to express information involved in the event that the coin lands on tails? How about heads? There are many ways to approach this problem, but an easy way would be to use binary numbers. For example, we might ascribe meaning to 0 and 1 such that 0 represents heads and 1 represents tails. Of course, there might be other ways to encode information, such as setting 111 to heads and 000 to tails, but obviously this is wasting information. In other words, it is not the most efficient method of encoding. Even under a single digit binary number scheme, we would be able to decode a series of transmitted information without loss. It is not difficult to see, therefore, that all we need to express the result of a fair coin toss is a single digit of binary numbers 0 and 1. Typically, we use bits to denote the number of digits required to express information in binary numbers. In this case, the information involved in  is equal to 1 bit; by symmetry, the same goes for .",0,0,0,0,0,1,0,0
