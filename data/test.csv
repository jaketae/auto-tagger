title,body,deep_learning,linear_algebra,machine_learning,from_scratch,statistics,pytorch,analysis,probability_distribution
Fourier Series,"Taylor series is used in countless areas of mathematics and sciences. It is a handy little tool in the mathematicians arsenal that allows us to decompose any function into a series of polynomials, which are fairly easy to work with. Today, we are going to take a brief look at another type of series expansion, known as Fourier series. Note that these concepts are my annotations of Professor Gilbert Strang’s amazing lecture, available on YouTube. The biggest difference between Taylor series and Fourier series is that, unlike Taylor series, whose basic fundamental unit is a polynomial term, the building block of a Fourier series is a trigonometric function, namely one of either sine or cosine. Concretely, a generic formula of a Fourier expansion looks as follows: Personally, I found this formula to be more difficult to intuit than the Taylor series. However, once you understand the underlying mechanics, it’s fascinating to see how periodic wave functions can be decomposed as such. First, let’s begin with an analysis of orthogonality. Commonly, we define to vectors math variable and math variable as being orthogonal if That is, if their dot product yields zero. This follows from the definition of a dot product, which has to do with cosines. With a stretch of imagination, we can extend this definition of orthogonality to the context of functions, not just vectors. For vectors, a dot product entails summing the element-wise products of each component. Functions don’t quite have a clearly defined, discrete component. Therefore, instead of simply adding, we integrate over a given domain. For example, The same applies to cosines and sines: where math variable and math variable can be any integer. In other words, cosine functions of different frequencies are orthogonal to each other, as are cosines are with sines! Now, why is orthogonality relevant at all for understanding the Fourier series? It’s time to sit back and let the magic unfold when we multiply math variable to (1) and integrate the entire expression. If we divide both sides of (5) by math variable, you will realize that we have derived an expression for the constant corresponding to the math variable expansion term: The key takeaway here is this: by exploiting orthogonality, we can knock out every term but one, the very term that we multiplied to the expansion. By the same token, therefore, we can deduce that we can do the same for the sine terms: The only small caveat is that the case is a bit more specific for math variable. When math variable, math variable reduces to a constant of one, which is why we end up with math variable instead of math variable. In other words, Hence, we end up with This exceptional term has a very intuitive interpretation: it is the average of the function math variable over the domain of integration. Indeed, if we were to perform some expansion, it makes intuitive sense that we start from an average. One observation to make about Fourier expansion is the fact that it is a combination of sines and cosines—and we have seen those before with, lo and behold, Euler’s formula. Recall that Euler’s formula is a piece of magic that connects all the dots of mathematics. Here is the familiar equation: Using Euler’s formula, we can formulate an alternative representation of Fourier series: Let’s unequivocally drive this concept home with a simple example involving the Dirac delta function. The delta function is interesting function that looks like this: The delta function has two nice properties that make it great to work with. First, it integrates to one if the domain includes math variable. This is the point where the graph peaks in the diagram. Second, the delta function is even. This automatically tells us that when we perform a Fourier expansion, we will have no sine functions—sine functions are by nature odd. With this understanding in mind, let’s derive the Fourier series of the Dirac delta by starting with math variable. The equality is due to the first property of the delta function outlined in the previous paragraph. The derivation of the rest of the constants can be done in a similar fashion. The trick is to use the fact that the delta function is zero in all domains but math variable. Therefore, the oscillations of math variable will be nullified by the delta function in all but that one point, where math variable is just one. Therefore, (13) simply reduces to integrating the delta function itself, which is also one! To sum up, we have the following: I find it fascinating to see how a function so singular and unusual as the Dirac delta can be reduced to a summation of cosines, which are curvy, oscillating harmonics. This is the beauty of expansion techniques like the Fourier and Taylor: as counterintuitive as it may seem, these tools tell us that any function can be approximated through an infinite summation, even if the original function may not resemble the building block of the expansion technique at all at a glance.",0,0,0,0,1,0,1,0
A Simple Autocomplete Model,"You might remember back in the old days when autocomplete was just terrible. The suggestions provided by autocomplete would be useless if not downright stupid—I remember that one day when I intended to type “Gimme a sec,” only to see my message get edited into “Gimme a sex” by the divine touches of autocomplete. On the same day, the feature was turned off on my phone for the betterment of the world. Now, times have changed. Recently, I decided to give autocorrect a chance on my iPhone. Surprisingly, I find myself liking autocomplete more than hating it, especially now that the weather is getting colder by each day: when my frost-numbed finger tips touch on the wrong places of the phone screen to produce words that aren’t really words, iPhone’s autocomplete somehow magically reads my mind to rearrange all that inscrutable alphabet soup into words that make actual, coherent sense. Sometimes, it’s so good at correcting my typos that I intentionnally make careless mistakes on the keyboard just to see how far it can go. One of the obvious reasons behind such drastic improvements in autocomplete functionality is the development of deep neural networks. As we know, neural networks are great at learning hidden patterns as long as we feed it with enough data. In this post, we will implement a very simple version of a generative deep neural network that can easily form the backbone of some character-based autocomplete algorithm. Let’s begin! Let’s first go ahead and import all dependencies for this tutorial. As always, we will be using the functional API to build our neural network. We will be training our neural network to speak like the great German philosopher Friedrich Nietzsche (or his English translations, to be more exact). First, let’s build a function that retrieves the necessary text file document from the web to return a Python string. Let’s take a look at the text data by examining its length. Just to make sure that the data has been loaded successfully, let’s take a look at the first 100 characters of the string. ## Preprocessing It’s time to preprocess the text data to make it feedable to our neural network. As introduced in this previous post on recurrent neural networks, the smart way to deal with text preprocessing is typically to use an embedding layer that translates words into vectors. However, text embedding is insuitable for this task since our goal is to build a character-level text generation model. In other words, our model is not going to generate word predictions; instead, it will spit out a character each prediction cycle. Therefore, we will use an alternative technique, namely mapping each character to an integer value. This isn’t as elegant as text embedding or even one-hot encoding but for a character-level analysis, it should work fine. The function takes a string text data as input and returns a list of training data, each of length , sampled every characters. It also returns the training labels and a hash table mapping characters to their respective integer encodings. Let’s perform a quick sanity check to see if the function works as expected. Specifying to 60 means that each instance in the training data will be 60 consecutive characters sampled from the text data every characters. The result tells us that we have a total of 200278 training instances, which is probably plenty to train, test, and validate our model. The result also tells us that there are 57 unique characters in the text data. Note that these unique characters not only include alphabets but also and other miscellaneous white spacing characters and punctuations. Let’s now design our model. Because there is obviously going to be sequential, temporal structure underlying the training data, we will use an LSTM layer, a type of advanced recurrent neural network we saw in the previous post. In fact, this is all we need, unless we want to create a deep neural network spanning multiple layers. However, training such a model would cost a lot of time and computational resource. For the sake of simplicity, we will build a simple model with a single LSTM layer. The output layer is going to be a dense layer with number of neurons, activated with a softmax function. We can thus interpret the index of the biggest value of the final array to correspond to the most likely character. Below is a full plot of the model that shows the dimensions of the input and output tensors of all layers. Now, all we have to do is to train the model with the data. Let’s run this for 50 epochs, just to give our model enough time to explore the loss function and settle on a good minimum. As I was training this model on Google Colab, I noticed that training even this simple model took a lot of time. Therefore, I decided that it is a good idea to probably save the trained model—in the worst case scenario that poor network connection suddenly caused the Jupyter kernel to die, saving a saved model file would be of huge help since I can continue training again from there. Saving the model on Google Colab requires us to import a simple module, . The process is very simple. To load the model, we can simply call the command below. Let’s take a look at the loss curve of the model. We can simply look at the value of the loss function as printed throughout the training scheme, but why not visualize it if we can? As expected, the loss decreases throughout each epoch. The reason I was not paticularly worried about overfitting was that we had so much data to work with, especially in comparison with the relatively constrained memory capacity of our one-layered model. One of the objectives of this tutorial was to demonstrate the fun we can have with generative models, namely neural networks that can be used to generate data themselves, not just classify or predict data points. To put this into perspective, let’s compare the objectives of a generative model with that of a discriminative model. Simply put, the goal of a discriminative model is to model and calculate where math variable is a label and math variable is some input vector. As you can see, discriminative models arise most commonly from the context of supervised machine learning, such as regression or classification. In contrast, the goal of a generative model is to approximate the distribution which we might construe to be the probability of observing evidence or data. By modeling this distribution, the goal is that we might be able to generate samples that appear to have been sampled from this distribution. In other words, we want our model to generate likely data points based on an approximation of the true distribution from which these observations came from. In the context of this tutorial, our neural network should be able to somewhat immitate the speech of the famous German philosopher based on the training it went through with text data, although we would not expect the content generated by our neural network to have the same level of depth and profoundity as those of his original writings. As mentioned above, the objective of a generative model is to model the distribution of the latent space from which observed data points came from. At this point, our trained model should be able to model this distribution, and thus generate predictions given some input vector. However, we want to add some element of randomness of noise in the prediction. Why might we want to do this? Well, an intuitive pitfall we might expect is that the model might end up generating a repetition of some likely sequence of characters. For example, let’s say the model’s estimated distribution deems the sequence “God is dead” to be likely. Then, the output of our model might end up being something like this: …(some input text) God is dead God is dead God is dead… (repetition elided) We don’t want this to happen. Instead, we want to introduce some noise so that the model faces subtle obstructions, thereby making it get more “creative” with its output instead of getting trapped in an infinite loop of some likely sequence. Below is a sample implementation of adding noise to the output using log and exponential transformations to the output vector of our model. The transformation might be expressed as follows: where math variable denotes a transformation, math variable denotes a prediction as a vector, math variable denotes temperature as a measure of randomness, and math variable is a normalizing constant. Although this might appear complicated, all it’s doing is that it is adding some perturbation or disturbance to the output data so that it is possible for less likely characters to be chosen as the final prediction. Below is a sample implementation of this process in code. Note that due to the algebraic quality of the vector transformation above, randomness is increased for large values of math variable. Now it’s finally time to put our Nietzsche model to the test. How we will do this is pretty simple. First, we will feed a 60-character excerpt from the text to our model. Then, the model will output a prediction vector, which is then passed onto given a specified . We will finally have a prediction that is 1 character. Then, we incorporate that one character prediction into the original 60-character data we started with. We slice the new augmented data set from to end up with another prediction. We would then slice the data set from, you guessed it, and repeat the process as outlined above. When we iterate through this cycle many times, we would eventually end up with some generated text. Below is the function that implements the iteration process. We’re almost done! To get a better sense of what impact temperature has on the generation of text, let’s quickly write up a function that will allow us to generate text for differing values of . The time has come: let’s test our model for four different temperature values from 0.3 to 1.2, evenly spaced. We will make our model go through 1000 iterations to make sure that we have a long enough text to read, analyze, and evaluate. For the sake of readability, I have reformatted the output result in markdown quotations. Generated text at temperature 0.3: is a woman–what then? is there not ground for suspecting that the experience and present strange of the soul is also as the stand of the most profound that the present the art and possible to the present spore as a man and the morality and present self instinct, and the subject that the presence of the surcessize, and also it is an action which the philosophers and the spirit has the consider the action to the philosopher and possess and the spirit is not be who can something the predicess of the constinate the same and self-interpatence, the disconsises what is not to be more profound, as if it is a man as a distance of the same art and ther strict to the presing to the result the problem of the present the spirit what is the consequences and the development of the same art of philosophers and security and spirit and for the subjective in the disturce, as in the contrary and present stronger and present could not be an inclination and desires of the same and distinguished that is the discoverty in such a person itself influence and ethers as Generated text at temperature 0.6: is a woman–what then? is there not ground for suspecting to and the world will had to a such that the basis of the incussions of the spirit as the does not because actian free spirits of intellect of the commstical purtious expression of men are so much he is not unnor experiences of self-conturity, and as anifegently religious in the man would not consciously, his action is not be actian at in accombs life for the such all procees of great and the heart of this conduct the spirity of the man can provate for in any once in any of the suriticular conduct that which own needs, when they are therefore, as such action and some difficulty that the strength, it, himself which has to its fine term of pricismans the exacte in its self-recuphing and every strength and man to wist the action something man as the worst, that the was of a longent that the whole not be all the very subjectical proves the stronger extent he is necessary to metaphysical figure of the faith in the bolity in the pure belief–as “the such a successes of the values–that is he ​ Generated text at temperature 0.9: is a woman–what then? is there not ground for suspecting that they grasutes, and so farmeduition of the does not only with this constrbicapity have honour–and who distical seclles are denie’n, is one samiles are no luttrainess, and ethic and matficulty, concudes of morality to rost were presence of lighters caseful has prescally here at last not and servicatity, leads falled for child real appreparetess of worths–the resticians when one to persans as a what a mean of that is as to the same heart tending noble stimptically and particious, we pach yought for that mankind, that the same take frights a contrady has howevers of a surplurating or in fact a sort, without present superite fimatical matterm of our being interlunally men who cal scornce. the shrinking’s proglish, and traints he way to demitable pure explised and place can deterely by the compulse in whom is phypociative cinceous, and the higher and will bounthen–in itsiluariant upon find the “first the whore we man will simple condection and some than us–a valuasly refiges who feel Generated text at temperature 1.2: is a woman–what then? is there not ground for suspecting that he therefore when shre, mun, a schopenhehtor abold gevert. 120 =as in find that is _know believinally bad, euser of view.–bithic iftel canly in any knowitumentially. the charm surpose again, in swret feathryst, form of kinne of the world bejud–age–implaasoun ever? but that the is any appearance has clenge: the? a plexable gen preducl=s than condugebleines and aligh to advirenta-nasure; findiminal it as, not take. the ideved towards upavanizing, would be thenion, in all pespres: it is of a concidenary, which, well founly con-utbacte udwerlly upon mansing–frauble of “arrey been can the pritarnated from their christian often–think prestation of mocives.” legt, lenge:–this deps telows, plenhance of decessaticrances). hyrk an interlusally” tone–under good haggy,” is have we leamness of conschous should it, of sicking ummenfeckinal zerturm erienweron of noble of himself-clonizing there is conctumendable prefersy exaitunia states,” whether they deve oves any of hispyssesss. int The results are fascinating. Granted, our model is still bad at immitating Nietzsche’s style of writing, but I think the performance is impressive given that this was a character-based text generation model. Think about it for a second: to write even a single word, say “present,” the model has to correctly predict “p”, “r”, “e”, “s”, “e”, “n”, and “t,” all in tandem. Imagine doing this for extended cycles, long enough to generate text that is comfortably a paragraph long. It’s amazing how the text it generates even makes some sense at all. Then, as temperature rises, we see more randomness and “creativity” at work. We start to see more words that aren’t really words (the one I personally like is “farmeduition”—it sounds like it could be either some hard, obscure word that no one knows, or a failed jumble of “farm,” “education,” and “intuition”). At temperature 1.2, the model is basically going crazy with randomness, adding white spaces where there shouldn’t be and sounding more and more like a speaker of Old English or German, something that one might expect to see in English scripts written in pre-Shakesperean times. At any rate, it is simply fascinating to see how a neural network can be trained to immitate some style of writing. Hopefully this tutorial gave you some intuition of how autocomplete works, although I presume business-grade autocomplete functions on our phones are based on much more complicated algorithms. Thanks for reading this post. In the next post, we might look at another example of a generative model known as generative adversarial networks, or GAN for short. This is a burgeoning field in deep learning with a lot of prospect and attention, so I’m already excited to put out that post once it’s done. See you in the next post. Peace!",1,0,0,0,0,0,0,0
(Attempt at) Knowledge Distillation,"For the past couple of months or so, I’ve been spending time looking into transformers and BERT. Transformers are state of the art NLP models that are now replacing traditional RNNs in countless NLP tasks. One benefit of transformers is that, unlike RNNs, which require data to be input as a sequence, they can handle tokens all at once in parallel. This post is not going to be about transformers or BERT, but it does touch on a relevant topic. Transformers are huge architectures that can have as many as a few million trainable parameters. Hence, training transformers are extremely expensive, and only companies with a lot of computing resources can afford to train such models in their GPU or TPU clusters for extended periods of time. This is why so many people are interested in model compression. Huge models are not only difficult to train, but also requires good enough computing power. In other words, on-device inference is very difficult. People have thus started to look at various model compression techniques. Today, we will be discussing one such technique, known as knowledge distillation, introduced in Distilling the Knowledge in a Neural Network, by Hinton et. al. Let’s get started. In knowledge distillation, we assume two models: a teacher and student models. The teacher is the big, cumbersome model we seek to compress. As you may have guessed, the student is the compressed result of the teacher model. The object of knowledge distillation is to train the student to mimic the logits produced by the teacher. In other words, the student adjusts its parameters in the training phase such that its own logits output by the final layer is as close to that of the teacher model as possible. This is different from the typical training scheme in which we train the model on a set of hard, one-hot encoded labels in a supervised learning context. Why might we want to do this? After all, don’t we want the model to simply learn the dataset labels? The motivation is that the teacher model possesses what the authors of the paper refer to as “dark knowledge.” To make this more concrete, let’s consider a simple example. Say we have trained a reasonably good model on the MNIST dataset. The model would, of course, accurately be able to predict which digit each image represents. If we show the model a 6, chances are, it will accurately predict that the number is a 6. No big deal here. The interesting part occurs when we look at its logits. After a softmax activation, we might expect to see something like We can see that the network correctly outputs 6 as the most probably digit by assigning it the largest logit. However, this is not the end of the story: we also see that the network has assigned a somewhat high probability to other numbers, such as 0 and 9. If you think about it, it kind of makes sense: given the shape of the digits, one would reasonably expect a 6 to look like 0, or vice versa, depending on the handwriting, and ditto the 9. Such implicit knowledge the model learns about the dataset is referred to as black knowledge. The goal of knowledge distillation is to be able to help the student model learn not only the true labels from the dataset, but also this dark knowledge that the larger teacher model has about the data. This is why we train the student model on the logits of the teacher model. There are several simplifications we’ve made so far in our explanation. One major simplification is how we treat the logits from the teacher model. If you look at the diagram above, the amount of dark knowledge that can be gained is relatively little. If the model is well-trained enough, perhaps it will predict the digit 6 with an even higher number of confidence, thus ascribing lesser probabilities to other labels like 0 or 9. So ironically, the better the teacher model is, the lesser dark knowledge there is to be gained by the student model. The authors circumvent this problem by adjusting the final softmax output of the teacher model. Recall that the softmax activation function looks as follows: Due to the nature of the exponential function, the softmax function is designed in such a way that larger logits are amplified, whereas smaller ones are squashed near zero. From a dark knowledge point of view, this is not great, as it makes it more difficult for the student model to learn the dark knowledge from the teacher. Thus, the authors of the paper introduce a temperature parameter to modify the softmax function. Let’s denote this as softmax prime. When math variable, it is identical to the softmax function. When math variable gets larger, it smoothens the distribution. This is because the derivative of the exponential function near smaller values are smaller compared to that at larger values. To put this into perspective, let’s write a simple function that demonstrates this observation visually. is a code translation of (2) above. Note that we could have made this more numerically stable by subtracting the maximum value from the logits. But for sake of simplicity, we implement the formula as-is. Now, we can see how different temperature values yield different softmax outputs. Here, we consider integer temperature values from 1 to 5, inclusive. From the graph, one can easily see that the higher the temperature value, the smoother the distribution becomes. In other words, it makes it easier for the student model to learn dark knowledge. This softmax variation is key to knowledge distillation. Since we now train the student model on both the soft label logits of the teacher model and the hard labels from the dataset, it is necessary to revisit what the loss function is. To cut to the chase, the loss function in knowledge distillation is a combined loss of weighted averages: the cross entropy between the student model outputs and labels, as well as the Kullback–Leibler divergence between the temperature-adjusted student and teacher model logits. More concisely, where math variable denotes the softmax function, math variable denotes softmax prime with adjusted temperature, math variable denotes the raw logits of the teacher model, and math variable is a weighted average parameter. While this might seem like a lot, in reality it is just the weighted average of the normal cross entropy loss that we would use in typical training, combined with the KL divergence loss between the temperature-adjusted softmax outputs of the student and teacher model. Intuitively, we can understand the KL divergence as an additional form of regularization that forces the student network to not only train on hard ground-truth labels, but also the dark knowledge coming from the outputs of the teacher model. Now that we have an overall idea of how knowledge distillation works, let’s start implementing knowledge distillation training. Before we move on, full disclaimer that this experiment did not go as I had expected. As you will see below, somehow the distilled student model ended up outperforming the teacher, which is something that normally does not happen. In a later section, I will try to provide some hypotheses as to why this happened. Let’s import necessary modules for this tutorial. We define a transformation we will be applying to each image. We can then create a training set and a testing set. Note that I didn’t have to download anything because I already had all my files on my local machine. Next, we create data loaders with a batch size of 64. Modeling is very dull and uninteresting here; we simply define a relatively large teacher model and a smaller student model. As per the experiment outlined in the original paper, we define a multi-layer perceptron with two hidden layers, each with 1200 units. The teacher network also uses dropout for regularization. Next up is the student model. The student model is a smaller version of the teacher network. It has two hidden layers, but 800 units as opposed to 1200. It also does not employ any dropout. To get a sense of how large the teacher model is in comparison to the student network, let’s count the number of trainable parameters in each. Below is a function that accomplishes this task. Now we can check how many parameters the teacher and student models have. From this comparison, we can see that the student model is roughly half the size of the teacher model. We could consider this percentage the degree of compression we wish to perform through knowledge distillation. This is where all the fun stuff happens. First, let’s implement the knowledge distillation loss function as defined in (3). Note that we don’t calculate the weighted average yet; we simply return the two loss sub-components. The first component, , is no different from loss value calculations we saw in typical classification tasks. is the added component, which is the KL divergence between the temperature-adjusted softmax outputs of the student and teacher models. Now that we have all the ingredients we need, let’s write the knowledge distillation training loop. In the loop, we invoke the function, apply a weighted average, and backpropagate on the combined loss. One technicality we have not touched upon earlier is the fact that we need to apply some scaling to the KL divergence loss. The gradient of the term, compared to , is smaller by a factor of math variable. Thus, we need to multiply math variable to correct this difference in magnitude. Let’s also write a training loop for just training the model as is. We will essentially conduct a controlled experiment where we compare the result of knowledge distillation and vanilla training. And here is a simple evaluation function we will use to check the performance of each model at the end of training. Since we are merely evaluating the model, we don’t need to keep gradients. First, we train the teacher model. After 10 epochs, the teacher model manages to get an accuracy of around 98 percent. Next, we train the student model using the typical training scheme without knowledge distillation. Using this method, the student model manages to get an accuracy of 98 percent; in fact, 0.1 percentage point higher than that of the teacher model. At this point, I already knew that something was off, but I decided to continue with the experiment. Finally, we train a brand new student model through knowledge distillation. And interestingly enough, the knowledge distilled student model records an accuracy score of 98.1 percent. Why might have been the case? There are several things I think were happening in this example. The first is that the teacher model did not have enough helpful dark knowledge. In this experiment, I trained both the teacher and student models for 10 epochs. In retrospect, the teacher model should have been trained for a longer period of time to achieve the best performance possible, and thus learn enough helpful dark knowledge that could then be distilled onto the student network. Had we used some huge ImageNet-based model on CIFAR 100, we would have probably seen more interesting results through distillation. This relates to my second hypothesis, which is that the dataset was perhaps too simple for the networks. The student network was able to achieve a 98 percent point accuracy by training on the hard labels. This perhaps suggests that what we used as the student network itself could have been a teacher network of its own. Perhaps the student model should have been an even smaller network than the one we experimented with here. Although our attempt at knowledge distillation did not quite go as we had expected, nonetheless I think it was a great opportunity for me to learn about an exciting area of research. I also think understanding knowledge distillation will be helpful when using larger transformer models. I personally prefer smaller models like DistilBERT than full-fledged super transformers like RoBERTA large. In using transformers and fiddling with them, I think understanding distillation will open up more possibilities for me, and at least give me more confidence when using distilled models for my experiments. I hope you’ve enjoyed reading this post. Catch you up in the next one!",1,0,0,0,0,1,0,0
Wonders of Monte Carlo,"I have been putting off with blog postsings lately, largely because I was preoccupied with learning new languages I decided to pick up out of whim. Although I’m still learning the basics of these languages, namely HTML, CSS, and Javascript, I’m enjoying the process. I still have no idea where this spontaneous journey will take me, but hopefully I can make use of it in one way or another. The topic for today’s post is Monte Carlo methods, something that I have been very interested in for a long time, admittedly because of its eye-catching name. Contrary to my original expectation, Monte Carlo is not named after an eponymous mathematician, but a gambling hot spot in Monaco. The logic behind this nomenclature is that the simulation of random outcomes, such as in the context of an unpredictable gambling game, is what Monte Carlo methods are best suited for. To present a more formal definition, Monte Carlo methods refer to a broad category of algorithms that use repeated random sampling to make estimations of unknown parameters. Basically, MC methods work by cleverly sampling from a distribution to estimate a variable of interest. This versatility is why MC method is such a powerful tool in the statistician’s arsenal. In today’s post, we will attempt to solve various bite-sized tasks using MC methods. These tasks will be of varying difficulty, but taken together, they will collectively demonstrate the useful applications of MC methods. Let’s get started with the first up on the list: estimating math expression. We all know from basic geometry that the value of math expression approximates to math expression. There are obviously various ways to derive this value. Archimedes famously used hexagons to estimate that the value of math expression lies between math expression. With later advances in math, mathematicians began to approach this problem from the angle of infinite series or products, the result of which were the Leibniz formula, Wallis product, Euler product, and the likes. And of course, modern computing now allows us to borrow the prowess of machinery to calculate this quantity with extreme accuracy. While the maths behind these derivations are fascinating, our approach will not take these routes; instead, we will use a crude Monte Carlo method. First, we draw a two-by-two square, inside of which we inscribe a circle of radius 1. For convenience purposes, let’s center this circle and square both at the origin. Next, we generate a series of random coordinates within the region of the square. Then, we count the percentage of dots that fall within the area of the cricle. Using a simple formula of proportions, we can calculate the area of the circle, through which we can then estimate the value of math expression. Before we get into the specifics of this algorithm, let’s see hwo this plays out in code. Now that we have the function ready, let’s try calling it with some input parameters. Below, we perform our little crude Monte Carlo simulation with a hundred randomly generated data points. Invoking the function returns the value of the estimation. The returned result is not abysmal, but we clearly can do a lot better. The reason behind this spotty estimation can be checked by drawing the plot of the Monte Carlo simulation, as shown below. As we can see, ten samples simply aren’t enough to really cover the entire area of the plane or the circle. Although we do get some points in the circle, it’s really hard to tell if the proportion of points in and out of the circle is going to be representative of the actual proportion of area between the circle and the square. Let’s push our MC algorithm to do a bit more by sampling more data points, this time with 100000 randomly generated points. As expected, with more data points, we get a better estimation of pi. Although the randomness created by the call in our function means that this estimation will fluctuate with each execution, the value is reliably close to the actual value of math expression, differing only by about 0.01 or less. If we draw the plot for our experiment, it is clear that our data points accurately capture the proportionality between the area of the square and the circle. We can systematically verify that more samples tend to yield better results by graphing the magnitude of the error of our estimation plotted against the number of random samples generated. The plot shows that, with larger sample sizes, the error quickly converges to around 0. Although the rate of convergence dramatically decreases after the first few iterations, the pattern of convergence is apparrent. So how does this work? The mechanism is extremely simple: if we were to randomly generate an infinite number of dots, the proportion of the number of dots that fall within the circle versus those that do not fall within it would converge to some constant, i.e. math expression. Why is this the case? Intuitively, the larger the area, the larger the number of points that fall into that area. Given this proportional relationship, the number of randomly generated points in an area after a simulation would mirror the actual area of the circle and the rectangle, hence the proportional expression above. By following this line of reasoning, we can then resort to Monte Carlo to generate these random points, after which we can make a reasonable estimation of math expression. But approximation is not the only domain in which Monte Carlo methods become useful–they can also be used to calculate complicated integrals. We all know from calculus class that integration can be difficult. Everyone has encountered integrals of varying monstrosity at one point in their lives, scrambling to solve it with integration by parts or some obscure, creative substitution, only to realize that everything wounds up in the middle of nowhere. Well, good news for all of us—Monte Carlo methods can be used to estimate the value of mind-pulverizing, complicated definite integrals. Let’s say we want to estimate the value of an integral of a function math expression over some domain math expression. Now assume that there is some probability density function math expression defined over math expression. Then, we can alter this integral as shown below. Notice that this integral can now be understood as an expected value for some continuous random variable. In other words, math expression collapses into the following expression. What does this tell us? This means that we can simply calculate an integral by randomly sampling the values of math expression such that math expression follows some probability distribution math expression. The probability distribution part simply ensures that values of math expression that are more probable are sampled more often than others. Intuitively, we are effectively taking a weighted mean of the values of math expression, which is the loose definition of expected values. Now, to simplify things a bit, we are going to take a look at an example that does not involve much probability distributions. Conside the following integal of sine, a classic in calculus 101: The reason why we chose this integral is that we know how to calculate it by hand. Therefore, we can match the accuracy of our crude Monte Carlo against an actual, known value. Let’s fist quickly compute this integral. Now time for Monte Carlo. Notice that there is no probability distribution explicitly defined over the domain of integration in our example. In other words, math expression simply follows a continuous uniform distribution, meaning that all values of math expression within math expression are equally likely. All we have to do, therefore, is to compute the expected value of the integrand by randomly generating a series of numbers within the specified domain, plug those values into the function math expression, and take their average. This is a very elementary function that simply generates a specified number of samples given within the domain . These numbers are then plugged into the function , after which an unweighted mean of these values are computed to approximate an integral. Let’s test the accuracy of this crude Monte Carlo method by using our example of math expression computed earlier. The result is a very poor approximation that is way off, most likely because we only used ten randomly generated numbers. Much like earlier, however, we would expect Monte Carlo to perform better with larger samples. The example we have analyzed so far was a very simple one, so simple that we would probably have been better off calculating the integral by hand than writing code. That’s why it’s time to put our crude Monte Carlo to the test with integrals more difficult to compute. Consider the following expression: One might try calculating this integral through substitution or integration by parts, but let’s choose not to for the sake of our mental health. Instead, we can model the integrand in Python and ask Monte Carlo to do the work for us. Concretely, this process might look as follows. We can now plug this function into our crude Monte Carlo and hope that the algorithm will provide us with an accurate estimation of this expression. So we have a number! But how do we know if this is an accurate estimation? Unlike in previous problems, where we already knew the true value of our estimate and measured the error of our simulation by comparing it with the known value, the true value of the integral expression is unknown in this problem because we have not evaluated the integral by hand. One way we can go about this dilemma is to calculate variance. Intuitively, if our estimate is indeed accurate, running the same Monte Carlo simulation would yield a value very similar to that of the previous. Conversely, if our estimate is inaccurate, the variance would be large, suggesting that our estimate has not converged to a value yet. Indeed, this is exactly what we attempted to visualize with the error plot above in our math expression estimation example. However, in most cases where Monte Carlo methods are used, we have no idea about the true value of the quantity we wish to estimate, like the complicated integral problem in this case, which is why we cannot simply calculate error by substracting our estimate from the true value. As we might recall, variance measures, quite simply, the degree of variability in our data. The well-known formula for variation goes as follows. Using this formula, let’s plot variance against the number of samples to see what effect increasing the sample size has on variance. The function accepts a list as an argument and returns the variance seen in the given data set. Now that we have this function ready, let’s use it to plot variance against the number of samples used in crude Monte Carlo integration Notice that variance quickly converges to near zero as the number of samples gets larger! This means that, even if we do not know the true value of the integral expression, we can now be confident that the output of the crude Monte Carlo will have converged to an approximation of the true value with sampling size as big as 1000, or even something like 400. This gives us more confidence in saying that the integral expression in (1) is approximates to 0.247. The crude Monte Carlo algorithm we employed here used simple random sampling to generate a series of random numbers to be used for our estimation. Crude Monte Carlo is powerful, but in a way it is inefficient because we have to sample large amounts to ensure that the resulting sample is representative, which is a condition that must be satisfied to produce a reliable estimate. There are a plethora of mathematical techniques that build on top of crude Monte Carlo to ensure that sampling is done correctly and more efficiently, such as importance sampling, but for the purposes of this post, we will stop here and move onto the last task: simulating random walk. The last task we will deal with in this post is simulating what is known as the drunkard’s walk, a version of which is introduced here. The drunkard’s walk is a type of random walk with a specified termination condition. As the name suggests, the drunkard’s walk involves a little story of an intoxicated man trying to reach (or avoid) some destination, whether that be a cliff or, in our case, a restroom. Because he is drunk, he cannot walk to the restroom in a straight path as a normal person would do; instead, he stumbles this way and that, therefore producing a random walk. Our goal here is to simulate this motion many times to estimate the probability that the man would successfully end up in the restroom to go about his business. This example was borrowed from this post by Zacharia Miller. Before we start typing up some code, let’s first lay down the ground rules of this simulation. First, we assume that the pub is modeled as a ten-by-ten grid, the bottom-left point defined as math expression and the top-right math expression. The drunkard will start his walk at his table, represented by the coordinate math expression. For each walk, function will generate a random number to determine the direrction of his movement. The magnitude of each walk is 1 by default. Beforer a walk is performed, we will invoke another function to check if his movements are legal, i.e. whether he stepped out of the boundary of the pub. If his moves are legal, we continue with the movement; if not, we stop and assume that the trial has yielded a failure. The goal of this random walk is to end up in the top-right portion of the pub, a square defined by coordinates math expression, and math expression. Now that we have established the basics of this game, let’s start coding away. Our little random walk simulator is now ready to go! Let’s perform a quick sanity check to see if the code works as expected. We see that the returned tuple contains the flag boolean value as well as a list containing the coordinates of the first ten steps the drunkard took in this experiment, which is exactly what we expected. Now it is time to put this Monte Carlo application to the test by simulating the walk many times and counting the instances of successes verses failures. The function calls the function as many times specified by . The function compiles all the results to return two pieces of information: the percentage of success, represented in decimals, and the average number of steps it took for the drunkard to reach the restroom. Notice that there is a 100-step cap, meaning if the drunkard was not able to find the restroom after a hundred steps, the trial was assumed a failure. We can verify the functionality of our design by calling the function. Cool! The Monte Carlo algorithm thus tells us that the probability of success is about 10 percent, which is a lot smaller thant I had personally anticipated. Think about how complicated it would have been to calculate this probability by hand. By simulating this game multiple times and counting the instances of successes, we can derive an estimation of the success rate of our particular random walk model. Let’s see what happens when we simulate the drunkard’s walk thirty times. In the particular instance that I have below, we see that the drunkard successfully reached the rest room four out of thirty attempts, which roughly equals the success probability of ten percent we saw earlier. By now, hopefully you have been convinced that Monte Carlo is a wonderful method of solving problems. Although the examples we looked at were mostly simple, these algorithms can easily be applied to solve much harder ones. Simply put, Monte Carlo uses a brute force approach to simulate a particular instance of a model multiple times. Through such repeated sampling, we are able to gain a better understanding of the parameters underlying the issue at hand, no matter how complex. This is a pattern that we saw with all three tasks we dealt with in today’s post. This sums up our post on Monte Carlo methods. In a future post, we will take a look at Markov Chain Monte Carlo, particularly Metropolis-Hastings, which uses the best of both worlds to analyze complicated probability distributions. I’m already excited for that post, because MCMC methods will bring together so many concepts that we have dealt with on this blog so far—ranging from Bayesian inference, probability distributions, Markov chains, and so many more. Catch you up on the next one!",0,0,0,0,1,0,0,0
Principal Component Analysis,"Principal component analysis is one of those techniques that I’ve always heard about somewhere, but didn’t have a chance to really dive into. PCA would come up in papers on GANs, tutorials on unsupervised machine learning, and of course, math textbooks, whether it be on statistics or linear algebra. I decided that it’s about time that I devote a post to this topic, especially since I promised one after writing about singular value decomposition on this blog some time ago. So here it goes. What do we need principal component analysis for? Or more importantly, what is a principal component to begin with? Well, to cut to the chase, PCA is a way of implementing dimensionality reduction, often referred to as lossy compression. This simply means that we want to transform some data living in high dimensional space into lower dimensions. Imagine having a data with hundreds of thousands of feature columns. It would take a lot of computing power to apply a machine learning model to fit the data and generate predictions. This is when PCA comes in: with PCA, we can figure out which dimensions are the most important and apply a transformation to compress that data into lower dimensions, making it a lot more tractable and easier to work with. And in case you’re still wondering, principal components refer to those new extracted dimensions used to newly represent data! Let’s derive PCA with some good old linear algebra tricks. I used Ian Goodfellow’s Deep Learning and a lecture slide from Columbia references for this post. The setup of a classic PCA problem might be summarized as follows. Suppose we have a dataset of math variable points, each living in math variable-dimensional space. In other words, math expression where Our goal is to find a way to compress the data into lower dimensional space math variable where math variable. We might imagine this as a transformation, i.e. the objective is to find a transformation So that applying math variable will yield a new vector math variable living in lower dimensional space. We can also imagine there being a reverse transformation or a decoding function math variable that achieves Because PCA is in essence a linear transformation, it is most natural to express and understand it as a matrix. Let’s define this transformation as math variable, and the matrix corresponding to the decoding math variable. In other words, PCA makes a number of assumptions to simplify this problem. The most important assumption is that each column of math variable is orthogonal to each other. As we will see later in an alternate derivation with statistics, this has to do with the notion of covariance. Another restriction is that the columns of math variable must have a Euclidean norm of one. This constraint is necessary for us to find a unique matrix math variable that achieves compression—otherwise, we could have any multiples, leading to an infinite number of such matrices. We make one more convenient assumption about the given data points, math variable. That is, math variable is assumed to have a mean of zero, i.e. math variable. If this is not the case, we can easily perform standardization by subtracting the mean from the data. With this setup in mind, let’s finally start the derivation. As said earlier, the goal of PCA is to compress data (and be able to uncompress it) with as little loss of information as possible. We don’t want to compress data in a haphazard fashion; instead, we want the compression scheme to be able to preserve the structure of the data as much as possible in its lower dimensional representation. From this, we can come up with the following equation: In other words, the goal is to find math variable that which minimizes the difference between the original data and the reconstructed data. Note that finding this optimal math variable amounts to finding math variable that most effectively compresses given data. Instead of the L2 norm, let’s consider the squared L2 norm for convenience purposes. Note that minimizing the L2 norm is equal to minimizing the squared L2 norm, so there is no semantic difference. By definition of vector transpose, we can now express the squared L2 norm versions of (3) as follows: where the second to last equality is due to the fact that math variable and math variable are both constants that denote the same value. Also, the argument of the minimum is with respect to math variable, we can omit the first term, which is purely in terms of math variable. It’s time to take derivatives. But in order to do so, we need to unpack math variable , since we have no idea how to take its derivative. Using (2), we can reorganize (4) as follows: The last equality is due to the fact that we constrained the columns of math variable to be unit vectors that are orthogonal to each other. Now we can take a derivative of the argument with respect to math variable and set it equal to zero to find the minimum. This tells us that the optimal way of compressing math variable is simply by multiplying it by the transpose of the decoding matrix. In other words, we have found the transformation math variable in (2). For those of you who are confused about how gradients and matrix calculus work, here is a very short explanation. First, notice that math variable is just a scalar, since math variable is a column vector. Taking a gradient with respect to this quantity would mean that we get another column vector of equal dimensions with math variable with the following elements: And we know how to go from there. The same line of thinking can be applied to think about the second term, math variable. We know that math variable is a row vector since its dot product with math variable should be possible dimensionally speaking. Then, we know that the gradient with respect to math variable should give each of the elements of math variable, but in column vector format—hence the need for a transpose. In general, the rule of thumb is that the gradient of a scalar with respect to a vector or a matrix should return a vector or matrix of the same dimension. Recall from (1) that reconstruction can be achieved by applying compression followed by a decoding operation: Since we know that math variable is just math variable and math variable is math variable by definition, we can express (1) in a different way. In retrospect, this is somewhat intuitive since math variable can roughly be thought of as a pseudo-orthonormal matrix—pseudo since there is no guarantee that it is a square matrix. Now, all that is left is to find the matrix math variable. The way to go about this is to reconsider (3), the notion of minimizing data loss, given our findings in (6). In other words, Instead of considering a single observation, here we consider the design matrix in its entirety. Note that math variable is a design matrix whose rows correspond to a single observation. And because we are dealing with matrices, the Euclidean norm was replaced with its matrix equivalent, the Frobenius norm. Observe that the first term math variable can safely be removed from the argument since it is a constant with respect to math variable; let’s also change the argument of the minimum to the maximum given the negative sign. The Frobenius norm of a real matrix can be calculated as Therefore, The last equality is due to a useful property of trace, which is that we can cycle the order of matrices without changing its value. Let’s consider a single column in math variable, denoted as math variable. You might also imagine this as a situation where math variable is one-dimensional, meaning we want to compress data into a single scalar value. It is not difficult to see that the trace of math variable, which is a scalar in the one-dimensional case, is maximized when math variable is an eigenvector of math variable with the largest eigenvalue. Generalizing this result back to math variable, we see that math variable is a matrix whose columns correspond to the eigenvectors of math variable in descending order. If you had prior exposure to PCA, you might know that the standard way of obtaining principal components is by calculating the covariance matrix of the data and finding its eigenvectors. Here, I attempt to present an explanation of how and why the procedure outlined in the preceding section is essentially achieving the same tasks, albeit through a different frame of thought. The unbiased sample covariance matrix is given by Of course, this is operating under the assumption that math variable has already been standardized such that the mean of the data is zero. You might be thinking that the formulation in (14) looks different from the one introduced previously on this post on SVD. In that particular post, I stated that covariance could be calculated as (14) and (15) certainly look different. However, under the hood, they express the same quantity. So in a nutshell, the conclusion we arrived at in the preceding section with the minimization of residual sums ultimately amounts to finding the covariance matrix and its eigenvectors. I found this to be the more dominant interpretation of PCA, since indeed it is highly intuitive: the goal of PCA is to find the axes—or the principal components—that which maximize the variance seen in the data. setosa.io has some excellent visualizations on the notion of covariance and how it relates to PCA, so I highly recommend that you go check it out. If were to derive PCA from the gecko with the covariance approach, we would be using an iterative approach to find a single principal component at a time. Specifically, our goal would be to find math variable that which maximizes Hence the problem is now framed as a constrained optimization problem. We use Lagrangians to solve constrained optimization. The intuition for the Lagrangian method is that the gradient of the constraint and the argument should be parallel to each other at the point of optimization. We go about this by taking the gradient of the argument with respect to math variable: Since 2 is just a constant, we can absorb it into math variable to form a more concise expression. Also, since the covariance matrix is by definition symmetric, we can simplify things further to end up with And once again, we have shown that the principal components are the eigenvectors of the covariance matrix. But the procedure outlined above can be used to find only one principal component, that is the eigenvector with the largest eigenvalue. How do we go about searching for multiple eigenvectors? This can be done, once again, with Lagrangians, with the added caveat that we will have more trailing terms in the end. Let’s elaborate on this point further. Here, we assume that we have already obtained the first component, math variable, and our goal is to find the next component, math variable. With induction, we can easily see how this analysis would apply to finding math variable. Simply put, the goal is to maximize math variable under the constraint that math variable is orthogonal to math variable while also satisfying the constraint that it is a unit vector. (In reality, the orthogonality constraint is automatically satisfied since the covariance matrix is symmetric, but we demonstrate this nonetheless.) Therefore, Using Lagrangians, In the last equality, we make a trivial substitution to simplify and get rid of the constant. We also use the fact that the covariance matrix is symmetric. If we left multiply (18) by math variable, But since math variable, the first two terms go to zero. Also, the last term reduces to math variable since math variable. This necessarily means that math variable. If we plug this result back into (18), we end up with the definition of the eigenvector again, but this time for math variable. Essentially, we iterate this process to find a specified number of principal components, which amounts to finding math variable number of eigenvectors of the sample covariance matrix. A while back, we discussed both eigendecomposition as well as singular value decomposition, both of which are useful ways of decomposing matrices into discrete factors. In this section, we will see how PCA is essentially a way of performing and applying these decomposition techniques under the hood. Recall that eigendecomposition is a method of decomposing matrices as follows: where math variable is a diagonal matrix of eigenvalues and math variable is a matrix of eigenvectors. PCA is closely related to eigendecomposition, and this should come as no surprise. Essentially, by finding the eigenvalues and eigenvectors of math variable, we are performing an eigendecomposition on the covariance matrix: Notice that math variable is a matrix of principal components. Of course, in this case, math variable is a square matrix of full rank; to apply dimension compression, we need to slice the first math variable entries of math variable. At any rate, it is clear that PCA involves eigendecomposition of the covariance matrix. Eigendecomposition can only be applied to matrices of full rank. However, there is a more generalized method for non-square matrices, which is singular value decomposition. Here is a blueprint of SVD: Where math variable is a matrix containing the roots of the eigenvalues, with appropriate dimensional configurations to accommodate the shape of the original matrix. We cannot perform eigendecomposition on math variable, which has no guarantee that it is square; however, SVD is definitely an option. Assume that math variable can be decomposed into math variable, math variable, and math variable. Then the covariance matrix becomes And we end up in the same place as we did in (25). This is no surprise given that the derivation of SVD involves eigendecomposition. In this post, we took a deep dive into the mathematics behind principal component analysis. PCA is a very useful technique used in many areas of machine learning. One of the most common applications is to apply PCA to a high-dimensional dataset before applying a clustering algorithm. This makes it easier for the ML model to cluster data, since the data is now aligned in such a way that it shows the most variance. Upon some more research, I also found an interesting paper that shows that there is a solid mathematical relationship between K-means clustering and PCA. I haven’t read the paper from top to bottom, but instead glossed over a summary of the paper on this thread on stack overflow. It’s certainly a lot of information to take in, and I have no intent of covering this topic in this already rather lengthy post on PCA. So perhaps this discussion will be tabled for a later time, as interesting as it seems. I hope you enjoyed reading this post. Amidst the chaos of the COVID19 pandemic, let’s try to stay strong and find peace ruminating over some matrices and formulas. Trust me, it works better than you might think.",0,1,0,0,1,0,0,0
BLEU from scratch,"Recently, I joined the Language, Information, and Learning at Yale lab, led by Professor Dragomir Radev. Although I’m still in what I would consider to be the incipient stages of ML/DL/NLP studies—meaning it will take time for me to be able to actively participate in an contribute to research and publications—I think it will be a great learning experience from which I can glean valuable insight into what research at Yale looks like. One of the first projects I was introduced to at the lab is domain-independent table summarization. As the name implies, the goal is to train a model such that it can extract some meaningful insight from the table and produce a human-readable summary. Members are the lab seem to be making great progress in this project, and I’m excited to see where it will go. In the meantime, I decided to write a short post on BLEU, a metric that I came across while reading some of the survey papers related to this topic. Let’s dive into it. Before going into code and equations, a high-level overview of what BLEU is might be helpful here. BLEU, which stands for Bilingual Evaluation Understudy, is an metric that was introduced to quantitatively evaluate the quality of machine translations. The motivation is clear: as humans, we are able to get an intuitive sense of whether or not a given translation is accurate and of high quality; however, it is difficult to translate this arbitrary linguistic intuition to train NLP models to produce better translations. This is where BLEU comes to the rescue. The way BLEU works is simple. Given some candidate translation of a sentence and a group of reference sentences, we use a bag-of-word approach to see how many occurences of BOWs co-occur in both the translation and reference sentences. BOW is a simple yet highly effective way of ensuring that the machine translation contains key phrases or words that reference translations also contain. In other words, BLEU compares candidate translations with human-produced, annotated reference translations and compares how many hits there are in the candidate sentence. The more BOW hits there are, the better the translation. Of course, there are many more details that go beyond this. For instance, BLEU is able to account for situations in which meaningless words are repeated throughout the machine translation to simply increase BOW hits. It can also penalize translations that are too short. By combining this BOW precision-based approach with some penalization terms, BLEU provides a robust means of evaluating machine translations. With this high-level overview in mind, let’s start implementing BLEU from scratch. First, let’s begin by defining some simple preprocessing and helper functions that we will be using throughout this tutorial. The first on the list is , which converts a given sentence into lowercase and splits it into tokens, which are, in this case, English words. We could make this more robust using regular expressions to remove punctuations, but for the purposes of this demonstration, let’s make this simpler. I decided to use anonymous functions for the sake of simplicity and code readability. Next, let’s write a function that creates n-grams from a given sentence. This involves tokenizing the given sentence using , then looping through the tokens to create a bag of words. And here is a quick sanity check of what we’ve done so far. The BLEU score is based on a familar concept in machine learning: precision. Formally, precision is defined as where math variable and math variable stand for true and false positives, respectively. In the context of machine translations, we can consider positives as roughly corresponding to the notion of hits or matches. In other words, the positives are the bag of word n-grams we can construct from a given candidate translation. True positives are n-grams that appear in both the candidate and some reference translation; false positives are those that only appear in the candidate translation. Let’s use this intuition to build a simple precision-based metric. First, we need to create some n-grams from the candidate translation. Then, we iterate through the n-grams to see if they exist in any of the n-grams generated from reference translations. We count the total number of such hits, or true positives, and divide that quantity by the total number of n-grams produced from the candidate translation. Below are some candidate sentences and reference translations that we will be using as an example throughout this tutorial. Comparing with , it is pretty clear that the former is the better translation. Let’s see if the simple precision metric is able to capture this intuition. And indeed that seems to be the case! However, the simple precision-based metric has some huge problems. As an extreme example, consider the following candidate translation. Obviously, is a horrible translation, but the simple precision metric fails to flag it. This is because precision simply involves checking whether a hit occurs or not: it does not check for repeated bag of words. Hence, the original authors of BLEU introduces modified precision as a solution, which uses clipped counts. The gist of it is that, if some n-gram is repeated many times, we clip its count through the following formula: Here, math variable refers to the number of hits we assign to a certain n-gram. We sum this value over all distinct n-grams in the candidate sentence. Note that the distinction requirement effectively weeds out repetitive translations such as we looked at earlier. math variable refers to the number of occurrences of a n-gram in the candidate sentence. For example, in , the unigram appears 13 times, and so math variable. This value, however, is clipped by math variable, which is the maximum number of occurrence of that n-gram in any one of the reference sentences. In other words, for each reference, we count the number of occurrence of that n-gram and take the maximum value among them. This can seem very confusing, but hopefully it’s clearer once you read the code. Here is my implementation using . Notice that we use a in order to remove redundancies. corresponds to math variable; corresponds to math variable. Using this modified metric, we can see that the is now penalized quite a lot through the clipping mechanism. But there are still problems that modified precision doesn’t take into account. Consider the following example translation. To us, it’s pretty obvious that is a bad translation. Although some of the key words might be there, the order in which they are arranged violates English syntax. This is the limitation of using unigrams for precision analysis. To make sure that sentences are coherent and read fluently, we now have to introduce the notion of n-grams, where math variable is larger than 1. This way, we can preserve some of the sequential encoding in reference sentences and make better comparison. The fact that unigrams are a poor way of evaluating translations becomes immediately clear once we plot the math variable in n-grams against modified precision. As you can see, precision score decreases as math variable gets higher. This makes sense: a larger math variable simply means that the window of comparison is larger. Unless whole phrases co-occur in the translation and reference sentences—which is highly unlikely—precision will be low. People have generally found that a suitable math variable value lies somewhere around 1 and 4. As we will see later, packages like use what is known as cumulative 4-gram BLEU score, or BLEU-4. The good news is that our current implementation is already able to account for different math variable values. This is because we wrote a handy little function, . By passing in different values to , we can deal with different n-grams. Now we’re almost done. The last example to consider is the following translation: This is obviously a bad translation. However, due to the way modified precision is currently being calculated, this sentence will likely earn a high score. To prevent this from happening, we need to apply what is known as brevity penalty. As the name implies, this penalizes short candidate translations, thus ensuring that only sufficiently long machine translations are ascribed a high score. Although this might seem confusing, the underlying mechanism is quite simple. The goal is to find the length of the reference sentence whose length is closest to that of the candidate translation in question. If the length of that reference sentence is larger than the candidate sentence, we apply some penalty; if the candidate sentence is longer, than we do not apply any penalization. The specific formula for penalization looks as follows: The brevity penalty term is multiplied to the n-gram modified precision. Therefore, a value of 1 means that no penalization is applied. Let’s perform a quick sanity check to see whether the brevity penalty function works as expected. Finally, it’s time to put all the pieces together. The formula for BLEU can be written as follows: First, some notation clarifications. math variable specifies the size of the bag of word, or the n-gram. math variable denotes the weight we will ascribe to the modified precision—math variable—produced under that math variable-gram configuration. In other words, we calculate the weighted average of log precision, exponentiate that sum, and apply some brevity penalty. Although this can sound like a lot, really it’s just putting all the pieces we have discussed so far together. Let’s take a look at the code implementation. The weighting happens in the part within the generator expression within the statement. In this case, we apply weighting across math variable that goes from to . Now we’re done! Let’s test out our final implementation with for math variable from 1 to 4, all weighted equally. The package offers functions for BLEU calculation by default. For convenience purposes, let’s create a wrapper functions. This wrapping isn’t really necessary, but it abstracts out many of the preprocessing steps, such as applying . This is because the BLEU calculation function expects tokenized input, whereas and are untokenized sentences. And we see that the result matches that derived from our own implementation! In this post, we took a look at BLEU, a very common way of evaluating the fluency of machine translations. Studying the implementation of this metric was a meaningful and interesting process, not only because BLEU itself is widely used, but also because the motivation and intuition behind its construction was easily understandable and came very naturally to me. Each component of BLEU addresses some problem with simpler metrics, such as precision or modified precision. It also takes into account things like abnormally short or repetitive translations. One area of interest for me these days is seq2seq models. Although RNN models have largely given way to transformers, I still think it’s a very interesting architecture worth diving into. I’ve also recently ran into a combined LSTM-CNN approach for processing series data. I might write about these topics in a future post. I hope you’ve enjoyed reading this post. Catch you up later!",1,0,0,1,0,0,0,0
Introduction to tf-idf,"Although I’ve been able to automate some portion of the blog workflow, there’s always been a challenging part that I wanted to further automate myself using deep learning: automatic tagging and categorization. Every post requires some form of YAML front matter, containing information such as the title, tag, and category of the blog post to be uploaded. Although I sometimes create new tags or categories if existing ones seem unfit, I only deal with a limited number of topics on this blog, which is why I’ve always thought that some form of supervised learning be able to automate the process by at least generating some possible tags for me. I’m currently in the process of preparing the data (my previous blog posts) and building a simple NLP document classification model for the job. However, NLP is a field that I’m admittedly not well-acquainted with, not to mention the fact that I’ve not bee posting a lot about deep learning implementations for a while now. So in today’s short post, I decided to write about tf-idf vectorization, which is a very simple yet powerful technique that is often used in routine tasks like document classification, where SOTA models aren’t really required. As always, this post is going to take a hands-on approach by demonstrating a simple way of implementing tf-idf vectorization from scratch. Let’s get started. tf-idf stands for term frequency-inverse document frequency. This is all there is to it—in fact, the formula for tf-idf can simply be expressed as where math variable denotes a single term; math variable, a singe document, and math variable, a collection of documents. So simply put, tf-idf is simply a product of the term frequency, denoted above as math variable, and inverse document frequency, math variable. All there is left, then, is to figure out what term frequency and inverse document frequency are. Without much explanation, you can probably guess what term frequency is: it simply indicates how frequently a word appeared in a given document. For example, if there were a total of 3 distinct words in a document (very short, I know), then each of the three words would have a tf score of math variable. Put differently, the sum of the tf vector for each document should sum to one. The definition of a tf score might be thus expressed as where the denominator denotes the count of all occurrences of the term math variable in document math variable, and the numerator represents the total number of terms in the document. Roughly speaking, inverse document frequency is simply the reciprocal of document frequency. Therefore, it suffices to show what document frequency is, since idf would immediately follow from df. Before getting into the formula, I think it’s instructive to consider the motivation behind tf-idf, and in particular what role idf plays in the final score. The motivation behind tf-idf commences from a simple question: how do we determine the semantic importance of a word in a set of documents? On one hand, words the appear a lot are probably worth paying attention to. For example, in one of my posts on Gaussian distributions, the word “Gaussian” probably appears many times throughout the post. A keyword probably appears frequently in the document; hence the need to calculate tf. On the other hand, there might be words that appear a lot, but aren’t really that important at all. For example, consider the word “denote.” I know that I use this word a lot before writing down equations or formulas, just for the sake of notational clarity. However, the word itself carries little information on what the post is about. The same goes for other words, such as “example,” “however,” and so on. So term frequency only doesn’t really tell us much; instead, we want to pay attention to words that occur frequently in a given document, but doesn’t appear a lot in others—such words are most likely to be unique keywords that potentially capture the gist of that document. Given this analysis, it isn’t difficult to see why tf-idf is designed the way it is. Although we give priority weight to words with high term frequency, we discount words that appear frequently across all documents by dividing tf by idf, or inverse document frequency. In short, document frequency tells us how frequently a given word appears throughout all documents; the inverse is the reciprocal of that quantity. In practice, we often apply a logarithm to prevent the idf score from exploding. Also, we add some smoothing to prevent division by zero. There seems to be many variations of how smoothing is implemented in practice, but here I present one way that seems to be adopted by scikit-learn. For other schemes, refer to this table on Wikipedia. However, this is a mere technically; the intuition we motivated earlier still applies regardless. With these ideas in mind, let’s go implement tf-idf vectorization in Python! In this section, we will develop a simple set of methods to convert a set of raw documents to tf-idf vectors, using a dummy dataset. Below are four documents (again, I know they’re short) that we will be using throughout this tutorial. The first step is to preprocess and tokenize the data. Although the specifics of preprocessing would probably differ from task to task, in this simple example, we simply remove all punctuations, change documents to lower case letters, and tokenize them by breaking down documents into a bag of words. Other possible techniques not discussed here include stemming and lemmatization. The function accepts as input a set of documents and removes all the punctuation in each document. Here is the result of applying our function to the dummy data. Next, we need to tokenize the strings by splitting them into words. In this process, we will also convert all documents to lower case as well. Note that works on each documents, not the entire collection. Let’s try calling the function with the first document in our dummy example. Finally, as part of the preprocessing step, let’s build the corpus. The corpus simply refers to the entire set of words in the dataset. Specifically for our purposes, the corpus will be a dictionary whose keys are the words and values are an ordinal index. Another way to think about the corpus in this context is to consider it as a word-to-index mapping. We will be using the indices to represent each word in the tf, idf, and tf-idf vectors later on in the tutorial. Because we have a very simple example, our corpus only contains 9 words. This also means that our tf-idf vectors for each document will also be a list of length 9. Thus it isn’t difficult to see how tf-idf vectorization can result in extremely high-dimensional matrices, which is why we often apply techniques such as lemmatization or PCA on the final result. Also note that published modules use sparse representations to minimize computational load, as we will later see with scikit-learn. Now it’s time to implement the first step: calculating term frequency. In Python, this simply amounts to looping through each document, creating a tf vector per iteration, and making sure that they are normalized as frequencies at the very end. In creating tf vectors for each document, we will be referencing the word-to-index mapping in our corpus. Let’s see what we get for the four documents in our dummy example. Due to floating point arithmetic, the decimals don’t look the most pleasing to the eye, but it’s clear that normalization has been performed as expected. Also note that we get 4 vectors of length 9 each, as expected. Next, it’s time to implement the idf portion of the vectorization process. In order to calculate idf, we first need a total count of each number in the entire document collection. A module that is perfect for this job is , which accepts as input an iterable and outputs a dictionary-like object whose values represent the count of each key. Let’s test in on our dummy dataset to see if we get the count of each tokenized word. This is precisely what we need to calculate idf. Recall the formula for calculating idf As noted earlier, the intuition behind idf was that important keywords probably appear only in specific relevant documents, whereas generic words of comparatively lesser importance appear throughout all documents. We transcribe (4) into code as follows: Now, we have the idf vectors for the nine terms in the dummy dataset. At this point, all there is left to do is to multiply the term frequencies with their corresponding idf scores. This is extremely easy, since we are essentially performing a dot product of the tf and idf vectors for each document. As a final step, we normalize the result to ensure that longer documents do not overshadow shorter ones. Normalizing is pretty simple, so we’ll assume that we have a function that does the job for now. Before we test the code, we obviously need to implement . This can simply done by obtaining the sum of the L2 norm of each vector, then dividing each element by that constant. Here is an easy contrived example we can do in our heads: And now we’re done! If let’s print the tf-idf vectors for each of the four documents in the dummy example. It seems about right, as all the vectors appear normalized and are of the desired dimensions. However, to really verify the result, it’s probably a good idea to pit our algorithm against scikit-learn’s implementation. In scikit-learn, the does all the job. To transform the data to tf-idf vectors, we need to create an instance of the and call its method, . And here are the results: There are several observations to be made about this result. First, note that the default return type of is a sparse matrix. Sparse matrices are a great choice since many of the entries of the matrix will be zero—there is probably no document that contains every word in the corpus. Therefore, sparse representations can save a lot of space and compute time. This is why we had to call on the result. Second, you might be wondering why the order of elements are different. This is because the way we built ordinal indexing in corpus is probably different from how scikit-learn implements it internally. This point notwithstanding, it’s clear that the values of each vectors are identical, disregarding the fact that the result produced by our algorithm has more decimal points due to floating point arithmetic. This was a short introductory post on tf-idf vectors. When I first heard about tf-idf vectors from a friend studying computational linguistics, I was intimidated. However, now that I have a project I want to complete, namely an auto-tagging and classification NLP model, I’ve mustered more courage and motivation to continue my study the basics of NLP. I hope you’ve enjoyed reading this post. Catch you up in the next one! (Yes, this is a trite ending comment I use in almost every post, so the idf scores for the words in these two sentences are going to be very low.)",0,0,0,1,0,0,0,0
Convex Combinations and MAP,"In a previous post, we briefly explored the notion of maximum a posteriori and how it relates to maximum likelihood estimation. Specifically, we derived a generic formula for MAP and explored how it compares to that for MLE. Today’s post is going to be an interesting sequel to that story: by performing MAP on the univariate Gaussian, we will show how MAP can be interpreted as a convex combination, thus motivating a more intuitive understanding of what MAP actually entails under the hood. Let’s jump right into it. The univariate Gaussian is a good example to work with because it is simple and intuitive yet also complex enough for meaningful analysis. After all, it is one of the most widely used probability distributions and also one that models many natural phenomena. With that justification firmly in mind, let’s take a look at the setup of the MAP of the mean for the univariate Gaussian. As always, we begin with some dataset of math variable independent observations. In this case, because we are dealing with the univariate Gaussian, each observations will simply be a scalar instead of a vector. In other words, Let’s assume that the random variable is normally distributed according to some parameter math variable. We will assume that the standard deviation of the random variable is given as math variable. We could have considered standard deviation to be a parameter, but since the goal of this demonstration is to conduct MAP estimation on the mean of the univariate Gaussian, we assume that the standard deviation is known. Next, we have to define a prior distribution for the parameter. Let’s say that math variable is also normally distributed around some mean math variable with a standard deviation of 1, as shown below. Recall that the goal of MAP is, as the same suggests, to maximize the posterior distribution. To derive the posterior, we need two ingredients: a prior and a likelihood function. We already have the first ingredient, the prior, as we have just defined it above. The last piece of the puzzle, then, is the likelihood function. Since we have assumed our data to be independently distributed, we can easily calculate the likelihood as follows: All that is left is to compute the posterior according to Baye’s formula for Bayesian inference. We can thus calculate the MAP estimate of math variable as shown below. The second equality is due to proportionality, whereby math variable is independent of math variable and thus can be removed from the argmax operation. The fourth equality is due to the monotonically increasing nature of the logarithmic function. We always love using logarithms to convert products to sums, because sums are almost always easier to work with than products, especially when it comes to integration or differentiation. If any of these points sounds confusing or unfamiliar, I highly recommend that you check out my articles on MAP and MLE. To proceed, we have to derive concrete mathematical expressions for the log likelihood and the log prior. Recall the formula for the univariate Gaussian that describes our data: Then, from (1), we know that the likelihood function is simply going to be a product of the univariate Gaussian distribution. More specifically, the log likelihood is going to be the sum of the logs of the Gaussian probability distribution function. There is the log likelihood function! All we need now is the log prior. Recall that the prior is a normal distribution centered around mean math variable with standard deviation of 1. In PDF terms, this translates to The log prior can simply be derived by casting the logarithmic function to the probability distribution function. Now we are ready to enter the maximization step of the sequence. To calculate the maximum of the posterior distribution, we need to derive the posterior and set the gradient equal to zero. For a more robust analysis, it would be required to show that the second derivative is smaller than 0, which is indeed true in this case. However, for the sake of simplicity of demonstration, we skip that process and move directly to calculating the gradient. Let’s rearrange the final equality in (7). From (8), we can finally derive an expression for math variable. This value of the parameter is one that which maximizes the posterior distribution. And we have derived the MAP estimate for the mean of the univariate Gaussian! Maximum a posteriori analysis is great and all, but what does the final result exactly tell us? While there might be many ways to interpret understand the result as derived in (9), one particular useful intuition to have relates to the concept of convex combinations. Simply put, a convex combination is a linear combination of different points or quantities in which the coefficients of the linear combinations add up to one. More concretely, We can also imagine that math variable and math variable are each math variable-dimensional vectors, and that a convex combination is simply a dot product of these two vectors given that the elements of math variable sum up to one. Why did I suddenly bring up convex combinations out of no where? Well, it turns out that the result in (9) in fact an instance of a convex combination of two points satisfying the form Indeed, it is not difficult to see that the coefficient of math variable and math variable add up to 1, which is precisely the condition for a linear combination to be considered convex. Now here is the important part: the implication of this observation is that we can consider the MAP estimate of parameter math variable as an interpolation, or more simply, some weighted average between math variable and math variable. This interpretation also aligns with the whole notion of Bayesian inference: our knowledge of the parameter is partially defined by the prior, but updated as more data is introduced. And as we obtain larger quantities of data, the relative importance of the prior distribution starts to diminish. Imagine that we have an infinite number of data points. Then, math variable will soley be determined by the likelihood function, as the weight ascribed to the prior will decrease to zero. In other words, Conversely, we can imagine how having no data points at all would cause the weight values to shift in favor of the prior such that no importance is ascribed to the MLE estimate of the parameter. In this short article, we reviewed the concept of maximum a posteriori and developed a useful intuition about its result from the perspective of convex combinations. Maximum a posteriori, alongside its close relative maximum likelihood estimation, is an interesting topic that deserives our attention. Hopefully through this post, you gained a better understanding of what the result of an MAP estimation actually means from a Bayesian point of view: a weighted average between the prior mean and the MLE estimate, where the weight is determined by the number of data points at our disposal. Also, I just thought that the name “convex conbinations” is pretty cool. The fancier the name, the more desire to learn—it is a natural human instinct indeed. I hope you enjoyed reading this article. In the next post, we will continue our discussion by exploring the concept of conjugate priors, which is something that we have always assumed as true and glossed over, yet is arguably what remains at the very core of Bayesian analysis.",0,0,0,0,1,0,0,0
On Expectations and Integrals,"Expectation is a core concept in statistics, and it is no surprise that any student interested in probability and statistics may have seen some expression like this: In the continuous case, the expression is most commonly presented in textbooks as follows: However, this variant might throw you off, which happened to me when I first came across it a few weeks ago: I mean, my calculus is rusty, but it kind of makes sense: the probably density function is, after all, a derivative of the cumulative density function, and so notationally there is some degree of coherency here. But still, this definition of the expected value threw me off quite a bit. What does it mean to integrate over a distribution function instead of a variable? After some research, however, the math gurus at Stack Exchange provided me with an answer. So here is a brief summary of my findings. The integral that we all know of is called the Riemann integral. The confusing integral is in fact a generalization of the Riemann integral, known as the Riemann-Stieltjes integral (don’t ask me how to pronounce the name of the Dutch mathematician). There is an even more general interpretation of integrals called the Lebesgue integral, but we won’t get into that here. First, let’s take a look at the definition. The definition of the integral is actually a lot simpler than what one might imagine. Here, math variable is a value that falls within the interval math variable. In short, we divide the interval of integration math variable into math variable infinitesimal pieces. Imagine this process as being similar to what we learn in Calculus 101, where integrals are visualized as an infinite sum of skinny rectangles as the limit approaches zero. Essentially, we are doing the same thing, except that now, the base of each rectangle is defined as the difference between math variable and math variable instead of math variable and math variable as is the case with the Riemann integral. Another way to look at this is to consider the integral as calculating the area beneath the curve represented by the parameterization math variable. This connection becomes a bit more apparent if we consider the fact that the Riemann integral is calculating the area beneath the curve represented by math variable. In other words, the Riemann-Stieltjes integral can be seen as dealing with a change of variables. You might be wondering why the Riemann-Stieltjes integral is necessary in the first place. After all, the definition of expectation we already know by heart should be enough, shouldn’t it? To answer this question, consider the following function: This cumulative mass function is obviously discontinuous since it is a step-wise function. This also means that it is not differentiable; hence, we cannot use the definition of expectation that we already know. However, this does not mean that the random variable math variable does not have an expected value. In fact, it is possible to calculate the expectation using the Riemann-Stieltjes integral quite easily, despite the discontinuity! The integral we wish to calculate is the following: Therefore, we should immediately start visualizing splitting up the domain of integration, the real number line, into infinitesimal pieces. Each box will be of height math variable and width math variable. In the context of the contrived example, this definition makes the calculation extremely easy, since math variable equals zero in all locations but the jumps where the discontinuities occur. In other words, We can easily extend this idea to calculating things like variance or other higher moments. A more realistic example might be the Dirac delta function. Consider a constant random variable (I know it sounds oxymoronic, but the idea is that the random variable takes only one value and that value only). In this case, we can imagine the probability density function as a literal spike in the sense that the PDF will peak at math variable and be zero otherwise. The cumulative density function will thus exhibit a discontinuous jump from zero to 1 at math variable. And by the same line of logic, it is easy to see that the expected value of this random variable is math variable, as expected. Although this is a rather boring example in that the expectation of a constant is of course the constant itself, it nonetheless demonstrates the potential applications of Riemann-Stieltjes. I hope you enjoyed reading this post. Lately, I have been busy working on some interesting projects. There is a lot of blogging and catching up to do, so stay posted for exciting updates to come!",0,0,0,0,1,0,0,0
Recommendation Algorithm with SVD,"I’ve been using a music streaming service for the past few weeks, and it’s been a great experience so far. I usually listen to some smoothing new age piano or jazz while I’m working, while I prefer K-pop on my daily commutes and bass-heavy house music during my workouts. Having processed these information through repeated user input on my part, the streaming application now regularly generates playlists each reflective of the three different genres of music that I enjoy most. This got me wondering: what is the underlying algorithm beind content selection and recommendation? How do prominent streaming services such as Netflix and Spotify provide recommendations to their users that seem to reflect their personal preferences and tastes? From a business perspective, these questions carry extreme significance since the accuracy of a recommendation algorithm may directly impact sales revenue. In this post, we will dive into this question by developing an elementary recommendation engine. The mechanism we will use to achieve this objective is a technique in linear algebra known as singular value decomposition or SVD for short. SVD is an incredibly powerful way of processing data, and also ties in with other important techniques in applied statistics such as principal component analysis, which we might also take a look at in a future post. Enough with the preface, let’s dive right into developing our model. Before we start coding away, let’s first try to understand what singular value decomposition is. In a previous post on Markov chains, we examined the clockwork behind eigendecomposition, a technique used to decompose non-degenerate square matrices. Singular value decomposition is similar to eigendecomposition in that it is a technique that can be used to factor matrices into distinct components. In fact, in deriving the SVD formula, we will later inevitably run into eigenvalues and eigenvectors, which should remind us of eigendecomposition. However, SVD is distinct from eigendecomposition in that it can be used to factor not only square matrices, but any matrices, whether square or rectangular, degenerate or non-singular. This wide applicability is what makes singular decomposition such a useful method of processing matrices. Now that we have a general idea of what SVD entails, let’s get down into the details. In this section, we take a look at the mathematical clockwork behind the SVD formula. In doing so, we might run into some concepts of linear algebra that requie us to understand some basic the properties of symmetric matrices. The first section is devoted to explaining the formula using these properties; the second section provides explanations and simple proofs for some of the properties that we reference duirng derivation. We might as well start by presenting the formula for singular value decomposition. Given some math expression-by-math expression matrix math expression, singular value decomposition can be performed as follows: There are two important points to be made about formula (1). The first pertains to the dimensions of each factor: math expression, math expression, math expression. In eigendecomposition, the factors were all square matrices whose dimension was identical to that of the matrix that we sought to decompose. In SVD, however, since the target matrix can be rectangular, the factors are always of the same shape. The second point to note is that math expression and math expression are orthogonal matrices; math expression, a diagonal matrix. This decomposition structure is similar to that of eigendecomposition, and this is no coincidence: in fact, formula (1) can simply be shown by performing an eigendecomposition on math expression and math expression. Let’s begin by calculating the first case, math expression, assuming formiula (1). This process looks as follows: The last equality stands since the inverse of an orthogonal matrix is equal to its transpose. Substituting math expression for math expression, equation (2) simplifies to And we finally have what we have seen with eigendecomposition: a matrix of independent vectors equal to the rank of the original matrix, a diagonal matrix, and an inverse. Indeed, what we have in (3) is an eigendecomposition of the matrix math expression. Intuitively speaking, because matrix math expression is not necessarily square, we calculate math expression to make it square, then perform the familiar eigendecomposition. Note that we have orthogonal eigenvectors in this case because math expression is a symmetric matrix—more specifically, positive semi-definite. We won’t get into this subtopic too much, but we will explore a very simple proof for this property, so don’t worry. For now, let’s continue with our exploration of the SVD formula by turning our attention from matrix math expression—a factor of eigendecomposition on math expression—to the matrix math expression. Much like we understood math expression as a factor of eigendecomposition, math expression can be seen as a factor of eigendecomposition, this time on the matrix math expression. Concretly, Notice the parallel between (2) and (4). It’s not difficult to see that, by symmetry, math expression is also going to be an orthogonal matrix containing the eigenvectors of math expression. The most important difference between math expression and math expression concerns dimensionality: while math expression is a math expression-by-math expression matrix, V is an math expression-by-math expression. This disparity originates from the fact that math expression itself is a rectangular matrix, meaning that the dimensions of math expression and math expression are also different. Another point that requires clarification pertains to math expression. Earlier, we made a substitution of math expression for math expression. This tells us that math expression contains the square roots of the eigenvalues of math expression and math expression, which, it is important to note, has identical non-zero eigenvalues. If this point brings confusion, I recommend that you peruse over the next subsection on linear algebra. Let’s conclude this section with the formula for singular value decomposition: Hopefully, now it is clear what math expression, math expression, and math expression are. Singular value decomposition can intuitively be thought of as a square root version of eigendecomposition, since essentially math expression and math expression are all derivatives that come from the “square” of a matrix, the two transpose multiples. This intuition also aligns with the fact that math expression is a diagonal matrix containing the square roots of eigenvalues of the transpose products. With these in mind, let’s get ready to build the recommendation model. In this optional section, we take a look at two mathematical propositions we referenced while motivating the SVD formula: first, that symmetric matrices have orthogonal eigenvectors; second, that math expression and math expression have identical non-zero eigenvalues. The proof for both of these statements are simple, but feel free to gloss over this section if you just want to see SVD at work instead of the mathematical details behind singular value decomposition. Let math expression be some symmetric matrix, i.e. math expression. Also assume that math expression has two distinct eigenvectors, math expression and math expression with corresponding eigenvalues math expression and math expression. With this setup, we start from the definition of eigenvectors and eigenvalues: If we apply transpose on both sides, We can legally multiply both sides by math expression, which results in the following: However, since math expression, Furthermore, we can use the fact that the eigenvalue corresponding to math expression is math expression. Then, Since math expression, the only way for (5) to make sense is if math expression—and this is exactly what we have been trying to show. Since math expression and math expression are two distinct eigenvectors of the symmetric matrix math expression, we have successfully shown that any two eigenvectors of math expression will be orthogonal, i.e. their dot product is going to be zero. Let’s start by assuming that math expression has some non-zero eigenvector math expression whose corresponding eigenvalue is math expression. Then, we have If we left multiply both sides by math expression, we get By the definition of an eigenvector, it is not difficult to see that math expression has an eigenvector math expression whose corresponding eigenvalue is math expression. In short, the reason why SVD works is that the eigenvalue matrix math expression can be obtained either way by performing an eigendecomposition of the matrix math expression or math expression. Now that we have a mathematical understanding of how singular value decomposition, let’s see how we can apply SVD to build a simple recommendation algorithm. This section will continue as follows. First, we examine SVD as a technique of data compression and dimensionality reduction. Next, we generate some toy data of movie reviews and apply SVD to see how we can build a simple function that gives movie recommendations to users given their movie ratings history. Let’s jump right in. Why is singular value decomposition so important? Sure, it should now be fairly clear that SVD is a decomposition technique that can be applied to any matrix, whether square or not, which in and of itself makes it a very powerful tool in the statistician’s arsenal. But the true beauty of singular value decomposition comes from the fact that we can perform data compression by extracting meaningful information from the given data. This process is otherwise known as dimensionality reduction, and it is one of the most common applications of singular value decomposition. Let’s see what this means with an example. Here is math expression, a target matrix for singuluar value decomposition. Calculating math expression, we get which is symmetric as we expect. We can calculate the eigenvalues of this matrix by finding the roots of the following characteristic polynomial: Since math expression in SVD is the diagonal matrix that contains the square roots of the eigenvalues of math expression, we can conclude that where math expression denotes the value of the math expressionth diagonal entry in math expression. Therefore, given the dimensionality of math expression, we can conclude that Next, we find the eigenvalues of math expression. This process can be performed by identifying the null space of the matrix math expression. For instance, given math expression, Given the orientation of this matrix, we see that By doing the same for math expression, we can construct the matrix math expression: Repeating the procedure for math expression to obtain the factor math expression, we can complete the singular value decomposition on A: The key to dimensionality reduction is that the first few columns of math expression, its corresponding eigenvalues in math expression, and the corresponding first few rows of math expression contain the most amount of information on matrix math expression. As we go down the diagonal entries of math expression, we see that the eigenvalues get smaller. The rule of thumb is that the smaller the eigenvalue, the lesser contribution it has on expressing data on math expression. In other words, we can obtain an approximation of math expression by extracting the first few columns and rows of each factor. For example, This may seem like a very clumsy way of approximating math expression. However, this is because the toy matrix we dealt with was a mere two-by-three matrix with only two non-zero entries in the diagonal of math expression. Imagine performing the same analysis on a much larger matrix, from which we extract math expression number of non-trivial entries of math expression. On scale, singular value decomposition becomes more powerful, as it allows large amounts of data to be processed in managable bites. This is more than enough theory on SVD. Now is finally the time to jump into building our recommendation model with singular value decomposition. In this section, we will generate some random data, namely the ratings matrix. The row of the ratings matrix can be interpreted as users; the columns, movies. In other words, math expression denotes the ratings the math expressionth user gave for the math expressionth movie. The example we will use was borrowed from this post by Zacharia Miller. Let’s quickly build this ratings matrix using and as shown below. Let’s first see what this matrix looks like. We can do this simply by calling the function and saving it to some variable. For notational consistency, let’s name this variable . Great! Now we have a matrix of binary numbers, where denotes the fact that the user liked the movie and the fact that they disliked it. We can make some cursory qualitative observations of this toy data. Note, for instance, that users who like Movie 2 also tend to like Movie 3. Also, User 6 and User 8 have identical prefernece for movies—perhaps they both like a particular genre, or tend to like the movie starred by some actor or actress. We would expect singular value decomposition to capture these observations in some way, albeit approximately. Now, let’s actually perform singular value decomposition on the ratings matrix. We could try to do this manually by hand, but let’s utilize the power of modern computing to save ourselves of the time and mental effort involved in calculating the eigenvalues and eigenvectors of a ten-by-ten matrix. Luckily for us, the module contains some excellent functionality to help us with singular value decomposition. Using this library, singular value decomposition can very simply be achieved with just a few lines of code. The parameters of the function are , the ratings matrix, and , the number of non-trivial entries of math expression to select for dimensionality reduction, as we have seen earlier. More technically speaking, corresponds to the number of “concepts” or dimensions that we will extract from the matrix. Let’s see what this means by actually running this function. Great! This is what dimensionality reduction means in the loosest sense. Instead of having 5 entries each row, as we had with the original ratings matrix , we now have 3 entries per row. In other words, the information on users has been compressed into three dimensions. Unlike in , where each column corresponded to some movie, we don’t really know what the columns of stand for. It might be some genre, actress, or any hidden patterns in the data set that we are not aware of. Regardless, what’s important here is that we can now understand data more easily in smaller dimensions. An impotant observation to make is that, as we have noted earlier, User 6 and User 8 have rows that are identical. While this should not be a surprise given that the two users had what seemed to be an identical taste in movies, it is still interesting to see how SVD is able to extract this information and display it onto a new axis. Next, let’s see what looks like. Shown above is the transpose of , which means that is just really math expression. What’s important here is that the five movies have also been reduced to three dimensions. We don’t really know what the columns of this matrix means; all we know is that it is some distillation and amalgamation of information about the ten users on some unknown axis. At any rate, the previous ten dimensional vectors have now been reduced to three dimensions, which is great news for us—as three dimensional beings, it’s always easier to visualize and deal with three dimensions or less than 10D. Movie 2 and Movie 3 do not look as similar as they did before on the ratings matrix. However, perhaps this is due to the fact that all entries of this matrix have pretty small values, and it is difficult to see how the difference between Movie 2 and 3 compares to, say, the distance between Movies 1 and 4. Perhaps we should scale this in terms of relative distances or plot it on a three dimensional space, which is exactly what we are going to in a moment. Before we jump into visualizations, however, let’s deal with the elephant in the room first: is it okay to simply chop off a few dimensions to reduce a high dimensional image to fit into three-dimensional space? To answer this question, let’s check the math expression matrix for this particular instance of singular value decomposition. Note that we already have the first three values of math expression in our hands given that in our instantiation of singular value decomposition. The information we lose pertains to the last two values, given by and . These values are smaller in order of magnitude compared to, for instance, the largest value of math expression, which is . This supports the idea that the information we lose amid dimensionality reduction is minimal. Alternatively, we can also see this by taking a look at the full, unreduced version of the matrix math expression or math expression. For example, the code snippet below displays the full version of the factor math expression. It is not difficult to see that the last few columns of math expression contain values so small that their contribution to data is going to be minimal at best. This is not the most mathematical way of presenting the concept of data—doing so would require us to take a look at other metrics such as covariance—but this basic analysis will suffice for our purposes for now. The takeaway is that dimensionality reduction is a meaningful way to extract important information from our data. Now that we have performed SVD on the ratings matrix, let’s move onto the last step: crafting a model for our recommendation algorithm. My personal pet theory is that using any word in conjunction with “algorithm” makes the concept sound more complex than it actually is. This is exactly what we are doing here, because in reality, our so-called algoithm for movie recommendations is going to be very simple. The intuition behind the recommendation system is distance calculation. Simply put, if users have similar movie preferences, the points representing the two users will appear to be close when plotted on a graph. Let’s see what this means by plotting using . This can be achieved with the following code. We can pass as an argument for the function to see a three-dimensional plot of users’ movie preferences, as shown below. Note that the points corresponding to User 6 and User 8 exactly overlap, which is why the points look darker despite being positioned near the corner of the plot. This is also why we can only count seven points in total despite having plotted eight data points. In short, this visualization shows how we might be able to use distance calculation to give movie recommendations to a new user. Assume, for instance, that we get a new suscriber to our movie application. If we can plot onto the space above, we will be able to see to whom User 10’s preference is most similar. This comparison is useful since User 10 will most likely like the movie that the other use also rated highly. We can also create a similar plot for movies instead of users. The plot is shown below: With some alteration of the viewing angle, now we see through visualization that Movies 2 and 3 are close, as we had expected from the original ratings matrix . This is an interesting result, and it shows just how powerful singular value decomposition is at extracting important patterns from given data sets. Now that we understand what SVD does for us, it’s time to code our recommender function that uses distance calculation to output movie recommendations. In this post, we will be using the dot product as a means of determining distance, although other metrics such as Euclidean distance would suit our purposes as well. An advantage of using the dot product is that it is computationally less expensive and easy to achieve with code, as shown below. The function recommends an number of movies given that the user rated highly. For example, let’s say some user really liked Movie 2 and is looking for two more movies that are similar to Movie 2. Then, we can simply call the function above by passing in appropriate arguments as follows. This function tells us that our movie application should recommend to our user Movies 3 and 4, in that order. This result is not surprising given the fact that we have already observed the closeness between Movies 2 and 3—if a user likes Movie 2, we should definitely recommend Movie 3 to them. Our algorithm also tells us that the distance between Movie 2 and 4 is also pretty close, although not as close as the distance between Movies 2 and 3. What is happening behind the scene here? Our function simply calculates the distance between the vector representation of each movies as a dot product. If we were to print the local variable array defined within the function, for instance, we would see the following result. This tells us how close Movies 0, 1, 3, and 4 are with Movie 2. The larger the dot product, the closer the movie; hence, the more compelling that recommendation. The function then sorts the array and outputs the first movies as a recommendation. Of course, we could think of an alternate implementation of this algorithm that makes use of the matrix instead of , but that would be a slightly different recommendation system that uses past user’s movie ratings as information to predict whether or not the particular individual would like a given movie. As we can see, SVD can be used in countless ways in the domain of recommendation algorithms, which goes to show how powerful it is as a tool for data analysis. In today’s post, we dealt primarily with singular value decomposition and its application in the context of recommendation systems. Although the system we built in this post is extremely simple, especially in comparison to the complex models that companies use in real-life situations, nonetheless our exploration of SVD is valuable in that we started from the bare basics to build our own model. What is even more fascinating is that many recommendation systems involve singular value decomposition in one way or another, meaning that our exploration is not detached from the context of reality. Hopefully this post has given you some intuition behind how recommendation systems work and what math is involved in those algorithms. On a tangential note, recently, I have begun to realize that linear algebra as a university subject is very different from linear algebra as a field of applied math. Although I found interest in linear algebra last year when I took the course as a first-year, studying math on my own has endowed me with a more holistic understanding of how the concepts and formulas we learned in linear algebra class can be used in real-life contexts. While I am no expert in pedagogy or teaching methodology, this makes me believe that perhaps linear algebra could be taught better if students were exposed to applications with appropriate data to toy around with. Just a passing thought. Anyhow, that’s enough blogging for today. Catch you up in the next one.",0,1,0,1,0,0,0,0
