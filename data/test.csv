title,body,machine_learning,deep_learning,analysis,pytorch,probability_distribution,from_scratch,linear_algebra,statistics
Gaussian Mixture Models,"We’ve discussed Gaussians a few times on this blog. In particular, recently we explored Gaussian process regression, which is personally a post I really enjoyed writing because I learned so much while studying and writing about it. Today, we will continue our exploration of the Gaussian world with yet another machine learning model that bears the name of Gauss: Gaussian mixture models. After watching yet another inspiring video by mathematicalmonk on YouTube, I meant to write about Gaussian mixture models for quite some time, and finally here it is. I would also like to thank ritvikmath for a great beginner-friendly explanation on GMMs and Expectation Maximization, as well as fiveMinuteStats for a wonderful exposition on the intuition behind the EM algorithm. Without further ado, let’s jump right into it. The motivating idea behind GMMs is that we can model seemingly complicated distributions as a convex combination of Gaussians each defined by different parameters. One visual analogy I found particularly useful is imagining Gaussians as some sort of hill or mountain on a contour map. If we have multiple hills adjacent to one another, we can essentially model the topography of the region as a combination of Gaussians. At peaks, we would see circular contour lines, but where the hills meet, we might see different patterns, most likely circular patterns overlapping with each other. The key point here is that the combination is convex; in other words, the mixing coefficient for each Gaussian should add up to one. If we consider GMM to be a generative model, then we can imagine the generating process as follows: At this point, for clarity’s sake, let’s introduce some notations and concretize what has been elaborated above. First, we define a categorical distribution that will represent the mixing coefficients described above. Let $\pi$ be an $m$-dimensional vector that parametrizes this categorical distribution. In other words, This means that we assume the data to have $m$ different clusters. Each $\pi_n$ is then a mixing coefficient that establishes the convexness of the linear combinations of the underlying Gaussians. Now we can sample the cluster index, denoted as $Z$, from the categorical distribution as shown below. Note that we use $z$ for the cluster index, since it is considered a latent variable—we don’t observe it directly, yet it is deeply involved in the data generation process. Quite simply, the probability that a data point belongs to the $k$th cluster is represented by $\alpha_k$. The last step to data generation, as outlined in the bullet points above, is sampling a point from the corresponding Gaussian. where $\mu_k$ and $\Sigma_k$ are the mean and covariance matrices that parameterize the $k$th Gaussian in the mixture model. Now that we have an idea of how GMMs can serve as a generative model that describes the underlying data generation process, let’s think about the marginal probability—that is, the probability that some point $x$ in the sample space was generated by the GMM. After all, what we observe is the final output, not the latent variables. Therefore, it would be convenient to be able to come up with some expression for the marginal distribution. We can come up with a simple expression using the law of total probability and marginalization. where $z$ can take values between 1 and $m$. Notice that we can thus simplify $p(z)$, as this is simply the categorical distribution for the mixing coefficients. In other words, We can also simplify the condition probability expression, since we already know that it follows a normal distribution. Therefore, we end up with And there we have it, the density function of the Gaussian mixture model! We have a convex combination of $m$ different Gaussian PDFs that compose the Gaussian mixture model. In the context of machine learning, the goal is to find the parameters of the model that best describe some given set of data. In the case of GMMs, this is no different. The parameters that we have to estimate are Of course, $\mu$ and $\Sigma$ are not single vectors or matrices, but $m$ collection of such objects. But for notational simplicity, I opted to write them as such as shown above. Given a collection of $n$ data points, denoted as $X$, we can now come up with the following expression: $\mathcal{L}$ denotes the likelihood function, and $\theta$ is a collection of all parameters that define the mixture model. In other words, All this is doing is that we are using the marginal distribution expression we derived earlier and applying that to a situation in which we have $N$ data points instead of just one, which is the context we have been operating in so far. We multiply the probabilities given the assumption of independence. From the perspective of maximum likelihood estimation, the goal then would be to maximize this likelihood to find the optimal parameters for $\pi, \mu$ and $\Sigma$. Before we attempt MLE with the likelihood function, let’s first try to calculate the log likelihood, as this often makes MLE much easier by decoupling products as summations. Let $\mathcal{l}(\theta) = \log( \mathcal{L}(\theta \vert X))$. And now we see a problem: the log is not applied to the inside of the function due to the summation. This is bad news since deriving this expression by $\theta$ will become very tedious. The result is for sure not going to look pretty, let’s try deriving the log likelihood by $\mu_k$ and set it equal to zero. At least one good news is that, for now, we can safely ignore the outer summation given the linearity of derivatives. We ultimately end up with the expression below: This is not pretty at all, and it seems extremely difficult, if not impossible, to solve analytically for $\mu_k$. This is why we can’t approach MLE the way we usually do, by directly calculating derivatives and setting them equal to zero. This is where Expectation Maximization, or EM algorithm comes in. Before we get into the details of what the EM algorithm is, it’s perhaps best to provide a very brief overview of how the EM algorithm works. A very simple way to understand EM is to think of the Gibbs sampler. Simply put, Gibbs sampling is a way of approximating some joint distribution given conditional distributions. The underlying idea was to sample from one distribution and use that sampled result to in turn generate another sample from the next conditional distribution. One might visualize this as a chain of circular dependence: if we obtain a sample from past samples, the new sample can then be used to generate the next sample. Repeat this process until convergence, and we are done. Turns out that the Gibbs sampler can be considered a specific flavor of the EM method. Although I am still far away from fully understanding the inner-workings of the EM algorithm, the underlying idea is clear: given some sort of dependence relationship, much like we saw in the case of Gibbs sampling above, we can generate some sample in one iteration and use that sample in the next. As we will see in this section, such a relationship can clearly be established, which is why EM is so commonly used in the context of Gaussian mixture models. Let’s begin by defining some quantities, the first being the posterior distribution. In other words, given some data, what is the probability that it will belong to a certain class? Using the definition of conditional probability, we can arrive at the following conclusion: This represents the probability that, given some point $x_a \in X$, the point belongs in the $k$th cluster. With this result, we can now rewrite the MLE calculation that we were performing earlier. Using the new $\gamma$ notation, we can thus simplify the result down to We can then simplify this expression to derive an expression for $\mu_k$. An important trick is here to use the fact that the covariance matrix is positive semi-definite. Therefore, the covariance matrix plays no role in driving the value down to zero. With some algebraic manipulations, we arrive at Let’s introduce another notational device to simplify the expresison even further. Let Recall that $\gamma_{a, k}$ was defined to be the posterior probability that a given point $x_a$ belongs to the $k$th cluster. Then, since we are essentially summing up this quantity across the entire $n$ data points in the dataset $X$, we can interpret $N_k$ to effectively be the number of points in the dataset that are assinged to the $k$th cluster. Then, we can now simplify the MLE estimate of the mean as But we can now observe something interesting. Notice that a$\mu_k$ depend on $\gamma$. In turn, $\gamma$ is defined in terms of $\mu_k$. This is the very circular dependency that we discussed earlier as we were introducing the EM algorithm and comparing it with the Gibbs sampler. Now it becomes increasingly apparent why the EM algorithm is needed to find a converging solution for the MLE estimates. We can take a similar approach to calculate the MLE of the other two remaining paramters, namely $\Sigma_k$ and $\pi_k$. The derivation is more complicated since $\Sigma$ is a matrix; $\pi$ is subject to constraints that apply to any categorical distribution: all elements must be positive and must sum up to one. For my own personal reference and the curious-minded, here is a link to a resource that contains the full derivation. But the fundamental idea is that we would commence from the log likelihood function and derive our way to the solution. The solutions are presented below: So the full picture is now complete: given the inter-dependence of derived quantities, we seek to optimize them using the Expectation Maximization algorithm. Specifically, the EM method works as follows: In today’s post, we took a deep dive into Gaussian mixture models. I find GMMs to be conceptually very intuitive and interesting at the same time. I’m also personally satisfied and glad that I have learned yet another ML/mathematical concept that starts with the word Gaussian. Much like how I felt when learning about Gaussian process regression, now I have an even greater respect for the Gaussian distribution, although I should probably be calling it normal instead, just like everybody else. I’m also happy that I was able to write this blog post in just a single day. Of course, this is in large part due to the fact that I had spent some time a few weeks ago studying this material, but nonetheless I think I’m starting to find the optimal balance between intern dev work and self-studying of math and machine learning. I hope you’ve enjoyed reading this post. In a future post, I will be implementing Gaussian mixture models in Python from scrach. Stay tuned for more!",1,0,0,0,0,0,0,1
Natural Gradient and Fisher,"In a previous post, we took a look at Fisher’s information matrix. Today, we will be taking a break from the R frenzy and continue our exploration of this topic, meshing together related ideas such as gradient descent, KL divergence, Hessian, and more. The typical formula for batch gradient descent looks something like this: This is the familiar gradient descent algorithm that we know of. While this approach works and certainly makes sense, there are definite limitations; hence the introduction of other more efficient algorithms such as SGD, Adam, and et cetera. However, these algorithms all have one thing in common: they adjust the parameter in the parameter space according to Euclidean distance. In other words, gradient descent essentially looks at regions that are some Euclidean distance away from the current parameter and chooses the direction of steepest descent. This is where the notion of natural gradients come into play: if our goal is to minimize the cost function, which is effectively equivalent to maximizing the likelihood, why not search within the distribution space of the likelihood function instead? After all, this makes more sense since gradient descent in parameter space is likely to be easily perturbed by the mode of parametrization, such as using precision instead of variance in a normal distribution, whereas searching in the distribution space would not be subject to this limitation. So the alternative to this approach would be to search the distribution space and find the distribution that which makes value of the cost function the smallest. This is the motivation behind the notion of a natural gradient. Now you might be wondering how all this has anything to do with the Fisher matrix, which we looked at in the previous post. Well, it turns out there are some deep, interesting questions to be posed and connections to be uncovered. If we’re going to search around the distribution space, one natural question to consider is what distance metric we will use for our search. In case of batch gradient descent, we used Euclidean distance. This made sense since we were simply measuring the distance between two parameters, which are effectively scalars or vector quantities. If we want to search the distribution space, on the other hand, we would have to measure the distance between two probability distributions, one that is defined by the previous parameter and the other defined by the newly found parameter after natural gradient descent. Well, we know one great candidate for this task right off the bat, and that is KL divergence. Recall that KL divergence is a way of quantifying the pseudo-distance between two probability distributions. The formula for KL divergence is shown below. And while we’re at it, let’s throw cross entropy and entropy into the picture as well, both for review and clarity’s sake: For a short, simple review of these concepts, refer to this previous article, or Aurelien Geron’s video on YouTube. In most cases, $p$ is the true distribution which we seek to model, while $q$ is some more tractable distribution at our disposal. In the classic context of ML, we want to minimize the KL divergence. In this case, however, we’re simply using KL divergence as a means of measuring distance between two parameters in defined within a distribution space. As nicely stated in layman’s term in this Medium article, … instead of “I’ll follow my current gradient, subject to keeping the parameter vector within epsilon distance of the current vector,” you’d instead say “I’ll follow my current gradient, subject to keeping the distribution my model is predicting within epsilon distance of the distribution it was previously predicting” I see this as an intuitive way of nicely summarizing why we’re using KL divergence in searching the distribution space, as opposed to using Euclidean distance in searching the parameter space. Now it’s time for us to connect the dots between KL divergence and Fisher’s matrix. Before we diving right into computations, let’s think about how or why these two concepts might be related at all. One somewhat obvious link is that both quantities deal with likelihood, or to be more precise, log likelihood. Due to the definition of entropy, KL divergence ends up having a log likelihood term, while Fisher’s matrix is the negative expected Hessian of the log likelihood function, or the covariance matrix of Fisher’s score, which is the gradient of the log likelihood. Either way, we know that likelihood is the fundamental bridge connecting the two. Let’s try to compute the KL divergence between $p(x \vert \theta)$ and $p(x \vert \theta’)$. Conceptually, we can think of $\theta$ as the previous point of the parameter and $\theta’$ as the newly updated parameter. In this context, the KL divergence would tell us the effect of one iteration of natural gradient descent. This time, instead of using integral, let’s try to simplify a bit by expressing quantities as expectations. We see the familiar log likelihood term. Given the fact that the Fisher matrix is the negative expected Hessian of the log likelihood, we should be itching to derive this expression twice to get a Hessian out of it. Let’s first obtain the gradient, then get its Jacobian to derive a Hessian. This derivation process was heavily referenced from Agustinus Kristiadi’s blog. Let’s do this one more time to get the Hessian. This conclusion tells us that the curvature of KL divergence is defined by Fisher’s matrix. In hindsight, this is not such a surprising result given that the KL divergence literally had a term for expected log likelihood. Applying the Leibniz rule twice to move the derivative into the integral, we quickly end up with Fisher’s matrix. At this point, you might be wondering about the implications of this conclusion. It’s great that KL divergence and the Fisher matrix are closely related via the Hessian, but what implication does it have for the gradient descent algorithm in distribution space? To answer this question, we first need to perform a quick multivariate second order Taylor expansion on KL divergence. Recall that the simple, generic case of multivariate Taylor expansion looks as follows: This is simply a generalization of the familiar univariate Taylor series approximation we saw earlier. (In most cases, we stop at the second order because computing the third order in the multivariate case requires us to obtain a three-dimensional symmetric tensor. I might write a post on this topic in the future, as I only recently figured this out and found it very amusing.) Continuing our discussion of KL divergence, let’s try to expand the divergence term using Taylor approximation. Here, $\delta$ is small distance in the distribution space defined by KL divergence as the distance metric. This can be a bit obfuscating notation-wise because of the use of $\theta’$ as our variable, assuming $\theta$ as a fixed constant, and evaluating the gradient and the Hessian at the point where $\theta’ = \theta$  since we want to approximate the value of KL divergence at the point where where $\theta’ = \theta + \delta$. But really, all that is happening here is that in order to approximate KL divergence, we’re starting at the point where $\theta’ = \theta$, and using the slope and curvature obtained at that point to approximate the value of KL divergence at distance $\delta$ away. Picturing the simpler univariate situation in the Cartesian plane might help. The bottom line is that the KL divergence is effectively defined by the Fisher matrix. The implication of this is that now, the gradient descent algorithm is subject to the constraint where $c$ is some constant. Now, the update rule would be To solve for the argument minima operation, we will resort to the classic method for optimization: Lagrangians. In this case, the Lagrangian would be This immediately follows from using the constraint condition. To make progress, let’s use Taylor approximation again, both on the term for the loss function and the KL divergence. The good news is that we have already derived the expression for the latter. Noting the fact that there are several constants in this expression, we can simplify this into To minimize this expression, we set its gradient equal to zero. Note that we are deriving with respect to $\delta$. Therefore, We are finally done with our derivation. This equation tells us that the direction of steepest descent is defined by the inverse of the Fisher matrix multiplied by the gradient of the loss function, up to some constant scaling factor. This is different from the vanilla batch gradient descent we are familiar with, which was simply defined as Although the difference seems very minor—after all, all that was changed was the addition of Fisher’s matrix—yet the underlying concept, as we have seen in the derivation, is entirely different. This was definitely a math-heavy post. Even after having written this entire post, I’m still not certain if I have understood the details and subtleties involved in the derivation. And even the details that I understand now will become confusing and ambiguous later when I return back to it. Hopefully I can retain most of what I have learned from this post. Before I close this post, I must give credit to Agustinus Kristiadi, whose blog post was basically the basis of this entire writing. I did look at a few Stack Overflow threads, but the vast majority of what I have written are either distillations or adaptations from their blog. It’s a great resource for understanding the mathematics behind deep learning. I hope you enjoyed reading this blog. See you in the next one!",1,0,0,0,0,0,0,1
A Step Up with  Variational Autoencoders,"In a previous post, we took a look at autoencoders, a type of neural network that receives some data as input, encodes them into a latent representation, and decodes this information to restore the original input. Autoencoders are exciting in and of themselves, but things can get a lot more interesting if we apply a bit of twist. In this post, we will take a look at one of the many flavors of the autoencoder model, known as variational autoencoders, or VAE for short. Specifically, the model that we will build in this tutorial is a convolutional variational Autoencoder, since we will be using convolutional layers for better image processing. The model architecture introduced in this tutorial was heavily inspired by the one outlined in François Chollet’s Deep Learning with Python, as well as that from a separate article on the Keras blog. Let’s start by importing the modules necessary for this demonstration. The objective of today’s task is to build an autoencoder model that produces MNIST hand-written digits. The hidden dimension, or the latent space of the model, is going to a random vector living in two-dimensional space. Let’s specify this setup, along with some other miscellaneous configurations, before we proceed with constructing the model architecture. It’s time to build our model… or not quite now. Before we start stacking layers for the encoder and the decoder, we need to define a sampling function that will perform the meat of the variational inference involved in VAE. Let’s start out by taking a look at the sampling function we will use to define one of the layers of the variational Autoencoder network. Simply put, the  above below takes as arguments  and  in the form of a bundled list. As you can guess from the name of the variables, these two  parameters refer to the mean and log variance of the random vector living in our predefined latent space. Note that we are assuming a diagonal Gaussian here: in other words, the covariance matrix of the multi-dimensional Gaussian is assumed to be diagonal, meaning that each elements of the vector are independent. If any of this sounds foreign to you, I recommend that you read this post on the Gaussian distribution. Let’s continue our discussion with the sampling function. The goal here is to sample a random vector in the latent space from the distribution specified by the two parameters, mean and log variance. The sampling process can be expressed as follows: where $\mu_z$ denotes the mean, corresponding to , $\epsilon$ denotes a tensor of random numbers sampled from the standard normal distribution, and $\sigma_z$ denotes the standard deviation (we will see how this is related to  in just a moment). Essentially, the goal here is to use a resampling technique such that we can sample from a standard normal distribution centered around mean 0 and a standard deviation of 1, but consequentially sample from a distribution of $z$ living in the latent space. If you are wondering how (1) translates to the return statement, then the following equation might resolve your curiosity. This is the promised elaboration on the relationship between log variance and standard deviation: Therefore, multiplying 0.5 is just a simple algebraic manipulation to morph log variance to standard deviation. The reason why we use log variance instead of just variance or standard deviation is to ensure numerical stability in computation. Now that this part has been cleared, let’s start stacking away layers! Just like the autoencoder, VAEs are composed of two discrete components: the encoder and the decoder. Here, we take a look at the first piece of the puzzle, the encoder network. There are several things to note about this model. First, I decided to use a  loop to simplify the process of stacking layers. Instead of repeating the same code over multiple lines, I found this approach to be more succinct and concise. Second, we define a custom layer at the end, shown as , that uses the  function we defined earlier. This is the final key that enables us to build an encoder model that receives as input a 28-by-28 image, then output a two-dimensional latent vector representation of that image to pass onto the decoder network. Below is the summary of what our model looks like. Note that the model outputs a total of three quantities: , , and . We need the first two parameters to later sample from the latent distribution; , of course, is needed to train the decoder. The decoder network looks similar to the the encoder, except that much of the architecture is in reverse order. Most notably, we use  to undo the convolution done by the encoder. This allows us to effectively scale up the input back to its original dimension, which is what we want to do with a generative model like a VAE. One subtly worth mentioning is the fact that we use a sigmoid activation in the end. This is because we want the pixel values of the output to be between 0 and 1, just as the original input was normalized before it was fed into the encoder network via division by 255. The summary of the decoder network is presented below: Now that we have both the encoder and the decode network fully defined, it’s time to wrap them together into one autoencoder model. This can simply achieved by defining the input as the input of the encoder—the normalized MNIST images—and defining the output as the output of the decoder when fed a latent vector. Concretely, this process might look as follows: Let’s look a the summary of the CVAE. Note that the encoder and the decoder look like individual layers in the grand scheme of the VAE architecture. We have almost everything we need, but there is one crucial step that is missing: compiling the model with an optimizer and a loss function. Normally, defining a loss function is very easy: in most cases, we  use pre-made loss functions that are available through the TensorFlow API, such as cross entropy or mean squared error. In the case of variational autoencoders, however, this is not such an easy task: how do we judge the robustness or the effectiveness of the decoder, which is essentially a generative algorithm? Of course, we could stop training once the figures it generates becomes reasonable, i.e. the mock MNIST digits it creates looks compelling to the human eye. However, this is a subjective metric at best, and we can’t expect there to be a ML engineer peering at the screen, looking at the outputs of the decoder per each epoch. To tackle this challenge, we need to dive into some math. Let’s take a look. First, let’s carefully review what our goal is for this task. The motivating idea behind variational autoencoders is that we want to model a specific distribution, namely the distribution of the latent space given some input. As you recall, this latent space is a two dimensional vector modeled as a multivariate diagonal Gaussian. Using Bayes’ theorem, we can express this distribution as follows: By now, it is pretty clear what the problem its: the evidence sitting in the denominator is intractable. Therefore, we cannot directly calculate or derive $p(z \vert x)$ in its closed form; hence the need for variational inference. The best we can do is to find a distribution $q(z \vert x)$ that best approximates $p(z \vert x)$. How do we find this distribution? Well, we know one handy concept that measures the difference or the pseudo-distance between two distributions, and that is Kullback-Leibler divergence. As we discussed in this post on entropy, KL divergence tells us how different two distributions are. So the goal here would be find a distribution that minimizes the following expression: Using the definition of conditional probability, we can simplify (4) as follows: The trick is to notice that $\log p(x)$ is a constant that can break out of the expectation calculation. Let’s continue by deriving an expression for the evidence term. A useful property to know about KL divergence is the fact that it is always non-negative. We will get into why this is the case in a moment. For now, let’s assume non-negativity to be true and transform (6) into an inequality: The term on the right of the inequality is known as the Evidence Lower Bound, or ELBO for short. Why are we interested in ELBO? First, note that $\log p(x)$, the evidence, is a constant. Therefore, minimizing KL divergence amounts to maximizing ELBO. This is the key to variational inference: instead of calculating the intractable integral in (3), we can find a distribution $q$ that which minimizes KL divergence by maximizing ELBO, which is a tractable operation. Let’s prove why KL divergence is always greater or equal to zero, which is a condition we assumed to be true in the derivation of ELBO above. For the sake of completeness, I present two ways of proving the same property. In the context of probability, Jensen’s inequality can be summarized as follows. Given a convex function $f(x)$, We won’t get into rigorous proofs here, but it’s not difficult to see why this inequality stands with some basic geometric intuition. Due to its bow-like shape, the expected value of a convex function evaluated across a given interval will always be greater or equal to the function evaluated at the expected value of the random variable.  How is Jensen’s inequality related to the non-negativity of KL divergence? Let’s return back to the definition of KL divergence. For simplicity and to reduce notational burden, we briefly depart from conditional probabilities $.(z \vert x)$ and return back to generic distributions $p$ and $q$. Notice that the definition of KL divergence itself is an expected value expression. Also, note that $- \log(x)$ is a convex function—$\log(x)$ itself is concave, but the negative sign flips the concavity the other way. With these observations in mind, we can apply Jensen’s inequality to derive the following: Therefore, we have shown that KL divergence is always greater or equal to zero, which was our end goal. There is another version of a proof that I found a lot more intuitive and easier to follow than the previous approach. This derivation was borrowed from this post. We start from the simple observation that a logarithmic function is always smaller than  a linear one. In other words, This is no rocket science, and one can easily verify (11) by simply plotting the two functions on a Cartesian plane. Using (11), we can proceed in a different direction from the definition of KL divergence. Once again, we have shown that KL divergence is positive! Proving this isn’t really necessary in the grand scheme of exploring the mathematics behind VAEs, yet I thought it would help to have this adjunctive section to better understand KL divergence and familiarize ourselves with some standard algebraic manipulations that are frequently invoked in many derivations. Let’s jump back into variational inference and defining the cost function with ELBO. Recall from the setup of our Variational Autoencoder model that we have defined the latent vector as living in two-dimensional space following a multivariate Gaussian distribution. It’s time to apply the ELBO equation to this specific context and derive a closed-form expression of our loss function. Let’s recall the formula for ELBO: After some rearranging, we can decompose ELBO into two terms, one of which is a KL divergence: Now, it’s finally time for us to dive deep into math: let’s unpack the closed form expression in (13). Note that the ELBO expression applies to just about any distribution, but since we chose a multivariate Gaussian to be the base distribution, we will see how it unfolds specifically in this context. Let’s begin by assuming the distribution of our models to be Gaussian. Namely, Because $q$ is an approximation of $p$, we naturally assume the same model for the approximate distribution: Now we can derive an expression for the negative KL divergence sitting in the ELBO expression: This may seem like a lot, but it’s really just plugging in the distributions into the definition of KL divergence as an expectation and using some convenient properties of logarithms to perform simple algebraic simplifications. To proceed further, observe that the first term is a constant that can escape out of the expectation: From the definition of variance and expectation, we know that Therefore, we can simplify (17) as follows: Let’s zoom in on the expected value term in (19). Our goal is to use (18) again so that we can flesh out another one half from that term. This can be achieved through some clever algebraic manipulation: But since the the expected value of $(\mu_q - \mu_p)^2$ is constant and that of $(z - \mu_q)$ is zero, We can now plug this simplified expression back into the calculation of KL divergence, in (19): Since we will standardize our input such that $\mu_p = 0$ and $\sigma_p = 1$, we can plug these quantities into (22) and show that We are almost done with deriving the expression for ELBO. I say almost, because we still have not dealt with the trailing term in (13): At this point, it is extremely useful to recall the definition of cross entropy, which is generically defined as follows: Therefore, we see that the trailing term in (13) is just a cross entropy between two distributions! This was a circumlocutions journey, but that is enough math we will need for this tutorial. It’s time to get back to coding. All that math was for this simple code snippet shown below: As you can see, this short code snippet shows, in essence, how we can define a compile a model with a custom loss function. In this case,  refers to the reconstruction loss, which is the cross entropy term we saw earlier. , as you might expect, simply refers to KL divergence. Notice how there is a  multiplying factor in the  expression, just like we did when we derived it in the section above. With some keen observations and comparisons, you will easily see that the code is merely a transcription of (13), with some minor differences given dimensionality. One important fact to note is that the gradient descent algorithm, by default, seeks to minimize the loss function. However, we discussed above how the objective of VAE is to maximize ELBO. Therefore, we modify ELBO into a loss function that is to be minimized by defining the loss function as the negative of ELBO. In other words, the cost function $J$ is defined as $- ELBO$; hence the difference in sign. It’s finally time to test the model. Let’s first begin with data preparation and preprocessing. Now, we should have the training and test set ready to be fed into our network. Next, let’s define a simple callback application using the  monitor so that training can be stopped when no substantial improvements are being made to our model. This was included because training a VAE can take some time, and we don’t want to waste computing resources seeing only submarginal increments to the model performance. Training begins! After 14 epochs, training has stopped, meaning that no meaningful improvements were being made. Let’s visualize the representation of the latent space learned by the VAE. Visualizing this representation is easy in this case because we defined the latent space to be two-dimensional; in other words, all points can be plotted on a Cartesian plane. Let’s take a look:  This plot shows us how each numbers are distributed across the latent space. Notice that numbers that belong to the same class seem to be generally clustered around each other, although there is a messy region in the middle. This is a reasonable result: while we would expect ones to be fairly easy to distinguish from, say, eights, numbers like zeros and sixes might look very similar, and hence appear mixed as a lump in the fuzzy region in the middle. One cool thing about VAEs is that we can use their learned representation to see how numbers slowly morph and vary across a specified domain. This is why VAEs are considered to be generative models: if we feed the VAE some two-dimensional vector living in the latent space, it will spit out a digit. Whether or not that digit appears convincing depends on the random vector the decoder was provided as input: if the vector is close to the learned mean, $\mu_q$, then the result will be convincing; if not, we might see a confusing blob of black and white. Let’s see what exactly is going on in the fuzzy region of the image, because that is apparently where all the digits mingle together and seem indistinguishable from one another. Put differently, if we vary the random vector little by little across that region, we will be able to see how the digit slowly morphs into another number.  How cool is that? We were able to get a VAE to show us how one digit can shift across a certain domain of the latent space. This is one of the many cool things we can do with a generative model like a variational autoencoder. In this post, we took a deep dive into the math behind variational autoencoders. It was a long journey, but definitely worth it because it exposed us to many core concepts in deep learning and statistics. At the same time, I found it fascinating to see how a model could learn from a representation to generate numbers, as we saw in the very last figure. In a future post, we will look at generative adversarial networks, or GANs, which might be considered as the pinnacle of generative models and a successor to autoencoders. GANs resemble autoencoders in that it is also composed of two models. One core difference, however, is that in GANs, the two models are in a competing relationship, whereas in autoencoders, the encoder and the decoder play distinct, complementary roles. If any of this sounds exciting, make sure to check out the next post. I hope you enjoyed reading. Catch you up in the next one!",0,1,0,0,0,0,0,0
Likelihood and Probability,"“I think that’s very unlikely.” “No, you’re probably right.” These are just some of the many remarks we use in every day conversations to express our beliefs. Linguistically, words such as “probably” or “likely” serve to qualify the strength of our professed belief, that is, we express a degree of uncertainty involved with a given statement. In today’s post, I suggest that we scrutinize the concept of likelihood—what it is, how we calculate it, and most importantly, how different it is from probability. Although the vast majority of us tend to conflate likelihood and probability in daily conversations, mathematically speaking, these two are distinct concepts, though closely related. After concretizing this difference, we then move onto a discussion of maximum likelihood, which is a useful tool frequently employed in Bayesian statistics. Without further ado, let’s jump right in. As we have seen in an earlier post on Bayesian analysis, likelihood tells us—and pardon the circular definition here—how likely a certain parameter is given some data. In other words, the likelihood function answers the question: provided some list of observed or sampled data \(D\), what is the likelihood that our parameter of interest takes on a certain value \(\theta\)? One measurement we can use to answer this question is simply the probability density of the observed value of the random variable at that distribution. In mathematical notation, this idea might be transcribed as: At a glance, likelihood seems to equal probability—after all, that is what the equation (1) seems to suggest. But first, let’s clarify the fact that \(P(D \mid \theta)\) is probability density, not probability. Moreover, the interpretation of probability density in the context of likelihood is different from that which arises when we discuss probability; likelihood attempts to explain the fit of observed data by altering the distribution parameter. Probability, in contrast, primarily deals with the question of how probable the observed data is given some parameter \(\theta\). Likelihood and probability, therefore, seem to ask similar questions, but in fact they approach the same phenomenon from opposite angles, one with a focus on the parameter and the other on data. Let’s develop more intuition by analyzing the difference between likelihood and probability from a graphical standpoint. To get started, recall the that This is the good old definition of probability as defined for a continuous random varriable \(X\), given some probability density function \(f(p)\) with parameter \(\theta\). Graphically speaking, we can consider probability as the area or volume under the probability density function, which may be a curve, plane, or a hyperplane depending on the dimensionality of our context. Unlike probability, likelihood is best understood as a point estimate on the PDF. Imagine having two disparate distributions with distinct parameters. Likelihood is an estimate we can use to see which of these two distributions better explain the data we have in our hands. Intuitively, the closer the mean of the distribution is to the observed data point, the more likely the parameters for the distribution would be. We can see this in action with a simple line of code. This code block creates two distributions of different parameters, \(N_1~(1, 0)\) and \(N_2~(0.5, 0.7)\). Then, we assume that a sample of value 1 is observed. Then, we can compare the likelihood of the two parameters given this data by comparing the probability density of the data for each of the two distributions. In this case, \(N_2\) seems more likely, i.e. it better explains the data \(X = 1\) since \(L(\theta_{N_1} \mid 1) \approx 0.4416\), which is larger than \(L(\theta_{N_2} \mid 1) \approx 0.2420\). To sum up, likelihood is something that we can say about a distribution, specifically the parameter of the distribution. On the other hand, probabilities are quantities that we ascribe to individual data. Although these two concepts are easy to conflate, and indeed there exists an important relationship between them explained by Bayes’ theorem, yet they should not be conflated in the world of mathematics. At the end of the day, both of them provide interesting ways to analyze the organic relationship between data and distributions. Maximum likelihood estimation, or MLE in short, is an important technique used in many subfields of statistics, most notably Bayesian statistics. As the name suggests, the goal of maximum likelihood estimation is to find the parameters of a distribution that maximizes the probability of observing some given data \(D\). In other words, we want to find the optimal way to fit a distribution to the data. As our intuition suggests, MLE quickly reduces into an optimization problem, the solution of which can be obtained through various means, such as Newton’s method or gradient descent. For the purposes of this post, we look at the simplest way that involves just a bit of calculus. The best way to demonstrate how MLE works is through examples. In this post, we look at simple examples of maximum likelihood estimation in the context of normal distributions. We have never formally discussed normal distributions on this blog yet, but it is such a widely used, commonly referenced distribution that I decided to jump into MLE with this example. But don’t worry—we will derive the normal distribution in a future post, so if any of this seems overwhelming, you can always come back to this post for reference. The probability density function for the normal distribution, with parameters \(\mu\) and \(\sigma\), can be written as follows: Assume we have a list of observations that correspond to the random variable of interest, \(X\). For each \(x_i\) in the sample data, we can calculate the likelihood of a distribution with parameters \(\theta = (\mu, \sigma)\) by calculating the probability densities at each point of the PDF where \(X = x_i\). We can then make the following statement about these probabilities: In other words, to maximize the likelihood simply means to find the value of a parameter that which maximizes the product of probabilities of observing each data point. The assumption of independence allows us to use multiplication to calculate the likelihood in this manner. Applied in the context of normal distributions with \(n\) observations, the likelihood function can therefore be calculated as follows: But finding the maximum of this function can quickly turn into a nightmare. Recall that we are dealing with distributions here, whose PDFs are not always the simplest and the most elegant-looking. If we multiply \(n\) terms of the normal PDF, for instance, we would end up with a giant exponential term. To prevent this fiasco, we can introduce a simple transformation: logarithms. Log is a monotonically increasing function, which is why maximizing some function \(f\) is equivalent to maximizing the log of that function, \(\log(f)\). Moreover, the log transformation expedites calculation since logarithms restructure multiplication as sums. With that in mind, we can construct a log equation for MLE from (3) as shown below. Because we are dealing with Euler’s number, \(e\), the natural log is our preferred base. Using the property in (3), we can simplify the equation above: To find the maximum of this function, we can use a bit of calculus. Specifically, our goal is to find a parameter that which makes the first derivative of the log likelihood function to equal 0. To find the optimal mean parameter \(\mu\), we derive the log likelihood function with respect to \(\mu\) while considering all other variables as constants. From this, it follows that Rearranging this equation, we are able to obtain the final expression for the optimal parameter \(\mu\) that which maximizes the likelihood function: As part 2 of the trilogy, we can also do the same for the other parameter of interest in the normal distribution, namely the standard deviation denoted by \(\sigma\). We can simplify this equation by multiplying both sides by \(\sigma^3\). After a little bit of rearranging, we end up with Finally, we have obtained the parameter values for the mean and variance of a normal distribution that maximizes the likelihood of our data. Notice that, in the context of normal distributions, the ML parameters are simply the mean and standard deviation of the given data point, which closely aligns with our intuition: the normal distribution that best explains given data would have the sample mean and variance as its parameters, which is exactly what our result suggests. Beyond the specific context of normal distributions, however, MLE is generally very useful when trying to reconstruct or approximate the population distribution using observed data. Let’s wrap this up by performing a quick verification of our formula for maximum likelihood estimation for normal distributions. First, we need to prepare some random numbers that will serve as our supposed observed data. We then calculate the optimum parameters \(\mu\) and \(\sigma\) by using the formulas we have derived in (5) and (6). We then generate two subplots of the log likelihood function as expressed in (4), where we vary \(\mu\) while keeping \(\sigma\) at  in one and flip this in the other. This can be achieved in the following manner. Executing this code block produces the figure below. From the graph, we can see that the maximum occurs at the mean and standard deviation of the distribution as we expect. Combining these two results, we would expect the maximum likelihood distribution to follow \(N~(\mu, \sigma)\) where \(\mu\) =  and \(\sigma\) =  in our code. And that concludes today’s article on (maximum) likelihood. This post was motivated from a rather simple thought that came to my mind while overhearing a conversation that happened at the PMO office. Despite the conceptual difference between probability and likelihood, people will continue to use employ these terms interchangeably in daily conversations. From a mathematician’s point of view, this might be unwelcome, but the vernacular rarely strictly aligns with academic lingua. In fact, it’s most often the reverse; when jargon or scholarly terms get diffused with everyday language, they often transform in meaning and usage. I presume words such as “likelihood” or “likely” fall into this criteria. All of this notwithstanding, I hope this post provided you with a better understanding of what likelihood is, and how it relates to other useful statistical concepts such as maximum likelihood estimation. The topic for our next post is going to be Monte Carlo simulations and methods. If “Monte Carlo” just sounds cool to you, as it did to me when I first came across it, tune in again next week. Catch you up in the next one.",0,0,0,0,0,0,0,1
A sneak peek at Bayesian Inference,"So far on this blog, we have looked the mathematics behind distributions, most notably binomial, Poisson, and Gamma, with a little bit of exponential. These distributions are interesting in and of themselves, but their true beauty shines through when we analyze them under the light of Bayesian inference. In today’s post, we first develop an intuition for conditional probabilities to derive Bayes’ theorem. From there, we  motivate the method of Bayesian inference as a means of understanding probability. Suppose a man believes he may have been affected with a flu after days of fever and coughing. At the nearest hospital, he is offered to undergo a clinical examination that is known to have an accuracy of 90 percent, i.e. it will return positive results to positive cases 90 percent of the time. However, it is also known that the test produces false positives 50 percent of the time. In other words, a healthy, unaffected individual will test positive with a probability of 50 percent. In cases like these, conditional probability is a great way to package and represent information. Conditional probability refers to a measure of the probability of an event occurring, given that another event has occurred. Mathematically, we can define the conditional probability of event \(A\) given \(B\) as follows: This equation simple states that the conditional probability of \(A\) given \(B\) is the fraction of the marginal probability \(P(B)\) and the area of intersection between those two events, \(P(A \cap B)\). This is a highly intuitive restatement of the definition of conditional probability introduced above: given that event \(B\) has already occurred, conditional probability tells us the probability that event \(A\) occurs, which is then synonymous to that statement that \(A \cap B\) has occurred. By the same token, we can also define the reverse conditional probability of \(B\) given \(A\) through symmetry and substitution. Notice that the numerator stays unchanged since the operation of intersection is commutative. Now let’s develop an intuition for conditional probabilities by applying it to our example of clinical trials and the potentially affected patient. The purported accuracy of the clinical test is 90 percent, which we might express as follows, using the conditional probability notation: By the same token, we can also express the information on false positives as shown below. This conditional probability statement espouses that, given an individual who is not sick, the test returns a false positive 50 percent of the time. Conditional probability provides us with an interesting way to analyze given information. For instance, let \(R\) be the event that it rains tomorrow, and \(C\) be the event that it is cloudy at the present moment. Although we are no experts in climatology and weather forecast, common sense tells us that since with the additional piece of information that current weather conditions are cloudy, we are inclined to believe that it will likely rain tomorrow, or in the near future. Like this, conditional probability allows us to update our beliefs on uncertainty given new information, and we will see in the later sections that this is the core idea behind Bayesian inference. Let’s return back to the example of the potential patient with a flu. Shortly afterwards at the hospital, the the man was convinced by the doctor and decided to take the clinical test, the result of which was positive. We cannot assume that the man is sick, however, since the test has a rather high rate of false positives as we saw earlier. In this situation, the parameter that is of interest to us can be expressed as In other words, given a positive test result, what is the probability that the man is actually sick? However, we have no means as of yet to directly answer this question; the two pieces of information we have are that \(P(\text{test +} \mid \text{sick}) = 0.9\), and that \(P(\text{test +} \mid \text{¬sick}) = 0.5\). To calculate the value of \(P(\text{sick} \mid \text{test +})\), we need Bayes’s theorem to do its trick. Let’s quickly derive Bayes’ theorem using the definition of conditional probabilities delineated earlier. Recall that Multiply \(P(B)\) and \(P(A)\) on both sides of (1) and (2) respectively to obtain the following result: Notice that the two equations describe the same quantity, namely \(P(A \cap B)\). We can use equivalence to put these two equations together in the following form. Equation (3) can be manipulated in the following manner to finally produce a simple form of Bayes’ theorem: We can motivate a more intricate version this rule by modifying the denominator. Given that \(A\) and \(B\) are discrete events, we can break down \(A\) as a union of intersections between \(A\) and \(B_i\), where \(B_i\) represents subsets within event \(B\). In concrete form, we can rewrite this as Additionally, we can rewrite the conditional probability \(P(A \cap B_i)\) in terms of \(P(B_i)\) and \(P(A \mid B_i)\) according to the definition of conditional probability we observed earlier. Applying these alterations to (4) to rewrite \(P(A)\) produces equation (5): This is the equation of Bayes’ theorem. In simple language, Bayes’ theorem tells us that the conditional probability of some subset \(B_k\) given \(A\) is equal to its relevant fraction within a weighted summation of the conditional probabilities \(A\) given \(B_i\). Although this equation may seem complicated at a glance, we can develop an intuition for this formula by reminding ourselves of the definition of conditional probabilities, as well as the fact that independent events can be expressed as a union of intersections. At the end of the day, Bayes’ theorem provides a powerful tool through which we can calculate a conditional probability in terms of its reverse, i.e. calculate \(P(B \mid A)\) by utilizing \(P(A \mid B)\). Why is this important at all? Let’s return back to our example of the potential patient. Recall that the conditional probability of our interest was while the pieces of information we were provided were This is where Bayes’ theorem comes in handy. Notice that we have expressed \(P(\text{sick} \mid \text{test +})\) in terms of \(P(\text{test +} \mid \text{sick})\) and \(P(\text{test +} \mid \text{¬sick}) P(\text{¬sick})\). From a statistics point of view, all we have to do now is conduct a random survey of the population to see the percentage of the demographic infected with the flu. Let’s say that 15 percent of the population has been affected with this flu. Plugging in the relevant value yields Using Bayes’ theorem, we are able to conclude that there is roughly a 24 percent chance that the man who tests positive on this examination is affected by the flu. That seems pretty low given the 90 percent accuracy of the test, doesn’t it? This ostensible discrepancy originates from the fact that the test has a substantial false positive of 50 percent, and also that the vast majority of the population is unaffected by the disease. This means that, if the entire population were to conduct this test, there would be more false positives than there would be true positives; hence the distortion in the value of the conditional probability. But what if the man were to take the same test again? Intuition tells us that the more test he takes, the more confident we can be on whether the man is or is not affected by the disease. For instance, if the man repeats the exam once and receives a positive report, the conditional probability that he is sick given two consecutive positive test results should be higher than the 24 percent we calculated above. We can see this in practice by reapplying Bayes’ theorem with updated information, as shown below: We see that the value of the conditional probability has indeed increased, lending credence to the idea that the man is sick. Like this, Like this, Bayes’ theorem is a powerful tool that can be used to calculate conditional probabilities and to update them continuously through repeated trials. From a Bayesian perspective, we begin with some expectation, or prior probability, that an event will occur. We then update this prior probability by computing conditional probabilities with new information obtained for each trial, the result of which yields a posterior probability. This posterior probability can then be used as a new prior probability for subsequent analysis. In this light, Bayesian statistics offers a new way to compute new information and update our beliefs about an event in probabilistic terms. Bayesian inference is nothing more than an extension of Bayes’ theorem. The biggest difference between the two is that Bayesian inference mainly deals with probability distributions instead of point probabilities. The case of the potential patient we analyzed above was a simple yet illuminating example, but it was limiting in that we assumed all parameters to be simple constants, such as \(0.9\) for test accuracy and \(0.5\) for false positive frequency. In reality, most statistical estimates exist as probability distributions since there are limitations to our ability to measure and survey data from the population. For example, a simple random sampling of the population might reveal that 15 percent of the sample population is affected with the flu, but this would most likely produce a normal distribution with mean centered around 0.15 instead of a point probability. From a Bayesian standpoint, we would then replace the point probability in our example above with an equation for the distribution, from which we can proceed with the Bayesian analysis of updating our prior with the posterior through repeated testing and computation. Bayes’ theorem, specifically in the context of statistical inference, can be expressed as where \(D\) stands for observed or measured data, \(\theta\) stands for parameters, and \(f\) stands for some probability distribution. In the language of Bayesian inference, \(f(\theta \mid D)\) is the posterior distribution for the parameter \(\theta\), \(f(D \mid \theta)\) is the likelihood function that expresses the likelihood of having parameter \(\theta\) given some observed data \(D\), \(f(\theta)\) is the prior distribution for the parameter \(\theta\), and \(f(D)\) is evidence, the marginal probability of seeing the data, which is determined by summing or integrating across all possible values of the parameter, weighted by how strongly we believe in those particular values of \(\theta\). Concretely, Notice that this is not so different from the expansion of the denominator we saw with Bayes’ theorem, specifically equation (5). The only difference here is that the integral takes continuous probability density functions into account, as opposed to discrete point probabilities we dealt with earlier. If we temporarily disregard the constants that show up in (6), we can conveniently trim down the equation for Bayesian inference as follows: This idea is not totally alien to us—indeed, this is precisely the insight we gleaned from the example of the potential patient. This statement is also highly intuitive as well. The posterior probability would be some mix of our initial belief, expressed as a prior, and the data newly presented, the likelihood. Bayesian inference, then, can be understood as a procedure for incorporating prior beliefs with evidence in order to derive an updated posterior. What makes Bayesian inference such a powerful technique is that the derived posterior can themselves be used as a prior for subsequent inference conducted with new data. To see Bayesian inference in action, let’s dive into the most classic, beaten-to-death yet nonetheless useful example in probability and statistics: the coin flip. This example was borrowed from the following post. Assume that we have a coin whose fairness is unknown. To be fair, most coins are approximately fair (no pun intended) given the physics of metallurgy and center of mass, but for now let’s assume that we are ignorant of coin’s fairness, or the lack thereof. By employing Bayesian inference, we can update our beliefs on the fairness of the coin as we accumulate more data through repeated coin flips. For the purposes of this post, we will assume that each coin flip is independent of others, i.e. the coin flips are independent and identically distributed. Let’s start by coming up with a model representation of the likelihood function, which we might recall is the probability of having a parameter value of \(\theta\) given some data \(D\). It is not difficult to see that the best distribution for the likelihood function given the setup of the problem is the binary distribution since each coin flip is a Bernoulli trial. Let \(X\) denote a random variable that represents the number of tails in \(n\) coin flips. For convenience purposes, we define 1 to be heads and 0 to be tails. Then, the conditional probability of obtaining \(k\) heads given a fairness parameter \(\theta\) can be expressed as We can perform a quick sanity check on this formula by observing that, when \(\theta = 0\), the probability of observing \(k\) heads diminishes to 0, unless \(k = 0\), in which case the probability becomes 1. This behavior is expected since \(\theta = 0\) represents a perfectly biased coin that always shows tails. By symmetry, the same logic applies to a hypothetical coin that always shows heads, and represents a fairness parameter of 1. Now that we have derived a likelihood function, we move onto the next component necessary for Bayesian analysis: the prior. Determining a probability distribution for the prior is a bit more challenging than coming up with the likelihood function, but we do have certain clues as to what characteristics our prior should look possess. First, the domain of the prior probability distribution should be contained within \(\). This is because the range of the fairness parameter \(\theta\) is also defined within this range. This constraint immediately tells us that where \(f(x)\) is represents the probability density function that represents the prior. Recall that some of the other functions we have looked at, namely binomial, Poisson, Gamma, or exponential are all defined within the unclosed interval \(\), making it unsuitable for our purposes. The Beta distribution nicely satisfies this criterion. The Beta distribution is somewhat similar to the Gamma distribution we analyzed earlier in that it is defined by two shape parameters, \(\alpha\) and \(\beta\). Concretely, the probability density function of the Beta distribution goes as follows: The coefficient, expressed in terms of a fraction of Gamma functions, provides a definition for the Beta function. The derivation of the Beta distribution and its apparent relationship with the Gamma function deserves an entirely separate post devoted specifically to the said topic. For the purpose of this post, an intuitive understanding of this distribution and function will suffice. A salient feature of the Beta distribution that is domain is contained within \(\). This means that, application-wise, the Beta distribution is most often used to model a distribution of probabilities, say the batting average of a baseball player as shown in this post. It is also worth noting that the Beta function, which serves as a coefficient in the equation for the Beta PDF, serves as a normalization constant to ensure that integrating the function over the domain \(\) would yield 1 as per the definition of a PDF. To see this, one needs to prove This is left as an exercise for the keen reader. We will revisit this problem in a separate post. Another reason why the Beta distribution is an excellent choice for our prior representation is that it is a conjugate prior to the binomial distribution. Simply put, this means that using the Beta distribution as our prior, combined with a binomial likelihood function, will produce a posterior  that also follows a Beta distribution. This fact is crucial for Bayesian analysis. Recall that the beauty of Bayesian inference originates from repeated applicability: a posterior we obtain after a single round of calculation can be used as a prior to perform the next iteration of inference. In order to ensure the ease of this procedure, intuitively it is necessary for the prior and the posterior to take the same form of distribution. Conjugate priors streamline the Bayesian process of updating our priors with posteriors by ensuring that this condition is satisfied. In simple language, mathematicians have found that certain priors go well with certain likelihoods. For instance, a normal prior goes along with a normal likelihood; Gamma prior, Poisson likelihood; Gamma prior, normal likelihood, and so on. Our current combination, Beta prior and binomial likelihood, is also up on this list. To develop some intuition, here is a graphical representation of the Beta function for different values of \(\alpha\) and \(\beta\). This code block produces the following diagram. Graphically speaking, the larger the value of \(\alpha\) and \(\beta\), the more bell-shaped it becomes. Also notice that a larger \(\alpha\) corresponds to a rightward shift, i.e. a head-biased coin; a larger \(\beta\), a tail-oriented one. When \(\alpha\) and \(\beta\) take the same value, the local extrema of the Beta distribution is established at \(\theta = 0.5\), when the coin is perfectly fair. Now that we have established the usability of the Beta function as a conjugate prior to the binomial likelihood function, let’s finally see Bayesian inference at work. Recall the simplified version of Bayes’ theorem for inference, given as follows: For the prior and the likelihood, we can now plug in the equations corresponding to each distribution to generate a new posterior. Notice that \(D\), which stands for data, is now given in the form \((k, n)\) where \(k\) denotes the number of heads; \(n\), the total number of coin flips. Notice also that constants, such as the combinatorial expression or the reciprocal of the Beta function, can be dropped since we are only establishing a proportional relationship between the left and right hand sides. Further simplifications can be applied: But notice that this expression for the posterior can be encapsulated as a Beta distribution since Therefore, we started from a prior of \(f_B(\theta; \alpha, \beta)\) to end up with a posterior of \(f_B(\theta; k + \alpha, n - k + \beta)\). This is an incredibly powerful mechanism of updating our beliefs based on presented data. This process also proves that, as purported earlier, the Beta distribution is indeed a conjugate prior of a binomial likelihood function. Now, it’s time to put our theory to the test with concrete numbers. Suppose we start our experiment with completely no expectation as to the fairness of the coin. In other words, the prior would appear to be a uniform distribution, which is really a specific instance of a Beta distribution with \(\alpha = \beta = 0\). Presented below is a code snippet that simulates 500 coin flips, throughout which we perform five calculations to update our posterior. Executing this code block produces the following figure. This plot shows us the change in our posterior distribution that occurs due to Bayesian update with the processing of each data chunk. Specifically, we perform this Bayesian update after  trials. When no coin flips are performed, as shown in the first subplot, the prior follows a uniform distribution as detailed above. As more coin tosses are performed, however, we start to develop an understanding of the fairness of the coin. When we only have a few data points, the more probability there is that we obtain skewed data, which is why the mean estimate of our posterior seems skewed as well. However, with a larger number of trials, the law of large numbers guarantees that we will eventually be able to identify the value of our parameter \(\theta\), which is indeed the case. The key takeaway from this code block is the line . This is all the Bayesian method there is in this updating procedure. Notice that this line of code directly corresponds to the formula for the updated Beta posterior distribution we found earlier, which is \(k\) refers to , \(n\) corresponds to , and both \(\alpha\) and \(\beta\) are set to  in order to take into account the initial prior which tends to a uniform distribution. An interesting observation we can make about this result is that the variance of the Beta posterior decreases with more trials, i.e. the narrower the distribution gets. This is directly reflective of the fact that we grow increasingly confident about our estimate of the parameter with more tosses of the coin. At the end of the 500th trial, we can conclude that the coin is fair indeed, which is expected given that we simulated the coin flip using the command . If we were to alter the argument for this method, say , then we would expect the final result of the update to reflect the coin’s bias. Bayes’ theorem is a powerful tool that is the basis of Bayesian statistical analysis. Although our example was just a simple coin toss, the sample principle and mechanism can be extended to countless other situations, which is why Baye’s theorem remains highly relevant to this day, especially in the field of machine learning and statistical analysis. Bayesian statistics presents us with an interesting way of understanding probability. The classical way of understanding probability is the frequentist approach, which purports that a probability for an event is the limit of its frequency in infinite trials. In other words, to say that a coin is fair is to say that, theoretically, performing an infinite number of coin flips would result in 50 percent heads and 50 percent tails. However, the Bayesian approach we explored today presents a drastically different picture. In Bayesian statistics, probability is an embodiment of our subjective beliefs about a parameter, such as the fairness of a coin. By performing trials, infinite or not, we gain more information about the parameter of our interest, which affects the posterior probability. Both interpretations of probability are valid, and they help complement each other to help us gain a broader understanding of what the notion of probability entails. I hope this post gave you a better understanding as to why distributions are important—specifically in the context of conjugate priors. In a future post, we will continue our exploration of the Beta distribution introduced today, and connect the dots between Beta, Gamma, and many more distributions in the context of Bayesian statistics. See you in the next one.",0,0,0,0,1,0,0,1
"Beta, Bayes, and Multi-armed Bandits","Recently, I fortuitously came across an interesting blog post on the multi-armed bandit problem, or MAB for short. I say fortuitous because the contents of this blog nicely coincided with a post I had meant to write for a very long time: revisiting the Beta distribution, conjugate priors, and all that good stuff. I decided that the MAB would be a refreshing way to discuss this topic. “Bayesian” is a buzz word that statisticians and ML people love anyway, me shamelessly included. In this post, we will start off with a brief introduction into what the MAB problem is, why it is relevant, and how we can use some basic Bayesian analysis with Beta and Bernoulli distributions to derive a nice sampling algorithm, known as Thompson sampling. Let’s dive right into it. The multi-armed bandit problem is a classical gambling setup in which a gambler has the choice of pulling the lever of any one of $k$ slot machines, or bandits. The probability of winning for each slot machine is fixed, but of course the gambler has no idea what these probabilities are. To ascertain out the values of these parameters, the gambler must learn through some trial and error. Given this setup, what is the optimal way of going about the problem? One obvious way is to start randomly pulling on some slot machines until they get a rough idea of what the success probabilities are. However, this is obviously not the most systematic approach, and there is not even a clear guideline as to how they should go about pulling these levers. Here is where the Bayesian approach comes in handy. It isn’t difficult to frame this as a Bayesian problem. Given a pull of the slot machine, the result of which we will denote as $x$, a Bernoulli random variable, we can then formulate the problem as follows: And as the classical Bayesian analysis goes, the more data we collect through repeated experiments, the closer the posterior distribution will get to the target distribution. Through this process, we are able to approximate the parameter $\theta$. Now we have to think about what distributions we want to use for the prior and likelihood. Let’s start with the easier one: the likelihood. The natural choice that makes sense is the binomial distribution. Each pull on the lever can be considered a Bernoulli trial, and as we start exploring with more pulls to accumulate data, we will essentially be sampling from a binomial distribution, with $s$ success and $f$ failures out of a total of $(s + f)$ trials. Therefore, more concretely, we can say The more interesting part is the prior distribution. Intuitively, the one might be inclined to say that we need a uniform prior, and indeed that answer would technically be true. However, I qualify with “technically” since the real answer has an added layer of complexity. Having a uniform prior only makes sense on the very first try, when there is no historical data to estimate the parameter from. However, once we start pulling the lever and seeing the results of each consecutive pull, we should be able to build some prior expectation as to what the true value of the parameter is. This suggests that the uniform prior approach is missing some important key pieces. To really drive this point home, let’s discuss more about the notion of conjugate priors, specifically in the context of the Beta and binomial distributions. The notion of conjugate priors is something that recurs extremely often in Bayesian analysis. This is mainly because conjugate priors offer a nice closed-form solution to posterior distribution computation. Simply put, a conjugate prior to some likelihood function is a type of prior that, when multiplied with a given likelihood, produces a posterior distribution that is of the same form as itself. For example, a Beta prior, when multiplied with a binomial likelihood, produces a Beta posterior. This is what we want in Bayesian analysis, since we can now assume that posterior to be our new prior in the next iteration of experiments. In this section, we will build on top of this high level understanding of conjugacy to show that a Beta distribution is indeed a conjugate prior to the binomial likelihood function. On that end, let’s start with a quick definition review of the Beta distribution and the Beta function. The Beta distribution looks as follows: where Comparing (3) and the integral representation of the Beta function in (4), you will see that there is a clear resemblance, and this is no coincidence. In the Beta distribution, the Beta function which appears in the denominator is simply a normalizing constant that ensures that integrating the probability distribution function from 0 to 1 produces 1. The specifics of the Beta function and how it relates to the Gamma function might be a topic for a another post. For now, it suffices to show the general probability function as well as its building blocks, namely the Beta function itself. These will become relevant later when we derive the posterior with a binomial likelihood function to show conjugacy. To prove conjugacy, it suffices to show that multiplying a Beta prior with a binomial likelihood produces a Beta posterior. Roughly speaking, the big picture we will be using is essentially equation (1). For notation consistency, I decided to use $\vert$ for conditional probability and $;$ for function parametrization. For a rather pedantic discussion on this notational convention, I recommend that you take a quick look at this cross validated post. Let’s now plug in the Beta and binomial distributions into (5) to see what comes out of it. This is going to be a lot of computation, but the nice part of it is that a lot of terms cancel out each other. We can easily see that there are constant that exist both in the numerator and the denominator. We can pull these constants out of the integral to simplify the expression. These constants include the binomial and the reciprocal Beta normalizing constants. One useful observation to make here is the fact that the numerator itself looks a lot like something we have seen before: the Beta distribution. In fact, you might also realize that the denominator is nothing but just a normalizing constant that ensures that our posterior distribution, when integrated from 0 to 1, integrates up to 1 as the axiom of probability states. We can also see the denominator as the definition of the Beta function. In other words, Therefore, we end up with Notice that offers an extremely nice interpretation: the number of success $s$ and failures $f$, which are determined by $\theta$, yield insight into what the true parameter is via the Beta distribution. This is why the Beta distribution is often referred to as a “probability distribution of probabilities.” I highly recommend that you read this post on cross validated for many intuitive explanations of what the Beta distribution is and how it can be useful, such as in this Bayesian context. Now we know that there is a closed-form solution for Bayesian problems involving conjugate priors and likelihood functions, such as a Beta prior coupled with the binomial likelihood. But we want to be able to interpret the posterior distribution. We might start from rather simple metrics like the mean to get a better idea of what the posterior tells us. Note that we can always generalize such quantities into moments. Given $X \sim \text{Beta}(\alpha, \beta)$, the expected value of the Beta random variable can be expressed as Proving this is pretty straightforward if we simply use the law of the unconscious statistician. Using the definition of expectation, we can derive the following: Here, we use the Gamma representation of the Beta function. This conclusion gives an even nicer, more intuitive interpretation of the Bayesian update we saw earlier with the Beta prior and binomial likelihood. Namely, given the new posterior we know that sampling from that new posterior will give us a mean value that somewhat resembles the crude approach we would have taken without the prior expectation. Here, the crude approach is referring to the raw frequentist estimate we would have made had we not taken the Bayesian approach to the problem. It is obvious that the two hyperparameters of the prior are acting as an initial weight of sorts, making sure that when little data is available, the prior overshadows observations, but when ample amount of data is collected and available, eventually yields to those observations to estimate the parameter. Now we can turn our attention back to the multi-armed bandit problem. Now we can build on top of our knowledge of the Beta-Binomial update and refine what the frequentist greedy approach. We will also write out some simple functions to simulate the bandit problem and thus demonstrate the effectiveness of the Bayesian approach. Note that a lot of the code in this post was largely inspired by Peter’s blog. Before we get into any modeling, let’s first import the modules we’ll need and set things up. Instead of using the approach outlined in the blog I’ve linked to, I decided to use objects to model bandits. The rationale is that this approach seems to make a little bit more intuitive sense to me. Also, working with Django these days has made me feel more affinity with Python classes. At any rate, let’s go ahead. Now, we can initialize a bandit with some predetermined parameter, . Of course, our goal would be to determine the true value of this parameter through sampling and Bayesian magic. In this case, we have created a fair bandit with a  of 0.5. We can also simulate level pulls by repeatedly calling on . Note that this will accumulate the result of each Bernoulli trial in the  list object. Notice that we also have . This is an a list object that c Now that we have performed all the basic sanity checks we need, let’s quickly go ahead and create three bandit objects for demonstration purposes. Now we get into the details of how to perform the Bayesian update. More specifically, we’re interested in how we are going to use posterior probabilities to make decisions on which slot machine to pull on. This is where Thompson sampling comes in. In the simple, greedy frequentist approach, we would determine which bandit to pull on given our historical rate of success. If the first slot machine approximately yielded success 60 percent of the time, whereas the second one gave us 40, we would choose the first. Of course, this approach is limited by the fact that, perhaps we only pulled on the second machine 5 times and got only 2 success out of them, whereas we pulled on the first bandit a hundred times and got 60 successes. Maybe it turns out that the second bandit actually has a higher success rate, and that we were simply unlucky those five turns. Thompson sampling remedies this problem by suggesting a different approach: now that we have Bayesian posteriors, we can now directly sample from those posteriors to get an approximation of the parameter values. Since we are sampling from a distribution instead of relying on a point estimate as we do for the greedy approach, this allows for both exploration and exploitation to happen at reasonable frequencies. If a posterior distribution has large variance, this means that we will explore that particular bandit slightly more than others. If a posterior has a large mean—a high success parameter—then we will exploit that machine a bit more to earn more profit, or, in this context, to minimize regret. Before we move on any farther, perhaps’ it’s worth discussing what the term “regret” means in this context. Simply put, regret refers to the amount that we have comparatively lost by making a sub-optimal choice from the get go. Here is a visual diagram I came across on Analytics Vidhya.  The maximum reward would obviously be achieved if we pull on the slot machine with the highest success parameter from trial 1. However, this does not happen since the gambler dives into the game without this prior knowledge. Hence, they have to learn what the optimal choice is through exploration and exploitation. It is of course in this learning process that the greedy algorithm or Bayesian analysis with Thompson sampling comes into play. The amount that we have theoretically lost—or, in other words, the extent to which we are far away from the maximum amount we could have earned—is denoted as regret. Thus, to maximize reward is to minimize regret, and vice versa. Now let’s simulate a hundred pulls on the lever using Bayesian analysis using the Beta-Binomial model and Thompson sampling. Nothing much fancy here, all we’re doing is Thompson sampling from the Beta distribution via , then obtaining the index of the bandit with the highest parameter, then pulling the machine that corresponds to that index. We will also keep cumulative track of our results to reproduce the regret diagram shown above. And we’re done with the hundred round of simulations! Hopefully our simulator gambler has made some good choices by following Bayesian update principles, with the Beta-Binomial model and Thompson sampling under their belt. Let’s take a look at the posterior distribution for each bandit. We can easily plot them using , as shown below. Notice that I have also included the uniform prior for reference purposes.  The result is as we would expect: the bandit with the highest success parameter of 0.7 seems to have been pulled on the most, which explains why its variance is the smallest out of the bunch. Moreover, the mean of that particular posterior is also close to 0.7, its true value. Notice that the rest of the posteriors also somewhat have this trend, although more uncertainty is reflected into the shape of the distributions via the spread. It is interesting to see how we can go from a uniform prior, or $\text{Beta}(1, 1)$, to almost normal-shaped distributions as we see above. To corroborate our intuition that the best bandit was indeed the most pulled, let’s quickly see the proportion of the pulls through a pie chart.  As you can see, there seems to be a positive correlation between the success parameter and the number of pulls. This is certainly good, since we want the gambler to pull on the best bandit the majority of times while avoiding the worse ones as much as possible. This certainly seems to be the case, given that the bandit with the worse parameter—0.1—was pulled the least. Last but not least, let’s revisit the cumulative regret graph introduced earlier. We can draw our own cumulative regret graph by first simulating what would have been the optimal result—in other words, we need to obtain the amount of reward the gambler would have earned had they simply pulled on the best bandit the entire time. Let’s quickly simulate that first. And it turns out that the maximum amount they would have earned, in this particular instance, is 74. I say in this particular instance, since the expected value of the maximum reward is simply 70, given that the highest success parameter is 0.7. This minor detail notwithstanding, we can now use the  quantity and the  list in order to recreate our own cumulative regret graph, as shown below.  It is obvious that the cumulative regret is highest when we start since the current reward is at 0. However, after some trial and error, the gambler starts to figure out which bandit is the best and starts pulling more of those, ultimately ending up at the point that is quite close to the maximum reward, though not quite due to the earlier opportunities that may have been lost due to exploration and sampling. In this post, we took a look at the multi-armed bandit problem and how it relates to Bayesian analysis with the Beta and Binomial distributions. I personally enjoyed writing this post, not only because I hadn’t written in a long time, but also because it helped me revisit some statistics, which is something that I desperately needed to do—I’m spending way too much time dealing with Django and Selenium these days. Time and again, I realize that there is an element of intuitiveness to Bayesian statistics that, though not obvious at first, starts to make more sense as I explore more into that realm. Of course, frequentist statistics offers a quick and easy way of interpreting certain metrics, but the notion of expectation is something that I had thought was complicated and unnecessary at first, but have gradually come to understand, embrace, and even support. Perhaps writing this post has reaffirmed my Bayesian identity as a budding statistician. I hope you’ve enjoyed reading this post. Catch you up in the next one!",0,0,0,0,1,0,0,1
PyTorch RNN from Scratch,"In this post, we’ll take a look at RNNs, or recurrent neural networks, and attempt to implement parts of it in scratch through PyTorch. Yes, it’s not entirely from scratch in the sense that we’re still relying on PyTorch autograd to compute gradients and implement backprop, but I still think there are valuable insights we can glean from this implementation as well. For a brief introductory overview of RNNs, I recommend that you check out this previous post, where we explored not only what RNNs are and how they work, but also how one can go about implementing an RNN model using Keras. This time, we will be using PyTorch, but take a more hands-on approach to build a simple RNN from scratch. Full disclaimer that this post was largely adapted from this PyTorch tutorial this PyTorch tutorial. I modified and changed some of the steps involved in preprocessing and training. I still recommend that you check it out as a supplementary material. With that in mind, let’s get started. The task is to build a simple classification model that can correctly determine the nationality of a person given their name. Put more simply, we want to be able to tell where a particular name is from. We will be using some labeled data from the PyTorch tutorial. We can download it simply by typing This command will download and unzip the files into the current directory, under the folder name of . Now that we have downloaded the data we need, let’s take a look at the data in more detail. First, here are the dependencies we will need. We first specify a directory, then try to print out all the labels there are. We can then construct a dictionary that maps a language to a numerical label. We see that there are a total of 18 languages. I wrapped each label as a tensor so that we can use them directly during training. Let’s store the number of languages in some variable so that we can use it later in our model declaration, specifically when we specify the size of the final output layer. Now, let’s preprocess the names. We first want to use  to standardize all names and remove any acute symbols or the likes. For example, Once we have a decoded string, we then need to convert it to a tensor so that the model can process it. This can first be done by constructing a  mapping, as shown below. We see that there are a total of 59 tokens in our character vocabulary. This includes spaces and punctuations, such as ` .,:;-‘(num_char, 59)(59,)`. We can now build a function that accomplishes this task, as shown below: If you read the code carefully, you’ll realize that the output tensor is of size , which is different from the explanation above. Well, the reason for that extra dimension is that we are using a batch size of 1 in this case. In PyTorch, RNN layers expect the input tensor to be of size . Since every name is going to have a different length, we don’t batch the inputs for simplicity purposes and simply use each input as a single batch. For a more detailed discussion, check out this forum discussion. Let’s quickly verify the output of the  function with a dummy input. Now we need to build a our dataset with all the preprocessing steps. Let’s collect all the decoded and converted tensors in a list, with accompanying labels. The labels can be obtained easily from the file name, for example . We could wrap this in a PyTorch  class, but for simplicity sake let’s just use a good old  loop to feed this data into our model. Since we are dealing with normal lists, we can easily use ’s  to separate the training data from the testing data. Let’s see how many training and testing data we have. Note that we used a  of 0.1. We will be building two models: a simple RNN, which is going to be built from scratch, and a GRU-based model using PyTorch’s layers. Now we can build our model. This is a very simple RNN that takes a single character tensor representation as input and produces some prediction and a hidden state, which can be used in the next iteration. Notice that it is just some fully connected layers with a sigmoid non-linearity applied during the hidden state computation. We call  at the start of every new batch. For easier training and learning, I decided to use  to initialize these hidden states. We can now build our model and start training it. I realized that training this model is very unstable, and as you can see the loss jumps up and down quite a bit. Nonetheless, I didn’t want to cook my 13-inch MacBook Pro so I decided to stop at two epochs. Now we can test our model. We could look at other metrics, but accuracy is by far the simplest, so let’s go with that. The model records a 72 percent accuracy rate. This is very bad, but given how simple the models is and the fact that we only trained the model for two epochs, we can lay back and indulge in momentary happiness knowing that the simple RNN model was at least able to learn something. Let’s see how well our model does with some concrete examples. Below is a function that accepts a string as input and outputs a decoded prediction. I don’t know if any of these names were actually in the training or testing set; these are just some random names I came up with that I thought would be pretty reasonable. And voila, the results are promising. The model seems to have classified all the names into correct categories! This is cool and all, and I could probably stop here, but I wanted to see how this custom model fares in comparison to, say, a model using PyTorch layers. GRU is probably not fair game for our simple RNN, but let’s see how well it does. Let’s declare the model and an optimizer to go with it. Notice that we are using a two-layer GRU, which is already one more than our current RNN implementation. The training appeared somewhat more stable at first, but we do see a weird jump near the end of the second epoch. This is partially because I didn’t use gradient clipping for this GRU model, and we might see better results with clipping applied. Let’s see the accuracy of this model. And we get an accuracy of around 80 percent for this model. This is better than our simple RNN model, which is somewhat expected given that it had one additional layer and was using a more complicated RNN cell model. Let’s see how this model predicts given some raw name string. The last one is interesting, because it is the name of a close Turkish friend of mine. The model obviously isn’t able to tell us that the name is Turkish since it didn’t see any data points that were labeled as Turkish, but it tells us what nationality the name might fall under among the 18 labels it has been trained on. It’s obviously wrong, but perhaps not too far off in some regards; at least it didn’t say Japanese, for instance. It’s also not entirely fair game for the model since there are many names that might be described as multi-national: perhaps there is a Russian person with the name of Demirkan. I learned quite a bit about RNNs by implementing this RNN. It is admittedly simple, and it is somewhat different from the PyTorch layer-based approach in that it requires us to loop through each character manually, but the low-level nature of it forced me to think more about tensor dimensions and the purpose of having a division between the hidden state and output. It was also a healthy reminder of how RNNs can be difficult to train. In the coming posts, we will be looking at sequence-to-sequence models, or seq2seq for short. Ever since I heard about seq2seq, I was fascinated by tthe power of transforming one form of data to another. Although these models cannot be realistically trained on a CPU given the constraints of my local machine, I think implementing them themselves will be an exciting challenge. Catch you up in the next one!",0,1,0,1,0,1,0,0
Introduction to tf-idf,"Although I’ve been able to automate some portion of the blog workflow, there’s always been a challenging part that I wanted to further automate myself using deep learning: automatic tagging and categorization. Every post requires some form of YAML front matter, containing information such as the title, tag, and category of the blog post to be uploaded. Although I sometimes create new tags or categories if existing ones seem unfit, I only deal with a limited number of topics on this blog, which is why I’ve always thought that some form of supervised learning be able to automate the process by at least generating some possible tags for me. I’m currently in the process of preparing the data (my previous blog posts) and building a simple NLP document classification model for the job. However, NLP is a field that I’m admittedly not well-acquainted with, not to mention the fact that I’ve not bee posting a lot about deep learning implementations for a while now. So in today’s short post, I decided to write about tf-idf vectorization, which is a very simple yet powerful technique that is often used in routine tasks like document classification, where SOTA models aren’t really required. As always, this post is going to take a hands-on approach by demonstrating a simple way of implementing tf-idf vectorization from scratch. Let’s get started. tf-idf stands for term frequency-inverse document frequency. This is all there is to it—in fact, the formula for tf-idf can simply be expressed as where $t$ denotes a single term; $d$, a singe document, and $D$, a collection of documents. So simply put, tf-idf is simply a product of the term frequency, denoted above as $\text{tf}$, and inverse document frequency, $\text{idf}$. All there is left, then, is to figure out what term frequency and inverse document frequency are. Without much explanation, you can probably guess what term frequency is: it simply indicates how frequently a word appeared in a given document. For example, if there were a total of 3 distinct words in a document (very short, I know), then each of the three words would have a tf score of $1/3$. Put differently, the sum of the tf vector for each document should sum to one. The definition of a tf score might be thus expressed as where the denominator denotes the count of all occurrences of the term $t$ in document $d$, and the numerator represents the total number of terms in the document. Roughly speaking, inverse document frequency is simply the reciprocal of document frequency. Therefore, it suffices to show what document frequency is, since idf would immediately follow from df. Before getting into the formula, I think it’s instructive to consider the motivation behind tf-idf, and in particular what role idf plays in the final score. The motivation behind tf-idf commences from a simple question: how do we determine the semantic importance of a word in a set of documents? On one hand, words the appear a lot are probably worth paying attention to. For example, in one of my posts on Gaussian distributions, the word “Gaussian” probably appears many times throughout the post. A keyword probably appears frequently in the document; hence the need to calculate tf. On the other hand, there might be words that appear a lot, but aren’t really that important at all. For example, consider the word “denote.” I know that I use this word a lot before writing down equations or formulas, just for the sake of notational clarity. However, the word itself carries little information on what the post is about. The same goes for other words, such as “example,” “however,” and so on. So term frequency only doesn’t really tell us much; instead, we want to pay attention to words that occur frequently in a given document, but doesn’t appear a lot in others—such words are most likely to be unique keywords that potentially capture the gist of that document. Given this analysis, it isn’t difficult to see why tf-idf is designed the way it is. Although we give priority weight to words with high term frequency, we discount words that appear frequently across all documents by dividing tf by idf, or inverse document frequency. In short, document frequency tells us how frequently a given word appears throughout all documents; the inverse is the reciprocal of that quantity. In practice, we often apply a logarithm to prevent the idf score from exploding. Also, we add some smoothing to prevent division by zero. There seems to be many variations of how smoothing is implemented in practice, but here I present one way that seems to be adopted by scikit-learn. For other schemes, refer to this table on Wikipedia. However, this is a mere technically; the intuition we motivated earlier still applies regardless. With these ideas in mind, let’s go implement tf-idf vectorization in Python! In this section, we will develop a simple set of methods to convert a set of raw documents to tf-idf vectors, using a dummy dataset. Below are four documents (again, I know they’re short) that we will be using throughout this tutorial. The first step is to preprocess and tokenize the data. Although the specifics of preprocessing would probably differ from task to task, in this simple example, we simply remove all punctuations, change documents to lower case letters, and tokenize them by breaking down documents into a bag of words. Other possible techniques not discussed here include stemming and lemmatization. The  function accepts as input a set of documents and removes all the punctuation in each document. Here is the result of applying our function to the dummy data. Next, we need to tokenize the strings by splitting them into words. In this process, we will also convert all documents to lower case as well. Note that  works on each documents, not the entire collection. Let’s try calling the function with the first document in our dummy example. Finally, as part of the preprocessing step, let’s build the corpus. The corpus simply refers to the entire set of words in the dataset. Specifically for our purposes, the corpus will be a dictionary whose keys are the words and values are an ordinal index. Another way to think about the corpus in this context is to consider it as a word-to-index mapping. We will be using the indices to represent each word in the tf, idf, and tf-idf vectors later on in the tutorial. Because we have a very simple example, our corpus only contains 9 words. This also means that our tf-idf vectors for each document will also be a list of length 9. Thus it isn’t difficult to see how tf-idf vectorization can result in extremely high-dimensional matrices, which is why we often apply techniques such as lemmatization or PCA on the final result. Also note that published modules use sparse representations to minimize computational load, as we will later see with scikit-learn. Now it’s time to implement the first step: calculating term frequency. In Python, this simply amounts to looping through each document, creating a tf vector per iteration, and making sure that they are normalized as frequencies at the very end. In creating tf vectors for each document, we will be referencing the word-to-index mapping in our corpus. Let’s see what we get for the four documents in our dummy example. Due to floating point arithmetic, the decimals don’t look the most pleasing to the eye, but it’s clear that normalization has been performed as expected. Also note that we get 4 vectors of length 9 each, as expected. Next, it’s time to implement the idf portion of the vectorization process. In order to calculate idf, we first need a total count of each number in the entire document collection. A module that is perfect for this job is , which accepts as input an iterable and outputs a dictionary-like object whose values represent the count of each key. Let’s test in on our dummy dataset to see if we get the count of each tokenized word. This is precisely what we need to calculate idf. Recall the formula for calculating idf As noted earlier, the intuition behind idf was that important keywords probably appear only in specific relevant documents, whereas generic words of comparatively lesser importance appear throughout all documents. We transcribe (4) into code as follows: Now, we have the idf vectors for the nine terms in the dummy dataset. At this point, all there is left to do is to multiply the term frequencies with their corresponding idf scores. This is extremely easy, since we are essentially performing a dot product of the tf and idf vectors for each document. As a final step, we normalize the result to ensure that longer documents do not overshadow shorter ones. Normalizing is pretty simple, so we’ll assume that we have a function  that does the job for now. Before we test the code, we obviously need to implement . This can simply done by obtaining the sum of the L2 norm of each vector, then dividing each element by that constant. Here is an easy contrived example we can do in our heads: And now we’re done! If let’s print the tf-idf vectors for each of the four documents in the dummy example. It seems about right, as all the vectors appear normalized and are of the desired dimensions. However, to really verify the result, it’s probably a good idea to pit our algorithm against scikit-learn’s implementation. In scikit-learn, the  does all the job. To transform the data to tf-idf vectors, we need to create an instance of the  and call its method, . And here are the results: There are several observations to be made about this result. First, note that the default return type of  is a sparse matrix. Sparse matrices are a great choice since many of the entries of the matrix will be zero—there is probably no document that contains every word in the corpus. Therefore, sparse representations can save a lot of space and compute time. This is why we had to call  on the result. Second, you might be wondering why the order of elements are different. This is because the way we built ordinal indexing in corpus is probably different from how scikit-learn implements it internally. This point notwithstanding, it’s clear that the values of each vectors are identical, disregarding the fact that the result produced by our algorithm has more decimal points due to floating point arithmetic. This was a short introductory post on tf-idf vectors. When I first heard about tf-idf vectors from a friend studying computational linguistics, I was intimidated. However, now that I have a project I want to complete, namely an auto-tagging and classification NLP model, I’ve mustered more courage and motivation to continue my study the basics of NLP. I hope you’ve enjoyed reading this post. Catch you up in the next one! (Yes, this is a trite ending comment I use in almost every post, so the idf scores for the words in these two sentences are going to be very low.)",0,0,0,0,0,1,0,0
"Newton-Raphson, Secant, and More","Recently, I ran into an interesting video on YouTube on numerical methods (at this pont, I can’t help but wonder if YouTube can read my mind, but now I digress). It was a channel called numericalmethodsguy, run by a professor of mechanical engineering at the University of Florida. While the videos themselves were recorded a while back in 2009 at just 240p, I found the contents of the video to be very intriguing and easily digestable. His videos did not seem to assume much mathematical knowledge beyond basic high school calculus. After watching a few of his videos, I decided to implement some numerical methods algorithms in Python. Specifically, this post will deal with mainly two methods of solving non-linear equations: the Newton-Raphson method and the secant method. Let’s dive right into it. Before we move on, it’s first necessary to come up with a way of representing equations in Python. For the sake of simplicity, let’s first just consider polynomials. The most obvious, simplest way of representing polynomials in Python is to simply use functions. For example, we can express $f(x) = x^3 - 20$ as However, a downside of this approach is the fact that it’s difficult to directly derive this equation despite the fact that it is nothing more than just a simple polynomial. So instead, we will use a list index-based representation. Namely, the  $i$th element of a list represents the coefficient of the $i$th power in a polynomial equation. In other words, $f(x) = x^2 - 20$ would translate into . The  is a function that returns a Python function given a list that conforms to this list index representation. Let’s see if this works as expected. $3^3 - 20 = 7$, so the function passes our quick sanity test. One useful helper function that I also implemented for the sake of convenience is a array-to-equation parser that translates a list representation into a mathematical expression in Python. This is best demonstrated than explained, so I’ll defer myself to an example. Below is the full definition of the  function. At this point, I also thought that it would be useful and interesting to compose a function that translates the string output of  into a proper Python function we can use to calculate values. Below is the  function that receives as input some parsed output string and returns a corresponding Python function. Now, we can do something like this: Now that we have more than enough tools we can use relating to the list index representation we decided to use to represent polynomials, it’s time to exploit the convenience that this representation affords us to calculate derivatives. Calculating derivatives using the list index representation is extremely easy and convenient: in fact, it can be achieved in just a single line. Let’s test this function with the  example we have been using previously. Let’s also use the  function to make the final result for human-readable. Seems like the derivative calculation works as expected. In the process, I got a little bit extra and also wrote a function that integrates a function in list index representation format. If we integrate $f(x) = x^3 - 20$, we end up with $F(x) = \frac14 x^4 - 20 x + C$, where $C$ is the integration constant. Excluding the integration constant, we get a result that is consistent with the  function. While it’s great that we can calculate derivatives and integrals, one very obvious drawback of this direct approach is that we cannot deal with non-polynomial functions, such as exponentials or logarithms. Moreover, the list index representation is unable to represent polynomials that include terms whose powers are not positive integers. For these reasons, we will need some other methods of calculating derivatives as well. Hence the motivation for approximation methods, outlined in the section below. If you probe the deepest depths of your memory, somewhere you will recall the following equation, which I’m sure all of us saw in some high school calculus class: This equation, commonly known as the definition of a derivative, is also known as the forward divided difference formula of calculating derivatives. There is another variant, known as the backward divided difference formula: (1) and (2) are almost nearly identical, but the difference lies in which term is subtracted from who. In (1), we go an infinitesimal step forward—hence the $f(x + h)$—and subtract the value at the point of approximation, $f(x)$. In (2), we go backwards, which is why we get $f(x - h)$. As $h$ approaches 0, (1) and (2) asymptotically gives us identical results. Below is a Python variant of the backward divided difference formula. Some tweaks have been made to the formula for use in the section that follows, but at its core, it’s clear that the function uses the approximation logic we’ve discussed so far. Another variant of the forward and backward divided difference formula is the center divided difference. By now, you might have some intuition as to what this formula is—as the name implies, we somehow use the center point, instead of going forward or backwards from the point of approximation. Here is the formula: Heuristically, this formula also makes sense. We can imagine going both a step forward and backward, then dividing the results by the total of two steps we’ve taken, one in each direction. Shown below is the Python implementation of the center divided difference formula. According to conventional mathematical wisdom, the center divided method normally provides a more robust way of approximating first order derivatives. In this subsection, we discuss why this is the case. Using Taylor expansion, we can approximate the value of $f(a + h)$ as follows, given that $h$ goes to 0 under the limit. Notice that we can manipulate (4) to derive the forward divided difference equation in (1). If we move the $f(a)$ term to the LHS, then divide both sides by $h$, we end up with Here, we used big-O notation to denote the order of magnitude of the trailing terms. The trailing terms are significant since they are directly related to the accuracy of our approximation. An error term of $O(h)$ means that, if we halve the step size, we will also halve the error. This is best understood as a linear relationship between error and the step size. We can conduct a similar mode of analysis with backward divided difference. By symmetry, we can express $f(a - h)$ as If we rearrange (6), we end up with (2). Again, we see that backward divided difference yields linear error, or a trailing term of $O(h)$. Here’s where things get more interesting: in the case of center divided difference, the magnitude of the error term is $O(h^2)$, meaning that halving the step size decreases the error by four-folds. This is why center divided difference yields much more accurate approximations than forward or backward divided difference. To see this, we subtract (5) from (4), then move some terms, and divide both sides by $2h$. Notice that subtracting these two expression results in a lot of term cancellations. Dividing both sides by $2h$ yields From this result, we now know why the central divided difference method provides more accurate approximations of the derivative than do forward and backward divided difference. This is somewhat related to what we will be doing in the next section, so it’s a good intuition to have throughout when reading the rest of this article. Now that we have these tools for differential calculus, now comes the exciting part: solving non-linear equations. Specifically, we will be taking a look at two numerical methods: the Newton-Raphson method and the secant method. It’s time to put the methods we developed in the preceding sections to use for solving non-linear equations. Specifically, we’ll begin by taking look at a classic algorithm, the Newton-Raphson method. The Newton-Raphson method is one of the many ways of solving non-linear equations. The intuition behind the Newton-Raphson method is pretty straightforward: we can use tangent lines to approximate the x-intercept, which is effectively the root of the equation $f(x) = 0$. Specifically, we begin on some point on the graph, then obtain the tangent line on that point. Then, we obtain the $x$-intercept of that tangent line, and repeat the process we’ve just completed by starting on a point on the graph whose $x$-value is equal to that $x$-intercept. The following image from Wikipedia illustrates this process quite well. (A digression: It’s interesting to see how “function” and “tangent” are written in German—in case you are wondering, I don’t know a word of German.)  Mathematically, the Newton-Raphson method can be expressed recursively as follows: Deriving this formula is quite simple. Say we start at a point on the graph, $(x_i, f(x_i))$. The tangent line from that point will have a slope of $f’(x_i)$. Therefore, the equation of the tangent line can be expressed as Then, the $x$-intercept can simpy be obtained by finding an $x$ value that which makes $y = 0$. Let $x^*$ denote that point. Then, we arrive at the following update rule. Since we will be using $x^$ as the value for the next iteration, $x^ = x_{i + 1}$, and now we have the update rule as delineated in (4). Below is an implementation of the Newton-Raphson method in Python. I’ve added some parameters to the function for functionality and customization.  is simply some small value we use to decide when to stop the update; if the change in the value of the root is so small that it is not worth the extra compute, we should stop.  determines how many iterations we want to continue. If the algorithm is unable to find the root within  iterations, it likely means that the function provided does not have a root, or at the very least, the root is not discoverable via the algorithm. Lastly,  is a flag that determines whether we return the full update history or simply the last value in the iteration as a single value. One peculiarity that deserves attention is the  exception, which occurs in this case if the number of arguments passed into the function does not match. I added this   block to take into account the fact that the  method and other approximate derivative calculation methods such as  have differing numbers of parameters. Let’s see if this actually works by using the example we’ve been reusing thus far, $f(x) = x^3 - 20$, or  and , both of which we have already defined and initialized above. The root seems to be around 2.7. And indeed, if we cube it, we end up with a value extremely close to 20. In other words, we have successfully found the root to $y = x^3 - 20$. Instead of the direct derivative, , we can also use approximation methods. In the example below, we show that using  results in a very similar value (in fact, it is identical in this case, but we need to take into other factors such as numerical stability and overflow which might happen with such high-precision numbers). This result aligns with the earlier observation that center divided difference provides very accurate approximations of the first derivative. Note that the advantage of using  is that we can now apply Newton-Raphson to non-polynomial equations that cannot be formulated in list index representation format. For instance, let’s try something like . To verify that this is indeed correct, we can plug  back into . Also, given that $e \approx 2.7$, we can use some heuristics to realize that the answer must be only ever so slightly larger than 1. Notice that the result is extremely close to zero, suggesting that we have found the correct root. Now that we have seen the robustness of the Newton-Raphson method, let’s take a look at another similar numerical method that uses backward divided difference for derivative approximation. In this section, we will look at the secant method, which is another method for identifying the roots of non-linear equations. Before we get into a description of how this method works, here’s a quick graphic, again from Wikipedia, on how the secant method works.  As the name implies, the secant function works by drawing secant lines that cross the function at each iteration. Then, much like the Newton-Raphson method, we find the $x$-intercept of that secant line, find a new point on the graph whose $x$-coordinate corresponds to that intercept, and use the point from the previous iteration to find a new secant line. This process is very intuitively outlined in this video by numericalmethodsguy. The update rule for the secant method can be expressed as We can derive (7) simply by slightly modifying the update rule we saw for Newton-Raphson. Recall that the Newton-Raphson update rule was written as The only modification we need to make to this update rule is to replace $f’(x_i)$ with an approximation using the backward divided difference formula. Here, we make a slight modification to (2), specifically by using values from previous iterations. If we plug (8) back into (4), with some algebraic simplifications, we land on (7), the update rule for the secant method. This is left as an exercise for the reader. Now let’s take a look at how we might be able to implement this numerical method in code. Presented below is the  method, which follows the same general structure as the  function we looked at earlier. The only part that differs is the part where we use backward divided difference, which requires that we look up not only the immediately previous root approximation, but the value prior to that as well. In other words, we need both $x_i$ and $x_{i - 1}$ to calculate $x_{i + 1}$ via an iterative update. And here is an obligatory sanity check using our previous example. 2.7 is a familiar value, and indeed it is what was returned by the Newton-Raphson method as well. We confirm that this is indeed the root of the equation. Now that we have looked at both methods, it’s time to make a quick comparison. We will be comparing three different methods: By setting  to be , we can obtain a full list of root updates as the three methods begin their quest for the root of the function. We can then see which method converges the quickest. Let’s see how this little experiment turns out. We first begin by importing some dependencies to plot the history of values. Then, we obtain the history for each of the three approaches and plot them as a scatter plot. The result is shown below.  You might have to squint your eye to see that  (Netwon-Raphson with direct derivatives) and  (Newton-Raphson with center divided difference) almost coincide exactly at the same points. I was honestly somewhat surprised by the result: although we had verified the advantage of using center divided difference over forward or backward divided difference via some simple big-O analysis with trailing error terms, I did not expect the two to coincide with such exactitude. Another interesting observation is that the secant method seems to take slightly longer than the Newton-Raphson method. This is probably due to the fact that the secant method uses backward divided difference, and also the fact that it requires two previous at each iteration instead of one. The reason why the first update seems rather ineffective is that the two initial guesses that we fed into the model was probably not such a good starting point. The topic of today’s post was somewhat different from what we had previously dealt with in this blog, but it was an interesting topic for me nonetheless. I had encountered the Newton-Raphson method previously when going down my typical Wikipedia rabbit holes, but it is only today that I feel like I’ve finally got a grasp of the concept. I consider this post to be a start of many more posts on numerical methods to come. I hope you’ve enjoyed reading this post. See you in the next one.",0,0,1,0,0,0,0,0
Maximum A Posteriori Estimation,"In a previous post on likelihood, we explored the concept of maximum likelihood estimation, a technique used to optimize parameters of a distribution. In today’s post, we will take a look at another technique, known as maximum a posteriori estimation, or MAP for short. MLE and MAP are distinct methods, but they are more similar than different. We will explore the similar mathematical underpinnings behind the methods to gain a better understanding of how distributions can be tweaked to best fit some given data. Let’s begin! Before we jump right into comparing MAP and MLE, let’s refresh our memory on how maximum likelihood estimation worked. Recall that likelihood is defined as In other words, the likelihood of some model parameter $\theta$ given data observations $X$ is equal to the probability of seeing $X$ given $\theta$. Thus, likelihood and probability are inevitably related concepts that describe the same landscape, only from different angles. The objective of maximum likelihood estimation, then, is to determine the values for a distribution’s parameters such that the likelihood of observing some given data is maximized under that distribution. In the example in the previous post on likelihoods, we showed that MLE for a normal distribution is equivalent to setting $\mu$ as the sample mean; $\sigma$, sample variance. But this convenient case was specific only to the Gaussian distribution. More generally, maximum likelihood estimation can be expressed as: It is not difficult to see why trying to compute this quantity may not be as easy as it seems: because we are dealing with probabilities, which are by definition smaller than 1, their product will quickly diverge to 0, which might cause arithmetic underflow. Therefore, we typically use log likelihoods instead. Maximizing the log likelihood amounts to maximizing the likelihood function since log is a monotonically increasing function. Finding the maximum could be achieved multiple ways, such as through derivation or gradient descent. As the name suggests, maximum a posteriori is an optimization method that seeks to maximize the posterior distribution in a Bayesian context, which we dealt with in this post. Recall the Bayesian analysis commences from a number of components, namely the prior, likelihood, evidence, and posterior. Concretely, The objective of Bayesian inference is to estimate the posterior distribution, whose probability distribution is often intractable, by computing the product of likelihood and the prior. This process could be repeated multiple times as more data flows in, which is how posterior update can be performed. We saw this mechanism in action with the example of a coin flip, given a binomial likelihood function and a beta prior, which are conjugate distribution pairs. Then what does maximizing the posterior mean in the context of MAP? With some thinking, we can convince ourselves that maximizing the posterior distribution amounts to finding the optimal parameters of a distribution that best describe the given data set. This can be seen by simply interpreting the posterior from a conditional probability point of view: the posterior denotes the probability of the value of the model parameter is $\theta$ given data $X$. Put differently, the value of $\theta$ that maximizes the posterior is the optimal parameter value that best explains the sample observations. This is why at its heart, MAP is not so much different from MLE: although MLE is frequentist while MAP is Bayesian, the underlying objective of the two methods are fundamentally identical. And indeed, this similarity can also be seen through math. And we see that (5) is almost identical to (3), the formula for MLE! The only part where (5) differs is the inclusion of an additional term in the end, the log prior. What does this difference intuitively mean? Simply put, if we specify a prior distribution for the model parameter, the likelihood is no longer just determined by the likelihood of each data point, but also weighted by the specified prior. Consider the prior as an additional “constraint”, construed in a loose sense. The optimal parameter not only has to conform to the given data, but also not deviate too much from the established prior. To get a more intuitive hold of the role that a Bayesian prior plays in MAP, let’s assume the simplest, most uninformative prior we can consider: the uniform distribution. A uniform prior conveys zero beliefs about the distribution of the parameter, i.e. all values of $\theta$ are equally probable. The implication of this decision is that the prior collapses to a constant. Given the nature of the derived MAP formula in (5), constants can safely be ignored as it will not contribute to argument maximization in any way. Concretely, Therefore, in the case of a uniform prior, we see that MAP essentially boils down to MLE! This is an informative result that tells us that, at their core, MLE and MAP seek to perform the same operation. However, MAP, being a Bayesian approach, takes a specified prior into account, whereas the frequenting MLE simply seeks to dabble in data only, as probabilities are considered objective results of repeated infinite trials instead of subjective beliefs as a Bayesian statistician would purport. I hope you enjoyed reading this post. See you in the next one!",0,0,0,0,0,0,0,1
