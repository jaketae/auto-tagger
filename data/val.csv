title,body,from_scratch,analysis,linear_algebra,machine_learning,probability_distribution,tensorflow,statistics,deep_learning
Logistic Regression Model from Scratch,"Now what’s next? Since we have a loss function, we need to build an algorithm that will allow us to minimize this cost function. One of the most common methods used to achieve cost minimization is gradient descent. As you might be able to tell, this algorithm has a lot to do with gradients, which can loosely be understood as a fancy way of saying derivatives. Below is an illustration of the gradient descent algorithm in action, sourced from this blog.  Basically, what gradient descent does is that it takes the derivative of the loss function with respect to the weight vector every epoch, or iteration, and takes a small step in the opposite direction of that derivative. If you think of this in the context of two dimensions as shown in the illustration, the gradient descent algorithm ends up moving down the parabola, taking little steps each time, until it eventually reaches the global minimum. In mathematical notation, we might express this process as follows: If we were to perform this in vectorized format, where  represents a vector containing the weight coefficients of the logistic regression model: The  notation is used to denote gradients, an important operation in matrix calculus which we explored in when deriving the normal equation solution to linear regression on this blog. The  denotes a hyperparameter known as the learning rate, which essentially determines how big of a step the gradient descent model takes with each iteration.",1,0,0,1,0,0,0,0
Logistic Regression Model from Scratch,"If the output of an algorithm given some data point is larger than 0.5, it is likely that the given input is a spam; if it is smaller than the 0.5 threshold, chances are the email is not spam. Let’s take a look at the shape of the sigmoid function, which is a special case of the logistic function that we will use throughout this post. To plot the sigmoid function, we need to import some libraries. The sigmoid function is defined as follows: We can express this as a Python function, as demonstrated in the code snippet below. Let’s quickly plot the graph to see what the sigmoid function looks like.  As we can see, the sigmoid is a smooth, differentiable function that is bounded between 0 and 1. It is also symmetrical around the point , which is why we can use 0.5 as a threshold for determining the class of a given data point. The logistic regression model uses the sigmoid function to generate predictions, but how exactly does it work? Recall that, in the case of linear regression, our goal was to determine the coefficients of some linear function, specifically Logistic regression is not so different from linear regression.",1,0,0,1,0,0,0,0
Bayesian Linear Regression,"Although it may seem as if we made zero progress by unpacking , this process is in fact necessary to complete the square of the exponent according to the Gaussian form after making the substitutions By now, you should be comfortable with this operation of backtracking a quadratic and rearranging it to complete the square, as it is a standard operation we have used in multiple parts of this process. Finally, we have derived the predictive distribution in closed form: With more simplification using the , it can be shown that And there’s the grand formula for Bayesian linear regression! This result tells us that, if we were to simply get the best point estimate of the predicted value , we would simply have to calculate , which is the tranpose product of the MAP estimate of the weights and the input vector! In other words, the answer that Bayesian linear regression gives us is not so much different from vanilla linear regression, if we were to reduce the returned predictive probability distribution into a single point. But of course, doing so would defeat the purpose of performing Bayesian inference, so consider this merely an intriguing food for thought. As promised, we will attempt to visualize Bayesian linear regression using the  library. Doing so will not only be instructive from a perspective of honing probabilistic programming skills, but also help us better understand and visualize Bayesian inference invovled in linear regression as explored in the context of this article.",0,0,1,0,0,0,0,0
Wonders of Monte Carlo,"All we have to do, therefore, is to compute the expected value of the integrand by randomly generating a series of numbers within the specified domain, plug those values into the function \(\sin(x)\), and take their average. This is a very elementary function that simply generates a specified number of samples given  within the domain . These numbers are then plugged into the function , after which an unweighted mean of these values are computed to approximate an integral. Let’s test the accuracy of this crude Monte Carlo method by using our example of \(\sin(x)\) computed earlier. The result is a very poor approximation that is way off, most likely because we only used ten randomly generated numbers. Much like earlier, however, we would expect Monte Carlo to perform better with larger samples. The example we have analyzed so far was a very simple one, so simple that we would probably have been better off calculating the integral by hand than writing code. That’s why it’s time to put our crude Monte Carlo to the test with integrals more difficult to compute. Consider the following expression: One might try calculating this integral through substitution or integration by parts, but let’s choose not to for the sake of our mental health. Instead, we can model the integrand in Python and ask Monte Carlo to do the work for us. Concretely, this process might look as follows.",0,0,0,0,0,0,1,0
Word2vec from Scratch,"This aligns with the behavior we want, since we want the predicted probability to be as close to 1 as possible. So let’s summarize the entire process a little bit. First, embeddings are simply the rows of the first weight matrix, denoted as . Through training and backpropgation, we adjust the weights of , along with the weight matrix in the second layer, denoted as , using cross entropy loss. Overall, our model takes on the following structure: where  is the matrix contains the prediction probability vectors. With this in mind, let’s actual start building and train our model. Let’s start implement this model in code. The implementation we took here is extremely similar to the approach we took in this post. For an in-depth review of backpropagation derivation with matrix calculus, I highly recommend that you check out the linked post. The representation we will use for the model is a Python dictionary, whose values are the weight matrices and keys, the name with which we will refer to the weight matrices. In accordance with the nomenclature established earlier, we stick with  and  to refer to these weights. Let’s specify our model to create ten-dimensional embeddings. In other words, each token will be represented as vectors living in ten-dimensional space. Note that actual models tend to use much higher dimensions, most commonly 300, but for our purposes this is not necessary. Let’s begin with forward propagation. Coding the forward propagation process simply amounts to transcribing the three matrix multiplication equations in (6) into NumPy code.",1,0,0,0,0,0,0,1
Markov Chain and Chutes and Ladders,"Unfortunately, the stochastic matrix is singular because \(81 < 101\), the number of columns or rows. This implies that our matrix is degenerate, and that the best alternative to eigendecomposition is the singular value decomposition. But for the sake of simplicity, let’s resort to the brute force calculation method instead and jump straight into some statistical analysis. We first write a simple function that simulates the Chutes and Ladders game given a starting position vector . Because a game starts at the \(0\)th cell by default, the function includes a default argument on  as shown below: Calling this function will give us \(T^nx_0\), which is a 101-by-1 vector whose th entry represents the probability of the player being on the \(i\)th cell after a single turn. Now, we can plot the probability distribution of the random variable \(N\), which represents the number of turns necessary for a player to end the game. This analysis can be performed by looking at the values of  since the last entry of this vector encodes the probability of the player being at the \(100\)th cell, i.e. successfully completing the game after  rounds. This block produces the following figure: I doubt that anyone would play Chutes and Ladders for this long, but after about 150 rolls of the dice, we can expect with a fair amount of certainty that the game will come to an end.",0,0,1,0,0,0,0,0
Moments in Statistics,"Although calculating the third order derivative may sound intimidating, it may seem easier in comparison to evaluating the integral which would require us to use integration by parts. In the end, both (12) and (13) are pointing at the same quantity, namely the third moment of the exponential distribution. Perhaps the complexity of calculating either quantity is similar, and the question might just boil down to a matter of preference. However, this example shows that the MGF is a robust method of calculating moments of a distribution, and even more, potentially less computationally expensive than using the brute force method to directly calculate expected values. This was a short post on moments and moment generating functions. Moments was one of these terms that I had come across on Wikipedia or math stackexchange posts, but never had a chance to figure out. Hopefully, this post gave you some intuition behind the notion of moments, as well as how moment generating functions can be used to compute useful properties that explain a distribution. In the next post, we will take a brief break from the world of distributions and discuss some topics in information theory that I personally found interesting. If you would like to dwell on the question like “how do we quantify randomness,” don’t hesitate to tune in again in a few days!.",0,0,0,0,1,0,1,0
PyTorch RNN from Scratch,"In this post, we’ll take a look at RNNs, or recurrent neural networks, and attempt to implement parts of it in scratch through PyTorch. Yes, it’s not entirely from scratch in the sense that we’re still relying on PyTorch autograd to compute gradients and implement backprop, but I still think there are valuable insights we can glean from this implementation as well. For a brief introductory overview of RNNs, I recommend that you check out this previous post, where we explored not only what RNNs are and how they work, but also how one can go about implementing an RNN model using Keras. This time, we will be using PyTorch, but take a more hands-on approach to build a simple RNN from scratch. Full disclaimer that this post was largely adapted from this PyTorch tutorial this PyTorch tutorial. I modified and changed some of the steps involved in preprocessing and training. I still recommend that you check it out as a supplementary material. With that in mind, let’s get started. The task is to build a simple classification model that can correctly determine the nationality of a person given their name. Put more simply, we want to be able to tell where a particular name is from. We will be using some labeled data from the PyTorch tutorial. We can download it simply by typing This command will download and unzip the files into the current directory, under the folder name of .",1,0,0,0,0,0,0,1
A Simple Autocomplete Model,"To put this into perspective, let’s compare the objectives of a generative model with that of a discriminative model. Simply put, the goal of a discriminative model is to model and calculate where  is a label and  is some input vector. As you can see, discriminative models arise most commonly from the context of supervised machine learning, such as regression or classification. In contrast, the goal of a generative model is to approximate the distribution which we might construe to be the probability of observing evidence or data. By modeling this distribution, the goal is that we might be able to generate samples that appear to have been sampled from this distribution. In other words, we want our model to generate likely data points based on an approximation of the true distribution from which these observations came from. In the context of this tutorial, our neural network  should be able to somewhat immitate the speech of the famous German philosopher based on the training it went through with text data, although we would not expect the content generated by our neural network to have the same level of depth and profoundity as those of his original writings. As mentioned above, the objective of a generative model is to model the distribution of the latent space from which observed data points came from. At this point, our trained model should be able to model this distribution, and thus generate predictions given some input vector.",0,0,0,0,0,1,0,1
The Magic of Euler’s Identity,"Notice that introducing this new parameter does not alter the fundamental shape or path of the vector \(e^{ix}\); it merely specifies the speed at which the particle is traversing the complex plane. You might recall from high school physics that the velocity vector is a derivative of the position vector with respect to time. In other words, Where \(r(t)\) is a vector that denotes the position of an object at time \(t\). Now, let’s assume that \(e^{it}\) is such a position vector. Then, it follows from the principles of physics that its derivative will be a velocity vector. Therefore, we have What is so special about this velocity vector? For one, we can see that it is a scalar multiple of the original position vector, \(e^{it}\). Upon closer examination, we might also convince ourselves that this vector is in fact orthogonal to the position vector. This is because multiplying a point or vector by \(i\) in the complex plane effectively flips the object’s \(x\) and \(y\) components, which is precisely what a 90 degree rotation entails. What does it mean to have a trajectory whose instantaneous velocity is perpendicular to that of the position vector? Hint: think of planetary orbits. Yes, that’s right: this relationship is characteristic of circular motions, a type of movement in which an object rotates around a center of axis. The position vector of a circular motion points outward from the center of rotation, and the velocity vector is tangential to the circular trajectory.",0,1,0,0,0,0,0,0
Logistic Regression Model from Scratch,"There is one more tiny little step we have to make to concretize this equation, and that is to consider the average of the total gradient, since (13) as it stands applies to only one data observation. Granted, this derivation is not meant to be a rigorous demonstration of mathematical proof, because we glossed over some details concerning matrix transpose, dot products, and dimensionality. Still, it provides a solid basis for the construction of the gradient descent algorithm in code, as shown below. To avoid compensating code readability, I made a stylistic choice of using  and  to denote the vector of coefficients instead of using  for notational consistency. Other than adding some switch optional parameters such as  or , the code simply follows the gradient descent algorithm outlined above. Note that equation (6) is expressed via ; equation (14) is expressed by the line . Let’s quickly check that the  function works as expected using the dummy data we created earlier. Great! We see that the average cross entropy decreases with more iterations. The returned  array contains the coefficients of the logistic regression model, which we can use to now make predictions. We can stop here, but just like we did in the post on k-nearest neighbors, let’s wrap all the functions we have created so far into a single function that represents the logistic regression model. Our model is ready. Time for testing with some real-world data. Let’s import some data from the web.",1,0,0,1,0,0,0,0
Dissecting the Gaussian Distribution,"We can also see why (9) is coherent by unpacking the expected values expression as shown below: Using the linearity of expectation, we can rewrite the equation as Therefore, we end up with which almost exactly parallels the definition of variance, which we might recall is where \(\mu = \mathbf{E}(X)\). The key takeaway is that the covariance matrix constructed from the random vector \(X\) is the multivariable analogue of variance, which is a function of the random variable \(x\). To gain a better idea of what the covariance matrix actually looks like, however, it is necessary to review its structure element-by-element. Here is the brief sketch of the \(n\)-by-\(n\) covariance matrix. This might seem complicated, but using the definition of covariance in (8), we can simplify the expression as: Note that the covariance matrix is a symmetric matrix since \(\Sigma = \Sigma^{T}\). More specifically, the covariance matrix is a positive semi-definite matrix. This flows from the definition of positive semi-definiteness. Let \(u\) be some arbitrary non-zero vector. Then, You might be wondering how (9) ends up as (10). Although this relationship may not be immediately apparent, that the two expressions are identical can be seen by setting the random vector as and performing basic matrix vector multiplication operations. For the sake of brevity, this is left as an exercise for the reader. We now have all the pieces we need to complete the puzzle.",0,0,0,0,1,0,1,0
Complex Fibonacci,"A simple hack that nonetheless makes a lot of sense in this case is to use the real-number line for two purposes: representing the input dimension, namely the real number line, and one component of the output dimension—the real number portion of the output to Binet’s formula. This admittedly results in a loss of information, since finding the point where  won’t give us the th fibonacci number; instead, it will only tell us what the fibonacci number is whose real number component equals . Nonetheless, this is an approach that makes sense since the real number line is a common dimension in both the input and output data. With this in mind, let’s go ahead and try to plot the interpolation of the fibonacci sequence on the complex plane. First, we import the modules we will need. Then, we simply specify the domain on the real number line and generate the fibonacci numbers, separating out the real and imaginary components. Note that  is not going to be used for plotting; instead, we use  as the -axis, and this is where the loss of temporal information comes in, as mentioned earlier. Now, let’s go ahead and plot it out!  And there it is, the full fibonacci sequence, interpolated across the real numbers. When I first saw this pattern in Matt Parker’s video, I was simply in awe, a loss of words. There’s something inexplicably beautiful and wonderful at this pattern, almost as if it was some part of God’s plan.",0,1,0,0,0,0,0,0
Bayesian Linear Regression,"where  is a design matrix given by and  is a column vector given by Before calculating the posterior, let’s recall what the big picture of Bayesian inference looks like. where  denotes the parameter of interest for inference. In plain terms, the proposition above can be written as In other words, the posterior distribution can be obtained by calculating the product of the prior distribution and the likelihood function. In many real-world cases, this process can be intractable, but because we are dealing with two Gaussian distributions, the property of conjugacy ensures that this problem is not only tractable, but also that the resulting posterior would also be Gaussian. Although this may not be immediately apparent, observe that the exponent is a quadratic that follows the form after making appropriate substitutions Therefore, we know that the posterior for  is indeed Gaussian, parameterized as follows: Let’s try to obtain the MAP estimate of of , i.e. simplify  Notice the similarity with the MLE estimate, which is the solution to normal equation, which I otherwise referred to as vanilla linear regression: This is no coincidence: in a previous post on MAP and MLE, we observed that the MAP and MLE become identical when we have a uniform prior. In other words, the only cause behind the divergence between MAP and MLE is the existence of a prior distribution. We can thus consider the additional term in (10) absent in (11) as a vestige of the prior we defined for .",0,0,1,0,0,0,0,0
Building Neural Network From Scratch,"Welcome back to another episode of “From Scratch” series on this blog, where we explore various machine learning algorithms by hand-coding them from scratch. So far , we have looked at various machine learning models, such as kNN, logistic regression, and naive Bayes. Now is time for an exciting addition to this mix: neural networks. Around last year December, I bought my first book on deep learning, titled Deep Learning from Scratch,  by Saito Goki. It was a Korean translation of a book originally published in Japanese by O’Reilly Japan. Many bloggers recommended the book as the go-to introductory textbook on deep learning, some even going as far as to say that it is a must-have. After reading a few pages in, I could see why: as the title claimed, the author used only  to essentially recreate deep learning models, ranging from simple vanilla neural networks to convolutional neural networks. As someone who had just started to learn Python, following the book was a lot harder than expected, but it was a worthwhile read indeed. Inspired by that book, and in part in an attempt to test the knowledge I gained from having read that bok, I decided to implement my own rendition of a simple neural network supported by minibatch gradient descent. Let’s jump right into it. The default setup of my Jupyter Notebook, as always: Before we start building our model, we should first prepare some data.",1,0,1,0,0,0,0,1
Demystifying Entropy (And More),"” While working at Bell Labs, Shannon was experimenting with methods to most efficiently encode and transmit data without loss of information. It is in this context that Shannon proposed the notion of entropy, which he roughly defined as the smallest possible size of lossless encoding of a message that can be achieved for transmission. Of course, there is a corresponding mathematical definition for entropy. But before we jump straight into entropy, let’s try to  develop some preliminary intuition on the concept of information, which is the building block of entropy. What is information? Warren Weaver, who popularized Shannon’s works and together developed the field of information theory, pointed out that information is not related to what is said, but what could be said. This element of uncertainty involved in one’s degree of freedom is what makes the notion of information inseparable from probability and randomness. As Ian Goodfellow put it in Deep Learning, The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred. In other words, a low probability event expresses a lot of information, while a high probability event expresses low information as its occurrence provides little information of value to the informed. Put differently, rare events require more information to represent than common ones. Consider, for example, how we might represent the amount of information involved in a fair coin toss.",0,0,0,0,0,0,1,0
Building Neural Network From Scratch,"ReLU is a piece-wise function, and hence introduces nonlinearity, which is one of the purposes of having an activation function in a neural network. The formula for ReLU is extremely simple. If the input value  i s greater or equal to zero, the ReLU function outputs the value without modification. However, if  is smaller than zero, the returned value is also zero. There are other ways of expressing the ReLU function. One version that is commonly used and thus deserves our attention is written below. Although this appears different from (3), both formulas express the same operation at their core. We can get a better sense of what the function with the help of Python. Assuming that the input is a  vector, we can use vectorization to change only the elements in the input vector that are negative to zero, as shown below. Let’s see what the ReLU function looks like by plotting it on the plane.  The visualization makes clear the point that ReLU is a piece-wise function that flattens out negative values while leaving positive values unchanged. Now that we have all the ingredients ready, it’s time to build the neural network. Earlier, I said that a neural network can be reduced to matrix multiplication. This is obviously an oversimplification, but there is a degree of truth to that statement. Recall that a single neuron of a neural network can be expressed as a dot product of two vectors, as shown below. Following conventional notation,  represents weights; , input data; , bias.",1,0,1,0,0,0,0,1
Moments in Statistics,"The word “moment” has many meanings. Most commonly, it connotes a slice of time. In the realm of physics, moment refers to the rotational tendency of some object, similar to how torque measures the change in an object’s angular momentum. As statisticians, however, what we are interested in is what moment means in math and statistics. In this post, we will attempt to shed new light on the topic of probability distributions through moment generating functions, or MGF for short. The mathematical definition of moments is actually quite simple. And of course, we can imagine how the list would continue: the \(n\)th moment of a random variable would be \(\mathbf{E}(X^n)\). It is worth noting that the first moment corresponds to the mean of the distribution, \(\mu\). The second moment is related to variance, as \(\sigma^2 = \mathbf{E}(X^2) - \mathbf{E}(X)\mathbf{E}(X)\). The third moment relates to the symmetry of the distribution, or the lack thereof, a quality which goes by the name of skewness. The fourth moment relates to kurtosis, which is a measure of how heavy the tail of a distribution is. Higher kurtosis corresponds to many outliers, while the converse would signify that the distribution contains little deviations. As you can see, the common theme is that the moment contains information about the defining features of a distribution, which is why it is such a convenient way to present information about a distribution. As the name suggests, MGF is a function that generates the moments of a distribution.",0,0,0,0,1,0,1,0
The Math Behind GANs,"This is because the gradient of the function  is steeper near  than that of the function , meaning that trying to maximize , or equivalently, minimizing  is going to lead to quicker, more substantial improvements to the performance of the generator than trying to minimize . Now that we have defined the loss functions for the generator and the discriminator, it’s time to leverage some math to solve the optimization problem, i.e. finding the parameters for the generator and the discriminator such that the loss functions are optimized. This corresponds to training the model in practical terms. When training a GAN, we typically train one model at a time. In other words, when training the discriminator, the generator is assumed as fixed. We saw this in action in the previous post on how to build a basic GAN. Let’s return back to the min-max game. The quantity of interest can be defined as a function of  and . Let’s call this the value function: In reality, we are more interested in the distribution modeled by the generator than . Therefore, let’s create a new variable, , and use this substitution to rewrite the value function: The goal of the discriminator is to maximize this value function.",0,0,0,0,0,0,1,1
An Introduction to Markov Chain Monte Carlo,"We covered the topic of Markov chains on two posts, one on PageRank and the other on the game of chutes and ladders. Nonetheless, some recap would be of help. [Wikepedia] defines Markov chains as follows: A Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. In other words, a Markov chain is a method of generating a sequence of random variables where the current value of that random variable probabilistically dpends on its prior value. By recursion, this means that the next value of that random variable only depends on its current state. To put this into context, we used Markovian analysis to assess the probability that a user on the internet would move to one site to another in the context of analyzing Google’s PageRank algorithm. Markov chains also popped up when we dealt with chutes and ladders, since the next position of the player in game only depends on their current position on the game board. These examples all demonstrate the Markov property, also known as memorylessness. Later on, we will see how Markov chains come in handy when we decide to “jump” from one number to the next when sampling from the posterior distribution to derive an approximation of the parameter of interest. We also explored Monte Carlo in some detail here on this blog.",1,0,0,0,0,0,0,0
"Beta, Bayes, and Multi-armed Bandits","I personally enjoyed writing this post, not only because I hadn’t written in a long time, but also because it helped me revisit some statistics, which is something that I desperately needed to do—I’m spending way too much time dealing with Django and Selenium these days. Time and again, I realize that there is an element of intuitiveness to Bayesian statistics that, though not obvious at first, starts to make more sense as I explore more into that realm. Of course, frequentist statistics offers a quick and easy way of interpreting certain metrics, but the notion of expectation is something that I had thought was complicated and unnecessary at first, but have gradually come to understand, embrace, and even support. Perhaps writing this post has reaffirmed my Bayesian identity as a budding statistician. I hope you’ve enjoyed reading this post. Catch you up in the next one!.",0,0,0,0,1,0,1,0
Introduction to tf-idf,"The motivation behind tf-idf commences from a simple question: how do we determine the semantic importance of a word in a set of documents? On one hand, words the appear a lot are probably worth paying attention to. For example, in one of my posts on Gaussian distributions, the word “Gaussian” probably appears many times throughout the post. A keyword probably appears frequently in the document; hence the need to calculate tf. On the other hand, there might be words that appear a lot, but aren’t really that important at all. For example, consider the word “denote.” I know that I use this word a lot before writing down equations or formulas, just for the sake of notational clarity. However, the word itself carries little information on what the post is about. The same goes for other words, such as “example,” “however,” and so on. So term frequency only doesn’t really tell us much; instead, we want to pay attention to words that occur frequently in a given document, but doesn’t appear a lot in others—such words are most likely to be unique keywords that potentially capture the gist of that document. Given this analysis, it isn’t difficult to see why tf-idf is designed the way it is. Although we give priority weight to words with high term frequency, we discount words that appear frequently across all documents by dividing tf by idf, or inverse document frequency.",0,0,0,1,0,0,0,0
Building Neural Network From Scratch,"This is why deep learning is such a powerful tool: it can be trained to detect nonlinear, complex patterns in data that a human might otherwise be unable to identify. Our vanilla neural network will make use of two activation functions: softmax and ReLU. If you have read my previous post on the Keras functional API, you might recall that we used softmax and ReLU for certain dense layers. Back then, we considered them to be a blackbox without necessarily taking a look at what they do. Let’s explore the details and get our hands dirty today. Mathematically speaking, the softmax function is a function that takes a vector as input and outputs a vector of equal length. Concretely, where Although the formula may appear complex, the softmax function is a lot simpler than it seems. First, note that all  entries of the returned vector  add up to 1. From here, it is possible to see that the softmax function is useful for ascribing the probability that a sample belongs to one of  classes: the -th element of  would indicate the probability of the sample belonging to the -th class. Put another way, the index of the largest entry in  is the class label number that is most probable. Implementing the softmax function is extremely easy thanks to the vectorized computation made possible through . Presented below is one possible implementation of the softmax function in Python. This particular implementation, however, poses two problems. First, it is susceptible to arithematic overflow.",1,0,1,0,0,0,0,1
PyTorch RNN from Scratch,"It’s also not entirely fair game for the model since there are many names that might be described as multi-national: perhaps there is a Russian person with the name of Demirkan. I learned quite a bit about RNNs by implementing this RNN. It is admittedly simple, and it is somewhat different from the PyTorch layer-based approach in that it requires us to loop through each character manually, but the low-level nature of it forced me to think more about tensor dimensions and the purpose of having a division between the hidden state and output. It was also a healthy reminder of how RNNs can be difficult to train. In the coming posts, we will be looking at sequence-to-sequence models, or seq2seq for short. Ever since I heard about seq2seq, I was fascinated by tthe power of transforming one form of data to another. Although these models cannot be realistically trained on a CPU given the constraints of my local machine, I think implementing them themselves will be an exciting challenge. Catch you up in the next one!.",1,0,0,0,0,0,0,1
Gaussian Mixture Models,"We’ve discussed Gaussians a few times on this blog. In particular, recently we explored Gaussian process regression, which is personally a post I really enjoyed writing because I learned so much while studying and writing about it. Today, we will continue our exploration of the Gaussian world with yet another machine learning model that bears the name of Gauss: Gaussian mixture models. After watching yet another inspiring video by mathematicalmonk on YouTube, I meant to write about Gaussian mixture models for quite some time, and finally here it is. I would also like to thank ritvikmath for a great beginner-friendly explanation on GMMs and Expectation Maximization, as well as fiveMinuteStats for a wonderful exposition on the intuition behind the EM algorithm. Without further ado, let’s jump right into it. The motivating idea behind GMMs is that we can model seemingly complicated distributions as a convex combination of Gaussians each defined by different parameters. One visual analogy I found particularly useful is imagining Gaussians as some sort of hill or mountain on a contour map. If we have multiple hills adjacent to one another, we can essentially model the topography of the region as a combination of Gaussians. At peaks, we would see circular contour lines, but where the hills meet, we might see different patterns, most likely circular patterns overlapping with each other. The key point here is that the combination is convex; in other words, the mixing coefficient for each Gaussian should add up to one.",0,0,0,1,0,0,1,0
Fisher Score and Information,"The next step, as we all know, is to take the derivative of the term in the argument maxima, set it equal to zero, and voila! We have found the maximum likelihood estimate of the parameter. A quick aside that may become later is the fact that maximizing the likelihood amounts to minimizing the loss function. Now here comes the definition of Fisher’s score function, which really is nothing more than what we’ve done above: it’s just the gradient of the log likelihood function. In other words, we have already been implicitly using Fisher’s score to find the maximum of the likelihood function all along, just without explicitly using the term. Fisher’s score is simply the gradient or the derivative of the log likelihood function, which means that setting the score equal to zero gives us the maximum likelihood estimate of the parameter. An important characteristic to note about Fisher’s score is the fact that the score evaluated the true value of the parameter equals zero. Concretely, this means that given a true parameter , This might seem deceptively obvious: after all, the whole point of Fisher’s score and maximum likelihood estimation is to find a parameter value that would set the gradient equal to zero. This is exactly what I had thought, but there are subtle intricacies taking place here that deserves our attention. So let’s hash out exactly why the expectation of the score with respect to the true underlying distribution is zero.",0,0,0,0,0,0,1,0
Wonders of Monte Carlo,"There are a plethora of mathematical techniques that build on top of crude Monte Carlo to ensure that sampling is done correctly and more efficiently, such as importance sampling, but for the purposes of this post, we will stop here and move onto the last task: simulating random walk. The last task we will deal with in this post is simulating what is known as the drunkard’s walk, a version of which is introduced here. The drunkard’s walk is a type of random walk with a specified termination condition. As the name suggests, the drunkard’s walk involves a little story of an intoxicated man trying to reach (or avoid) some destination, whether that be a cliff or, in our case, a restroom. Because he is drunk, he cannot walk to the restroom in a straight path as a normal person would do; instead, he stumbles this way and that, therefore producing a random walk. Our goal here is to simulate this motion many times to estimate the probability that the man would successfully end up in the restroom to go about his business. This example was borrowed from this post by Zacharia Miller. Before we start typing up some code, let’s first lay down the ground rules of this simulation. First, we assume that the pub is modeled as a ten-by-ten grid, the bottom-left point defined as \((0, 0)\) and the top-right \((10, 10)\). The drunkard will start his walk at his table, represented by the coordinate \((5, 5)\).",0,0,0,0,0,0,1,0
Logistic Regression Model from Scratch,"In that post, we derived the formula for cross entropy and intuitively understood it as a way of measuring the “distance” between two distributions. This is exactly what we need: a way of quantifying how different the actual and predicted class labels are! Recall the formula for cross entropy: We can consider class labels as a Bernoulli distribution where data that belongs to class 1 has probability 1 of belonging to that class 1 and probability 0 of belonging to class 0, and vice versa for observations in class 0. The logistic regression model will output a Bernoulli distribution, such as , which means that the given input has a 60 percent chance of belonging to class 1; 40 percent to class 0. Applying this to (3), we get: And that is the loss function we will use for logistic regression! The reason why we have two terms, one involving just  and another involving  is due to the structure of the Bernoulli distribution, which by definition can be written as Now that we have a loss function to work with, let’s build a function that computes cross entropy loss given  and  using (4). The  function returns the average cross entropy over all input data. We use average cross entropy instead of total cross entropy, because it doesn’t make sense to penalize the model for high cross entropy when the input data set was large to begin with.",1,0,0,1,0,0,0,0
Demystifying Entropy (And More),"This yields Recall that the definition of entropy goes as Plugging in this definition to (11) yields the simplified definition of cross entropy: If KL divergence represents the average amount of additional information needed to represent an event with  instead of , cross entropy tells us the average amount of total information needed to represent a stochastic event with  instead of . This is why cross entropy is a sum of the entropy of the distribution  plus the KL divergence between  and . Instead of dwelling in the theoretical realm regurgitating different definitions and interpretations of cross entropy and KL divergence, let’s take a look at a realistic example to gain a better grasp of these concepts. Say we have constructed a neural network to solve a task, such as MNIST hand-written digit classification. Let’s say we have fed our neural network an image corresponding to the number 2. In that case, the true distribution that we are trying to model, represented in vector form, will be  as shown below. The  statement is there to make sure that the probabilities sum up to 1. Let’s assume that our neural network made the following prediction about image. These two distributions, although similar, are different. But the question is, how different? Creating a visualization might give us some idea about the difference between the two distributions.  The two distributions are quite similar, meaning that our neural network did a good job of classifying given data.",0,0,0,0,0,0,1,0
Recommendation Algorithm with SVD,"In eigendecomposition, the factors were all square matrices whose dimension was identical to that of the matrix that we sought to decompose. In SVD, however, since the target matrix can be rectangular, the factors are always of the same shape. The second point to note is that \(U\) and \(V\) are orthogonal matrices; \(\Sigma\), a diagonal matrix. This decomposition structure is similar to that of eigendecomposition, and this is no coincidence: in fact, formula (1) can simply be shown by performing an eigendecomposition on \(A^{T}A\) and \(AA^{T}\). Let’s begin by calculating the first case, \(A^{T}A\), assuming formiula (1). This process looks as follows: The last equality stands since the inverse of an orthogonal matrix is equal to its transpose. Substituting \(\Sigma^2\) for \(\Lambda\), equation (2) simplifies to And we finally have what we have seen with eigendecomposition: a matrix of independent vectors equal to the rank of the original matrix, a diagonal matrix, and an inverse. Indeed, what we have in (3) is an eigendecomposition of the matrix \(A^{T}A\). Intuitively speaking, because matrix \(A\) is not necessarily square, we calculate \(A^{T}A\) to make it square, then perform the familiar eigendecomposition. Note that we have orthogonal eigenvectors in this case because \(A^{T}A\) is a symmetric matrix—more specifically, positive semi-definite. We won’t get into this subtopic too much, but we will explore a very simple proof for this property, so don’t worry. For now, let’s continue with our exploration of the SVD formula by turning our attention from matrix \(V\)—a factor of eigendecomposition on \(A^{T}A\)—to the matrix \(U\).",1,0,1,0,0,0,0,0
First Neural Network with Keras,"Lately, I have been on a DataCamp spree after unlocking a two-month free unlimited trial through Microsoft’s Visual Studio Dev Essentials program. If you haven’t already, make sure to check it out, as it offers a plethora of tools, journal subscriptions, and software packages for developers. Anyhow, one of the courses I decided to check out on DataCamp was titled “Introduction to Deep Learning with Python,” which covered basic concepts in deep learning such as forward and backward propagation. The latter half of the tutorial was devoted to the introduction of the Keras API and the implementation of neural networks. I created this notebook immediately after finishing the tutorial for memory retention and self-review purposes. First, we begin by importing the  library as well as other affiliated functions in the module. Note that Keras uses TensorFlow as backend by default. The warning in the code block below appears because this notebook was written on Google Colab, which informs users that the platform will be switching over to TensorFlow 2 in the future. As you might be able to guess from one of the imported modules, the objective of the neural network will be to classify hand-written digits. In doing so, we will be dealing with a classic in machine learning literature known as the the MNIST data set, which contains images of hand-written digits from 0 to 9, each hand-labeled by researchers. The  variable denotes the total number of class labels available in the classification task, which is 10.",0,0,0,1,0,1,0,1
The Exponential Family,"In this sense, the exponential family is particularly of paramount importance in the field of Bayesian inference, as we have seen many times in previous posts. Let’s concretize our understanding of the exponential family by applying factorization to actual probability distributions. The easiest example, as you might have guessed, is the exponential distribution. Recall that the formula for the exponential distribution is where the indicator function, denoted as , takes the following form: The indicator function is a simple  modification applied to ensure that the function is well-defined across the entire real number domain. Normally, we omit the indicator function since it is self-apparent, but for the sake of robustness in our analysis, I have added it here. How can we coerce equation (6) to look more like (3), the archetypal form that defines the exponential family? Well, now it’s just a matter of drag and match: by paying close attention to the variables, parameters, and the output of each function, we can reconstruct (3) to take the form of (6). The easeist starting point is to observe the exponent to identify  and , after which the rest of the surrounding functions can be inferred. The end result is presented below: After substituting each function with their prescribed value in (8), it isn’t difficult to see that the exponential distribution can indeed by factorized according to the form outlined in (3).",0,0,0,0,1,0,1,0
The Gibbs Sampler,"In this post, we will explore Gibbs sampling, a Markov chain Monte Carlo algorithm used for sampling from probability distributions, somewhat similar to the Metropolis-Hastings algorithm we discussed some time ago. MCMC has somewhat of a special meaning to me because Markov chains was one of the first topics that I wrote about here on my blog. It’s been a while since I have posted anything about math or statistics-related, and I’ll admit that I’ve been taking a brief break from these domains, instead working on some personal projects and uping my Python coding skills. This post is going to be a fun, exciting mesh of some Python and math. Without further ado, let’s get started. I remember struggling to understand Metropolis-Hastings a while back. Gibbs sampling, on the other hand, came somewhat very naturally and intuitively to me. This is not because I’ve suddenly grown intelligent over the past couple of months, but because Gibbs sampling is conceptually simpler, at least in my humble opinion. All that is necessary to understand Gibbs sampling is the notion of conditional probability distributions. We know the classic context in which MCMC comes into play in a Bayesian setting: there is some intractable distribution that we wish to sample from. Metropolis-Hastings was one simple way to go about this, and Gibbs sampling provides another method. A feature that makes Gibbs sampling unique is its restrictive context.",0,0,1,0,0,0,1,0
Building Neural Network From Scratch,"Internally, the  function calls the  gradient descent algorithm to update the weights and finally returns the  which contains updated parameters based on the training data. As mentioned above, each  and  are minibatches that will be feeded into our  gradient descent function. Note that the  function is simply an implementation of equation (7). At the core of the  function is the  function, which is our implementation of back propagation. This provides a nice point of transition to the next section. Back propagation is a smart way of calculating gradients. There are obviously many ways one might go about gradient calculation. We can simply imagine there being a loss function that is a function of all the thousands of weights and biases making up our neural network, and calculate partial derivatives for each parameter. However, this naive aproach is problematic because it is so computationally expensive. Moreover, if you think about it for a second, you might realize that doing so would result in duplicate computations due to the chain rule. Take the simple example below. If we were to calculate the gradient of the loss function with respect to  and , all we need to compute is the gradient of , since that of  will naturally be obtained along the way. In other words, computing the gradient simply requires that we start from the very end of the neural network and propagate the gradient values backwards to compute the partial derivatives according to the chain rule.",1,0,1,0,0,0,0,1
A Brief Introduction to Recurrent Neural Networks,"Last but not least, let’s visualize the training scheme of all four models to take a identify any possible signs of convergence and overfitting, if any. To do that, we will be using the  function shown below. The dense feed-forward network seems to have a very linear pattern. One immediate pattern we see is that the model seems to be overfitting right away, since the testing accuracy decreases with each epoch while the training accuracy increases. This is certainly not a good sign; in the best case scenario, we want to see that training and testing labels moving in the same direction. Perhaps this is the biggest indication that a simple feed forward network is a suboptimal model choice in the context of this problem.  The graphs for the  model seems a lot better. At the very least, we see the training and test labels moving in unison: the accuracy increases with each epoch, while the loss slowly decreases. However, we do see some overfitting happening at the last two epochs or so. Specifically, note that cross entropy loss for the testing data seems to pick up an incrementing pattern past the seventh epoch. This observation suggests that we need to configure the model differently, presumably by decreasing the number of tunable parameters.  Next comes the winner of the day, the LSTM network. An interesting point to note about the learning curve of this model is that the test data accuracy and loss seem to stay roughly stagnant despite the progression of epochs.",0,0,0,0,0,1,0,1
The Exponential Family,"Recall that Therefore, Finally, we have arrived at our destination: We finally know how to calculate the parameter under which the likelihood of observing given data is maximized. The beauty of this approach is that it applies to all probability distributions that belong to the exponential family because our analysis does not depend on which distribution is in question; we started from the canonical form of the exponential family to derive a set of generic equations. This is the convenience of dealing with the exponential family: because they are all defined by the same underlying structure, the MLE equations hold general applicability. In this post, we explored the exponential family of distributions, which I flippantly ascribed the title “The Medici of Probability Distributions.” This is obviously my poor attempt at an intellectual joke, to which many of you might cringe, but I personally think it somewhat captures the idea that many probability distributions that we see on the textbook are, in fact, surprisingly more related than we might think. At least to me, it wasn’t obvious from the beginning that the exponential and the Bernoulli distributions shared the same structure, not to mention the wealth of other distributions that belong to the exponential family.  Also, the convenient factorization is what allowed us to perform an MLE estimation, which is an important concept in statistics with wide ranging applications.",0,0,0,0,1,0,1,0
Revisiting Basel with Fourier,"Turns out that under certain conditions, we can exchange the summation and the integral (or, more strictly speaking, the limit and the integral), using things like the dominating convergence theorem of Fubini’s theorem. However, these are topics for another post. For now, we will assume that this trick is legal and continue on. Now we have Now that we have a summation representation of , let’s move onto (11). We use the same trick we used earlier to interchange the summation and the integral. This gives us Since we have to terms with negative ones with the same exponent, we can safely remove both of them: And notice that we now have the Basel problem! If you plug in  and increment  from there, it is immediately apparent that this is the case. So there we have it, the integral representation of the Basel problem! Let’s look at another example, this time using a double integral representation. The motivation behind this approach is simple. This is a useful result, since it means that we can express the Basel problem as an integral of two different variables. Now, all we need is a summation expression before the integration. And now we are basically back to the Basel problem. Note that we can also use the interchange of integral and summation technique again to reexpress (19) as shown below.",0,1,0,0,0,0,0,0
"0.5!: Gamma Function, Distribution, and More","In a previous post, we looked at the Poisson distribution as a way of modeling the probability of some event’s occurrence within a specified time frame. Specifically, we took the example of phone calls and calculated how lucky I was on the day I got only five calls during my shift, as opposed to the typical twelve. While we clearly established the fact that the Poisson distribution was a more accurate representation of the situation than the binomial distribution, we ran into a problem at the end of the post: how can we derive or integrate the Poisson probability distribution, which is discontinuous? To recap, let’s reexamine the Poisson distribution function: As you can see, this function is discontinuous because of that one factorial term shamelessly flaunting itself in the denominator. The factorial, we might recall, is as an operation is only defined for integers. Therefore, although we can calculate expression such as \(7!\), we have no idea what the expression \(0.5!\) evaluates to. Or do we? Here is where the Gamma function kicks in. This is going to be the crux of today’s post. Let’s jump right into it by analyzing the Gamma function, specifically Euler’s integral of the second kind: At a glance, it is not immediately clear as to why this integral is an interpolation of the factorial function. However, if we try to evaluate this expression through integration by parts, the picture becomes clearer: Notice that the first term evaluates to 0.",0,0,0,0,1,0,1,0
How lucky was I on my shift?,"If we divide our time frame of interest into infinite segments, smaller even than microseconds, we can theoretically model multiple successful events, which is something that the binomial distribution could not account for. Intuitively speaking, this approach is akin to modeling a continuous function as infinitely many stepwise functions such that two “adjacent” dots on the graph could be considered as identical points—or, in probabilistic terms, a simultaneous event. And because we have infinitely many trials and only a fixed number of success, this necessarily means that \(p\) would approach 0. Although this value may seem odd, the argument that the probability of receiving a call at this very instant is 0, since “instant” as a unit of time is infinitely short to have a clearly defined probability. From an algebraic standpoint, \(p = 0\) is necessary to ensure that \(n \cdot p\), the expected number of success, converges to a real value. Now let’s derive the Poisson formula by tweaking the PMF for a binomial distribution. We commence from this expression: We can substitute \(p\) for \(\frac{\lambda}{n}\) from the definition: Using the definition of combinatorics, Recall that \(e^x\) can alternately be defined as \(\lim\limits_{x \to \infty} (1 + \frac{1}{x})^x\). From this definition, it flows that: But then the last term converges to 1 as \(n\) goes to \(\infty\): We can further simplify the rest of the terms in the limit expression as well. Specifically, \(\frac {n!}{(n - k)!}\) collapses to \(n \cdot (n - 1) \cdot (n - 2) \dots (n - k + 1)\).",0,0,0,0,1,0,1,0
A sneak peek at Bayesian Inference,"So far on this blog, we have looked the mathematics behind distributions, most notably binomial, Poisson, and Gamma, with a little bit of exponential. These distributions are interesting in and of themselves, but their true beauty shines through when we analyze them under the light of Bayesian inference. In today’s post, we first develop an intuition for conditional probabilities to derive Bayes’ theorem. From there, we  motivate the method of Bayesian inference as a means of understanding probability. Suppose a man believes he may have been affected with a flu after days of fever and coughing. At the nearest hospital, he is offered to undergo a clinical examination that is known to have an accuracy of 90 percent, i.e. it will return positive results to positive cases 90 percent of the time. However, it is also known that the test produces false positives 50 percent of the time. In other words, a healthy, unaffected individual will test positive with a probability of 50 percent. In cases like these, conditional probability is a great way to package and represent information. Conditional probability refers to a measure of the probability of an event occurring, given that another event has occurred. Mathematically, we can define the conditional probability of event \(A\) given \(B\) as follows: This equation simple states that the conditional probability of \(A\) given \(B\) is the fraction of the marginal probability \(P(B)\) and the area of intersection between those two events, \(P(A \cap B)\).",0,0,0,0,1,0,1,0
Moments in Statistics,"The traditional, no-brainer way of doing this would be to refer to the definition of expected values to compute the sum Computing this sum is not difficult, but it requires some clever manipulations and substitutions. Let’s start by simplifying the factorial in the denominator, and pulling out some expressions out of the sigma. where the third equality stands due to the variant of the Taylor series for the exponential function we looked at earlier: Therefore, we have confirmed that the mean of a Poisson distribution is equal to \(\lambda\), which aligns with what we know about the distribution. Another way we can calculate the first moment of the Poisson is by deriving its MGF. This might sound a lot more complicated than just computing the expected value the familiar way demonstrated above, but in fact, MGFs are surprisingly easy to calculate, sometimes even easier than using the definition expectation. Let’s begin by presenting a statement of the MGF. Let’s factor out terms that contain lambda, which is not affected by the summation. Again, we refer to equation (9) to realize that the sigma expression simplifies into an exponential. In other words, From this observation, we can simplify equation (10) as follows: And there is the MGF of the Poisson distribution! All we have to do to obtain the first moment of the Poisson distribution, then, is to derive the MGF once and set \(t\) to 0.",0,0,0,0,1,0,1,0
Moments in Statistics,"where the second equality stands due to linearity of expectation. All the magic happens when we derive this function with respect to \(t\). At \(t = 0\), all terms in (5) except for the very first one go to zero, leaving us with In other words, deriving the MGF once and plugging in 0 to \(t\) leaves us with the first moment, as expected. If we derive the function again and do the same, And by induction, we can see how the \(n\)th derivative of the MGF at \(t = 0\) would give us the \(n\)th moment of the distribution, \(\mathbf{E}(X^n)\). The easiest way to demonstrate the usefulness of MGF is with an example. For fun, let’s revisit a distribution we examined a long time ago on this blog: the Poisson distribution. To briefly recap, the Poisson distribution can be considered as an variation of the binomial distribution where the number of trials, \(n\), diverges to infinity, with rate of success defined as \(\frac{\lambda}{n}\). This is why the Poisson distribution is frequently used to model how many random events are likely in a given time frame. Here is the probability distribution of the Poisson distribution. Note that \(x\) denotes the number of occurrences of the random event in question. The task here is to obtain the mean of the distribution, i.e. to calculate the first moment, \(\mathbf{E}(X)\).",0,0,0,0,1,0,1,0
Gaussian Process Regression,"Given any non-zero vector , The key takeaway is that, given some positive semi-definite matrix, we can easily factor it into what we might consider to be its square root in the form of . The Cholesky decomposition is extremely useful in the context of sampling. Recall that, in a univariate setting, we can model any normal distribution by simply sampling from a standard normal distribution with zero mean and unit variance: We can extend this simplle idea to the context of multivariate Gaussians. One natural complication, however, is that variance  is a matrix in a multivariate setting. Therefore, we would somehow have to find the standard deviation of the Gaussian, or effectively its square root. This is precisely where the Cholesky decomposition comes in handy. We will be using this means of sampling when implementing GP regression in the next section. Let’s put all the pieces together. The crux of GP regression is conditioning. Recall that Here, the setup was that we have some multivariate Gaussian vector . Given some values for a portion of this random vector, namely , we can then derive another multivariate Gaussian for  using conditioning. This is exactly what we are trying to do with GP regression. Assuming that the data is normally distributed, given a number of training points and their corresponding  values, how can we make predictions at test points? In other words,  are the test points; , the training points.",1,0,0,1,0,0,0,0
An Introduction to Markov Chain Monte Carlo,"The benefit of working with this dumb example is that we can analytically derive a closed-form exprerssion for the posterior distribution. This is because a normal prior is conjugate with a normal likelihood of known variance, meaning that the posterior distribution for the mean will also turn out to be normal. If you are wondering if this property of conjugacy is relevant to the notion of conjugacy discussed above with Bayesian inference, you are exactly correct: statisticians have a laundry list of distributions with conjugate relationships, accessible on this Wikipedia article. The bottom line is that we can calculate the posterior analytically, which essentially gives us an answer with which we can evaluate our implementation of the Metropolis-Hastings algorithm. The equation for the posterior is presented below. This assumes that the data is normally distributed with known variance and that the prior is normal, representable as For a complete mathematical derivation of (8), refer to this document. As we will see later on, we use (9) to calculate the likelihood and (10) to calculate the prior. For now, however, let’s focus on the analyticial derivation of the posterior. This can be achieved by translating the equation in (8) into code as shown below. Let’s see what the posterior looks like given the prior . For sample observations, we use the toy data set  we generated earlier.  There we have it, the posterior distribution given sample observations and a normal prior.",1,0,0,0,0,0,0,0
Logistic Regression Model from Scratch,"The data we will be looking at is the banknote authentification data set, publicly available on the UCI Machine Learning Repository. This data set contains 1372 observations of bank notes, classified as either authentic or counterfeit. The five features columns of this data set are: Let’s use some modules to import this data set onto our notebook, as shown below. The imported data set was slightly modified in two ways to fit our model. First, I separated the class label data from the data set and stored it as a separate  array. Second, I appended s to each observation to create a new column that accounts for intercept approximation. All this means is that we consider our linear model to be where for all available sample observations. This is something that we have been assuming all along throughout the gradient descent derivation process, but had not been stated explicitly to reduce confusion. Just consider it a strategic choice on our part to simplify the model while allowing for the logistic regression model to consider bias. Let’s check the shape of the imported data set to check that the data has been partitioned correctly. Now, it’s time to split the data into training and testing data. To do this, I recycled a function we built earlier in the previous post on k-nearest neighbors algorithm. Using , we can partition the data set into training and testing data. Let’s make 20 percent of observations as testing data and allocate the rest for training.",1,0,0,1,0,0,0,0
Logistic Regression Model from Scratch,"This tutorial is a continuation of the “from scratch” series we started last time with the blog post demonstrating the implementation of a simple k-nearest neighbors algorithm. The machine learning model we will be looking at today is logistic regression. If the “regression” part sounds familiar, yes, that is because logistic regression is a close cousin of linear regression—both models are employed in the context of regression problems. Linear regression is used when the estimation parameter is a continuous variable; logistic regression is best suited to tackle binary classification problems. Implementing the logistic regression model is slightly more challenging due to the mathematics involved in gradient descent, but we will make every step explicit throughout the way. Without further ado, let’s get into it. To understand the clockwork behind logistic regression, it is necessary to understand the logistic function. Simply put, the logistic function is a s-shaped curve the squishes real values between positive and negative infinity into the range . This property is convenient from a machine learning perspective because it allows us to perform binary classification. Binary classification is a type of classification problem where we are assigned the task of categorizing data into two groups. For instance, given the dimensions of a patient’s tumor, determine whether the tumor is malignant or benign. Another problem might involve classifying emails as either spam or not spam. We can label spam emails as 1 and non-spam emails as 0, feed the data into a predefined machine learning algorithm, and generate predictions using that model.",1,0,0,1,0,0,0,0
Dissecting the Gaussian Distribution,"This example illustrates the intuitive link between the multivariate and univariate Gaussian distributions. In this post, we took a look at the normal distribution from the perspective of probability distributions. By working from the definition of what constitutes a normal data set, we were able to completely build the probability density function from scratch. The derivation of the multivariate Gaussian was complicated by the fact that we were dealing with matrices and vectors instead of single scalar values, but the matrix-scalar parallel intuition helped us a lot on the way. Note that the derivation of the multivariate Gaussian distribution introduced in this post is not a rigorous mathematical proof, but rather intended as a gentle introduction to the multivariate Gaussian distribution. I hope you enjoyed reading this post on normal distributions. Catch you up in the next one.",0,0,0,0,1,0,1,0
A Step Up with  Variational Autoencoders,"How do we find this distribution? Well, we know one handy concept that measures the difference or the pseudo-distance between two distributions, and that is Kullback-Leibler divergence. As we discussed in this post on entropy, KL divergence tells us how different two distributions are. So the goal here would be find a distribution that minimizes the following expression: Using the definition of conditional probability, we can simplify (4) as follows: The trick is to notice that  is a constant that can break out of the expectation calculation. Let’s continue by deriving an expression for the evidence term. A useful property to know about KL divergence is the fact that it is always non-negative. We will get into why this is the case in a moment. For now, let’s assume non-negativity to be true and transform (6) into an inequality: The term on the right of the inequality is known as the Evidence Lower Bound, or ELBO for short. Why are we interested in ELBO? First, note that , the evidence, is a constant. Therefore, minimizing KL divergence amounts to maximizing ELBO. This is the key to variational inference: instead of calculating the intractable integral in (3), we can find a distribution  that which minimizes KL divergence by maximizing ELBO, which is a tractable operation. Let’s prove why KL divergence is always greater or equal to zero, which is a condition we assumed to be true in the derivation of ELBO above. For the sake of completeness, I present two ways of proving the same property.",0,0,0,0,0,1,0,1
Gaussian Process Regression,"Therefore, we see little variation on test points near the data. In sparse regions where there is no training data, the model reflects our uncertainty, which is why we observe variation within the sampled functions. Comparing the region  where there is a lot of training data, and  where there is little data, this point becomes apparent. Overall, the average of the fifty samples seems to somewhat capture the overall sinusoidal trend present in the training data, notwithstanding the extraneous curvature observed in some regions. My first attempt at understanding Gaussian processes probably dates back to earlier this year, when I obtained an electronic copy of Rasmussen’s Gaussian Process for Machine Learning. I gave up on chapter 1. The book is still far beyond my current level of mathematics, but nonetheless I am glad that I was able to gain at least a cursory understanding of GP regression. I hope you’ve enjoyed reading this post. In a future post, I hope to dive into another topic I’ve not been able to understand back then: Gaussian mixture models. See you in the next one!.",1,0,0,1,0,0,0,0
Markov Chain and Chutes and Ladders,"” This simply means that the progress of the game depends only on the players’ current positions, not where they were or how they got there. A player might have ended up where they are by taking a ladder or by performing a series of regular dice rolls. In the end, however, all that matters is that the players eventually hit the hundredth cell. To perform a Markov chain analysis on the Chutes and Ladders game, it is first necessary to convert the information presented on the board as a stochastic matrix. How would we go about this process? Let’s assume that we start the game at the \(0\)th cell by rolling a dice. There are six possible events, each with probability of \(1/6\). More specifically, we can end up at the index numbers 38, 2, 3, 14, 5, or 6. In other words, at position 0, where \(C\) and \(X\) denote the current and next position of the player on the game board, respectively. We can make the same deductions for other cases where \(C = 1 \ldots 100\). We are thus able to construct a 101-by-101 matrix representing the transition probabilities of our Chutes and Ladders system, where each column represents the system at a different state, i.e. the \(j\)th entry of the \(i\)th column vector represents the probabilities of moving from cell \(i\) to cell \(j\). To make this more concrete, let’s consider a program that constructs the stochastic matrix , without regards to the chutes and ladders for now.",0,0,1,0,0,0,0,0
PyTorch Tensor Basics,"Not only do the two functions look similar, they also practically do the same thing. Upon more observation, however, I realized that there were some differences, the most notable of which was the .  seemed to be unable to infer the data type from the input given. On the other hand,  was sable to infer the data type from the given input, which was a list of integers. Sure enough,  is generally non-configurable, especially when it comes to data types.  can accept  as a valid argument. The conclusion of this analysis is clear: use  instead of . Indeed, this SO post also confirms the fact that  should generally be used, as  is more of a super class from which other classes inherit. As it is an abstract super class, using it directly does not seem to make much sense. In PyTorch, there are two ways of checking the dimension of a tensor:  and . Note that the former is a function call, whereas the later is a property. Despite this difference, they essentially achieve the same functionality. To access one of the  elements, we need appropriate indexing. In the case of , it suffices to consider the size as a list, meaning that square bracket syntax can be used. In the case of , indices can directly be passed into as an argument to index individual elements in the size tensor. These past few days, I’ve spent a fair amount of time using PyTorch for basic modeling.",0,0,0,0,0,0,0,1
A Step Up with  Variational Autoencoders,"If you are wondering how (1) translates to the return statement, then the following equation might resolve your curiosity. This is the promised elaboration on the relationship between log variance and standard deviation: Therefore, multiplying 0.5 is just a simple algebraic manipulation to morph log variance to standard deviation. The reason why we use log variance instead of just variance or standard deviation is to ensure numerical stability in computation. Now that this part has been cleared, let’s start stacking away layers! Just like the autoencoder, VAEs are composed of two discrete components: the encoder and the decoder. Here, we take a look at the first piece of the puzzle, the encoder network. There are several things to note about this model. First, I decided to use a  loop to simplify the process of stacking layers. Instead of repeating the same code over multiple lines, I found this approach to be more succinct and concise. Second, we define a custom layer at the end, shown as , that uses the  function we defined earlier. This is the final key that enables us to build an encoder model that receives as input a 28-by-28 image, then output a two-dimensional latent vector representation of that image to pass onto the decoder network. Below is the summary of what our model looks like. Note that the model outputs a total of three quantities: , , and .",0,0,0,0,0,1,0,1
Gaussian Process Regression,"However, we would expect them to look even smoother had we augmented the dimensions of the test data to 100 dimensions or more. Next, we need a function from which we generate dummy train data. For the purposes of demonstration, let’s choose a simple sine function. Let’s generate 15 training data points from this function. Note that we are performing a noiseless GP regression, since we did not add any Gaussian noise to . However, we already know how to perform GP with noise, as we discussed earlier how noise only affects the diagonal entries of the kernel. Now it’s time to model the posterior. Recall that the posterior distribution can be expressed as If we use  or  functions to calculate the inverse of the kernel matrix components, out life would admittedly be easy. However, using inversion is not only typically costly, but also prone to inaccuracy. Therefore, we instead opt for a safer method, namely using . In doing so, we will also be introducing some intermediate variables for clarity. Let’s begin with the expression for the posterior mean , which is . The underlying idea is that we can apply Cholesky decomposition on , and use that as a way to circumvent the need for direct inversion. Let , then Let’s make another substitution, .",1,0,0,1,0,0,0,0
Principal Component Analysis,"In this section, we will see how PCA is essentially a way of performing and applying these decomposition techniques under the hood. Recall that eigendecomposition is a method of decomposing matrices as follows: where  is a diagonal matrix of eigenvalues and  is a matrix of eigenvectors. PCA is closely related to eigendecomposition, and this should come as no surprise. Essentially, by finding the eigenvalues and eigenvectors of , we are performing an eigendecomposition on the covariance matrix: Notice that  is a matrix of principal components. Of course, in this case,  is a square matrix of full rank; to apply dimension compression, we need to slice the first  entries of . At any rate, it is clear that PCA involves eigendecomposition of the covariance matrix. Eigendecomposition can only be applied to matrices of full rank. However, there is a more generalized method for non-square matrices, which is singular value decomposition. Here is a blueprint of SVD: Where  is a matrix containing the roots of the eigenvalues, with appropriate dimensional configurations to accommodate the shape of the original matrix. We cannot perform eigendecomposition on , which has no guarantee that it is square; however, SVD is definitely an option. Assume that  can be decomposed into , , and . Then the covariance matrix becomes And we end up in the same place as we did in (25). This is no surprise given that the derivation of SVD involves eigendecomposition. In this post, we took a deep dive into the mathematics behind principal component analysis.",0,0,1,0,0,0,1,0
Traveling Salesman Problem with Genetic Algorithms,"I therefore decided to create a much more contrived example, but with many coordinates, so that we can easily verify whether the path decided on by the algorithm is indeed the optimal path. Namely, we will be arranging city coordinates to lie on a semi-circle, using the very familiar equation Let’s create 100 such fake cities and run the genetic algorithm to optimize the path. If the algorithm does successfully find an optimal path, it will be a single curve from one end of the semi-circle fully connected all the way up to its other end.  The algorithm seems to have converged, but the returned  does not seem to be the optimal path, as it is not a sorted array from 0 to 99 as we expect. Plotting this result, the fact that the algorithm hasn’t quite found the most optimal solution becomes clearer. This point notwithstanding, it is still worth noting that the algorithm has found what might be referred to as optimal segments: notice that there are some segments of the path that contain consecutive numbers, which is what we would expect to see in the optimal path.  An optimal path would look as follows.  Comparing the two, we see that the optimal path returned by the genetic algorithm does contain some wasted traveling routes, namely the the chords between certain non-adjacent cities. Nonetheless, a lot of the adjacent cities are connected (hence the use of the aforementioned term, optimal segments).",1,0,0,0,0,0,0,0
Dissecting LSTMs,"But if our LSTM network only keeps forgetting, obviously this is going to be problematic. Instead, we also want to update the cell state using the new input values. Let’s take a look at the cell state equation again: Previously when discussing the forget gate, we focused only on the first term. Taking a look at the second term, we note that the term is adding some value to the cell state that has been updated to forget information. It only makes sense, then, for the second term to perform the information update sequence. But to understand the second term, we need to take a look at two other equations: (2) is another forward pass involving concatenation, much like we saw in (1) with . The only difference is that, instead of forgetting,  is meant to simulate an update of the cell state. In some LSTM variants,  is simply replaced with , in which case the cell state update would be rewritten as However, we will stick to the convention that uses  instead of the simpler variant as shown in (4-2). The best way to think of  or  is a filter:  is a filter that determines which information to be updated and passed onto the cell state. Now all we need are the raw materials to pass into that filter. The raw material is , defined in (3).",0,0,0,0,0,0,0,1
Fisher Score and Information,"Although the derivation is by no means mathematically robust, it nonetheless vindicates a notion that is not necessary apparently obvious, yet makes a lot of intuitive sense in hindsight. I personally found this video by Ben Lambert to be particularly helpful in understanding the connection between likelihood and information. The gist of it is simple: if we consider the Hessian or the second derivative to be indicative of the curvature of the likelihood function, the variance of our estimate of the optimal parameter  would be larger if the curvature was smaller, and vice versa. In a sense, the larger the value of the information matrix, the more certain we are about the estimate, and thus the more information we know about the parameter. I hope you enjoyed reading this post. Catch you up on another post, most likely on the Leibniz rule, then natural gradient descent!.",0,0,0,0,0,0,1,0
The Magic of Euler’s Identity,"It is also interesting to see that the Taylor series for \(\sin(x)\) is an odd function, while that for \(\cos(x)\) is even, which is coherent with the features of their respective original functions. Last but not least, notice that the derivative of Taylor polynomial of \(e^x\) gives itself, as it should. Now that we have the Taylor polynomials, proving Euler’s identity becomes a straightforward process of plug and play. Let’s plug \(ix\) into the Taylor polynomial for \(e^x\): Notice that we can separate the terrms with and without \(i\): In short, \(e^{ix} = \cos(ix) + i\sin(ix)\)! With this generalized equation in hand, we can plug in \(\pi\) into \(x\) to see Euler’s identity: The classic proof, although fairly straightforward, is not my favorite mode of proving Euler’s identity because it does not reveal any properties about the exponentiation of an imaginary number, or an irrational number for that matter. Instead, I found geometric interpretations of Euler’s formula to be more intuitive and thought-provoking. Below is a version of a proof for Euler’s identity. Let’s start by considering the complex plane. There are two ways of expressing complex numbers on the Argand diagram: points and vectors. One advantage of the vector approach over point representation is that we can borrow some simple concepts from physics to visualize \(e^{it}\) through the former: namely, a trajectory of a point moving along the complex plane with respect to some time parameter \(t\).",0,1,0,0,0,0,0,0
The Magic of Euler’s Identity,"Where would the particle be on the trajectory now? After some thinking, we can convince ourselves that it would lie on the point \((-1, 0)\), since the unit circle has a total circumference of \(2\pi\). And so we have proved that \(e^{i\pi} = -1\), Euler’s identity. But we can also go a step further to derive the generalized version of Euler’s identity. Recall that a unit circle can be expressed by the following equation in the Cartesian coordinate system: On the complex plane mapped in polar coordinates, this expression takes on an alternate form: Notice that this contains the same exact information that Euler’s identity provides for us. It expresses: From this geometric interpretation, we can thus conclude that We now know the exact value that \(e^{ix}\) represents in the complex number system! Urban legend goes that mathematician Benjamin Peirce famously said the following about Euler’s identity: Gentlemen, that is surely true, it is absolutely paradoxical; we cannot understand it, and we don’t know what it means. But we have proved it, and therefore we know it must be the truth. But contrary to his point of view, Euler’s identity is a lot more than just an interesting, coincidental jumble of imaginary and irrational numbers that somehow churn out a nice, simple integer. In fact, it can be used to better understand fundamental operations such as logarithms and powers. Consider, for example, the value of the following expression: Imaginary powers are difficult to comprehend by heart, and I no make no claims that I do.",0,1,0,0,0,0,0,0
