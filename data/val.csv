title,body,from_scratch,statistics,pytorch,machine_learning,linear_algebra,probability_distribution,deep_learning,analysis
The Magic of Euler’s Identity,"At a glance, Euler’s identity is a confusing, mind-boggling mishmash of numbers that somehow miraculously package themselves into a neat, simple form: I remember staring at this identity in high school, trying to wrap my head around the seemingly discordant numbers floating around the equation. Today, I want to share some ideas I have learned since and demonstrate the magic that Euler’s identity can play for us. The classic proof for Euler’s identity flows from the famous Taylor series, a method of expressing any given function in terms of an infinite series of polynomials. I like to understand Taylor series as an approximation of a function through means of differentiation. Recall that a first-order derivative gives the slope of the tangent line at any given point of a function. The second-order derivative provides information regarding the convexity of the function. Through induction, we can convince ourselves that higher order derivatives will convey information about the curvature of the function throughout coordinate system, which is precisely the underlying mechanism behind Taylor’s series. In a more concise notation, we have Notice that  is the starting point of our approximation. Therefore, the Taylor series will provide the most accurate estimation of the original function around that point, and the farther we get away from , the worse the approximation will be. For the purpose of our analysis, let’s examine the Taylor polynomials for the following three functions: , and . Recall that the derivative of  is , which is precisely what the Taylor series suggests. It is also interesting to see that the Taylor series for  is an odd function, while that for  is even, which is coherent with the features of their respective original functions. Last but not least, notice that the derivative of Taylor polynomial of  gives itself, as it should. Now that we have the Taylor polynomials, proving Euler’s identity becomes a straightforward process of plug and play. Let’s plug  into the Taylor polynomial for : Notice that we can separate the terrms with and without : In short, ! With this generalized equation in hand, we can plug in  into  to see Euler’s identity: The classic proof, although fairly straightforward, is not my favorite mode of proving Euler’s identity because it does not reveal any properties about the exponentiation of an imaginary number, or an irrational number for that matter. Instead, I found geometric interpretations of Euler’s formula to be more intuitive and thought-provoking. Below is a version of a proof for Euler’s identity. Let’s start by considering the complex plane. There are two ways of expressing complex numbers on the Argand diagram: points and vectors. One advantage of the vector approach over point representation is that we can borrow some simple concepts from physics to visualize  through the former: namely, a trajectory of a point moving along the complex plane with respect to some time parameter . Notice that introducing this new parameter does not alter the fundamental shape or path of the vector ; it merely specifies the speed at which the particle is traversing the complex plane. You might recall from high school physics that the velocity vector is a derivative of the position vector with respect to time. In other words, Where  is a vector that denotes the position of an object at time . Now, let’s assume that  is such a position vector. Then, it follows from the principles of physics that its derivative will be a velocity vector. Therefore, we have What is so special about this velocity vector? For one, we can see that it is a scalar multiple of the original position vector, . Upon closer examination, we might also convince ourselves that this vector is in fact orthogonal to the position vector. This is because multiplying a point or vector by  in the complex plane effectively flips the object’s  and  components, which is precisely what a 90 degree rotation entails. What does it mean to have a trajectory whose instantaneous velocity is perpendicular to that of the position vector? Hint: think of planetary orbits. Yes, that’s right: this relationship is characteristic of circular motions, a type of movement in which an object rotates around a center of axis. The position vector of a circular motion points outward from the center of rotation, and the velocity vector is tangential to the circular trajectory. The implication of this observation is that the trajectory expressed by the vector  is essentially that of a circle, with respect to time . More specifically, we see that at , , or , which means that the circle necessarily passes through the point  on the complex plane expressed as an Argand graph. From this analysis, we can learn that the trajectory is not just any circle, but a unit circle centered around the origin. But there’s even more! Recall that the velocity vector of the trajectory is a 90-degree rotation of the position vector, i.e. , . Earlier, we concluded that the trajectory expressed by the vector  is a unit circle, which necessarily means that  for all values of . Then, syllogism tells us that  is also one, i.e. the particle on the trajectory moves at unit speed along the unit circle! Now we finally have a full visualization of the position vector. The blue arrow represents the position vector at ; green, the velocity vector also at . Why is speed important? Unit speed implies that the particle moves by  distance units after  time units. Let’s say that  time units have passed. Where would the particle be on the trajectory now? After some thinking, we can convince ourselves that it would lie on the point , since the unit circle has a total circumference of . And so we have proved that , Euler’s identity. But we can also go a step further to derive the generalized version of Euler’s identity. Recall that a unit circle can be expressed by the following equation in the Cartesian coordinate system: On the complex plane mapped in polar coordinates, this expression takes on an alternate form: Notice that this contains the same exact information that Euler’s identity provides for us. It expresses: From this geometric interpretation, we can thus conclude that We now know the exact value that  represents in the complex number system! Urban legend goes that mathematician Benjamin Peirce famously said the following about Euler’s identity: Gentlemen, that is surely true, it is absolutely paradoxical; we cannot understand it, and we don’t know what it means. But we have proved it, and therefore we know it must be the truth. But contrary to his point of view, Euler’s identity is a lot more than just an interesting, coincidental jumble of imaginary and irrational numbers that somehow churn out a nice, simple integer. In fact, it can be used to better understand fundamental operations such as logarithms and powers. Consider, for example, the value of the following expression: Imaginary powers are difficult to comprehend by heart, and I no make no claims that I do. However, this mind-pulverizing expression starts to take more definite meaning once we consider the generalized form of Euler’s identity, . Let . Then we have Take both sides to the power of i: Interestingly enough, we see that  takes on a definitive, real value. We can somewhat intuit this through Euler’s identity, which is basically telling us that there exists some inextricable relationship between real and imaginary numbers. Understood from this point of view, we see that the power operation can be defined in the entire space that is complex numbers. We can also take logarithms of negative numbers. This can simply be shown by starting from Euler’s identity and taking the natural log on both sides. In fact, because  is a periodic function around the unit circle, any odd multiple of  will give us the same result. While it is true that logarithmic functions are undefined for negative numbers, this proposition is only true in the context of real numbers. Once we move onto the complex plane, what may appear as unintuitive and mind-boggling operations suddenly make mathematical sense. This is precisely the magic of Euler’s identity: the marriage of different numbers throughout the number system, blending them together in such a way that seems so simple, yet so incomprehensibly complex and profound.",0,0,0,0,0,0,0,1
"Newton-Raphson, Secant, and More","Recently, I ran into an interesting video on YouTube on numerical methods (at this pont, I can’t help but wonder if YouTube can read my mind, but now I digress). It was a channel called numericalmethodsguy, run by a professor of mechanical engineering at the University of Florida. While the videos themselves were recorded a while back in 2009 at just 240p, I found the contents of the video to be very intriguing and easily digestable. His videos did not seem to assume much mathematical knowledge beyond basic high school calculus. After watching a few of his videos, I decided to implement some numerical methods algorithms in Python. Specifically, this post will deal with mainly two methods of solving non-linear equations: the Newton-Raphson method and the secant method. Let’s dive right into it. Before we move on, it’s first necessary to come up with a way of representing equations in Python. For the sake of simplicity, let’s first just consider polynomials. The most obvious, simplest way of representing polynomials in Python is to simply use functions. For example, we can express  as However, a downside of this approach is the fact that it’s difficult to directly derive this equation despite the fact that it is nothing more than just a simple polynomial. So instead, we will use a list index-based representation. Namely, the  th element of a list represents the coefficient of the th power in a polynomial equation. In other words,  would translate into . The  is a function that returns a Python function given a list that conforms to this list index representation. Let’s see if this works as expected. , so the function passes our quick sanity test. One useful helper function that I also implemented for the sake of convenience is a array-to-equation parser that translates a list representation into a mathematical expression in Python. This is best demonstrated than explained, so I’ll defer myself to an example. Below is the full definition of the  function. At this point, I also thought that it would be useful and interesting to compose a function that translates the string output of  into a proper Python function we can use to calculate values. Below is the  function that receives as input some parsed output string and returns a corresponding Python function. Now, we can do something like this: Now that we have more than enough tools we can use relating to the list index representation we decided to use to represent polynomials, it’s time to exploit the convenience that this representation affords us to calculate derivatives. Calculating derivatives using the list index representation is extremely easy and convenient: in fact, it can be achieved in just a single line. Let’s test this function with the  example we have been using previously. Let’s also use the  function to make the final result for human-readable. Seems like the derivative calculation works as expected. In the process, I got a little bit extra and also wrote a function that integrates a function in list index representation format. If we integrate , we end up with , where  is the integration constant. Excluding the integration constant, we get a result that is consistent with the  function. While it’s great that we can calculate derivatives and integrals, one very obvious drawback of this direct approach is that we cannot deal with non-polynomial functions, such as exponentials or logarithms. Moreover, the list index representation is unable to represent polynomials that include terms whose powers are not positive integers. For these reasons, we will need some other methods of calculating derivatives as well. Hence the motivation for approximation methods, outlined in the section below. If you probe the deepest depths of your memory, somewhere you will recall the following equation, which I’m sure all of us saw in some high school calculus class: This equation, commonly known as the definition of a derivative, is also known as the forward divided difference formula of calculating derivatives. There is another variant, known as the backward divided difference formula: (1) and (2) are almost nearly identical, but the difference lies in which term is subtracted from who. In (1), we go an infinitesimal step forward—hence the —and subtract the value at the point of approximation, . In (2), we go backwards, which is why we get . As  approaches 0, (1) and (2) asymptotically gives us identical results. Below is a Python variant of the backward divided difference formula. Some tweaks have been made to the formula for use in the section that follows, but at its core, it’s clear that the function uses the approximation logic we’ve discussed so far. Another variant of the forward and backward divided difference formula is the center divided difference. By now, you might have some intuition as to what this formula is—as the name implies, we somehow use the center point, instead of going forward or backwards from the point of approximation. Here is the formula: Heuristically, this formula also makes sense. We can imagine going both a step forward and backward, then dividing the results by the total of two steps we’ve taken, one in each direction. Shown below is the Python implementation of the center divided difference formula. According to conventional mathematical wisdom, the center divided method normally provides a more robust way of approximating first order derivatives. In this subsection, we discuss why this is the case. Using Taylor expansion, we can approximate the value of  as follows, given that  goes to 0 under the limit. Notice that we can manipulate (4) to derive the forward divided difference equation in (1). If we move the  term to the LHS, then divide both sides by , we end up with Here, we used big-O notation to denote the order of magnitude of the trailing terms. The trailing terms are significant since they are directly related to the accuracy of our approximation. An error term of  means that, if we halve the step size, we will also halve the error. This is best understood as a linear relationship between error and the step size. We can conduct a similar mode of analysis with backward divided difference. By symmetry, we can express  as If we rearrange (6), we end up with (2). Again, we see that backward divided difference yields linear error, or a trailing term of . Here’s where things get more interesting: in the case of center divided difference, the magnitude of the error term is , meaning that halving the step size decreases the error by four-folds. This is why center divided difference yields much more accurate approximations than forward or backward divided difference. To see this, we subtract (5) from (4), then move some terms, and divide both sides by . Notice that subtracting these two expression results in a lot of term cancellations. Dividing both sides by  yields From this result, we now know why the central divided difference method provides more accurate approximations of the derivative than do forward and backward divided difference. This is somewhat related to what we will be doing in the next section, so it’s a good intuition to have throughout when reading the rest of this article. Now that we have these tools for differential calculus, now comes the exciting part: solving non-linear equations. Specifically, we will be taking a look at two numerical methods: the Newton-Raphson method and the secant method. It’s time to put the methods we developed in the preceding sections to use for solving non-linear equations. Specifically, we’ll begin by taking look at a classic algorithm, the Newton-Raphson method. The Newton-Raphson method is one of the many ways of solving non-linear equations. The intuition behind the Newton-Raphson method is pretty straightforward: we can use tangent lines to approximate the x-intercept, which is effectively the root of the equation . Specifically, we begin on some point on the graph, then obtain the tangent line on that point. Then, we obtain the -intercept of that tangent line, and repeat the process we’ve just completed by starting on a point on the graph whose -value is equal to that -intercept. The following image from Wikipedia illustrates this process quite well. (A digression: It’s interesting to see how “function” and “tangent” are written in German—in case you are wondering, I don’t know a word of German.)  Mathematically, the Newton-Raphson method can be expressed recursively as follows: Deriving this formula is quite simple. Say we start at a point on the graph, . The tangent line from that point will have a slope of . Therefore, the equation of the tangent line can be expressed as Then, the -intercept can simpy be obtained by finding an  value that which makes . Let  denote that point. Then, we arrive at the following update rule. Since we will be using  as the value for the next iteration, , and now we have the update rule as delineated in (4). Below is an implementation of the Newton-Raphson method in Python. I’ve added some parameters to the function for functionality and customization.  is simply some small value we use to decide when to stop the update; if the change in the value of the root is so small that it is not worth the extra compute, we should stop.  determines how many iterations we want to continue. If the algorithm is unable to find the root within  iterations, it likely means that the function provided does not have a root, or at the very least, the root is not discoverable via the algorithm. Lastly,  is a flag that determines whether we return the full update history or simply the last value in the iteration as a single value. One peculiarity that deserves attention is the  exception, which occurs in this case if the number of arguments passed into the function does not match. I added this   block to take into account the fact that the  method and other approximate derivative calculation methods such as  have differing numbers of parameters. Let’s see if this actually works by using the example we’ve been reusing thus far, , or  and , both of which we have already defined and initialized above. The root seems to be around 2.7. And indeed, if we cube it, we end up with a value extremely close to 20. In other words, we have successfully found the root to . Instead of the direct derivative, , we can also use approximation methods. In the example below, we show that using  results in a very similar value (in fact, it is identical in this case, but we need to take into other factors such as numerical stability and overflow which might happen with such high-precision numbers). This result aligns with the earlier observation that center divided difference provides very accurate approximations of the first derivative. Note that the advantage of using  is that we can now apply Newton-Raphson to non-polynomial equations that cannot be formulated in list index representation format. For instance, let’s try something like . To verify that this is indeed correct, we can plug  back into . Also, given that , we can use some heuristics to realize that the answer must be only ever so slightly larger than 1. Notice that the result is extremely close to zero, suggesting that we have found the correct root. Now that we have seen the robustness of the Newton-Raphson method, let’s take a look at another similar numerical method that uses backward divided difference for derivative approximation. In this section, we will look at the secant method, which is another method for identifying the roots of non-linear equations. Before we get into a description of how this method works, here’s a quick graphic, again from Wikipedia, on how the secant method works.  As the name implies, the secant function works by drawing secant lines that cross the function at each iteration. Then, much like the Newton-Raphson method, we find the -intercept of that secant line, find a new point on the graph whose -coordinate corresponds to that intercept, and use the point from the previous iteration to find a new secant line. This process is very intuitively outlined in this video by numericalmethodsguy. The update rule for the secant method can be expressed as We can derive (7) simply by slightly modifying the update rule we saw for Newton-Raphson. Recall that the Newton-Raphson update rule was written as The only modification we need to make to this update rule is to replace  with an approximation using the backward divided difference formula. Here, we make a slight modification to (2), specifically by using values from previous iterations. If we plug (8) back into (4), with some algebraic simplifications, we land on (7), the update rule for the secant method. This is left as an exercise for the reader. Now let’s take a look at how we might be able to implement this numerical method in code. Presented below is the  method, which follows the same general structure as the  function we looked at earlier. The only part that differs is the part where we use backward divided difference, which requires that we look up not only the immediately previous root approximation, but the value prior to that as well. In other words, we need both  and  to calculate  via an iterative update. And here is an obligatory sanity check using our previous example. 2.7 is a familiar value, and indeed it is what was returned by the Newton-Raphson method as well. We confirm that this is indeed the root of the equation. Now that we have looked at both methods, it’s time to make a quick comparison. We will be comparing three different methods: By setting  to be , we can obtain a full list of root updates as the three methods begin their quest for the root of the function. We can then see which method converges the quickest. Let’s see how this little experiment turns out. We first begin by importing some dependencies to plot the history of values. Then, we obtain the history for each of the three approaches and plot them as a scatter plot. The result is shown below.  You might have to squint your eye to see that  (Netwon-Raphson with direct derivatives) and  (Newton-Raphson with center divided difference) almost coincide exactly at the same points. I was honestly somewhat surprised by the result: although we had verified the advantage of using center divided difference over forward or backward divided difference via some simple big-O analysis with trailing error terms, I did not expect the two to coincide with such exactitude. Another interesting observation is that the secant method seems to take slightly longer than the Newton-Raphson method. This is probably due to the fact that the secant method uses backward divided difference, and also the fact that it requires two previous at each iteration instead of one. The reason why the first update seems rather ineffective is that the two initial guesses that we fed into the model was probably not such a good starting point. The topic of today’s post was somewhat different from what we had previously dealt with in this blog, but it was an interesting topic for me nonetheless. I had encountered the Newton-Raphson method previously when going down my typical Wikipedia rabbit holes, but it is only today that I feel like I’ve finally got a grasp of the concept. I consider this post to be a start of many more posts on numerical methods to come. I hope you’ve enjoyed reading this post. See you in the next one.",0,0,0,0,0,0,0,1
The Exponential Family,"Normal, binomial, exponential, gamma, beta, poisson… These are just some of the many probability distributions that show up on just about any statistics textbook. Until now, I knew that there existed some connections between these distributions, such as the fact that a binomial distribution simulates multiple Bernoulli trials, or that the continuous random variable equivalent of the geometric distribution is the exponential. However, reading about the concept of the exponential family of distributions has lent me new insight, and I wish to share that renewed understanding on probability distributions through this post. In this section, we will take a look at what the exponential family of distributions is all about. We will begin by laying out a few mathematical definitions, then proceed to see examples of probability distributions that belong to the exponential family. To cut to the chase, the exponential family simply denotes a  group of probability distributions that satisfy a certain condition, namely that they can be factorized and parametrized into a  specific form, as show below: Here,  is a log noramlizing constant that ensures that the probability distribution integrates to 1. There are other alternative forms that express the same factorization. One such variant that I prefer and find more intuitive uses a simple fractional approach for normalization instead of adding complications to the exponential term. For notational convenience, I will follow the fractional normalization approach shown below throughout this post. Before we proceed any further, it is probably a good idea to clarify the setup of the equations above. First,  denotes a -dimensional random variable of interest; , a -dimensional parameter that defines the probability distribution.  is known as the sufficient statistic function. Below is a brief summary concerning the mappings of these different functions. You will notice that I used  and  instead of  and  as shown in equation (3). This is because (3) assumes vectorization of these functions as follows. We could have expressed (3) without vectorization, but doing so would be rather verbose. So we instead adhere to the vectorized convention in (3) throughout this post. As I hinted earlier, the exponential family covers a wide range of probability distributions, most PDFs and PMFs. In fact, most probability distributions that force themselves onto the page of statistics textbooks belong to this powerful family. Below is a non-comprehensive list of distributions that belong to the exponential family. Probability Density Functions Probability Mass Functions Of course, there are examples of common distributions that do not fall under this category, such as the uniform distribution or the student -distribution. This point notwithstanding, the sheer coverage of the exponential family makes it worthy of exploration and analysis. Also, notion of an exponential family itself is significant in that it allows us to frame problems in meaningful ways, such as through the notion of conjugate priors: if you haven’t noticed, the distributions outlined above all have conjugate priors that also belong to the exponential family. In this sense, the exponential family is particularly of paramount importance in the field of Bayesian inference, as we have seen many times in previous posts. Let’s concretize our understanding of the exponential family by applying factorization to actual probability distributions. The easiest example, as you might have guessed, is the exponential distribution. Recall that the formula for the exponential distribution is where the indicator function, denoted as , takes the following form: The indicator function is a simple  modification applied to ensure that the function is well-defined across the entire real number domain. Normally, we omit the indicator function since it is self-apparent, but for the sake of robustness in our analysis, I have added it here. How can we coerce equation (6) to look more like (3), the archetypal form that defines the exponential family? Well, now it’s just a matter of drag and match: by paying close attention to the variables, parameters, and the output of each function, we can reconstruct (3) to take the form of (6). The easeist starting point is to observe the exponent to identify  and , after which the rest of the surrounding functions can be inferred. The end result is presented below: After substituting each function with their prescribed value in (8), it isn’t difficult to see that the exponential distribution can indeed by factorized according to the form outlined in (3). Although this is by no means a rigorous proof, we see not only the evident fact that the exponential distribution indeed belongs to the exponential family, but also that the factorization formula in (3) isn’t just a complete soup of equations and variables. We can do the same for the Bernoulli distribution, which also falls under the exponential family. The formula for the Bernoulli distribution goes as follows: Again, I have added a very simple indicator function to ensure that the the probability mass function is well-defined across the entire real number line. Again, the indicator function is a simple boolean gate function that checks whether  is an element within a set of zero and one: Factorizing the Bernoulli is slightly more difficult than doing the same for the exponential distribution, largely because it is not apparent from (9) how factorization can be achieved. For example, we do not see any exponential term embedded in (9) as we did in the case of the exponential distributions. Therefore, a simple one-to-one correspondence cannot be identified. The trick to get around this problem is to introduce a log transformation, then reapplying an exponential. In other words, By applying this manipulation, we can artificially create an exponential term to more easily coerce (9) into the factorization mold. Specifically, observe that the power of the exponent can be expressed as a dot product between two vectors, each parameterized by  and  , respectively. This was the hard part: now, all that is left is to configure the rest of the functions to complete the factorization. One possible answer is presented below: By now, it should be sufficienty clear that the definition of the exponential family is robust enough to encompass at least the two probability distributions: the exponential and the Bernoulli. Although we do not go over other examples in this article, the exponential family is a well-defined set of probability distributions that, at thei core, are defined by a common structure. And as we will see in the next section, this underlying similarity makes certain calculations surprisingly convenient. In a previous post, we explorerd the notion of maximum likelihood estimation, and contrasted it with maximum a posteriori estimation. The fundamental question that maximum likelihood estimation seems to answer is: given some data, what parameter of a distribution best explains that observation? This is an interesting question that merits exploration in and of itself, but the discussion becomes a lot more interesting and pertinent in the context of the exponential family. Before diving into MLE, let’s define what is known as the canonical form of the exponential family. Despite its grandiose nomenclature, the canonical form simply refers to a specific flavor of factorization scheme where in which case (3) simplifies to We will assume some arbitrary distribution in the exponential family following this canonical form to perform maxmimum likelihood estimation. Much like in the previous post on maximum likelihood estimation, we begin with some data set of  independent and identically distributed observations. This is going to be the setup of the MLE problem. Given this dataset, the objective of maximum likelihood estimation is to identify some parameter  that maximizes the likelihood, i.e. the probability of observing these data points under a probability distribution defined by . In other words, How do we identify this parameter? Well, the go-to equipment in a mathematician’s arsenal for an optimization problem like this one is calculus. Recall that our goal is to maximize the likelihood function, which can be calculated as follows: The first equality stands due to the assumption that all data are independent and identically distributed. Maximizing (16) is a complicated task, especially because we are dealing with a large product. Products aren’t bad, but we typically prefer sums because they are easier to work with. A simple hack that we almost always use when dealing with maximum likelihood, therefore, is to apply a log transformation to calculate the log likelihood, since the logarithm is a monotonically increasing function. In other words, What does the log likelihood look like? Well, all we have to do is to apply a log function to (16), which yields the following result. Maximizing the log liklihood can be achieved by setting the gradient to zero, as the gods of calculus would tell us. As you might recall from a previous post on some very basic matrix calculus, the gradient is simply a way of packaging derivatives in a multivariate context, typically involving vectors. If any of this sounds unfamilar, I highly recommend that you check out the linked post. We can compute the partial derivative of the log likelihood function with respect to  as shown below. Observe that the last term in (18) is eliminated because it is a constant with respect to . This is a good starting point, but we still have no idea how to derive the log of . To go about this problem, we have to derive an expression for . Recall from the definition of the exponential family that  is a normalizing constant that exists to ensure that the probability function integrates to one. In other words, This necessarily implies that Now that we have an expression for  to work with, let’s try to compute the derivative term we left unsolved in (19). The first and second equalities stand due to the chain rule, and the third equality is a simple algebraic manipulation that recreates the probability function within the integral, allowing us to ultimately express the partial derivative as an expected value of  for the random variable . This is a surprising result, and a convenient one indeed, because we can now use this observation to conclude that the gradient of the log likelihood function is simply the expected value of the sufficient statistic. Therefore, starting again from (19), we can continue our calculation of the gradient and set the quantity equal to zero to calculate the MLE estimate of the parameter. It then follows that How do we interpret the final result in equation (25)? It looks nice, simple, and concise, but what does it mean to say that the expected value of the sufficient statistic is the average of the sufficient statistic for each observed individual data points? To remove  abstractness, let’s employ a simple example, the exponential distribution, and attempt to derive a clearer understanding of the final picture. Recall that the probability density function of the exponential distribution takes the following form according to the factorizations outlined below: Computing the derivative of the log of the normalizing term  as we did in (22), Because we know that the resulting quantity is the expected value of the sufficient statistic, we know that And indeed, this is true: the expected value of the random variable characterized by an exponential distribution is simply the inverse of the parameter defining that distribution. Note that the parameter for the exponential distribution is most often denoted as , in which case the expected value of the distribution would simply be written as . This is all great, but there is still an unanswered question lingering in the air: what is the MLE estimate of the parameter  ? This moment is precisely when equation (25) comes in handy. Recall that Therefore, Finally, we have arrived at our destination: We finally know how to calculate the parameter under which the likelihood of observing given data is maximized. The beauty of this approach is that it applies to all probability distributions that belong to the exponential family because our analysis does not depend on which distribution is in question; we started from the canonical form of the exponential family to derive a set of generic equations. This is the convenience of dealing with the exponential family: because they are all defined by the same underlying structure, the MLE equations hold general applicability. In this post, we explored the exponential family of distributions, which I flippantly ascribed the title “The Medici of Probability Distributions.” This is obviously my poor attempt at an intellectual joke, to which many of you might cringe, but I personally think it somewhat captures the idea that many probability distributions that we see on the textbook are, in fact, surprisingly more related than we might think. At least to me, it wasn’t obvious from the beginning that the exponential and the Bernoulli distributions shared the same structure, not to mention the wealth of other distributions that belong to the exponential family.  Also, the convenient factorization is what allowed us to perform an MLE estimation, which is an important concept in statistics with wide ranging applications. This post in no way claims to give a full, detailed view of the exponential family, but hopefully it gave you some understanding of what it is and why it is useful. In the next post, we will take a look at maximum a posteriori estimation and how it relates to the concept of convex combinations. Stay tuned for more.",0,1,0,0,0,1,0,0
BLEU from scratch,"Recently, I joined the Language, Information, and Learning at Yale lab, led by Professor Dragomir Radev. Although I’m still in what I would consider to be the incipient stages of ML/DL/NLP studies—meaning it will take time for me to be able to actively participate in an contribute to research and publications—I think it will be a great learning experience from which I can glean valuable insight into what research at Yale looks like. One of the first projects I was introduced to at the lab is domain-independent table summarization. As the name implies, the goal is to train a model such that it can extract some meaningful insight from the table and produce a human-readable summary. Members are the lab seem to be making great progress in this project, and I’m excited to see where it will go. In the meantime, I decided to write a short post on BLEU, a metric that I came across while reading some of the survey papers related to this topic. Let’s dive into it. Before going into code and equations, a high-level overview of what BLEU is might be helpful here. BLEU, which stands for Bilingual Evaluation Understudy, is an metric that was introduced to quantitatively evaluate the quality of machine translations. The motivation is clear: as humans, we are able to get an intuitive sense of whether or not a given translation is accurate and of high quality; however, it is difficult to translate this arbitrary linguistic intuition to train NLP models to produce better translations. This is where BLEU comes to the rescue. The way BLEU works is simple. Given some candidate translation of a sentence and a group of reference sentences, we use a bag-of-word approach to see how many occurences of BOWs co-occur in both the translation and reference sentences. BOW is a simple yet highly effective way of ensuring that the machine translation contains key phrases or words that reference translations also contain. In other words, BLEU compares candidate translations with human-produced, annotated reference translations and compares how many hits there are in the candidate sentence. The more BOW hits there are, the better the translation. Of course, there are many more details that go beyond this. For instance, BLEU is able to account for situations in which meaningless words are repeated throughout the machine translation to simply increase BOW hits. It can also penalize translations that are too short. By combining this BOW precision-based approach with some penalization terms, BLEU provides a robust means of evaluating machine translations. With this high-level overview in mind, let’s start implementing BLEU from scratch. First, let’s begin by defining some simple preprocessing and helper functions that we will be using throughout this tutorial. The first on the list is , which converts a given sentence into lowercase and splits it into tokens, which are, in this case, English words. We could make this more robust using regular expressions to remove punctuations, but for the purposes of this demonstration, let’s make this simpler. I decided to use anonymous functions for the sake of simplicity and code readability. Next, let’s write a function that creates n-grams from a given sentence. This involves tokenizing the given sentence using , then looping through the tokens to create a bag of words. And here is a quick sanity check of what we’ve done so far. The BLEU score is based on a familar concept in machine learning: precision. Formally, precision is defined as where  and  stand for true and false positives, respectively. In the context of machine translations, we can consider positives as roughly corresponding to the notion of hits or matches. In other words, the positives are the bag of word n-grams we can construct from a given candidate translation. True positives are n-grams that appear in both the candidate and some reference translation; false positives are those that only appear in the candidate translation. Let’s use this intuition to build a simple precision-based metric. First, we need to create some n-grams from the candidate translation. Then, we iterate through the n-grams to see if they exist in any of the n-grams generated from reference translations. We count the total number of such hits, or true positives, and divide that quantity by the total number of n-grams produced from the candidate translation. Below are some candidate sentences and reference translations that we will be using as an example throughout this tutorial. Comparing  with , it is pretty clear that the former is the better translation. Let’s see if the simple precision metric is able to capture this intuition. And indeed that seems to be the case! However, the simple precision-based metric has some huge problems. As an extreme example, consider the following  candidate translation. Obviously,  is a horrible translation, but the simple precision metric fails to flag it. This is because precision simply involves checking whether a hit occurs or not: it does not check for repeated bag of words. Hence, the original authors of BLEU introduces modified precision as a solution, which uses clipped counts. The gist of it is that, if some n-gram is repeated many times, we clip its count through the following formula: Here,  refers to the number of hits we assign to a certain n-gram. We sum this value over all distinct n-grams in the candidate sentence. Note that the distinction requirement effectively weeds out repetitive translations such as  we looked at earlier.  refers to the number of occurrences of a n-gram in the candidate sentence. For example, in , the unigram  appears 13 times, and so . This value, however, is clipped by , which is the maximum number of occurrence of that n-gram in any one of the reference sentences. In other words, for each reference, we count the number of occurrence of that n-gram and take the maximum value among them. This can seem very confusing, but hopefully it’s clearer once you read the code. Here is my implementation using . Notice that we use a  in order to remove redundancies.  corresponds to ;  corresponds to . Using this modified metric, we can see that the  is now penalized quite a lot through the clipping mechanism. But there are still problems that modified precision doesn’t take into account. Consider the following example translation. To us, it’s pretty obvious that  is a bad translation. Although some of the key words might be there, the order in which they are arranged violates English syntax. This is the limitation of using unigrams for precision analysis. To make sure that sentences are coherent and read fluently, we now have to introduce the notion of n-grams, where  is larger than 1. This way, we can preserve some of the sequential encoding in reference sentences and make better comparison. The fact that unigrams are a poor way of evaluating translations becomes immediately clear once we plot the  in n-grams against modified precision.  As you can see, precision score decreases as  gets higher. This makes sense: a larger  simply means that the window of comparison is larger. Unless whole phrases co-occur in the translation and reference sentences—which is highly unlikely—precision will be low. People have generally found that a suitable  value lies somewhere around 1 and 4. As we will see later, packages like  use what is known as cumulative 4-gram BLEU score, or BLEU-4. The good news is that our current implementation is already able to account for different  values. This is because we wrote a handy little function, . By passing in different values to , we can deal with different n-grams. Now we’re almost done. The last example to consider is the following translation: This is obviously a bad translation. However, due to the way modified precision is currently being calculated, this sentence will likely earn a high score. To prevent this from happening, we need to apply what is known as brevity penalty. As the name implies, this penalizes short candidate translations, thus ensuring that only sufficiently long machine translations are ascribed a high score. Although this might seem confusing, the underlying mechanism is quite simple. The goal is to find the length of the reference sentence whose length is closest to that of the candidate translation in question. If the length of that reference sentence is larger than the candidate sentence, we apply some penalty; if the candidate sentence is longer, than we do not apply any penalization. The specific formula for penalization looks as follows: The brevity penalty term is multiplied to the n-gram modified precision. Therefore, a value of 1 means that no penalization is applied. Let’s perform a quick sanity check to see whether the brevity penalty function works as expected. Finally, it’s time to put all the pieces together. The formula for BLEU can be written as follows: First, some notation clarifications.  specifies the size of the bag of word, or the n-gram.  denotes the weight we will ascribe to the modified precision——produced under that -gram configuration. In other words, we calculate the weighted average of log precision, exponentiate that sum, and apply some brevity penalty. Although this can sound like a lot, really it’s just putting all the pieces we have discussed so far together. Let’s take a look at the code implementation. The weighting happens in the  part within the generator expression within the  statement. In this case, we apply weighting across  that goes from  to . Now we’re done! Let’s test out our final implementation with  for  from 1 to 4, all weighted equally. The  package offers functions for BLEU calculation by default. For convenience purposes, let’s create a wrapper functions. This wrapping isn’t really necessary, but it abstracts out many of the preprocessing steps, such as applying . This is because the  BLEU calculation function expects tokenized input, whereas  and  are untokenized sentences. And we see that the result matches that derived from our own implementation! In this post, we took a look at BLEU, a very common way of evaluating the fluency of machine translations. Studying the implementation of this metric was a meaningful and interesting process, not only because BLEU itself is widely used, but also because the motivation and intuition behind its construction was easily understandable and came very naturally to me. Each component of BLEU addresses some problem with simpler metrics, such as precision or modified precision. It also takes into account things like abnormally short or repetitive translations. One area of interest for me these days is seq2seq models. Although RNN models have largely given way to transformers, I still think it’s a very interesting architecture worth diving into. I’ve also recently ran into a combined LSTM-CNN approach for processing series data. I might write about these topics in a future post. I hope you’ve enjoyed reading this post. Catch you up later!",1,0,0,0,0,0,1,0
Traveling Salesman Problem with Genetic Algorithms,"The traveling salesman problem (TSP) is a famous problem in computer science. The problem might be summarized as follows: imagine you are a salesperson who needs to visit some number of cities. Because you want to minimize costs spent on traveling (or maybe you’re just lazy like I am), you want to find out the most efficient route, one that will require the least amount of traveling. You are given a coordinate of the cities to visit on a map. How can you find the optimal route? The most obvious solution would be the brute force method, where you consider all the different possibilities, calculate the estimated distance for each, and choose the one that is the shortest path. While this is a definite way to solve TSP, the issue with this approach is that it requires a lot of compute—the runtime of this brute force algorithm would be , which is just utterly terrible. In this post, we will consider a more interesting way to approach TSP: genetic algorithms. As the name implies, genetic algorithms somewhat simulate an evolutionary process, in which the principle of the survival of the fittest ensures that only the best genes will have survived after some iteration of evolutionary cycles across a number of generations. Genetic algorithms can be considered as a sort of randomized algorithm where we use random sampling to ensure that we probe the entire search space while trying to find the optimal solution. While genetic algorithms are not the most efficient or guaranteed method of solving TSP, I thought it was a fascinating approach nonetheless, so here goes the post on TSP and genetic algorithms. Before we dive into the solution, we need to first consider how we might represent this problem in code. Let’s take a look at the modules we will be using and the mode of representation we will adopt in approaching TSP. The original, popular TSP requires that the salesperson return to the original starting point destination as well. In other words, if the salesman starts at city A, he has to visit all the rest of the cities until returning back to city A. For the sake of simplicity, however, we don’t enforce this returning requirement in our modified version of TSP. Below are the modules we will be using for this post. We will be using , more specifically a lot of functions from  for things like sampling, choosing, or permuting.  arrays are also generally faster than using normal Python lists since they support vectorization, which will certainly be beneficial when building our model. For reproducibility, let’s set the random seed to 42. Now we need to consider the question of how we might represent TSP in code. Obviously, we will need some cities and some information on the distance between these cities. One solution is to consider adjacency matrices, somewhat similar to the adjacency list we took a look at on the post on Breadth First and Depth First Search algorithms. The simple idea is that we can construct some matrix that represent distances between cities  and  such that  represents the distance between those two cities. When , therefore, it is obvious that  will be zero, since the distance from city  to itself is trivially zero. Here is an example of some adjacency matrix. For convenience purposes, we will represent cities by their indices. Now it’s time for us to understand how genetic algorithms work. Don’t worry, you don’t have to be a biology major to understand this; simple intuition will do. The idea is that, we can use some sort of randomized approach to generate an initial population, and motivate an evolutionary cycle such that only superior genes will survive successive iterations. You might be wondering what genes are in this context. Most typically, genes can be thought of as some representation of the solution we are trying to find. In this case, an encoding of the optimal path would be the gene we are looking for. Evolution is a process that finds an optimal solution for survival through competition and mutation. Basically, the genes that have superior traits will survive, leaving offspring into the next generation. Those that are inferior will be unable to find a mate and perish, as sad as it sounds. Then how do these superior or inferior traits occur in the first place? The answer lies in random mutations. The children of one parent will not all have identical genes: due to mutation, which occurs by chance, some will acquire even more superior features that puts them far ahead of their peers. Needless to say, such beneficiaries of positive mutation will survive and leave offspring, carrying onto the next generation. Those who experience adversarial mutation, on the other hand, will not be able to survive. In genetic algorithm engineering, we want to be able to simulate this process over an extended period of time without hard-coding our solution, such that the end result after hundred or thousands of generations will contain the optimal solution. Of course, we can’t let the computer do everything: we still have to implement mutational procedures that define an evolutionary process. But more on that later. First, let’s begin with the simple task of building a way of modeling a population. First, let’s define a class to represent the population. I decided to go with a class-based implementation to attach pieces of information about a specific generation of population to that class object. Specifically, we can have things like  to represent the full population,  to represent th chosen, selected superior few,  to store the score of the best chromosome in the population,  to store the best chromosome itself, and , the adjacency matrix that we will be using to calculate the distance in the context of TSP. Here is a little snippet of code that we will be using to randomly generate the first generation of population. Let’s see if this everything works as expected by generating a dummy population. Now we need some function that will determine the fitness of a chromosome. In the context of TSP, fitness is defined in very simple terms: the shorter the total distance, the fitter and more superior the chromosome. Recall that all the distance information we need is nicely stored in . We can calculate the sum of all the distances between two adjacent cities in the chromosome sequence. Next, we evaluate the population. Simply put, evaluation amounts to calculating the fitness of each chromosome in the total population, determining who is best, storing the score information, and returning some probability vector whose each element represents the probability that the th element in the population bag is chosen as a parent. We apply some basic preprocessing to ensure that the worst performing chromosome has absolutely no chance of being selected. When we call , we get a probability vector as expected. From the result, it appears that the last element is the best chromosome; the second chromosome in the population bag is the worst. When we call , notice that we get the last element in the population, as previously anticipated. We can also access the score of the best chromosome. In this case, the distance is said to be 86.25. Note that the lower the score, the better, since these scores represent the total distance a salesman has to travel to visit all the cities. Now, we will select  number of parents to be the basis of the next generation. Here, we use a simple roulette model, where we compare the value of the probability vector and a random number sampled from a uniform distribution. If the value of the probability vector is higher, the corresponding chromosome is added to . We repeat this process until we have  parents. As expected, we get 4 parents after selecting the parents through . Now is the crucial part: mutation. There are different types of mutation schemes we can use for our model. Here, we use a simple swap and crossover mutation. As the name implies, swap simply involves swapping two elements of a chromosome. For instance, if we have , we might swap the first two elements to end up with . The problem with swap mutation, however, is the fact that swapping is a very disruptive process in the context of TSP. Because each chromosome encodes the order in which a salesman has to visit each city, swapping two cities may greatly impact the final fitness score of that mutated chromosome. Therefore, we also use another form of mutation, known as crossovers. In crossover mutation, we grab two parents. Then, we slice a portion of the chromosome of one parent, and fill the rest of the slots with that of the other parent. When filling the rest of the slots, we need to make sure that there are no duplicates in the chromosome. Let’s take a look at an example. Imagine one parent has  and the other has . Let’s also say that slicing a random portion of the first parent gave us . Then, we fill up the rest of the empty indices with the other parent, paying attention to the order in which elements occur. In this case, we would end up with . Let’s see how this works. Now, we wrap the swap and crossover mutation into one nice function to call so that we perform each mutation according to some specified threshold. Let’s test it on . When we call , we end up with the population bag for the next generation, as expected. Now it’s finally time to put it all together. For convenience, I’ve added some additional parameters such as  or , but for the most part, a lot of what is being done here should be familiar and straightforward. The gist of it is that we run a simulation of population selection and mutation over  generations. The key part is  and . Basically, we obtain the children from the mutation and pass it over as the population bag of the next generation in the  constructor. Now let’s test it on our TSP example over 20 generations. As generations pass, the fitness score seems to improve, but not by a lot. Let’s try running this over an extended period of time, namely 100 generations. For clarity, let’s also plot the progress of our genetic algorithm by setting  to .  After something like 30 iterations, it seems like algorithm has converged to the minimum, sitting at around 86.25. Apparently, the best way to travel the cities is to go in the order of . But this was more of a contrived example. We want to see if this algorithm can scale. So let’s write some functions to generate city coordinates and corresponding adjacency matrices. generates  number of random city coordinates in the form of a numpy array. Now, we need some functions that will create an adjacency matrix based on the city coordinates. Let’s perform a quick sanity check to see if  works as expected. Here, give vertices of a unit square as input to the function. While we’re at it, let’s also make sure that  indeed does create city coordinates as expected. Now, we’re finally ready to use these functions to randomly generate city coordinates and use the genetic algorithm to find the optimal path using  with the appropriate parameters. Let’s run the algorithm for a few iterations and plot its history.  We can see that the genetic algorithm does seems to be optimizing the path as we expect, since the distance metric seems to be decreasing throughout the iteration. Now, let’s actually try plotting the path along with the corresponding city coordinates. Here’s a helper function to print the optimal path. And calling this function, we obtain the following:  At a glance, it’s really difficult to see if this is indeed the optimal path, especially because the city coordinates were generated at random. I therefore decided to create a much more contrived example, but with many coordinates, so that we can easily verify whether the path decided on by the algorithm is indeed the optimal path. Namely, we will be arranging city coordinates to lie on a semi-circle, using the very familiar equation Let’s create 100 such fake cities and run the genetic algorithm to optimize the path. If the algorithm does successfully find an optimal path, it will be a single curve from one end of the semi-circle fully connected all the way up to its other end.  The algorithm seems to have converged, but the returned  does not seem to be the optimal path, as it is not a sorted array from 0 to 99 as we expect. Plotting this result, the fact that the algorithm hasn’t quite found the most optimal solution becomes clearer. This point notwithstanding, it is still worth noting that the algorithm has found what might be referred to as optimal segments: notice that there are some segments of the path that contain consecutive numbers, which is what we would expect to see in the optimal path.  An optimal path would look as follows.  Comparing the two, we see that the optimal path returned by the genetic algorithm does contain some wasted traveling routes, namely the the chords between certain non-adjacent cities. Nonetheless, a lot of the adjacent cities are connected (hence the use of the aforementioned term, optimal segments). Considering the fact that there are a total of  possibilities, the fact that the algorithm was able to narrow it down to a plausible route that beats the baseline is still very interesting. Genetic algorithms belong to a larger group of algorithms known as randomized algorithms. Prior to learning about genetic algorithms, the word “randomized algorithms” seemed more like a mysterious black box. After all, how can an algorithm find an answer to a problem using pseudo-random number generators, for instance? This post was a great opportunity to think more about this naive question through a concrete example. Moreover, it was also interesting to think about the traveling salesman problem, which is a problem that appears so simple and easy, belying the true level of difficulty under the surface. There are many other ways to approach TSP, and genetic algorithms are just one of the many approaches we can take. It is also not the most effective way, as iterating over generations and generations can often take a lot of time. The contrived semi-circle example, for instance, took somewhere around five to ten minutes to fully run on my 13-inch MacBook Pro. Nonetheless, I think it is an interesting way well worth the time and effort spent on implementation. I hope you’ve enjoyed reading this post. Catch you up in the next one!",1,0,0,0,0,0,0,0
A sneak peek at Bayesian Inference,"So far on this blog, we have looked the mathematics behind distributions, most notably binomial, Poisson, and Gamma, with a little bit of exponential. These distributions are interesting in and of themselves, but their true beauty shines through when we analyze them under the light of Bayesian inference. In today’s post, we first develop an intuition for conditional probabilities to derive Bayes’ theorem. From there, we  motivate the method of Bayesian inference as a means of understanding probability. Suppose a man believes he may have been affected with a flu after days of fever and coughing. At the nearest hospital, he is offered to undergo a clinical examination that is known to have an accuracy of 90 percent, i.e. it will return positive results to positive cases 90 percent of the time. However, it is also known that the test produces false positives 50 percent of the time. In other words, a healthy, unaffected individual will test positive with a probability of 50 percent. In cases like these, conditional probability is a great way to package and represent information. Conditional probability refers to a measure of the probability of an event occurring, given that another event has occurred. Mathematically, we can define the conditional probability of event  given  as follows: This equation simple states that the conditional probability of  given  is the fraction of the marginal probability  and the area of intersection between those two events, . This is a highly intuitive restatement of the definition of conditional probability introduced above: given that event  has already occurred, conditional probability tells us the probability that event  occurs, which is then synonymous to that statement that  has occurred. By the same token, we can also define the reverse conditional probability of  given  through symmetry and substitution. Notice that the numerator stays unchanged since the operation of intersection is commutative. Now let’s develop an intuition for conditional probabilities by applying it to our example of clinical trials and the potentially affected patient. The purported accuracy of the clinical test is 90 percent, which we might express as follows, using the conditional probability notation: By the same token, we can also express the information on false positives as shown below. This conditional probability statement espouses that, given an individual who is not sick, the test returns a false positive 50 percent of the time. Conditional probability provides us with an interesting way to analyze given information. For instance, let  be the event that it rains tomorrow, and  be the event that it is cloudy at the present moment. Although we are no experts in climatology and weather forecast, common sense tells us that since with the additional piece of information that current weather conditions are cloudy, we are inclined to believe that it will likely rain tomorrow, or in the near future. Like this, conditional probability allows us to update our beliefs on uncertainty given new information, and we will see in the later sections that this is the core idea behind Bayesian inference. Let’s return back to the example of the potential patient with a flu. Shortly afterwards at the hospital, the the man was convinced by the doctor and decided to take the clinical test, the result of which was positive. We cannot assume that the man is sick, however, since the test has a rather high rate of false positives as we saw earlier. In this situation, the parameter that is of interest to us can be expressed as In other words, given a positive test result, what is the probability that the man is actually sick? However, we have no means as of yet to directly answer this question; the two pieces of information we have are that , and that . To calculate the value of , we need Bayes’s theorem to do its trick. Let’s quickly derive Bayes’ theorem using the definition of conditional probabilities delineated earlier. Recall that Multiply  and  on both sides of (1) and (2) respectively to obtain the following result: Notice that the two equations describe the same quantity, namely . We can use equivalence to put these two equations together in the following form. Equation (3) can be manipulated in the following manner to finally produce a simple form of Bayes’ theorem: We can motivate a more intricate version this rule by modifying the denominator. Given that  and  are discrete events, we can break down  as a union of intersections between  and , where  represents subsets within event . In concrete form, we can rewrite this as Additionally, we can rewrite the conditional probability  in terms of  and  according to the definition of conditional probability we observed earlier. Applying these alterations to (4) to rewrite  produces equation (5): This is the equation of Bayes’ theorem. In simple language, Bayes’ theorem tells us that the conditional probability of some subset  given  is equal to its relevant fraction within a weighted summation of the conditional probabilities  given . Although this equation may seem complicated at a glance, we can develop an intuition for this formula by reminding ourselves of the definition of conditional probabilities, as well as the fact that independent events can be expressed as a union of intersections. At the end of the day, Bayes’ theorem provides a powerful tool through which we can calculate a conditional probability in terms of its reverse, i.e. calculate  by utilizing . Why is this important at all? Let’s return back to our example of the potential patient. Recall that the conditional probability of our interest was while the pieces of information we were provided were This is where Bayes’ theorem comes in handy. Notice that we have expressed  in terms of  and . From a statistics point of view, all we have to do now is conduct a random survey of the population to see the percentage of the demographic infected with the flu. Let’s say that 15 percent of the population has been affected with this flu. Plugging in the relevant value yields Using Bayes’ theorem, we are able to conclude that there is roughly a 24 percent chance that the man who tests positive on this examination is affected by the flu. That seems pretty low given the 90 percent accuracy of the test, doesn’t it? This ostensible discrepancy originates from the fact that the test has a substantial false positive of 50 percent, and also that the vast majority of the population is unaffected by the disease. This means that, if the entire population were to conduct this test, there would be more false positives than there would be true positives; hence the distortion in the value of the conditional probability. But what if the man were to take the same test again? Intuition tells us that the more test he takes, the more confident we can be on whether the man is or is not affected by the disease. For instance, if the man repeats the exam once and receives a positive report, the conditional probability that he is sick given two consecutive positive test results should be higher than the 24 percent we calculated above. We can see this in practice by reapplying Bayes’ theorem with updated information, as shown below: We see that the value of the conditional probability has indeed increased, lending credence to the idea that the man is sick. Like this, Like this, Bayes’ theorem is a powerful tool that can be used to calculate conditional probabilities and to update them continuously through repeated trials. From a Bayesian perspective, we begin with some expectation, or prior probability, that an event will occur. We then update this prior probability by computing conditional probabilities with new information obtained for each trial, the result of which yields a posterior probability. This posterior probability can then be used as a new prior probability for subsequent analysis. In this light, Bayesian statistics offers a new way to compute new information and update our beliefs about an event in probabilistic terms. Bayesian inference is nothing more than an extension of Bayes’ theorem. The biggest difference between the two is that Bayesian inference mainly deals with probability distributions instead of point probabilities. The case of the potential patient we analyzed above was a simple yet illuminating example, but it was limiting in that we assumed all parameters to be simple constants, such as  for test accuracy and  for false positive frequency. In reality, most statistical estimates exist as probability distributions since there are limitations to our ability to measure and survey data from the population. For example, a simple random sampling of the population might reveal that 15 percent of the sample population is affected with the flu, but this would most likely produce a normal distribution with mean centered around 0.15 instead of a point probability. From a Bayesian standpoint, we would then replace the point probability in our example above with an equation for the distribution, from which we can proceed with the Bayesian analysis of updating our prior with the posterior through repeated testing and computation. Bayes’ theorem, specifically in the context of statistical inference, can be expressed as where  stands for observed or measured data,  stands for parameters, and  stands for some probability distribution. In the language of Bayesian inference,  is the posterior distribution for the parameter ,  is the likelihood function that expresses the likelihood of having parameter  given some observed data ,  is the prior distribution for the parameter , and  is evidence, the marginal probability of seeing the data, which is determined by summing or integrating across all possible values of the parameter, weighted by how strongly we believe in those particular values of . Concretely, Notice that this is not so different from the expansion of the denominator we saw with Bayes’ theorem, specifically equation (5). The only difference here is that the integral takes continuous probability density functions into account, as opposed to discrete point probabilities we dealt with earlier. If we temporarily disregard the constants that show up in (6), we can conveniently trim down the equation for Bayesian inference as follows: This idea is not totally alien to us—indeed, this is precisely the insight we gleaned from the example of the potential patient. This statement is also highly intuitive as well. The posterior probability would be some mix of our initial belief, expressed as a prior, and the data newly presented, the likelihood. Bayesian inference, then, can be understood as a procedure for incorporating prior beliefs with evidence in order to derive an updated posterior. What makes Bayesian inference such a powerful technique is that the derived posterior can themselves be used as a prior for subsequent inference conducted with new data. To see Bayesian inference in action, let’s dive into the most classic, beaten-to-death yet nonetheless useful example in probability and statistics: the coin flip. This example was borrowed from the following post. Assume that we have a coin whose fairness is unknown. To be fair, most coins are approximately fair (no pun intended) given the physics of metallurgy and center of mass, but for now let’s assume that we are ignorant of coin’s fairness, or the lack thereof. By employing Bayesian inference, we can update our beliefs on the fairness of the coin as we accumulate more data through repeated coin flips. For the purposes of this post, we will assume that each coin flip is independent of others, i.e. the coin flips are independent and identically distributed. Let’s start by coming up with a model representation of the likelihood function, which we might recall is the probability of having a parameter value of  given some data . It is not difficult to see that the best distribution for the likelihood function given the setup of the problem is the binary distribution since each coin flip is a Bernoulli trial. Let  denote a random variable that represents the number of tails in  coin flips. For convenience purposes, we define 1 to be heads and 0 to be tails. Then, the conditional probability of obtaining  heads given a fairness parameter  can be expressed as We can perform a quick sanity check on this formula by observing that, when , the probability of observing  heads diminishes to 0, unless , in which case the probability becomes 1. This behavior is expected since  represents a perfectly biased coin that always shows tails. By symmetry, the same logic applies to a hypothetical coin that always shows heads, and represents a fairness parameter of 1. Now that we have derived a likelihood function, we move onto the next component necessary for Bayesian analysis: the prior. Determining a probability distribution for the prior is a bit more challenging than coming up with the likelihood function, but we do have certain clues as to what characteristics our prior should look possess. First, the domain of the prior probability distribution should be contained within . This is because the range of the fairness parameter  is also defined within this range. This constraint immediately tells us that where  is represents the probability density function that represents the prior. Recall that some of the other functions we have looked at, namely binomial, Poisson, Gamma, or exponential are all defined within the unclosed interval , making it unsuitable for our purposes. The Beta distribution nicely satisfies this criterion. The Beta distribution is somewhat similar to the Gamma distribution we analyzed earlier in that it is defined by two shape parameters,  and . Concretely, the probability density function of the Beta distribution goes as follows: The coefficient, expressed in terms of a fraction of Gamma functions, provides a definition for the Beta function. The derivation of the Beta distribution and its apparent relationship with the Gamma function deserves an entirely separate post devoted specifically to the said topic. For the purpose of this post, an intuitive understanding of this distribution and function will suffice. A salient feature of the Beta distribution that is domain is contained within . This means that, application-wise, the Beta distribution is most often used to model a distribution of probabilities, say the batting average of a baseball player as shown in this post. It is also worth noting that the Beta function, which serves as a coefficient in the equation for the Beta PDF, serves as a normalization constant to ensure that integrating the function over the domain  would yield 1 as per the definition of a PDF. To see this, one needs to prove This is left as an exercise for the keen reader. We will revisit this problem in a separate post. Another reason why the Beta distribution is an excellent choice for our prior representation is that it is a conjugate prior to the binomial distribution. Simply put, this means that using the Beta distribution as our prior, combined with a binomial likelihood function, will produce a posterior  that also follows a Beta distribution. This fact is crucial for Bayesian analysis. Recall that the beauty of Bayesian inference originates from repeated applicability: a posterior we obtain after a single round of calculation can be used as a prior to perform the next iteration of inference. In order to ensure the ease of this procedure, intuitively it is necessary for the prior and the posterior to take the same form of distribution. Conjugate priors streamline the Bayesian process of updating our priors with posteriors by ensuring that this condition is satisfied. In simple language, mathematicians have found that certain priors go well with certain likelihoods. For instance, a normal prior goes along with a normal likelihood; Gamma prior, Poisson likelihood; Gamma prior, normal likelihood, and so on. Our current combination, Beta prior and binomial likelihood, is also up on this list. To develop some intuition, here is a graphical representation of the Beta function for different values of  and . This code block produces the following diagram. Graphically speaking, the larger the value of  and , the more bell-shaped it becomes. Also notice that a larger  corresponds to a rightward shift, i.e. a head-biased coin; a larger , a tail-oriented one. When  and  take the same value, the local extrema of the Beta distribution is established at , when the coin is perfectly fair. Now that we have established the usability of the Beta function as a conjugate prior to the binomial likelihood function, let’s finally see Bayesian inference at work. Recall the simplified version of Bayes’ theorem for inference, given as follows: For the prior and the likelihood, we can now plug in the equations corresponding to each distribution to generate a new posterior. Notice that , which stands for data, is now given in the form  where  denotes the number of heads; , the total number of coin flips. Notice also that constants, such as the combinatorial expression or the reciprocal of the Beta function, can be dropped since we are only establishing a proportional relationship between the left and right hand sides. Further simplifications can be applied: But notice that this expression for the posterior can be encapsulated as a Beta distribution since Therefore, we started from a prior of  to end up with a posterior of . This is an incredibly powerful mechanism of updating our beliefs based on presented data. This process also proves that, as purported earlier, the Beta distribution is indeed a conjugate prior of a binomial likelihood function. Now, it’s time to put our theory to the test with concrete numbers. Suppose we start our experiment with completely no expectation as to the fairness of the coin. In other words, the prior would appear to be a uniform distribution, which is really a specific instance of a Beta distribution with . Presented below is a code snippet that simulates 500 coin flips, throughout which we perform five calculations to update our posterior. Executing this code block produces the following figure. This plot shows us the change in our posterior distribution that occurs due to Bayesian update with the processing of each data chunk. Specifically, we perform this Bayesian update after  trials. When no coin flips are performed, as shown in the first subplot, the prior follows a uniform distribution as detailed above. As more coin tosses are performed, however, we start to develop an understanding of the fairness of the coin. When we only have a few data points, the more probability there is that we obtain skewed data, which is why the mean estimate of our posterior seems skewed as well. However, with a larger number of trials, the law of large numbers guarantees that we will eventually be able to identify the value of our parameter , which is indeed the case. The key takeaway from this code block is the line . This is all the Bayesian method there is in this updating procedure. Notice that this line of code directly corresponds to the formula for the updated Beta posterior distribution we found earlier, which is  refers to ,  corresponds to , and both  and  are set to  in order to take into account the initial prior which tends to a uniform distribution. An interesting observation we can make about this result is that the variance of the Beta posterior decreases with more trials, i.e. the narrower the distribution gets. This is directly reflective of the fact that we grow increasingly confident about our estimate of the parameter with more tosses of the coin. At the end of the 500th trial, we can conclude that the coin is fair indeed, which is expected given that we simulated the coin flip using the command . If we were to alter the argument for this method, say , then we would expect the final result of the update to reflect the coin’s bias. Bayes’ theorem is a powerful tool that is the basis of Bayesian statistical analysis. Although our example was just a simple coin toss, the sample principle and mechanism can be extended to countless other situations, which is why Baye’s theorem remains highly relevant to this day, especially in the field of machine learning and statistical analysis. Bayesian statistics presents us with an interesting way of understanding probability. The classical way of understanding probability is the frequentist approach, which purports that a probability for an event is the limit of its frequency in infinite trials. In other words, to say that a coin is fair is to say that, theoretically, performing an infinite number of coin flips would result in 50 percent heads and 50 percent tails. However, the Bayesian approach we explored today presents a drastically different picture. In Bayesian statistics, probability is an embodiment of our subjective beliefs about a parameter, such as the fairness of a coin. By performing trials, infinite or not, we gain more information about the parameter of our interest, which affects the posterior probability. Both interpretations of probability are valid, and they help complement each other to help us gain a broader understanding of what the notion of probability entails. I hope this post gave you a better understanding as to why distributions are important—specifically in the context of conjugate priors. In a future post, we will continue our exploration of the Beta distribution introduced today, and connect the dots between Beta, Gamma, and many more distributions in the context of Bayesian statistics. See you in the next one.",0,1,0,0,0,1,0,0
GAN in PyTorch,"In this blog post, we will be revisiting GANs, or general adversarial networks. This isn’t the first time we’ve seen GANs on this blog: we’ve implemented GANs in Keras, and we have also looked at the mathematics behind GANs. Well, this is somewhat of a repeat of what we’ve done, since all we’re doing here is reimplementing GANs using PyTorch, but I still think it’s worth a revisit. As always, we start by importing the necessary modules in PyTorch, as well as other libraries. Although we’ll be using CPU instead of GPUs, it’s always good to have a  object set up so that we can utilize GPUs should we run this script or notebook in a different environment. We’re not going to do anything too fancy (and I learned from experience that vanilla GANs are incredibly hard to train and can produce artifact-ridden results). Let’s keep in simple this time and try to implement a GAN that generates MNIST images. Below are the configurations we will use for our GAN. We can define a simple transformation that converts images to tensors, then applies a standard normalization procedure for easier training. Next, we create a dataset object and a data loader that batches and shuffles post-transformation images for us. You might recall that GANs are composed of two different networks: a discriminator and a generator. The discriminator, also metaphorically referred to as the police, is a classier that tries to determine whether a given data is real or fake, i.e. produced by the generator network. The generator, on the other hand, tries to generate images that are as realistic as possible, and so is referred to as the counterfeiter. Below is our generator network. Although we could have created DCGANs, or deep convolutional adversarial networks, let’s go simple here and just use fully connected layers. Notice that I’ve used the  PyTorch API instead of using class-based models. In this particular instance, we won’t have a complicated forward method, so the sequential API will suffice. Next, we create the generator. It is also a sequential model, inside of which are stacks of linear layers with ReLU activations. Before we jump into training, let’s move these networks to the  object we’ve created earlier. The interesting part starts here. Notice that we have different optimizers for the discriminator and the generator. This is expected, since we are going to be training two different networks in a different manner. Before we get into the details of training, here is a simple utility function in which we zero the gradients for both the generator and the discriminator. We want the discriminator to be able to distinguish real from fake images. Therefore, the discriminator will have a combined loss: the loss that comes from falsely identifying real images as fake, and the loss that comes from confusing fake, generated images as real ones. For the generator, the loss is actually dependent upon the classifier: the loss comes from the classifier correctly identifying generated images as fake. Let’s see what this means below.      And we see that the generator was able to produce somewhat confusing hand-written images of digits! Granted, this is far from perfect, and there are images that look somewhat funky. However, there are also somewhat realistic images here and there, and it is impressive that a simple densely connected network was able to achieve this performance. Had we used CNNs, the result might have been even better. Personally, I enjoyed this short implementation experiment because I was able to see just how easy and intuitive it is to use PyTorch. I’ll come back in the next post with (probably) another PyTorch tutorial. Catch you in the next one!",0,0,1,0,0,0,1,0
"PyTorch, From Data to Modeling","These past few weeks, I’ve been powering through PyTorch notebooks and tutorials, mostly because I enjoyed the PyTorch API so much and found so many of it useful and intuitive. Well, the problem was that I ended up writing something like ten notebooks without ever publishing them on this blog. So really, I’m going over some old notebooks I’ve coded out more than a month ago to finally make it live. That’s enough excuses, let’s get into the basics of PyTorch modeling in this notebook with the CIFAR10 dataset and some basic CNNs. The setup is pretty simple here. We import some modules and functions from PyTorch, as well as  to be able to show some basic training plots. One thing I have noticed is that a lot of people do something like which I personally don’t really get, because you can easily just do If you ask me, I think the latter is more elegant and less cluttered (after all, we don’t have to repeat  twice). I don’t think the two import statements are functionally different, but if I do figure out any differences, I will make sure to update future notebooks. One of the many nice things about PyTorch is the clean, intuitive API. PyTorch comes with good GPU support, and one of the main ways through which this can be done is by creating a  object. Because I am running this notebook on my MacBook Pro, which obviously does not come with Nvidia cuda-enabled graphics cards, the device is set as the CPU. Now, I can “move” tensors and models up to the GPU by doing something like and these statements would allow inference and training to occur within the GPU. And below are some constants I will be using in this notebook. Namely, we will run training for a total of 4 epochs, with a batch size of 32 and a learning rate of 0.001. Now that we have all the things we need, let’s jump into some data preparation and modeling. Another thing I love about PyTorch is the sheer ease with which you can preprocess data. PyTorch makes it incredibly easy to combine and stack multiple transforms to create custom transformations to be applied to the dataset. The easiest way to go about this is to use the  method, which looks like this: Here, we are applying to transformations: the first changes the dataset and casts it into PyTorch tensors,, and the second one normalizes the dataset to have a mean of 0.5 and a standard deviation of also 0.5 across all three channels of RGB. How can we apply this transform? Well, we can pass it to initialize the datasets as shown below: Because I already have the CIFAR10 downloaded in the  directory of my local, PyTorch does not download the dataset again. We could go with the dataset as-is, but we can use the  class to further batch and shuffle the dataset, which we normally want 99 percent of the time. This is as simple as calling  and passing in the dataset we want to load. If we loop through the , for instance, we can see that it is giving us a nice batch of 32 photos. Note that the dimensions are in the form of . As for the labels, we get 32 values where each number corresponds to an image. As the last step, let’s just make sure that we know what each of these labels correspond to. The  is a tuple of strings that translate label indices to actual strings we can interpret. For example, if we see the label , we know that it denotes , which is . Modeling in PyTorch is the probably the part I love the most. TensorFlow’s sequential API is a great way to start, and PyTorch also provides the same sort of way of building sequential models. However, once you try to build anything that’s more complicated than that, I think PyTorch’s class-based way of approaching modeling makes a lot more intuitive sense and provides more room for experimentation and customization. Before getting into too much detail, below is a very simple CNN we will refer to as an example throughout this post. The first thing you will realize is that the model itself is a class that inherits from . This is a pattern you will see all the time with PyTorch models.  is a super class from which we can inherit to build anything from full-fledged models to custom blocks or layers to be used in some other larger model. In the initialization function, we also define a number of layers that will be used in forward propagation. You might be wondering why these have to initialized in the initialization function, as opposed to the forward function itself. While I don’t have a complete, technically cogent explanation to that question, intuitively, we can understand a model’s layers as being components of the model itself. After all, the weights of these layers are adjusted with each iteration or epoch. In that sense, we want the layers to be attached to the model instance itself; hence the OOP design of PyTorch’s model class. In this particular instance, we define a number of convolutional layers, a pooling layer, and two fully connected layers used for classification output. The declaration of the layers themselves are not too different from other frameworks, such as TensorFlow. Also, I’ve written out all the named arguments so that it is immediately clear what each argument is configuring. Once we’ve declared all the necessary components in the initialization function, the next steps to actually churn out forward propagation results given some input. In PyTorch, this is done by defining the  function. As you can see above, we basically call on the layers we’ve declared in the initialization function via  and pass in any parameters we want. Since this is a very simple CNN, you will see that there is nothing exciting going on; we are simply getting the output of the previous layer and passing that as input into the next. After going through some convolutions and fully connected layers, we can return the result. There are one caveats worth mentioning here, which is the use of . There is a  that I could have easily used, and indeed there is an entire discussion on the PyTorch forum on the difference between the two. The bottom line is that they are pretty similar for our purposes. The most salient difference between the two is that the functional approach cannot be used when declaring a sequential model. However, since we are defining a custom forward function, this limitation does not apply. Personally, I prefer the functional because it means that there is one less layer to declare in the initialization function. However, it’s probably better to err on the side of the  if you’re not totally sure. Now that we’ve designed and implemented a model, it’s time to train it. This is the part where people might argue that TensorFlow 2 or Keras is superior to PyTorch. In Keras, you can simply call  to train the model. However, in PyTorch, this is not necessarily the case, unless you really want to imitate the Keras API and define a  function yourself. PyTorch is more low-level in that you need to define a custom training loop manually. However, I actually prefer this low-levelness because it requires me to really think through what is happening in each iteration, namely what the dimension of each batch is, what the model expects to receive as input in the forward computation, and what loss function is appropriate given the output and label. Let’s see what all of this means. First, we begin by actually initializing an instance of the model, a loss function named , and an , which is Adam in this case. A quick note of caution: if you dig into the PyTorch documentation or look at other example classifiers, you will realize, like me, there are two loss functions you can typically use:  and , or negative log likelihood loss. The difference between the former and latter is that, while the former applies a softmax function to the output before calculating the actual loss, the latter does not. In our case, since we simply output the raw logits instead of applying a softmax calculation, we need to use the former. Let’s return where we were. Before we jump into training and defining the training loop, it’s always a good idea to see if the output of the model is what you’d expect. In this case, we can simply define some random dummy input and see if the output is correct. Now that we’ve verified the input and output dimensions, we can move onto defining the training loop. Defining the training loop may seem difficult at first, especially if you’re coming from a Keras background, but actually a lot of it is boiler plate, and things are not as difficult as they may seem. First, we define a list to hold the loss values per iteration. We will be using this list for visualization later. The exciting part comes next. For each epoch, we load the images in the . Note that the loader returns a tuple of images and labels, which we can unpack directly within the  loop itself. We then move the two objects to , which would be necessary if we were running this one a Cuda-enabled computer. Then, we calculate the loss by calling , the loss function, and append the loss to the  list. Note that we have to call  since  itself is a one-by-one PyTorch tensor. Then comes the important part where we perform backprop. The idea is that we would The three steps correspond to each of the lines in the code above, starting from . As you might be able to guess from the name of the function, we zero the gradients to make sure that we aren’t accumulating gradient values from one iteration to the next. Calling  corresponds to calculating the new gradient values, and  performs the backprop. The last block of code is simply a convenient print function I’ve written to see the progress of training at certain intervals. As you can see, the loss seems to be decreasing for the most part, although it is jumpy at times. Indeed, plotting  makes it clear that the loss has been decreasing, though not entirely in a steady, monotonous fashion.  In retrospect, we could have probably added a batch normalization layer to stabilize and expedite training. However, since this post is largely meant as an introduction to PyTorch modeling, not model optimization or design, the example suffices. Now we have finally reached the last step of the development cycle: testing and evaluating the model. This last step will also require us to write a custom loop as we receive batches of data from the  object we’ve created above. The good news, however, is that the testing loop is not going to look too much different from the training loop; the only difference will be that we will not be backpropagating per each iteration. We will also be using  to make sure that the model is in the evaluation mode, not its default training mode. This ensures that things like batch normalization and dropout work correctly. Let’s talk briefly about the details of this loop. Here, the metric we’re collecting is accuracy. First, we generally see how many correct predictions the model generates. Then, we also see per-class accuracy; that is, whether our model is good at predicting any particular class. This ensures that the model’s performance is balanced throughout all labels. And here is the result! An overall accuracy of 70 percent is definitely not impressive, and we certainly could have done better by building a deeper model, or by using more complex architectures. However, this isn’t the worst performance considering the fact that we only had three convolutional layers. The more important takeaway from this tutorial is how to prepare data, build models, and train and evaluate them through a custom loop. From the tone and style of my writing, it is perhaps immediately clear to you that I am not officially a PyTorch fanboy. Yes, I will admit that I loved Keras for its simplicity, but after having spent more time learning python and DL, I now much prefer the freedom provided by PyTorch’s reasonably abstract API. I hope this notebook provided you with a nice, simple introduction to PyTorch. In the coming notebooks, we will take a deeper dive into implementing models with PyTorch, starting from RNNs all the way up to classic SOTA vision models like InceptionNet, ResNet, and seq2seq models. I can definitely tell you that these are coming, because, as funny as this sounds, I already have all the notetbooks and code ready; I just have to annotate them. I hope you’ve enjoyed reading this post. Catch you up in the next one!",0,0,1,0,0,0,1,0
"Basel, Zeta, and some more Euler","The more I continue my journey down the rabbit hole of mathematics, the more often I stumble across one name: Leonhard Euler. Nearly every concept that I learn, in one way or another, seems to be built on top of some strand of his work, not to mention the unending list of constants, formulas, and series that bears his name. It is simply mind-blowing to imagine that a single person could be so creative, inventive, and productive to the extent that the field of mathematics would not be where it is today had it not been for his birth on April 15, 1707. Why such intensive fanboying, you might ask. Well, let’s remind ourselves of the fact that the interpolation of the factorial through the Gamma function was spearheaded by Euler himself. But this is just the start of the beginning. Consider, for example, the Basel problem, an infamous problem that mathematicians have been trying to solve for nearly a century with no avail, until the 28-year-old Euler came to the rescue. The Basel problem can be stated as follows: At a glance, this seems like a fairly simple problem. Indeed, we know that this series converges to a real value. We also know that integration would give us a rough approximation. However, how can evaluate this series with exactitude? Euler’s solution, simple and elegant, demonstrates his genius and acute heuristics. Euler begins his exposition by analyzing the Taylor expansion of the sine function, which goes as follows: Dividing both sides by , we obtain the following: Now it’s time to follow Euler’s amazing intuition. If we take a close look at the equation , we can convince ourselves that its solutions will adhere to the form , where  is a non-zero integer between . This is expected given the periodic behavior of the sine function and its intercepts with the -axis. Given these zeros, we can then reconstruct the original function  as an infinite product using the fundamental theorem of algebra, or more specifically, Weierstrass factorization. Let’s try to factor out the coefficient of the  term through some elementary induction. First, we observe that calculating the product of the first two terms produces the following expression: Then, we can express the target coefficient, denoted by , as follows: Where  denotes the coefficient of  obtained by expanding the rest of the terms following  in the infinite product above. If we repeat this process once more, a clear pattern emerges: Iterating over this process will eventually allow us to express our target coefficient as a sum of inverse squares multiplied by some constant, in this case : But then we already know the value of  from the modification of the Taylor polynomial for sine we saw earlier, which is ! Therefore, the Basel problem reduces to the following: Therefore, And there we have the solution to the Basel problem. In hindsight, solving the Basel problem is not rocket science; it is a mere application of the Taylor polynomial, coupled with some modifications and operations to mold the problem into a specific form. However, it takes extreme clairvoyance to see the link between the Basel problem and the Taylor polynomial of the sine function. At this point, it doesn’t even surprise us to know that Euler applied this line of thinking to calculate the value of the sum of higher order inverses. An interesting corollary of Euler’s solution to the Basel problem is the Wallis product, which is a representation of the quantity  as an infinite product, as shown below: It seems mathematically unintuitive to say that an irrational number such as  can be expressed as a product of fractions, which is a way of representing rational numbers. However, we can verify the soundness of the Wallis product by substituting  for  in (1): Taking the reciprocal of this entire expression reveals the Wallis product. The Basel problem is a specific case of the Riemann zeta function, whose general form can be written as follows. A small digression: when , the zeta function converges to a value known as Apéry’s constant, eponymously named after the French mathematician who proved its irrationality in the late 20th century. Beyond the field of analytics and pure math, the zeta function is widely applied in fields such as physics and statistics. Perhaps we will explore these topics in the future. So back to the zeta function. In the later segment of his life, Euler found a way to express the zeta function as, you guessed it, an infinite product. This time, however, Euler did not rely on Taylor polynomials. Instead, he employed a more general approach to the problem. It is here that we witness Euler’s clever manipulation of equations again. We commence from the zeta function, whose terms are enumerated below. Much like how we multiply the ratio to a geometric sequence to calculate its sum, we adopt a similar approach by multiplying the second term,  to the entire expression. This operations yields By subtracting this modified zeta function from the original, we derive the following expression below. Now, we only have what might be considered as the odd terms of the original zeta function. We then essentially repeat the operation we have performed so far, by multiplying the expression by  and subtracting the result from the odd-term zeta function. It is not difficult to see that iterating through this process will eventually yield Euler’s product identity for the zeta function. The key to understanding this identity is that only prime numbers will appear as a component of the product identity. We can see this by reminding ourselves of the clockwork behind the sieve of Eratosthenes, which is basically how the elimination and factorization works in the derivation of Euler’s identity. Taking this into account, we can deduce that Euler’s identity will take the following form: This expression is Euler’s infinite product representation of the zeta function. These days, I cannot help but fall in love with Euler’s works. His proofs and discoveries are simple and elegant yet also fundamental and deeply profound, revealing hidden relationships between numbers and theories that were unthought of during his time. I tend to avoid questions like “who was the best  in history” because they most often lead to unproductive discussions that obscure individual achievements amidst meaningless comparisons, but I dare profess here my belief that only a handful of mathematicians can rival Euler in terms of his genius and prolific nature. That is enough Euler for today. I’m pretty sure that this is not going to be the last post on Euler given the sheer amount of work he produced during his lifetime. My exploration of the field of mathematics is somewhat like a random walk, moving from one point to another with no apparent pattern or purpose other than my interest and Google’s search suggestions, but my encounter with Euler will recur continuously throughout this journey for sure. But for now, I’m going to take a brief break from Euler and return back to the topic of good old statistical analysis, specifically Bayesian methods and Monte Carlo methods. Catch you up in the next one!",0,0,0,0,0,0,0,1
