title,body,machine_learning,deep_learning,analysis,pytorch,probability_distribution,from_scratch,linear_algebra,statistics
Dissecting the Gaussian Distribution,"If there is one thing that the field of statistics wouldn’t be complete without, it’s probably normal distributions, otherwise referred to as “the bell curve.” The normal distribution was discovered and studied extensively by Carl Friedrich Gauss, which is why it is sometimes referred to as the Gaussian distribution. We have seen Gaussian distributions before in this blog, specifically on this post on likelihood and probability. However, normal distribution was introduced merely as an example back then. Today, we will put the Gaussian distribution on stage under undivided spotlight. Of course, it is impossible to cover everything about this topic, but it is my goal to use the mathematics we know to derive and understand this distribution in greater detail. Also, it’s just helpful to brush up on some multivariable calculus in a while. Let’s start with the simplest case, the univariate Gaussian distribution. The “univariate” part is just a fancier way of saying that we will dealing be dealing with one-dimensional random variables, i.e. the distribution is going to be plotted on a two-dimensional \(xy\) plane. We make this seemingly trivial distinction to distinguish it from the multivariate Gaussian, which can be plotted on three-dimensional space or beyond. We’ll take a look at the multivariate normal distribution in a later section. For now, let’s derive the univariate case. One of the defining properties of data that are said to be normally distributed when the rate at which the frequencies decrement is proportional to its distance from the mean and the frequencies themselves. Concretely, this statement might be translated as We can separate the variables to achieve the following expression: Integrating both sides yields Let’s get rid of the logarithm by exponentiating both sides. That’s an ugly exponent. But we can make things look better by observing that the constant term \(C\) can be brought down as a coefficient, since where we make the substitution \(A = e^C\). Now, the task is to figure out what the constants \(A\) and \(k\) are. There is one constraint equation that we have not used yet: the integral of a probability distribution function must converge to 1. In other words, Now we run into a problem. Obviously we cannot calculate this integral as it is. Instead, we need to make a clever substitution. Here’s a suggestion: how about we get rid of the complicated exponential through the substitution Then, it follows that Therefore, the integral in (1) now collapses into Now that looks marginally better. But we have a very dirty constant coefficient at the front. Our natural instinct when we see such a square root expression is to square it. What’s nice about squaring in this case is that the value of the expression is going to stay unchanged at 1. Because the two integrals are independent, i.e. calculating one does not impact the other, we can use two different variables for each integral. For notational convenience, let’s use \(x\) and \(y\). We can combine the two integrals to form an iterated integral of the following form: The term \((x + y)^2\) rings a bell, and that bell sounds like circles and therefore polar coordinates. Let’s implement a quick change of variables to move to polar coordinates. Now we have something that we can finally integrate. Using the chain rule in reverse, we get We can consider there to be 1 in the integrand and continue our calculation. The result: From (4), we can express \(A\) in terms of \(k\): After applying the substitution, now our probability density function looks as follows: To figure out what \(k\) is, let’s try to find the variance of \(x\), since we already know that the variance should be equal to \(\sigma\). In other words, from the definition of variance, we know that Using (5), we get We can use integration by parts to evaluate this integral. This integral seems complicated, but if we take a closer look, we can see that there is a lot of room for simplification. First, because the rate of decay of an exponential function is faster than the rate of increase of a first-order polynomial, the first term converges to zero. Therefore, we have But since Therefore, Great! Now we know what the constant \(k\) is: Plugging this expression back into (5), we finally have the equation for the probability distribution function of the univariate Gaussian. And now we’re done! Let’s perform a quick sanity check on (7) by identifying its critical points. Based on prior knowledge, we would expect to find the local maximum at \(x = \mu\), as this is where the bell curve peaks. If we were to dig a bit deeper into prior knowledge, we would expect the point of inflection to be one standard deviations away from the mean, left and right. Let’s verify if these are actually true. From good old calculus, we know that we can obtain the local extrema by setting the first derivative to zero. We can ignore the constants as they are non-zero. Then, we end up with Because the exponent is always positive, the only way for the expression to evaluate to zero is if This tells us that the local maximum of the univariate Gaussian occurs at the mean of the distribution, as we expect. The inflection point can be obtained by setting the second order derivative of the probability distribution function equal to zero. Luckily, we’re already halfway done with calculating the second order derivative since we’ve already computed the first order derivative above. As we have done above, let’s ignore the constants since they don’t affect the calculation. Because the first exponential term cannot equal zero, we can simplify the equation to Therefore, From this, we can see that the inflection point of the univariate Gaussian is exactly one standard deviation away from the mean. This is one of the many interesting properties of the normal distribution that we can see from the formula for the probability distribution. So far, we’ve looked at the univariate Gaussian, which involved only one random variable \(X = x\). However, what if the random variable in question is a vector that contains multiple random variables? It is not difficult to see that answering this question requires us to think in terms of matrices, which is the go-to method of packaging multiple numbers into neat boxes, known as matrices. Instead of deriving the probability distribution for the multivariate Gaussian from scratch as we did for the univariate case, we’ll build on top of the equation for the univariate Gaussian to provide an intuitive explanation for the multivariate case. In a previous post on linear regression, we took a look at matrix calculus to cover basic concepts such as the gradient. We established some rough intuition by associating various matrix calculus operations and their single-variable calculus analogues. Let’s try to use this intuition as a pivot point to extend the univariate Gaussian model to the multivariate Gaussian. For readability sake, here is the univariate model we have derived earlier. Examining (7), the first observation we might make is that \((x - \mu)^2\) is no longer a coherent expression in the multivariable context. The fix to this is extremely simple: recall that in vector world. Therefore, we can reexpress (7) as This is the result of simply changing the squared term. Continuing, the next subject of our interest would be \(\sigma\), as the variance is only strictly defined for one variable, as expressed by its definition below: where \(X\) is a random variable, which takes a scalar value. This necessarily begs the question: what is the multivariable equivalent of variance? To answer this question, we need to understand covariance and the covariance matrix. To jump right into the answer, the multivariable analogue of variance is covariance, which is defined as Notice that \(\text{Cov}(X, X)\) equals variance, which is why we stated earlier that covariance is the multivariate equivalent of variance for univariate quantities. The intuition we can develop from looking at the equation is that covariance measures how far our random variables are from the mean in the \(X\) and \(Y\) directions. More concretely, covariance is expresses the degree of association between two variables. Simply put, if there is a positive relationship between two variables, i.e. an increase in one variable results in a corresponding increase in the other, the variance will be positive; conversely, if an increase in one variable results in a decrease in the other, covariance will be negative. A covariance of zero signifies that there is no linear relationship between the two variables. At a glance, the concept of covariance bears strong resemblance to the notion of correlation, which also explains the relationship between two variables. Indeed, covariance and correlation are related: in fact, correlation is a function of covariance. The biggest difference between correlation and covariance is that correlation is bounded between -1 and 1, whereas covariance is unbounded. The bottom line is that both correlation and covariance measure the strength of linearity between two variables, with correlation being a normalized version of covariance. At this point in time, one might point out that covariance is not really a multivariate concept since it is defined for only two variables, not three or more. Indeed, the expression \(\text{Cov}(X, Y, Z)\) is mathematically incoherent. However, covariance can be  a multivariate metric since we can express the covariance of any pairs of random variables by constructing what is called the covariance matrix. Simply put, the covariance matrix is a matrix whose elements are the pairwise covariance of two random variables in a random vector. Before we get into the explanation, let’s take a look at the equation for the covariance matrix: where \(\Sigma \in \mathbb{R}^{n \times n}\) and \(\mathbf{E}(X) = \mu \in \mathbb{R}^{n \times 1}\). This is the matrix analogue of the expression which is an alternate definition of variance. It is natural to wonder why we replaced the squared expression with \((X - \mu)(X - \mu)^{T}\) instead of \((X - \mu)^{T}(X - \mu)\) as we did earlier with the term in the exponent. The simplest answer that covariance is expressed as a matrix, not a scalar value. By dimensionality, \((X - \mu)(X - \mu)^{T}\) produces a single scalar value, whereas \((X - \mu)(X - \mu)^{T}\) creates a matrix of rank one. We can also see why (9) is coherent by unpacking the expected values expression as shown below: Using the linearity of expectation, we can rewrite the equation as Therefore, we end up with which almost exactly parallels the definition of variance, which we might recall is where \(\mu = \mathbf{E}(X)\). The key takeaway is that the covariance matrix constructed from the random vector \(X\) is the multivariable analogue of variance, which is a function of the random variable \(x\). To gain a better idea of what the covariance matrix actually looks like, however, it is necessary to review its structure element-by-element. Here is the brief sketch of the \(n\)-by-\(n\) covariance matrix. This might seem complicated, but using the definition of covariance in (8), we can simplify the expression as: Note that the covariance matrix is a symmetric matrix since \(\Sigma = \Sigma^{T}\). More specifically, the covariance matrix is a positive semi-definite matrix. This flows from the definition of positive semi-definiteness. Let \(u\) be some arbitrary non-zero vector. Then, You might be wondering how (9) ends up as (10). Although this relationship may not be immediately apparent, that the two expressions are identical can be seen by setting the random vector as and performing basic matrix vector multiplication operations. For the sake of brevity, this is left as an exercise for the reader. We now have all the pieces we need to complete the puzzle. Recall that we were trying to derive the probability density function of the multivariate Gaussian by building on top of the formula for the univariate Gaussian distribution. We finished at then moved onto a discussion of variance and covariance. Now that we understand that the covariance matrix is the analogue of variance, we can substitute \(\sigma^2\) with \(\Sigma\), the covariate matrix. Instead of leaving \(\Sigma\) at the denominator, let’s use the fact that to rearrange the expression. This is another example of when the matrix-scalar parallel intuition can come in handy: the scalar multiplicative identity is 1, whereas the equivalent in matrix world is the identity matrix \(I\). Therefore, the reciprocal of a matrix can be interpreted as its inverse. From this observation, we can conclude that We are almost done, but not quite. Recall the the constant coefficient of the probability distribution originates from the fact that We have to make some adjustments to the constant coefficient since, in the context of the multivariate Gaussian, the integral translates into While it may not be apparent immediately, it is not hard to accept that the correcting coefficient in this case has to be as there are \(n\) layers of iterated integrals to evaluate for each \(x_1\) through \(x_n\). Instead of the matrix \(\Sigma\), we use its determinant \(\lvert \Sigma \rvert\) since we need the coefficient to be a constant, not a matrix term. We don’t go into much detail about the derivation of the constant term; the bottom line is that we want the integral of the probability distribution function over the relevant domain to converge to 1. If we put the pieces of the puzzle back together, we finally have the probability distribution of the multivariate Gaussian distribution: To develop a better intuition for the multivariate Gaussian, let’s take a look at a case of a simple 2-dimensional Gaussian random vector with a diagonal covariance matrix. This example was borrowed from this source. Using the formula for the multivariate Gaussian we derived in (11), we can construct the probability distribution function given \(X\), \(\mu\), and \(\Sigma\). Note that computing \(\Sigma^{-1}\), the inverse of the covariance matrix, can be accomplished simply by taking the reciprocal of its diagonal entries since \(\Sigma\) was assumed to be a diagonal matrix. Continuing, In other words, the probability distribution of seeing a random vector \(\begin{pmatrix} x_1 & x_2 \end{pmatrix}^T\) given \(x_1 \sim \mathcal{N}(\mu_1, \sigma_1^2)\) and \(x_2 \sim \mathcal{N}(\mu_2, \sigma_2^2)\) is equal to the product of the two univariate Gaussians. This result is what we would expect given that \(\text{Cov}(x_1, x_2) = 0\). For instance, if \(x_1\) and \(x_2\) are independent, i.e. observing a value of \(x_1\) does not inform us of anything about \(x_2\) and vice versa, it would make sense that the possibility of observing a random vector \(x\) with entries \(x_1\) and \(x_2\) is merely the product of the independent probabilities of each observing \(x_1\) and \(x_2\). This example illustrates the intuitive link between the multivariate and univariate Gaussian distributions. In this post, we took a look at the normal distribution from the perspective of probability distributions. By working from the definition of what constitutes a normal data set, we were able to completely build the probability density function from scratch. The derivation of the multivariate Gaussian was complicated by the fact that we were dealing with matrices and vectors instead of single scalar values, but the matrix-scalar parallel intuition helped us a lot on the way. Note that the derivation of the multivariate Gaussian distribution introduced in this post is not a rigorous mathematical proof, but rather intended as a gentle introduction to the multivariate Gaussian distribution. I hope you enjoyed reading this post on normal distributions. Catch you up in the next one.",0,0,0,0,1,0,0,1
A Brief Introduction to Recurrent Neural Networks,"Neural networks are powerful models that can be used to identify complex hidden patterns in data. There are many types of neural networks, two of which we have seen already on this blog: the vanilla, feed-forward neural network and convolutional neural networks, often abbreviated as convnets. Today, we will add a third kind to this exciting mix: recurrent neural networks, or RNNs. Let’s take a brief conceptual look at how recurrent neural networks work, then implement a toy RNN to see how it compares to other models on the IMDB movie reviews dataset. I heavily borrowed my examples from Deep Learning with Python by François Chollet and the tutorial on text classification available from the official TensorFlow documentation. Recurrent neural networks, as the name implies, refer to neural network models that contain some sort of internal looping structure that simulates a flow of information. A good way to conceptualize this loop is to think of something like  loops, where a certain operation is performed repeatedly for a specified number of cycles. Given these pieces of information, we might ask ourselves two questions. Firstly, what does this looping operation involve? Secondly, what is the purpose of having this loop in the first place? Let’s try to answer both questions in the following subsections. One of the most salient features of a recurrent neural network is that it is capable of emulating some primitive form of memory. Why might we want to do this? Well, take the example of reading a text. When we read, we don’t process a given text at once in its totality; instead, we break them down into pieces, such as a word or a bag of words, and build our understanding based on information obtained from the previous sequence of text. In other words, processing information through reading is at best understood as a process of continuously receiving new information while retaining information obtained from the previous sequence. This is why recurrent neural network models are frequently employed in the context of natural language processing. But the applications of RNNs extends beyond the domain of NLP. For example, say we are given a dataset of temperature recording of a city district. Obviously, the structural integrity of that dataset is very important, i.e. we should not shuffle the datasets because making weather predictions requires us to understand temporal patterns. In predicting the weather 24 hours from today, data points pertaining to the last two days will be obviously much more important that those recorded a month ago. In such instances of time series analysis, recurrent neural networks perform better than other models we have looked at thus far. To better understand how RNNs work, let’s try to build a very simple recurrent neural network from scratch with . We will only implement forward propagation for the sake of simplicity,  but with some matrix calculus, one can imagine how deriving the formula for back propagation will be possible. Let’s cut to the chase: RNNs emulate memory by using the output from the previous sequence as an input to the next. Perhaps writing this down in matrix notation might give you a better idea of what the statement above means. Here is one way we might implement a very simple recurrent neural network. If the word “recursion” pops up into your mind, then you are on the right track. Notice that in calculating $y_t$, the output of the current sequence, the output of the previous sequence, $y_{t - 1}$ is used. By using the output of the previous sequence, the recurrent neural network is able to “remember” some of the information that was produced by the previous input. Granted, this is not exactly what memory is or how it works in the strictest sense, but we can see how some information from the previous input is trickling down to affect the computation of the current sequence of input data. Note that I used $\tanh$ for the example, but we can just about use any other activation function. Here is one way we might implement this in Python. Although not necessary, I decided to opt for a class-based implementation to make things look tight and nicer. Let’s create a  class object to see that everything works properly. There is really not much point in seeing the output because the calculations are going to be based off of randomly generated data and randomly created weights, but perhaps there is something to be learned from the dimensionality of input and output data. First, note that our model accepts input of size . The dimensionality of the output is a little more tricky because of the  option. What concatenate does is that it basically flattens all  number of outputs into a single list. In this case, because we set the option to , we get a flattened list containing , or 6400 elements. The main takeaway is that recurrent neural networks can be used to implement some sort of memory functionality, which is useful when dealing with datasets where there exists some sort of sequential structure. One way to implement memory is by using the output of the previous sequence to define a  variable, which is used to compute the next output as we have done above. Now let’s get down to business with the  API. Implementing a recurrent neural network is not so much different from building a simple feed forward or convolutional neural network: we simply import a RNN-specific layer and arrange these layers to construct a working model. Before we proceed any further, let’s first import all necessary dependencies for this tutorial. As with any tutorial, we need to start by loading and preprocessing data. Luckily, there isn’t going to be much preprocessing involved since we will be using a dataset available from the  library, the IMBD movie reviews dataset. The dataset contains 25,000 movie reviews from IMDB, labeled as either positive or negative. Each review is encoded as a sequence of integers according to a consistent encoding scheme. In other words, each integer corresponds to a unique word in the vocabulary of the dataset. More specifically, the integer to which a word is mapped corresponds to the frequency with which the word appears, i.e. the word encoded as 10 corresponds to the 10th most frequent word in the data. We will apply some very basic preprocessing on the dataset so that we can feed it into our model. Specifically, we will preprocess the dataset such that only a  number of most frequently occurring words are considered. This step will weed out words that occur very infrequently, thus decreasing the amount of noise from the network’s perspective. Next, we will apply padding to the dataset so that all reviews are of length . This means that longer reviews will be truncated, whereas shorter reviews will be padded with zero entries. Below is a sample code implementation of this process. Let’s load the data using the  function after specifying necessary parameters. Now that the data is here and ready to go, it’s time to  build our neural network. To spice things up a bit, let’s create four different models and compare their performance. Before we jump into that, however, we first need to understand what an embedding layer is, as it is key to natural language processing. Simply put, an embedding layer is a layer that transforms words or integers that encode words into dense vectors. This is a necessary transformation since neural networks are incapable of dealing with non-quantitative variables. Why dense vectors, then? Can’t we simply use one-hot encoding? That is a valid point, since one-hot encoding is how we mostly deal with categorical variables in a dataset. However, one downside of this approach is that we end up with many sparse vectors. In other words, a lot of resources are wasted because the model now has to process vectors of thousands or millions of dimensions, depending on the vocabulary size. Instead of using sparse vectors to represent each word, we can simply use a denser vector of smaller dimensions to encode our data. Another advantage of this approach is that dense vectors can be used to encode semantic information. You might have heard of the famous example that “king minus male equals queen minus female.” If we were to represent the words king, queen, male, and female as vectors, we can add and subtract vectors to represent and distill meaningful information. This vector-based computation is the key to natural language processing with deep neural networks: by back propagating and adjusting the weights of our embedding layer, our model can eventually be trained to “understand” the meaning of words and their relationship with other words in the form of dense vectors. Enough talking, let’s use the embedding layer to build our neural networks, starting with the simple feed-forward model. The feed-forward neural network model will first have an embedding layer that processes input. Then, the output of this embedding layer will be flattened to be passed onto a dense layer with one output transformed by the sigmoid activation function. We use the sigmoid function since we want to conduct a sentiment analysis of determining whether a given movie review is positive or negative. That wasn’t so difficult. Let’s initialize our model by defining the model parameters , , and , then plot the model to see the structure of the network alongside the input and output dimensions of each layer. Note that we defined  to be 16, which means that each word is transformed into dense vectors living in 16 dimensions. By plotting the model, we can get a better idea of the layers that compose the model.  The next model we will build is a simple recurrent neural network. This neural network is going to have an embedding layer, just like the previous model. However, instead of a dense layer, it will have two consecutive  layers stacked on top of each other. The  layer is essentially the  implementation of the  model we built earlier. Let’s take a look. We instantiate the model and take plot the network, just as we have done above.  The  model we built is, as the name shamelessly puts out, pretty simple. There are a lot more advanced recurrent neural networks that have complicated internal cell structures to better emulate human memory, in a sense. The biggest difference between a simple recurrent neural network and an LSTM is that LSTMs have a unique parameter known as the carrier that encodes an additional layer of information about the state of the cell. I might write a separate post devoted to the intricacies of the LSTM, but if you’re an avid reader who’s itching to know more about it right away, I highly recommend this excellent post by Christiopher Olah. For now, let’s just say that LSTMs represent a huge improvement over conventional RNNs, and that we can implement them in  by simply calling the  layer as shown below: Because LSTM layers take a lot longer to train than others, and because the representational capacity of a single LSTM layer is higher than that of others, I decided to use only one LSTM layer instead of two. Let’s initialize this model to take a better look.  The last model we will create is a convnet, which we explored on this previous post on image classification. Convolutional neural networks are great at identifying spatial patterns in data, which is why they also perform reasonably well in natural language processing. Another huge advantage of convents over recurrent networks is that they took a lot lesser time and resources to train. This is why it is often a good idea to build a convent to establish a baseline performance metric. Let’s initialize the model with identical parameters and take a look at its internal structure.  Let’s train all four models using the training data. For control our experiment, we will train all four models over the same , , and . There isn’t much exciting here to look at it terms of code; it’s just a matter of patience, waiting for the models to hopefully converge to a global minimum. For future reference, all training history is dumped in the  object where  corresponds to the model number. After a long time of waiting, the training is finally complete! If you are following this tutorial on your local workstation, please note that the time required for training may vary depending on your hardware configurations or the specification of our instance if you are using a cloud-based platform like AWS. None of our models reached the threshold of ninety percent accuracy, but they all managed to converge to some reasonable number, hovering around the high seventies to low eighties. Let’s test the performance of our models by using the  and  data, both of which none of our models have seen before. Based on the results, it looks like the LSTM model performed best, beating other models by a small margin. At this point, we cannot conclude as to whether or not this marginal boost in performance is significant. Judging this would not only depend on the context, but also most likely require us to have a larger test dataset that captures the statistics of the population data. This point notwithstanding, it is certainly beneficial to know that LSTM networks are good at detecting sequential patterns in data. Last but not least, let’s visualize the training scheme of all four models to take a identify any possible signs of convergence and overfitting, if any. To do that, we will be using the  function shown below. The dense feed-forward network seems to have a very linear pattern. One immediate pattern we see is that the model seems to be overfitting right away, since the testing accuracy decreases with each epoch while the training accuracy increases. This is certainly not a good sign; in the best case scenario, we want to see that training and testing labels moving in the same direction. Perhaps this is the biggest indication that a simple feed forward network is a suboptimal model choice in the context of this problem.  The graphs for the  model seems a lot better. At the very least, we see the training and test labels moving in unison: the accuracy increases with each epoch, while the loss slowly decreases. However, we do see some overfitting happening at the last two epochs or so. Specifically, note that cross entropy loss for the testing data seems to pick up an incrementing pattern past the seventh epoch. This observation suggests that we need to configure the model differently, presumably by decreasing the number of tunable parameters.  Next comes the winner of the day, the LSTM network. An interesting point to note about the learning curve of this model is that the test data accuracy and loss seem to stay roughly stagnant despite the progression of epochs. To better understand this phenomena, we probably have to run more trials with more data over longer iterations than we have done in this tutorial. This point notwithstanding, it is interesting to see how a single layer LSTM network can outperform a stacked RNN network.  The last up on this list is the one-dimensional convolutional neural network. The convent produced very remarkable results in this experiment, especially given its extremely short training time. Recurrent neural networks typically take a lot of time to train—even when they are not stacked—because each neuron is defined by a rather complicated operation involving many parameters, such as states, carriage, and so on. Convents, on the other hand, are relatively simpler, and thus take noticeably shorter to train and deploy. This tutorial demonstrates that convents can perform as well as simple recurrent networks to establish a baseline performance metric.  In this post, we briefly introduced and explored the concept of recurrent neural networks, how they work, and how to build them using the  functional API. Recurrent neural networks are one of the hottest topics in the contemporary deep learning academia because it presents numerous possibilities for applications. Hopefully this post gave you a better understanding of what all the hype is about, why RNNs are effective at what they do, and how they can be used in the context of basic natural language processing. In the next post, we will take a look at another interesting natural language processing task. Peace! My deepest condolences to those affected by the Wuhan corona virus, as well as the families and fans of Kobe Bryant.",0,1,0,0,0,0,0,0
Stirling Approximation,"It’s about time that we go back to the old themes again. When I first started this blog, I briefly dabbled in real analysis via Euler, with a particular focus on factorials, interpolation, and the Beta function. I decided to go a bit retro and revisit these motifs in today’s post, by introducing Stirling’s approximation of the factorial. There are many variants of Stirling’s approximation, but here we introduce the general form as shown: Let’s begin the derivation by first recalling the Poisson distribution. The Poisson distribution is used to model the probability that a certain event occurs a specified number of times within a defined time interval given the rate at which these events occur. The formula looks as follows: One interesting fact about the Poisson distribution is that, when the parameter $\lambda$ is sufficiently large, the Poisson approximates the Gaussian distribution whose mean and variance are both $\lambda$. This happens when the random variable $X = \lambda$. We can easily simplify (2) since the power of the exponent is zero. Thus, we have By simply rearranging (3), we arrive at Stirling’s approximation of the factorial: This is cool, but we still haven’t really shown why a Poisson can be used to approximate a Gaussian—after all, this premise was the bulk of this demonstration. To see the intuition behind this approximation, it is constructive to consider what happens when we add independent Poisson random variables. Say we have $X_1$ and $X_2$, both of which are independent Poisson random variables with mean $\lambda_1$ and $\lambda_2$. Then, $X_1 + X_2$ will be a new Poisson random variable with mean $\lambda_1 + \lambda_2$. If we extend this idea to apply to $n$ independent random variables instead of just two, we can conclude that $n$ collection of independent random variables from $X_1$ to $X_n$ sampled from a population of mean $\frac{\lambda}{n}$ will have mean $\lambda$. And by the nature of the Poisson distribution, the same goes for variance (We will elaborate on this part more below). The Central Limit Theorem then tells us that the distribution of the sum of these random variables will approximate a normal distribution. This concludes a rough proof of the Stirling approximation. For those of you who are feeling rusty on the Poisson distribution as I was, here is a simple explanation on the Poisson—specifically, its mean and variance. By the virtue of the definition of the parameter, it should be fairly clear why $\mathbb{E} = \lambda$: $\lambda$ is a rate parameter that indicates how many events occur within a window of unit time. The expected calculation can easily be shown using Taylor expansion: Next, we prove that the variance of a Poisson random variable defined by parameter $\lambda$ is equal to $\lambda$. Let $X$ be a Poisson random variable. Then, Then, using the definition of variance, we know that From this, we are once again reminded of the defining property of the Poisson, which is that both the mean and variance of a Poisson random variable is defined by the parameter $\lambda$. Let’s tie this back to our original discussion of the Central Limit Theorem. CLT states that, even if a distribution of a random variable is not normal, the distribution of the sums of these random variables will approximate a normal distribution. Using this handy property on the $n$ independent and identically distributed Poisson random variables of mean and variance $\frac{\lambda}{n}$, we can see how the sum of these random variables approximates a Gaussian distribution $\mathcal{N}(\lambda, \lambda)$. Hence the Stirling approximation!
\(\lambda! \approx e^{- \lambda} \lambda^\lambda \sqrt{2 \pi \lambda} \tag{1}\)",0,0,1,0,1,0,0,1
InceptionNet in PyTorch,"In today’s post, we’ll take a look at the Inception model, otherwise known as GoogLeNet. I’ve actually written the code for this notebook in October 😱 but was only able to upload it today due to other PyTorch projects I’ve been working on these past few weeks (if you’re curious, you can check out my projects here and here). I decided to take a brief break and come back to this blog, so here goes another PyTorch model implementation blog post. Let’s jump right into it! First, we import PyTorch and other submodules we will need for this tutorial. Because Inception is a rather big model, we need to create sub blocks that will allow us to take a more modular approach to writing code. This way, we can easily reduce duplicate code and take a bottom-up approach to model design. The  module is a simple convolutional layer followed by batch normalization. We also apply a ReLU activation after the batchnorm. Next, we define the inception block. This is where all the fun stuff happens. The motivating idea behind InceptionNet is that we create multiple convolutional branches, each with different kernel (also referred to as filter) sizes. The standard, go-to kernel size is three-by-three, but we never know if a five-by-five might be better or worse. Instead of engaging in time-consuming hyperparameter tuning, we let the model decide what the optimal kernel size is. Specifically, we throw the model three options: one-by-one, three-by-three, and five-by-five kernels, and we let the model figure out how to weigh and process information from these kernels. In the  below, you will see that there are indeed various branches, and that the output from these branches are concatenated to produce a final output in the  function. Researchers who conceived the InceptionNet architecture decided to add auxiliary classifiers to intermediary layers of the model to ensure that the model actually learns something useful. This was included in InceptionV1; as far as I’m aware, future versions of InceptionNet do not include auxiliary classifiers. Nonetheless, I’ve added it here, just for the fun of it. Now we finally have all the ingredients needed to flesh out the entire model. This is going to a huge model, but the code isn’t too long because we’ve abstracted out many of the building blocks of the model as  or . I won’t get into the details here, as the number of parameters are simply from the original paper. As you can see, there are auxiliary classifiers here and there. If the model is training, we get three outputs in total: , , and . When the model is in , however, we only get , as that’s all we need as the final logits to be passed through a softmax function. Let’s see the gigantic beauty of this model. Great. To be honest, I don’t think the output of the print statement is that helpful; all we know is that the model is huge, and that there is a lot of room for error. So let’s conduct a quick sanity check with a dummy input to see if the model works properly. Great! We’ve passed to the model a batch containing two RGB images of size 224-by-224, which is the standard input assumed by the InceptionNet model. We get in return a tensor of shape , which means we got two predictions, as expected. I’m not going to train this model on my GPU-less MacBook, and if you want to use InceptionNet, there are plenty of places to find pretrained models ready to be used right out of the box. However, I still think implementing this model helped me gain a finer grasp of PyTorch. I can say this with full confidence because a full month has passed since I coded out this Jupyter notebook, and I feel a lot more confident in PyTorch than I used to before. I hope you’ve enjoyed reading this blog post. Catch you up in the next one (where I’ll probably post another old notebook that’s been sitting on my computer for a month).",0,1,0,1,0,0,0,0
Neural Style Transfer,"In today’s post, we will take a look at neural style transfer, or NMT for short. NMT is something that I first came across about a year ago when reading Francois Chollet’s Deep Learning with Python book. At that time, I was just getting my feet wet in deep learning with Keras, and I specifically remember myself skipping the chapter on NMT, feeling unprepared and intimidated by the implementation. After a full year, I feel ready to face the challenge, but this time with PyTorch. Let’s get started! Before we get into any specific implementation details, it’s probably helpful to provide some context on how NMT works. Note that, among the many variations of the NMT algorithm, we are going to be discussing the original one first introduced by Gatys et. al. The goal of NMT is simple: given a content image and a style image, transform the content image to have the look and feel of the style image. Below is an example taken from Yunjey’s PyTorch tutorial, which has been an amazing resource so far in my PyTorch journey.  One peculiarity in the original NMT algorithm is that, unlike in typical scenarios in which we update the model’s parameters training, in NMT we update the pixel values of the clone of the content image itself to gradually stylize it. There is no “NMT model” that transforms some image; rather, we merely calculate a loss that is the combination of the content loss and style loss, then optimize the image with respect to this combined loss. Given some style image $S$, content image $C$, and a resulting generated image $G$, we can write the expression for the total loss as where $\alpha$ is a weight parameter that determines the degree with which we want to prioritize style over content. Intuitively, the more stylized an image, the higher the content loss; the smaller the content loss, the higher the style loss. In a way, these two quantities are somewhat mutually exclusive, which is why we want to use a weight constant to ascribe some level of importance to one over the other. Some variations to this formula include those that include weights for both the style and content terms, such as At the end of the day, both formulations are identical, only scaled by some scalar value. (2) is a special case of (1) where $\beta = 1$. Thus, we can always go from (1) to (2) simply by multiplying by some constant. For simplicity reasons, we will assume (1) throughout this tutorial. A natural question to ask, then, is how we calculate each of these loss terms. Somehow, these loss terms should be able to capture how different two images are, content-wise or style-wise. This is where feature extractors come into play. Pretrained models, such as the VGG network, have filters that are capable of extracting features from an image. It is known that low level convolutional filters that are closer to the input can extract low-level features such as lines or curves, whereas deeper layers are trained to have activation maps that respond to specific shapes or patterns. Notice that this is in line with what the content loss should be able to encode: the general lines and curves of the image should remain similar, as well as the location or presence of general objects like eyes, nose, or hands, to give some concrete examples. Thus, the content loss is simply the L2 norm of the features extracted from each target layer of some pretrained model $m$. Do not let the notation confuse you. All this means is that we sum over each layers of the pretrained model $m$. For each of these layers, we calculate matrix element-wise L2 norm of the content and generated image features extracted by the $l$th layer of the model. If we sum all of them up, we obtain the value of the content loss. Intuitively, we can think of this as comparing both high level and low level features between the two images. The style loss is somewhat trickier, but not too much. The authors of the original NMT paper used what is called the Gram matrix, sometimes also referred to as the Gramian matrix. The Gram matrix, despite its fancy name, is something that you’ve already seen at some point in your study of linear algebra. Given some matrix $A$, the Gram matrix can be calculated as More strictly speaking, given a set of vectors $V$, a Gram matrix can be calculated such that So how does the Gram matrix encode the stylistic similarities or differences between two images? Before I attempt at an explanation in words, I recommend that you check out this Medium article, which has helped me wrapped my own head around the different dimensions involved in the style loss term. This Medium article has also helped me gain more intuition on why the style loss is the way it is. The motivating idea is that, given an image and a layer in the feature extractor model, the activations each encode information coming from a filter. The resulting feature maps, therefore, contain information about some feature the model has learned, such as the presence of some pattern, shape, or object. By flattening each feature map and constructing a matrix of activations $A \in \mathbb{R}^{n \times m}$, where $n$ is the number of filters and $m$ is the width times height of each activation, we can now construct the Gram matrix. Effectively, the Gram matrix is a dot product of each rows of $A$; thus, if some $i$th and $j$th features tend to co-occur, $G_{ij}$ will have a large value. The key here is that the Gram matrix is largely location agnostic; all the information related to locations or positions in the image is lost in the calculation. This is expected, and in some ways desirable, since the style of an image is largely independent from its spatial features. Another key point is that, the style of an image can be thought of as an amalgamation of different combinations of each feature. For instance, Van Gogh’s style of painting is often associated with strong, apparent brush strokes. It is possible to decompose and analyze this style into a set of co-occurring features, such as thick line edges, curves, and so on. So in a sense, the Gram matrix encodes such information at different depths of the pretrained feature extractor, which is why it is fitting to use the Gram matrix for calculating style loss. Concretely, the equation for style loss goes as follows: The style loss is similar to content loss in the sense that it is also a sum of element-wise L2 norms of two matrices. The differences are that we are using the Gram matrix instead of the raw activations themselves, and that we have a scaling constant. But even this constant is a pretty minor change, as I have seen implementations where the style weight was made a trainable parameter as opposed to a fixed scalar. As stated earlier, this tutorial seeks to explain the original NMT algorithm. Subsequent NMT methods use an actual model instead of formulating NMT as an optimization problem in which we modify the generated image itself. The benefit of using an actual model is that it is quicker and more efficient; after all, it takes a lot of time to create a plausible image from some white noise (which is why we are going to use the clone of the content image for this tutorial—but even then, it is still very slow). Now that we have an understanding of how NMT works, let’s get down to the details. Let’s begin by importing necessary modules and handling some configurations for this tutorial. We will be using VGG 19 as our pretrained feature extractor model. We will be using five layers of the network to obtain intermediate representations of the input image. Below is a simple code that lets us achieve this task. In this case, we use the zeroth, fifth, tenth, 19tht, and 28th layers of the model. The output is a list that contains the representations of the input image. It’s time to read, load, and preprocess some images. Below is a simple helper function we will use to read an image from some file directory, then apply any necessary resizing and transformations to the image. Next, let’s define some transformations we will need. The VGG model was trained with a specific transformation configuration, which involves normalizing RGB images according to some mean and standard deviation for each color channel. These values are specified below. We will apply this transformation after loading the image. The application of this transformation will be handled by the  function we’ve defined earlier. Later on in the tutorial, we will also need to undo the transformation to obtain a human presentable image. The following  operation accomplishes this task. While the numbers might look like they came out of nowhere, it’s actually just a reversal of the operation above. Specifically, given a normalizing operation we can undo this normalization via In other words, the reverse transformation can be summarized as And thus it is not too difficult to derive the values specified in the reverse transformation . Now, let’s actually load the  style and content images. We also create a target image. The original way to create the target image would be to generate some white noise, but I decided to copy the content image instead to make things a little easier and expedite the process. Note also that the code has references to some of my local directories; if you want to test this out yourself, make changes as appropriate. Now we are finally ready to solve the optimization problem! The next step would be to generate intermediate representations of each image, then calculate the appropriate loss quantities. Let’s start by defining some values, such as the learning rate, weights, print steps, and others. Below is a helper function that we will be using to save images as we optimize. This will help us see the changes in style as we progress throughout the optimization steps. Finally, this is where all the fun part takes place. For each step, we obtain intermediate representations by triggering a forward pass. Then, for each layer, we calculate the content loss and style loss. The code is merely a transcription of the loss equations as defined above. In particular, calculating the Gram matrix might appear a little bit complicated, but all that’s happening is that we are effectively flattening each activation to make it a single matrix, then calculating the Gram via matrix multiplication with its transpose. We see that style loss decreases quite a bit, whereas the content loss seems to slightly increase with each training step. As stated earlier, it is difficult to optimize on both the content and style, since altering the style of the picture will end up affecting its content in one way or another. However, since our goal is to stylize the target image via NMT, it’s okay to sacrifice a little bit of content while performing NMT, and that’s what is happening here as we can see from the loss values. And here is the result of the transformation!  The result is… interesting, and we certainly see that somethings have changed. We see some more interesting texture in the target image, and there appears to be some changes. However, at this point, my laptop was already on fire, and more training did not seem to yield any better results. So I decided to try out other sample implementations of NMT to see how using more advanced NMT algorithms could make things any better. I decided to try out fast neural style transform, which is available on the official PyTorch GitHub repository. Fast NMT is one of the more advanced, recent algorithms that have been studied after the original NMT algorithm, which we’ve implemente above, was introduced. One of the many benefits of fast neural style transfer is that, instead of framing NMT as an optimization problem, FNMT makes it a modeling problem. In this instance, TransformerNet is a pretrained model that can transform images into their stylized equivalents. The code below was borrowed from the PyTorch repository. I decided to try out FNMT on a number of different pictures of myself, just to see how different results would be for each. Here, we loop through the directory and obtain the file path to each content photo. And here are the results!  Among the 20 photos that have been stylized, I think some definitely look better than others. In particular, I think the third row looks kind of scary, as it made every photo have red hues all over my face. However, there are definitely ones that look good as well. Overall, FNMT using pretrained models definitely yielded better results than our implementation. Of course, this is expected since the original NMT was not the most efficient algorithm; perhaps we will explore FNMT in a future post. But all in all, I think diving into the mechanics behind NMT was an interesting little project. I hope you’ve enjoyed reading this post. Catch you up in the next one!",0,1,0,1,0,0,0,0
Convolutional Neural Network with Keras,"Recently, a friend recommended me a book, Deep Learning with Python by Francois Chollet. As an eager learner just starting to fiddle with the Keras API, I decided it was a good starting point. I have just finished the first section of Part 2 on Convolutional Neural Networks and image processing. My impression so far is that the book is more focused on code than math. The apparent advantage of this approach is that it shows readers how to build neural networks very transparently. It’s also a good introduction to many neural network models, such as CNNs or LSTMs. On the flip side, it might leave some readers wondering why these models work, concretely and mathematically. This point notwithstanding, I’ve been enjoying the book very much so far, and this post is a reflection of just that. Today, we will use TensorFlow’s  module to build a convolutional neural network for image detection. This code is based on what I have learned from the book, so much credit goes to Deep Learning with Python. I have also looked at Machine Learning Mastery blog for additional reference. Let’s begin! Below are the modules that we will need to import for this demonstration. Note that this Jupyter Notebook was written on Google Colaboratory. The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x. We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the  magic: more info. The  function loads the CIFAR10 data set from  module, then applies some basic preprocessing to make it more usable for deep learning purposes. The CIFAR10 data set contains 50000 training images, each labeled with 1 of 10 classes, ranging from cats, horses, and trucks to airplanes. This is a classic classification task. The preprocessing occurs on two levels: first, the images are normalized so that its pixels takes values between 0 and 1. The training labels are transformed from an integer class label to a one-hot encoded vector. Let’s load the data to proceed with our analysis. Let’s see what the dimensions of the data are. Because the CIFAR10 data set include color images for its training set, we would expect three channels, each corresponding to red, green, and blue (hence RGB). As expected, it appears that the training data is a tensor of four dimensions, while the target labels are 10 dimensional vectors. To get a better idea of what the CIFAR10 data looks like, here is a basic function that will display the images in the data set for us using . We can see that, although the images are very pixelated, it is somewhat possible to make out what each image is showing. Of course, this task is going to be a lot more difficult for our neural network.  Now is finally the time to buid the neural network. This network pretty much follows the standard vanilla convolutional neural network model, which typically involves stacking convolution, batch normalization, and pooling layers on top of each other. The dropout layers were also added so as to minimize any potential overfitting. The  argument was configured as , named after Kaiming He who found the optimal weight initialization kernel for convolutional layers. The  argument ensures that the the feature maps are not downsized too quickly due to repeated applications of convolution and pooling. The  layer simply normalizes the tensor returned from the previous layer. There are ongoing research as to what effect batch normalization has on neural networks, but the general consensus is that it helps the model learn more quickly and efficiently. The  function returns the predefined sequential model, compiled using the configurations as shown below. Let’s take a look at the summary of the model. The summary shows that this model has 551,466 trainable parameters. The memory capacity of this model is not big, but it is definitely larger than the network we built in the previous post using the Keras API. Now that the model is ready to be deployed, we need to train and test the model. But before that, let’s quickly define a function that will provide us with a visualization of how the model is learning. This function is very similar to the one used in the previous post—all it does it that it plots the model’s accuracy and cross entropy loss with each epoch. This visualization will help us see whether our model is actually learning with each epoch, and whether or not overfitting is occurring at any point in training. The last piece of the puzzle we need is the  function. This function is essentially wraps all the functions we have created previously, first by loading the data, training using that data, building a model, training a model, and calling on the  function to provide a visualization of the model’s learning curve. One minor tweak I used to spice up this function is the , which basically creates more images for the neural network to train on by slightly modifying existing images in the training set. These modifications involve shifting, zooming, and flipping. When we don’t have enough data to train our model on, using the  can be useful. Finally, let’s see how well our model performs!  The result shows that the model has learned decently well, with testing accuracy of approximately 80 percent. This is not the best result, but it is certainly not bad, especially given the fact that the images in the dataset, as we have seen above, are very pixelated and sometimes difficult for even humans to decipher and categorize. That’s all for today. It’s fascinating to see how CNNs are capable of perceiving images and categorizing them after appropriate training. But as we all know, the potential of neural networks far extends beyond image classificaiton. In a future post, we might look at more complicated models, such as RNN or LSTMs, to achieve even cooler tasks!",0,1,0,0,0,0,0,0
A Simple Autocomplete Model,"You might remember back in the old days when autocomplete was just terrible. The suggestions provided by autocomplete would be useless if not downright stupid—I remember that one day when I intended to type “Gimme a sec,” only to see my message get edited into “Gimme a sex” by the divine touches of autocomplete. On the same day, the feature was turned off on my phone for the betterment of the world. Now, times have changed. Recently, I decided to give autocorrect a chance on my iPhone. Surprisingly, I find myself liking autocomplete more than hating it, especially now that the weather is getting colder by each day: when my frost-numbed finger tips touch on the wrong places of the phone screen to produce words that aren’t really words, iPhone’s autocomplete somehow magically reads my mind to rearrange all that inscrutable alphabet soup into words that make actual, coherent sense. Sometimes, it’s so good at correcting my typos that I intentionnally make careless mistakes on the keyboard just to see how far it can go. One of the obvious reasons behind such drastic improvements in autocomplete functionality is the development of deep neural networks. As we know, neural networks are great at learning hidden patterns as long as we feed it with enough data. In this post, we will implement a very simple version of a generative deep neural network that can easily form the backbone of some character-based autocomplete algorithm. Let’s begin! Let’s first go ahead and import all dependencies for this tutorial. As always, we will be using the  functional API to build our neural network. We will be training our neural network to speak like the great German philosopher Friedrich Nietzsche (or his English translations, to be more exact). First, let’s build a function that retrieves the necessary  text file document from the web to return a Python string. Let’s take a look at the text data by examining its length. Just to make sure that the data has been loaded successfully, let’s take a look at the first 100 characters of the string. ## Preprocessing It’s time to preprocess the text data to make it feedable to our neural network. As introduced in this previous post on recurrent neural networks, the smart way to deal with text preprocessing is typically to use an embedding layer that translates words into vectors. However, text embedding is insuitable for this task since our goal is to build a character-level text generation model. In other words, our model is not going to generate word predictions; instead, it will spit out a character each prediction cycle. Therefore, we will use an alternative technique, namely mapping each character to an integer value. This isn’t as elegant as text embedding or even one-hot encoding but for a character-level analysis, it should work fine. The  function takes a string text data as input and returns a list of training data, each of length , sampled every  characters. It also returns the training labels and a hash table mapping characters to their respective integer encodings. Let’s perform a quick sanity check to see if the function works as expected. Specifying  to 60 means that each instance in the training data will be 60 consecutive characters sampled from the text data every  characters. The result tells us that we have a total of 200278 training instances, which is probably plenty to train, test, and validate our model. The result also tells us that there are 57 unique characters in the text data. Note that these unique characters not only include alphabets but also and other miscellaneous white spacing characters and punctuations. Let’s now design our model. Because there is obviously going to be sequential, temporal structure underlying the training data, we will use an LSTM layer, a type of advanced recurrent neural network we saw in the previous post. In fact, this is all we need, unless we want to create a deep neural network spanning multiple layers. However, training such a model would cost a lot of time and computational resource. For the sake of simplicity, we will build a simple model with a single LSTM layer. The output layer is going to be a dense layer with  number of neurons, activated with a softmax function. We can thus interpret the index of the biggest value of the final array to correspond to the most likely character. Below is a full plot of the model that shows the dimensions of the input and output tensors of all layers.  Now, all we have to do is to train the model with the data. Let’s run this for 50 epochs, just to give our model enough time to explore the loss function and settle on a good minimum. As I was training this model on Google Colab, I noticed that training even this simple model took a lot of time. Therefore, I decided that it is a good idea to probably save the trained model—in the worst case scenario that poor network connection suddenly caused the Jupyter kernel to die, saving a saved model file would be of huge help since I can continue training again from there. Saving the model on Google Colab requires us to import a simple module, . The process is very simple. To load the model, we can simply call the command below. Let’s take a look at the loss curve of the model. We can simply look at the value of the loss function as printed throughout the training scheme, but why not visualize it if we can?  As expected, the loss decreases throughout each epoch. The reason I was not paticularly worried about overfitting was that we had so much data to work with, especially in comparison with the relatively constrained memory capacity of our one-layered model. One of the objectives of this tutorial was to demonstrate the fun we can have with generative models, namely neural networks that can be used to generate data themselves, not just classify or predict data points. To put this into perspective, let’s compare the objectives of a generative model with that of a discriminative model. Simply put, the goal of a discriminative model is to model and calculate where $y$ is a label and $X$ is some input vector. As you can see, discriminative models arise most commonly from the context of supervised machine learning, such as regression or classification. In contrast, the goal of a generative model is to approximate the distribution which we might construe to be the probability of observing evidence or data. By modeling this distribution, the goal is that we might be able to generate samples that appear to have been sampled from this distribution. In other words, we want our model to generate likely data points based on an approximation of the true distribution from which these observations came from. In the context of this tutorial, our neural network  should be able to somewhat immitate the speech of the famous German philosopher based on the training it went through with text data, although we would not expect the content generated by our neural network to have the same level of depth and profoundity as those of his original writings. As mentioned above, the objective of a generative model is to model the distribution of the latent space from which observed data points came from. At this point, our trained model should be able to model this distribution, and thus generate predictions given some input vector. However, we want to add some element of randomness of noise in the prediction. Why might we want to do this? Well, an intuitive pitfall we might expect is that the model might end up generating a repetition of some likely sequence of characters. For example, let’s say the model’s estimated distribution deems the sequence “God is dead” to be likely. Then, the output of our model might end up being something like this: …(some input text) God is dead God is dead God is dead… (repetition elided) We don’t want this to happen. Instead, we want to introduce some noise so that the model faces subtle obstructions, thereby making it get more “creative” with its output instead of getting trapped in an infinite loop of some likely sequence. Below is a sample implementation of adding noise to the output using log and exponential transformations to the output vector of our model. The transformation might be expressed as follows: where $T$  denotes a transformation, $\hat{y}$ denotes a prediction as a vector, $t$ denotes temperature as a measure of randomness, and $K$ is a normalizing constant. Although this might appear complicated, all it’s doing is that it is adding some perturbation or disturbance to the output data so that it is possible for less likely characters to be chosen as the final prediction. Below is a sample implementation of this process in code. Note that due to the algebraic quality of the vector transformation above, randomness is increased for large values of $t$. Now it’s finally time to put our Nietzsche model to the test. How we will do this is pretty simple. First, we will feed a 60-character excerpt from the text to our model. Then, the model will output a prediction vector, which is then passed onto  given a specified . We will finally have a prediction that is 1 character. Then, we incorporate that one character prediction into the original 60-character data we started with. We slice the new augmented data set from  to end up with another prediction. We would then slice the data set from, you guessed it,  and repeat the process as outlined above. When we iterate through this cycle many times, we would eventually end up with some generated text. Below is the  function that implements the iteration process. We’re almost done! To get a better sense of what impact temperature has on the generation of text, let’s quickly write up a  function that will allow us to generate text for differing values of . The time has come: let’s test our model for four different temperature values from 0.3 to 1.2, evenly spaced. We will make our model go through 1000 iterations to make sure that we have a long enough text to read, analyze, and evaluate. For the sake of readability, I have reformatted the output result in markdown quotations. Generated text at temperature 0.3: is a woman–what then? is there not ground
 for suspecting that the experience and present strange of the soul is also as the stand of the most profound that the present the art and possible to the present spore as a man and the morality and present self instinct, and the subject that the presence of the surcessize, and also it is an action which the philosophers and the spirit has the consider the action to the philosopher and possess and the spirit is not be who can something the predicess of the constinate the same and self-interpatence, the disconsises what is not to be more profound, as if it is a man as a distance of the same art and ther strict to the presing to the result the problem of the present the spirit what is the consequences and the development of the same art of philosophers and security and spirit and for the subjective in the disturce, as in the contrary and present stronger and present could not be an inclination and desires of the same and distinguished that is the discoverty in such a person itself influence and ethers as Generated text at temperature 0.6: is a woman–what then? is there not ground 
for suspecting to and the world will had to a such that the basis of the incussions of the spirit as the does not because actian free spirits of intellect of the commstical purtious expression of men are so much he is not unnor experiences of self-conturity, and 
as anifegently religious in the man would not consciously, his action is not be actian at in accombs life for the such all procees of great and the heart of this conduct the spirity of the man can provate for in any 
once in any of the suriticular conduct that which own needs, when they are therefore, as 
such action and some difficulty that the strength, it, himself which has to its fine term of pricismans the exacte in its self-recuphing and every strength and man to wist the action something man as the worst, that the was of a longent that the whole not be all the very subjectical proves the stronger extent he is necessary to metaphysical figure of the faith in the bolity in the pure belief–as “the such a successes of the values–that is he ​ Generated text at temperature 0.9: is a woman–what then? is there not ground 
for suspecting that they grasutes, and so farmeduition of the does not only with this 
constrbicapity have honour–and who distical seclles are denie’n, is one samiles are no luttrainess, 
and ethic and matficulty, concudes of morality to 
rost were presence of lighters caseful has prescally here at last not and servicatity, leads falled for child real appreparetess of worths–the 
resticians when one to persans as a what a mean of that is as to the same heart tending noble stimptically and particious, we pach yought for that mankind, that the same take frights a contrady has howevers of a surplurating or in fact a sort, without present superite fimatical matterm of our being interlunally men who cal 
scornce. the shrinking’s 
proglish, and traints he way to demitable pure explised and place can 
deterely by the compulse in whom is phypociative cinceous, and the higher and will bounthen–in itsiluariant upon find the “first the whore we man will simple condection and some than us–a valuasly refiges who feel Generated text at temperature 1.2: is a woman–what then? is there not ground 
for suspecting that he therefore when shre, mun, a schopenhehtor abold gevert. 120 =as in 
find that is _know believinally bad,[
euser of view.–bithic 
iftel canly 
in any 
knowitumentially. the charm surpose again, in 
swret feathryst, form of kinne of the world bejud–age–implaasoun ever? but that the is any 
appearance has clenge: the? a plexable gen preducl=s than condugebleines and aligh to advirenta-nasure; 
findiminal it as, not take. the ideved towards upavanizing, would be 
thenion, in all pespres: it is of 
a concidenary, which, well founly con-utbacte udwerlly upon mansing–frauble of “arrey been can the pritarnated from their 
christian often–think prestation of mocives.” legt, lenge:–this deps 
telows, plenhance of decessaticrances). hyrk an interlusally” tone–under good haggy,” 
is have we leamness of conschous should it, of 
sicking ummenfeckinal zerturm erienweron of noble of 
himself-clonizing there is conctumendable prefersy 
exaitunia states,” whether 
they deve oves any of hispyssesss. int The results are fascinating. Granted, our model is still bad at immitating Nietzsche’s style of writing, but I think the performance is impressive given that this was a character-based text generation model. Think about it for a second: to write even a single word, say “present,” the model has to correctly predict “p”, “r”, “e”, “s”, “e”, “n”,  and “t,” all in tandem. Imagine doing this for extended cycles, long enough to generate text that is comfortably a paragraph long. It’s amazing how the text it generates even makes some sense at all. Then, as temperature rises, we see more randomness and “creativity” at work. We start to see more words that aren’t really words (the one I personally like is “farmeduition”—it sounds like it could be either some hard, obscure word that no one knows, or a failed jumble of “farm,” “education,” and “intuition”). At temperature 1.2, the model is basically going crazy with randomness, adding white spaces where there shouldn’t be and sounding more and more like a speaker of Old English or German, something that one might expect to see in English scripts written in pre-Shakesperean times. At any rate, it is simply fascinating to see how a neural network can be trained to immitate some style of writing. Hopefully this tutorial gave you some intuition of how autocomplete works, although I presume business-grade autocomplete functions on our phones are based on much more complicated algorithms. Thanks for reading this post. In the next post, we might look at another example of a generative model known as generative adversarial networks, or GAN for short. This is a burgeoning field in deep learning with a lot of prospect and attention, so I’m already excited to put out that post once it’s done. See you in the next post. Peace!",0,1,0,0,0,0,0,0
Complex Fibonacci,"A few days ago, a video popped up in my YouTube suggestions. We all know how disturbingly powerful the YouTube recommendation algorithm is: more than 90 percent of the times, I thoroughly enjoy all suggestions put forth by the mastermind algorithm. This time was no exception: in fact, I enjoyed it so much that I decided to write a short blog post about it. Also a quick plug: if you haven’t checked out Matt Parker’s channel, I highly recommend that you do. Let’s dive right into today’s topic: extending the fibonacci sequence to complex numbers. We all know what the fibonacci sequence looks like, but for formality and notational clarity’s sake, here is what the fibonacci sequence looks like: There are some different conventions as to where the sequence starts. I personally prefer the one that starts from zero, with zero indexing. Here is what I mean: Implementing fibonacci numbers in code is one of the most common exercises that are used to teach concepts such as recursion, memoization, and dynamic programming. This is certainly not the point of today’s post, but here is an obligatory code snippet nonetheless. The code above is the standard fibonacci function as we know it, implemented with simple bare bone recursion. While the code works perfectly fine, there is a fatal problem with this code: it recalculates so many values. Namely, in calling , the program goes all the way up to the $(n - 1)$th fibonacci number. Then, in the next call of , the program recalculates the same values calculated earlier, up until the $(n - 2)$th fibonacci number, just one short of the previous one. The classic way to deal with this problem is to use a technique known as memoization. The idea is simple: we keep some sort of memo or cache of values that have already been calculated and store them in some data structure that we don’t have to recalculate values that have already been computed prior. Here is a simple implementation. To see how effective memoization is compared to vanilla recursion, let’s use the  module. Comparing this to the  test on  with memoization, the benefits of caching becomes immediately clear: 3 second and 150 nanoseconds are different by orders of magnitude, and we only asked the functions to calculate the 35th fibonacci number. We can get a sense of how quickly this disparity would grow if we try to calculate something like the 1000th fibonacci number in the sequence. Another perk of using caching as above is that we can now get the full sequence up to the 35th fibonacci number. Although memoization is interesting, it is not the main topic of today’s post. Instead, I want to discuss Binet’s formula, a formula with which we can calculate the $n$th fibonacci number. Binet’s formula states that We can trivially verify that $F_0 = 0$ and that $F_1 = 1$. For more robust empirical verification, we will resort to code later. It is worth noting that the quantity in the parenthesis, namely is otherwise referred to as the Golden ratio. Also observe that the other quantity is the negative inverse of the Golden ratio. Let’s take a closer look at why the Binet’s formula makes sense. This is not going to be a rigorous proof or a derivation, but rather an attempt at cursory analysis to provide food for thought. This process was heavily referenced from this Quora post. Intuitively, Binet’s formula has to do with the well-known fact that the ratio between two consecutive fibonacci numbers approaches the Golden ratio as $n$ goes to infinity. In this light, we might understand the fibonacci sequence as a geometric sequence with a constant ratio. The goal, then, is to show that the ratio is in fact the Golden ratio. Then, we have the following recurrence relation between consecutive terms. Dividing both sides by $r^{n - 2}$, we get This is a simple quadratic equation that we can easily solve. With some precalculus algebra, we get And indeed we start to see the Golden ratio and its negative inverse as solutions to the quadratic. This means that we can express the fibonacci sequence as a linear combinations of these two solutions: Much like solving any difference equations, we have two initial conditions, namely that $F_0 = 0$, $F_1 = 1$. We also trivially know that $F_2 = 1$, but only two conditions suffice to ascertain the value of the coefficients, $\alpha$ and $\beta$. With some algebra, one can verify that Putting these together, we finally get Binet’s formula: An interesting point to note about Binet’s formula is that $n$ doesn’t necessarily have to be a non-negative integer as we had previously assumed. In fact, it can be any number: rational, irrational, real, or even complex. The fact that the fibonacci numbers can extend to real number indexing becomes more apparent once we code out the formula. Nothing special at all, this is just a literal transcription of the formula presented above. But now, watch what happens when we try to get the 1.1th fibonacci number, for instance: Lo and behold, we get a complex fibonacci number! I thought this was so fascinating, almost like seeing a magic of some sort. Although I had known about the fibonacci sequence for as long as I can remember, I had never thought about it in continuous terms: in my mind, the fibonacci sequence was, after all, a sequence—a discrete set of numbers adhering to the simple rule that the next number in line is the sum of the previous two. The intriguing part is that, even in this complex fibonacci madness, the simple rule still holds. For instance, You might be wondering why we don’t compare things exactly by means of This is because this equality doesn’t hold due to floating point arithmetic. Therefore, we simply verify equivalence by comparing their magnitude with an arbitrarily small number, . The takeaway from the code snippet is that holds, regardless of whether or not $n$ is a non-negative integer. Indeed, Binet’s formula gives us what we might refer to as the interpolation of the fibonacci sequence, in this case extended along the real number line. A corollary of the real number interpolation of the fibonacci sequence via Binet’s formula is that now we can effectively plot the complex fibonacci numbers on the Cartesian plane. Because $n$ can be continuous, we would expect some graph to appear, where the $x$-axis represents real numbers, and $y$, the imaginary. This requires a bit of a hack though; note that the result of Binet’s formula is a complex number, or a two-dimensional data point. The input to the function is just a one-dimensional real number. Therefore, we need a way of representing a map from a one-dimensional real number line to a two-dimensional complex plane. This is sort of tricky if you think about it: the normal two-dimensional plane as we know it can only represent a mapping from the $x$-axis to the $y$-axis—in other words, a transformation from one-dimensional space to another one-dimensional space. A three-dimensional $xyz$-coordinate system, on the other hand, represents a transformation from a two-dimensional space, represented by $x$ and $y$, to another one-dimensional space, namely $z$. We aren’t used to going to other way around, where a one-dimensional space is mapped to a two-dimensional space, as is the case here. A simple hack that nonetheless makes a lot of sense in this case is to use the real-number line for two purposes: representing the input dimension, namely the real number line, and one component of the output dimension—the real number portion of the output to Binet’s formula. This admittedly results in a loss of information, since finding the point where $n = k$ won’t give us the $k$th fibonacci number; instead, it will only tell us what the fibonacci number is whose real number component equals $k$. Nonetheless, this is an approach that makes sense since the real number line is a common dimension in both the input and output data. With this in mind, let’s go ahead and try to plot the interpolation of the fibonacci sequence on the complex plane. First, we import the modules we will need. Then, we simply specify the domain on the real number line and generate the fibonacci numbers, separating out the real and imaginary components. Note that  is not going to be used for plotting; instead, we use  as the $x$-axis, and this is where the loss of temporal information comes in, as mentioned earlier. Now, let’s go ahead and plot it out!  And there it is, the full fibonacci sequence, interpolated across the real numbers. When I first saw this pattern in Matt Parker’s video, I was simply in awe, a loss of words. There’s something inexplicably beautiful and wonderful at this pattern, almost as if it was some part of God’s plan. Okay, maybe I’m being too melodramatic about a graph, but there is no denying that this pattern is geometrically interesting and pleasing to the eye. Everything looks so intentional and deliberate. The comments on the aesthetics of the snail shell aside, one point that deserves our attention is what appears to be a straight line. Well, turns out that this is, in fact, not a straight line. The only reason why it appears straight is that the snail pattern overshadows the little vibrations on this portion of the graph. Indeed, zooming in, we see that there is an interesting damping motion going on. This is what the fibonacci sequence would have looked like had we plotted only the positive domain of the real number line.  In this post, we took a look at the fibonacci sequence and its interpolation across the real number line. We could go even crazier, as did Matt Parker in his own video, by attempting to interpolate the sequence on the complex number plane, at which point we would now have a mapping from two dimensions to two dimensions, effectively forcing us to think in terms of four dimensions. There is no fast, handy way of drawing or visualizing four dimensions, as we are creatures that are naturally accustomed to three dimensions. There are interesting observations to be made with the full-fledged complex interpolation of the sequence, but I thought this is already interesting as it is nonetheless. Nowadays, I’m reminded of just how many things that I thought I knew well—like the fibonacci sequence—are rife with things to study and rejoice in wonder. More so than the value of understanding something brand new, perhaps the value of intellectual exploration lies in realizing just how ignorant one is, as ironic as it sounds. I didn’t want to end on such a philosophical note, but things have already precipitated contrary to my intentions. Anyhow, I hope you’ve enjoyed reading this post. Catch you up in the next one.",0,0,1,0,0,0,0,0
Markov Chain and Chutes and Ladders,"In a previous post, we briefly explored the notion of Markov chains and their application to Google’s PageRank algorithm. Today, we will attempt to understand the Markov process from a more mathematical standpoint by meshing it together the concept of eigenvectors. This post was inspired and in part adapted from this source. In linear algebra, an eigenvector of a linear transformation is roughly defined as follows: a nonzero vector that is mapped by a given linear transformation onto a vector that is the scalar multiple of itself This definition, while seemingly abstract and cryptic, distills down into a simple equation when written in matrix form: Here, \(A\) denotes the matrix representing a linear transformation; \(x\), the eignevector; \(\lambda\), the scalar value that is multiplied onto the eigenvector. Simply put, an eigenvector \(x\) of a linear transformation is one that is—allow me to use this term in the loosest sense to encompass positive, negative, and even imaginary scalar values—“stretched” by some factor \(\lambda\) when the transformation is applied, i.e. multiplied by the matrix \(A\) which maps the given linear transformation. The easiest example I like to employ to demonstrate this concept is the identity matrix \(I\). For the purpose of demonstration, let \(a\) be an arbritrary vector \((x, y, z)^{T}\) and \(I\) the three-by-three identity matrix. Multiplying \(a\) by \(I\) produces the following result: The result is unsurprising, but it reveals an interesting way of understanding \(I\): identity matrices are a special case of diagonalizable matrices whose eigenvalues are 1. Because the multiplying any arbitrary vector by the identity matrix returns the vector itself, all vectors in the dimensional space can be considered an eigenvector to the matrix \(I\), with \(\lambda\) = 1. A formal way to calculate eigenvectors and eigenvalues can be derived from the equation above. Since \(x\) is assumed as a nonzero vector, we can deduce that the matrix \((A - \lambda I)\) is a singular matrix with a nontrivial null space. In fact, the vectors in this null space are precisely the eigenvectors that we are looking for. Here, it is useful to recall that the a way to determine the singularity of a matrix is by calculating its determinant. Using these set of observations, we can modify the equation above to the following form: By calculating the determinant of \((A - \lambda I)\), we can derive the characteristic polynomial, from which we can obtain the set of eigenvectors for \(A\) representing some linear transformation \(T\). Now that we have reviewed some underlying concepts, perhaps it is time to apply our knowledge to a concrete example. Before we move on, I recommend that you check out this post I have written on the Markov process, just so that you are comfortable with the material to be presented in this section. In this post, we turn our attention to the game of Chutes and Ladders, which is an example of a Markov process which demonstrates the property of “memorylessness.” This simply means that the progress of the game depends only on the players’ current positions, not where they were or how they got there. A player might have ended up where they are by taking a ladder or by performing a series of regular dice rolls. In the end, however, all that matters is that the players eventually hit the hundredth cell. To perform a Markov chain analysis on the Chutes and Ladders game, it is first necessary to convert the information presented on the board as a stochastic matrix. How would we go about this process? Let’s assume that we start the game at the \(0\)th cell by rolling a dice. There are six possible events, each with probability of \(1/6\). More specifically, we can end up at the index numbers 38, 2, 3, 14, 5, or 6. In other words, at position 0, where \(C\) and \(X\) denote the current and next position of the player on the game board, respectively. We can make the same deductions for other cases where \(C = 1 \ldots 100\). We are thus able to construct a 101-by-101 matrix representing the transition probabilities of our Chutes and Ladders system, where each column represents the system at a different state, i.e. the \(j\)th entry of the \(i\)th column vector represents the probabilities of moving from cell \(i\) to cell \(j\). To make this more concrete, let’s consider a program that constructs the stochastic matrix , without regards to the chutes and ladders for now. The indexing is key here: for each column, \([i + 1, i + 7)\)th rows were assigned the probability of \(1/6\). Let’s say that a player is in the \(i\)th cell. Assuming no chutes or ladders, a single roll of a dice will place him at one of the cells from \((i + 1)\) to \((i + 6)\); hence the indexing as presented above. However, this algorithm has to be modified for  bigger or equal to 95. For example if , there are only three probabilities: \(P(X = 98)\), \(P(X = 99)\), and \(P(X = 100)\), each of values \(1/6\), \(1/6\), and \(2/3\) respectively. The  statements are additional corrective mechanisms to account for this irregularity. So now we’re done with the stochastic matrix! … or not quite. Things get a bit more complicated once we throw the chutes and ladders into the mix. To achieve this, we first build a dictionary containing information on the jump from one cell to another. In this dictionary, the keys correspond to the original position; the values, the index of the cell after the jump, either through a chute or a ladder. For example,  represents the first ladder on the game board, which moves the player from the first cell to the thirty eighth cell. To integrate this new piece of information into our code, we need to build a permutation matrix that essentially “shuffles up” the entries of the stochastic matrix  in such a way that the probabilities can be assigned to the appropriate entries. For example,  does not reflect the fact that getting a 1 on a roll of the dice will move the player up to the thirty eighth cell; it supposes that the player would stay on the first cell. The new permutation matrix  would adjust for this error by reordering . For an informative read on the mechanics of permutation, refer to this explanation from Wolfram Alpha. Let’s perform a quick sanity check to verify that  contains the right information on the first ladder, namely the entry  in the  dictionary. Notice the \(1\) in the \(38\)th entry hidden among a haystack of 100 \(0\)s! This result tells us that  is indeed a permutation matrix whose multiplication with  will produce the final stochastic vector that correctly enumerates the probabilities encoded into the Chutes and Ladders game board. Here is our final product: We can visualize the stochastic matrix \(T\) using the  package. This produces a visualization of our stochastic matrix. So there is our stochastic matrix! Now that we have a concrete matrix to work with, let’s start by identifying its eigenvectors. This step is key to understanding Markov processes since the eigenvector of the stochastic matrix whose eigenvalue is 1 is the stationary distribution vector, which describes the Markov chain in a state of equilibrium. For an intuitive explanation of this concept, refer to this previous post. Let’s begin by using the  package to identify the eigenvalues and eigenvectors of the stochastic matrix. This code block produces the following output: The first entry of this array, which is the value , deserves our attention, as it is the eigenvalue which corresponds to the stationary distribution eigenvector. Since the index of this value is , we can identify its eigenvector as follows: Notice that this eigenvector is a representation of a situation in which the player is in the \(100\)th cell of the game board! In other words, it is telling us that once the user reaches the \(100\)th cell, they will stay on that cell even after more dice rolls—hence the stationary distribution. On one hand, this information is impractical given that a player who reaches the end goal will not continue the game to go beyond the \(100\)th cell. On the other hand, it is interesting to see that the eigenvector reveals information about the structure of the Markov chain in this example. Markov chains like these are referred to as absorbing Markov chains because the stationary equilibrium always involves a non-escapable state that “absorbs” all other states. One might visualize this system as having a loop on a network graph, where it is impossible to move onto a different state because of the circular nature of the edge on the node of the absorbing state. At this point, let’s remind ourselves of the end goal. Since we have successfully built a stochastic matrix, all we have to do is to set some initial starting vector \(x_0\) and perform iterative matrix calculations. In recursive form, this statement can be expressed as follows: The math-inclined thinkers in this room might consider the possibility of conducting an eigendecomposition on the stochastic matrix to simply the calculation of matrix powers. There is merit to considering this proposition, although later on we will see that this approach is inapplicable to the current case. Eigendecomposition refers to a specific method of factorizing a matrix in terms of its eigenvalues and eigenvectors. Let’s begin the derivation: let \(A\) be the matrix of interest, \(S\) a matrix whose columns are eigenvectors of \(A\), and \(\Lambda\), a matrix whose diagonal entries are the corresponding eigenvalues of \(S\). Let’s consider the result of multiplying \(A\) and \(S\). If we view multiplication as a repetition of matrix-times-vector operations, we yield the following result. But recall that \(s\) are eigenvectors of \(A\), which necessarily implies that Therefore, the result of \(AS\) can be rearranged and unpacked in terms of \(\Lambda\): \(\begin{pmatrix} \vert & \vert &        & \vert \\ As_1 & As_2 & \ldots & As_n \\ \vert & \vert &        & \vert \end{pmatrix} = \begin{pmatrix} \vert & \vert &        & \vert \\ \lambda_1 s_1 & \lambda_2 s_2 & \ldots & \lambda_n s_n \\ \vert & \vert &        & \vert \end{pmatrix}\)
\(= \begin{pmatrix} \vert & \vert &        & \vert \\ s_1 & s_2 & \ldots & s_n \\ \vert & \vert &        & \vert \end{pmatrix} \begin{pmatrix} \lambda_1 & \dots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \dots & \lambda_n \end{pmatrix} = S \Lambda\) In short, Therefore, we have \(A = S \Lambda S^{-1}\), which is the formula for eigendecomposition of a matrix. One of the beauties of eigendecomposition is that it allows us to compute matrix powers very easily. Concretely, Because \(S\) and \(S^{-1}\) nicely cross out, all we have to compute boils down to \(\Lambda^n\)! This is certainly good news for us, since our end goal is to compute powers of the stochastic matrix to simulate the Markov chain. However, an important assumption behind eigendecomposition is that it can only be performed on nonsingular matrices. Although we won’t go into the formal proofs here, having a full span of independent eigenvectors implies full rank, which is why we must check if the stochastic matrix is singular before jumping into eigendecomposition. Unfortunately, the stochastic matrix is singular because \(81 < 101\), the number of columns or rows. This implies that our matrix is degenerate, and that the best alternative to eigendecomposition is the singular value decomposition. But for the sake of simplicity, let’s resort to the brute force calculation method instead and jump straight into some statistical analysis. We first write a simple function that simulates the Chutes and Ladders game given a starting position vector . Because a game starts at the \(0\)th cell by default, the function includes a default argument on  as shown below: Calling this function will give us \(T^nx_0\), which is a 101-by-1 vector whose th entry represents the probability of the player being on the \(i\)th cell after a single turn. Now, we can plot the probability distribution of the random variable \(N\), which represents the number of turns necessary for a player to end the game. This analysis can be performed by looking at the values of  since the last entry of this vector encodes the probability of the player being at the \(100\)th cell, i.e. successfully completing the game after  rounds. This block produces the following figure: I doubt that anyone would play Chutes and Ladders for this long, but after about 150 rolls of the dice, we can expect with a fair amount of certainty that the game will come to an end. The graph above presents information on cumulative fractions, but we can also look at the graph for marginal probabilities by examining its derivative: And the result: From the looks of it, the maximum of the graph seems to exist somewhere around \(n = 20\). To be exact, \((x_{max}, y_{max}) = (19, 0.027917820873612303)\). This result tells us that we will finish the game in 19 rolls of the dice more often than any other number of turns. We can also use this information to calculate the expected value of the game length. Recall that Or if the probability density function is continuous, In this case, we have a discrete random variable, so we adopt the first formula for our analysis. The formula can be achieved in Python as follows: This result tells us that the typical length of a Chutes and Ladders game is approximately 36 turns. But an issue with using expected value as a metric of analysis is that long games with infinitesimal probabilities are weighted equally to short games of substantial probability of occurrence. This mistreatment can be corrected for by other ways of understanding the distribution, such as median: This function tries to find the point in the cumulative distribution where the value is closest to \(0.5\), i.e. the median of the distribution. The result tells us that about fifty percent of the games end after 29 turns. Notice that this number is smaller than \(E(X)\) because it discredits more of the long games with small probabilities. The Markov chain represents an in interesting way to analyze systems that are memoryless, such as the one in today’s post, the Chutes and Ladders game. Although it is a simple game, it is fascinating to see just how much information and data can be derived from a simple image of the game board. In a future post, we present another way to approach similar systems, known as Monte Carlo simulations. But that’s for another time. Peace!",0,0,0,0,0,0,1,0
